<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SunJackson Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 23 Jul 2018 11:47:48 +0800</pubDate>
    <lastBuildDate>Mon, 23 Jul 2018 11:47:48 +0800</lastBuildDate>
    <generator>Jekyll v3.8.3</generator>
    
      <item>
        <title>Defining data science in 2018</title>
        <description>&lt;p&gt;I got my first data science job in 2012, the year &lt;a href=&quot;https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century&quot;&gt;Harvard Business Review announced data scientist to be the sexiest job of the 21st century&lt;/a&gt;. Two years later, I published &lt;a href=&quot;https://yanirseroussi.com/2014/10/23/what-is-data-science&quot;&gt;a post on my then-favourite definition of data science&lt;/a&gt;, as the intersection between software engineering and statistics. Unfortunately, that definition became somewhat irrelevant as more and more people jumped on the data science bandwagon ‚Äì possibly to the point of &lt;a href=&quot;https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title&quot;&gt;making data scientist useless as a job title&lt;/a&gt;. However, I still call myself a data scientist. Even better ‚Äì &lt;a href=&quot;https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic&quot;&gt;I still get paid for being a data scientist&lt;/a&gt;. But what does it mean? What do I actually do here? This article is a short summary of my understanding of the definition of data science in 2018.&lt;/p&gt;

&lt;h2 id=&quot;its-not-all-about-machine-learning&quot;&gt;It‚Äôs not all about machine learning&lt;/h2&gt;

&lt;p&gt;As I was wrapping up my PhD in 2012, I started thinking about my next steps. I knew I wanted to get back to working in the tech industry, ideally with a small startup. But it wasn‚Äôt clear to me how to market myself ‚Äì my LinkedIn title at the time was &lt;em&gt;‚Äúsoftware engineer with a research background‚Äù&lt;/em&gt;, which is a bit of a mouthful. Around that time I heard about &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; and decided to try competing. &lt;a href=&quot;https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions&quot;&gt;This went pretty well&lt;/a&gt;, and exposed me to the data science community globally and in Melbourne, where I was living at the time. That‚Äôs how I first met Adam Neumann, the founder of Giveable, a startup that aimed to recommend gifts based on social networking data. Upon graduating, I joined Giveable as a data scientist. Changing my LinkedIn title quickly led to many other offers, but I was happy to be working on Giveable ‚Äì I felt fortunate to have found a startup job that was related to my PhD research on recommender systems.&lt;/p&gt;

&lt;p&gt;My understanding of data science at the time was heavily influenced by Kaggle and the tech industry. Kaggle was only about predictive modelling competitions back then, and so I believed that data science is about using machine learning to build models and deploy them as part of various applications. I was very comfortable with that definition, having spent my PhD years on several predictive modelling tasks, and having worked as a software engineer prior to that.&lt;/p&gt;

&lt;p&gt;Things have changed considerably since 2012. It is now much easier to deploy machine learning models, &lt;a href=&quot;https://www.youtube.com/watch?v=YOIo09qjVl4&quot;&gt;even without a deep understanding of how they work&lt;/a&gt;. Many more people call themselves data scientists, &lt;a href=&quot;https://eng.lyft.com/whats-in-a-name-ce42f419d16c&quot;&gt;including some who are more focused on data analysis than on building data products&lt;/a&gt;. Even Kaggle ‚Äì which is now owned by Google ‚Äì &lt;a href=&quot;https://www.youtube.com/watch?v=AoRSIdLpFqU&quot;&gt;has broadened its scope beyond modelling competitions to support other types of analysis&lt;/a&gt;. Numerous articles have been published on the meaning of data science in the past six years. We seem to be going towards a broad definition of the field, which includes any type of general data analysis. This trend of broadening the definition &lt;a href=&quot;https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title&quot;&gt;may make data scientist somewhat useless as a job title&lt;/a&gt;. However, I believe that data science tasks remain useful, as shown by the following definitions.&lt;/p&gt;

&lt;h2 id=&quot;recent-definitions-by-hern√°n-hawkins-and-dubossarsky&quot;&gt;Recent definitions by Hern√°n, Hawkins, and Dubossarsky&lt;/h2&gt;

&lt;p&gt;In a &lt;a href=&quot;https://arxiv.org/pdf/1804.10846.pdf&quot;&gt;recent article&lt;/a&gt;, Hern√°n et al. classify data science tasks into three types: &lt;em&gt;description&lt;/em&gt;, &lt;em&gt;prediction&lt;/em&gt;, and &lt;em&gt;causal inference&lt;/em&gt;. Like other authors, they argue that causal inference has been neglected by traditional statistics and some scientific disciplines. They claim that the emergence of data science is an opportunity to get causal inference ‚Äúright‚Äù. Further, they emphasise the importance of domain expert knowledge, &lt;a href=&quot;https://yanirseroussi.com/2016/05/15/diving-deeper-into-causality-pearl-kleinberg-hill-and-untested-assumptions&quot;&gt;which is essential in causal inference&lt;/a&gt;. Defining data science in this broad manner seems to capture the essence of what the field is about these days. However, purely descriptive tasks are still often performed by data &lt;em&gt;analysts&lt;/em&gt; rather than &lt;em&gt;scientists&lt;/em&gt;. And the distinction between prediction and causal inference can be a bit fuzzy, especially as the tools for the latter are at a lower level of maturity. In addition, while I agree with Hern√°n et al. that domain expertise is important, it seems unlikely that this will forever be the case. No one is born an expert ‚Äì expertise is gained by learning from and interacting with the world. Therefore, it‚Äôs plausible that gaining expertise can and will be automated. Further, there are numerous cases where experts were proven to be wrong. For example, it wasn‚Äôt so long ago that &lt;a href=&quot;https://www.healio.com/hematology-oncology/news/print/hemonc-today/%7B241d62a7-fe6e-4c5b-9fed-a33cc6e4bd7c%7D/cigarettes-were-once-physician-tested-approved&quot;&gt;doctors recommended smoking&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Despite the importance of domain knowledge, one can argue that scientists that specialise in a single domain are not data scientists. In fact, the ability to go beyond one domain and think of data in a more abstract manner is what makes a data scientist. Applying this abstract knowledge often requires some domain expertise or input from domain experts, but most data science techniques are not domain-specific ‚Äì they can be applied to many different problems. John Hawkins explains this point well in an article titled &lt;em&gt;&lt;a href=&quot;https://www.linkedin.com/pulse/why-all-scientists-data-john-hawkins&quot;&gt;why all scientists are not data scientists&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;Those scientists and statisticians who have focused themselves on understanding the limitations and possibilities of making inferences from experimental data are the ones who are the forerunners to data scientists. They have a skill which transcends the particulars of what it takes to do lab work on cell cultures, or field studies for ecology etc. Their core skill involves thinking about the data involved at an abstracted level. To ask the question ‚Äúgiven data with these properties, what conclusions can we draw?‚Äù&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://www.superdatascience.com/podcast-one-purpose-data-science-truth-analytics&quot;&gt;according to Eugene Dubossarsky&lt;/a&gt;, &lt;em&gt;‚Äúthere‚Äôs only one purpose to data science, and that is to support decisions. And more specifically, to make better decisions. That should be something no one can argue with.‚Äù&lt;/em&gt; This goal-focused definition is unsurprising, given the fact that Eugene runs a training and consulting business and has been working in the field for over 20 years. I‚Äôm not going to argue with him, but to put it all together, &lt;strong&gt;we can define data science as a field that deals with description, prediction, and causal inference from data in a manner that is both domain-independent and domain-aware, with the ultimate goal of supporting decisions&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-about-ai&quot;&gt;What about AI?&lt;/h2&gt;

&lt;p&gt;Everyone loves a good buzzword, and these days AI (Artificial Intelligence) is one of the hottest buzzwords. However, despite &lt;a href=&quot;https://www.forbes.com/sites/valleyvoices/2017/01/31/the-rise-of-ai-will-force-a-new-breed-of-data-scientist&quot;&gt;what some people may try to tell you&lt;/a&gt;, AI is unlikely to make data science obsolete any time soon. Following the above definition, as long as there is a need to make decisions based on data, there will be a need for data scientists. This includes decisions that aren‚Äôt made by humans, as data scientists are involved in building systems that make decisions autonomously.&lt;/p&gt;

&lt;p&gt;The resurgence of AI feels somewhat amusing given my personal experience. One of the reasons I decided to pursue a PhD in natural language processing and personalisation was my interest in what I considered to be AI back in 2008. My initial introduction to the field was through an AI course and a project I did as part of my bachelor‚Äôs degree in computer science. However, by the time I graduated from my PhD, saying that I‚Äôm an AI expert seemed less useful than calling myself a data scientist. It may be that the field is about to shift again, and that rebranding as an AI expert would be more beneficial (though I‚Äôd be doing exactly the same work). Titles are somewhat silly ‚Äì I‚Äôm going to continue working with data to support decisions for as long as there is demand for this kind of work and I continue enjoying it. There is plenty to learn and develop in this area, regardless of buzzwords and sexy titles.&lt;/p&gt;

&lt;h3 id=&quot;like-this&quot;&gt;Like this:&lt;/h3&gt;

&lt;p&gt;Like Loading‚Ä¶&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/e6f23d80deaf3435840eb7d38a566489/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/e6f23d80deaf3435840eb7d38a566489/</guid>
        
        <category>data science job</category>
        
        <category>scientists</category>
        
        <category>titled</category>
        
        <category>titles</category>
        
        <category>decisions</category>
        
        <category>domain expert</category>
        
        <category>definitions</category>
        
        <category>kaggle</category>
        
        <category>modelling</category>
        
        <category>models</category>
        
        <category>inferences</category>
        
        <category>description</category>
        
        <category>descriptive</category>
        
        <category>recommender</category>
        
        <category>recommended</category>
        
        <category>phd</category>
        
        <category>tasks</category>
        
        <category>experts</category>
        
        <category>language</category>
        
        <category>statistics</category>
        
        <category>fuzzy</category>
        
        <category>buzzwords</category>
        
        <category>engineering</category>
        
        <category>adam</category>
        
        <category>business</category>
        
        <category>industry</category>
        
        <category>numerous articles</category>
        
        <category>involves</category>
        
        <category>involved</category>
        
        <category>abstracted</category>
        
        <category>expertise</category>
        
        <category>john</category>
        
        <category>hawkins</category>
        
        <category>knowledge</category>
        
        
      </item>
    
      <item>
        <title>Document worth readingÔºö ‚ÄúUniversal gradient descent‚Äù</title>
        <description>&lt;p&gt;In this small book we collect many different and useful facts around gradient descent method. First of all we consider gradient descent with inexact oracle. We build a general model of optimized function that include composite optimization approache, level‚Äôs methods, proximal methods etc. Then we investigate primal-dual properties of the gradient descent in general model set-up. At the end we generalize method to universal one. &lt;a href=&quot;http://arxiv.org/abs/1711.00394v1&quot;&gt;Universal gradient descent&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;like-this&quot;&gt;Like this:&lt;/h3&gt;

&lt;p&gt;Like Loading‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/cae9e56f34773d2fb89a27fce818d1dc/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/cae9e56f34773d2fb89a27fce818d1dc/</guid>
        
        <category>small book</category>
        
        <category>useful facts</category>
        
        <category>optimized</category>
        
        <category>composite optimization</category>
        
        
      </item>
    
      <item>
        <title>DeepLearning-GithubÊéíË°å</title>
        <description>&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Pos1&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Language&lt;/th&gt;
      &lt;th&gt;Stars Today&lt;/th&gt;
      &lt;th&gt;Total Stars&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;pytorch&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Tensors and Dynamic neural networks in Python  with strong GPU acceleration&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;17245&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/keras-team/keras&quot;&gt;keras&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Learning for humans&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;31783&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/exacity/deeplearningbook-chinese&quot;&gt;deeplearningbook-chinese&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Learning Book Chinese Translation&lt;/td&gt;
      &lt;td&gt;TeX&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;18828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tensorflow/tfjs&quot;&gt;tfjs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A WebGL accelerated JavaScript library for training and deploying ML models.&lt;/td&gt;
      &lt;td&gt;JavaScript&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;8339&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap&quot;&gt;Deep-Learning-Papers-Reading-Roadmap&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;18229&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;Detectron&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;FAIR‚Äôs research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;15326&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/aymericdamien/TensorFlow-Examples&quot;&gt;TensorFlow-Examples&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TensorFlow Tutorial and Examples for Beginners with Latest APIs&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;23682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/udacity/deep-reinforcement-learning&quot;&gt;deep-reinforcement-learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Repo for the Deep Reinforcement Learning Nanodegree program&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/opencv/opencv&quot;&gt;opencv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Open Source Computer Vision Library&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;26315&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/fastai/fastai&quot;&gt;fastai&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;The fast.ai deep learning library, lessons, and tutorials&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;5948&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/matterport/Mask_RCNN&quot;&gt;Mask_RCNN&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;6727&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/CMU-Perceptual-Computing-Lab/openpose&quot;&gt;openpose&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;OpenPose: Real-time multi-person keypoint detection library for body, face, and hands estimation&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;8174&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/yunjey/pytorch-tutorial&quot;&gt;pytorch-tutorial&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;PyTorch Tutorial for Deep Learning Researchers&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;6500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/BVLC/caffe&quot;&gt;caffe&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Caffe: a fast open framework for deep learning.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;24930&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers&quot;&gt;awesome-deep-learning-papers&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;The most cited deep learning papers&lt;/td&gt;
      &lt;td&gt;TeX&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;15445&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ZuzooVn/machine-learning-for-software-engineers&quot;&gt;machine-learning-for-software-engineers&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A complete daily plan for studying to become a machine learning engineer.&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;19273&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/AcceptedDoge/machine-learning-yearning-cn&quot;&gt;machine-learning-yearning-cn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Machine Learning Yearning Official Chinese Translation&lt;/td&gt;
      &lt;td&gt;CSS&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;257&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ageron/handson-ml&quot;&gt;handson-ml&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8483&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tzutalin/labelImg&quot;&gt;labelImg&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:metal: LabelImg is a graphical image annotation tool and label object bounding boxes in images&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;3982&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/uber/horovod&quot;&gt;horovod&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Distributed training framework for TensorFlow, Keras, and PyTorch.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;3222&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/pjreddie/darknet&quot;&gt;darknet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Convolutional Neural Networks&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;8723&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/hunkim/DeepLearningZeroToAll&quot;&gt;DeepLearningZeroToAll&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TensorFlow Basic Tutorial Labs&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2548&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/davisking/dlib&quot;&gt;dlib&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A toolkit for making real world machine learning and data analysis applications in C++&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;5379&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;ray&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A high-performance distributed execution engine&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;3740&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/Microsoft/CNTK&quot;&gt;CNTK&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;14840&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tensorflow/tensor2tensor&quot;&gt;tensor2tensor&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4526&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/aleju/imgaug&quot;&gt;imgaug&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Image augmentation for machine learning experiments.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3251&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents&quot;&gt;ml-agents&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Unity Machine Learning Agents Toolkit&lt;/td&gt;
      &lt;td&gt;C#&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3499&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/fchollet/deep-learning-with-python-notebooks&quot;&gt;deep-learning-with-python-notebooks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Jupyter notebooks for the code samples of the book ‚ÄúDeep Learning with Python‚Äù&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3848&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/Tencent/ncnn&quot;&gt;ncnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ncnn is a high-performance neural network inference framework optimized for the mobile platform&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4253&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic&quot;&gt;conv_arithmetic&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/td&gt;
      &lt;td&gt;TeX&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/Hvass-Labs/TensorFlow-Tutorials&quot;&gt;TensorFlow-Tutorials&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TensorFlow Tutorials with YouTube Videos&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4773&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/keon/awesome-nlp&quot;&gt;awesome-nlp&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:book: A curated list of resources dedicated to Natural Language Processing (NLP)&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;6244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/amusi/awesome-object-detection&quot;&gt;awesome-object-detection&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Awesome Object Detection based on handong1587 github: https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/abhshkdz/ai-deadlines&quot;&gt;ai-deadlines&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:alarm_clock: AI conference deadline countdowns&lt;/td&gt;
      &lt;td&gt;HTML&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;686&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/salesforce/decaNLP&quot;&gt;decaNLP&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;The Natural Language Decathlon: A Multitask Challenge for NLP&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;929&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/oarriaga/face_classification&quot;&gt;face_classification&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2943&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/wepe/MachineLearning&quot;&gt;MachineLearning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Basic Machine Learning and Deep Learning&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2656&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ritchieng/the-incredible-pytorch&quot;&gt;the-incredible-pytorch&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;The Incredible PyTorch: a curated list of tutorials, papers, projects, communities and more relating to PyTorch.&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2601&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/brightmart/text_classification&quot;&gt;text_classification&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;all kinds of text classificaiton models and more with deep learning&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2361&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/Microsoft/MMdnn&quot;&gt;MMdnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1938&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/bharathgs/Awesome-pytorch-list&quot;&gt;Awesome-pytorch-list&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A comprehensive list of pytorch related content on github,such as different models,implementations,helper libraries,tutorials etc.&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1829&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tensorlayer/srgan&quot;&gt;srgan&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;909&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/allenai/allennlp&quot;&gt;allennlp&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;An open-source NLP research library, built on PyTorch.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2716&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/gluon-api/gluon-api&quot;&gt;gluon-api&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A clear, concise, simple yet powerful and efficient API for deep learning.&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2209&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;46&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/mozilla/DeepSpeech&quot;&gt;DeepSpeech&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A TensorFlow implementation of Baidu‚Äôs DeepSpeech architecture&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;7305&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;47&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-mxnet&quot;&gt;incubator-mxnet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;14557&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/explosion/spaCy&quot;&gt;spaCy&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;üí´ Industrial-strength Natural Language Processing (NLP) with Python and Cython&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;9952&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/cmusatyalab/openface&quot;&gt;openface&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Face recognition with deep neural networks.&lt;/td&gt;
      &lt;td&gt;Lua&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;10398&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/deeplearning4j/deeplearning4j&quot;&gt;deeplearning4j&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deeplearning4j, ND4J, DataVec and more - deep learning &amp;amp; linear algebra for Java/Scala with GPUs + Spark - From Skymind&lt;/td&gt;
      &lt;td&gt;Java&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;9360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/janishar/mit-deep-learning-book-pdf&quot;&gt;mit-deep-learning-book-pdf&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville&lt;/td&gt;
      &lt;td&gt;Java&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;52&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/oxford-cs-deepnlp-2017/lectures&quot;&gt;lectures&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Oxford Deep NLP 2017 course&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;12213&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/carla-simulator/carla&quot;&gt;carla&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Open-source simulator for autonomous driving research.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1508&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/DIGITS&quot;&gt;DIGITS&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Learning GPU Training System&lt;/td&gt;
      &lt;td&gt;HTML&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3255&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;55&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tflearn/tflearn&quot;&gt;tflearn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep learning library featuring a higher-level API for TensorFlow.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8328&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/chiphuyen/stanford-tensorflow-tutorials&quot;&gt;stanford-tensorflow-tutorials&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;This repository contains code examples for the Stanford‚Äôs course: TensorFlow for Deep Learning Research.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;6755&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/bulutyazilim/awesome-datascience&quot;&gt;awesome-datascience&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:memo: An awesome Data Science repository to learn and apply for real world problems.&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8290&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/junyanz/iGAN&quot;&gt;iGAN&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Interactive Image Generation via Generative Adversarial Networks&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2851&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/chainer/chainerrl&quot;&gt;chainerrl&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ChainerRL is a deep reinforcement learning library built on top of Chainer.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;501&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/alexjc/neural-enhance&quot;&gt;neural-enhance&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Super Resolution for images using deep learning.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8246&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;61&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/karpathy/char-rnn&quot;&gt;char-rnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch&lt;/td&gt;
      &lt;td&gt;Lua&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8229&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/fchollet/deep-learning-models&quot;&gt;deep-learning-models&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Keras code and weights files for popular deep learning models.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3791&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;63&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/zengarden/light_head_rcnn&quot;&gt;light_head_rcnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Light-Head R-CNN&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;479&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/karpathy/convnetjs&quot;&gt;convnetjs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Learning in Javascript. Train Convolutional Neural Networks (or ordinary ones) in your browser.&lt;/td&gt;
      &lt;td&gt;JavaScript&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;9201&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/KeKe-Li/book&quot;&gt;book&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:books: All programming languages books&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;716&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&quot;&gt;Deep_reinforcement_learning_Course&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Notebooks from the free course Deep Reinforcement Learning with Tensorflow&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/carpedm20/simulated-unsupervised-tensorflow&quot;&gt;simulated-unsupervised-tensorflow&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;TensorFlow implementation of ‚ÄúLearning from Simulated and Unsupervised Images through Adversarial Training‚Äù&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;458&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ChristosChristofidis/awesome-deep-learning&quot;&gt;awesome-deep-learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A curated list of awesome Deep Learning tutorials, projects and communities.&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;9344&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;69&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tensorpack/tensorpack&quot;&gt;tensorpack&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;A Neural Net Training Interface on TensorFlow&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2470&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/daviddao/deep-learning-book&quot;&gt;deep-learning-book&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:book: MIT Deep Learning Book in PDF format&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;246&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/1adrianb/face-alignment&quot;&gt;face-alignment&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;:fire: 2D and 3D Face alignment library build using pytorch&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1788&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/justadudewhohacks/opencv4nodejs&quot;&gt;opencv4nodejs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Asynchronous OpenCV 3.x nodejs bindings with JavaScript and TypeScript API, with examples for: Face Detection, Machine Learning, Deep Neural Nets, Hand Gesture Recognition, Object Tracking, Feature Matching, Image Histogram&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1798&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/SwiftBrain/awesome-CoreML-models&quot;&gt;awesome-CoreML-models&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Collection of models for Core ML&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/SnailTyan/deep-learning-papers-translation&quot;&gt;deep-learning-papers-translation&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Ê∑±Â∫¶Â≠¶‰π†ËÆ∫ÊñáÁøªËØëÔºåÂåÖÊã¨ÂàÜÁ±ªËÆ∫ÊñáÔºåÊ£ÄÊµãËÆ∫ÊñáÁ≠â&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;450&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/amusi/daily-paper-computer-vision&quot;&gt;daily-paper-computer-vision&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ËÆ∞ÂΩïÊØèÂ§©Êï¥ÁêÜÁöÑËÆ°ÁÆóÊú∫ËßÜËßâ/Ê∑±Â∫¶Â≠¶‰π†/Êú∫Âô®Â≠¶‰π†Áõ∏ÂÖ≥ÊñπÂêëÁöÑËÆ∫Êñá&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;798&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/philipperemy/keras-attention-mechanism&quot;&gt;keras-attention-mechanism&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Attention mechanism Implementation for Keras.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/caffe2/caffe2&quot;&gt;caffe2&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Caffe2 is a lightweight, modular, and scalable deep learning framework.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8173&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;78&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tensorflow/probability&quot;&gt;probability&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Probabilistic reasoning and statistical analysis in TensorFlow&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/transcranial/keras-js&quot;&gt;keras-js&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Run Keras models in the browser, with GPU support using WebGL&lt;/td&gt;
      &lt;td&gt;JavaScript&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ildoonet/tf-pose-estimation&quot;&gt;tf-pose-estimation&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference.&lt;/td&gt;
      &lt;td&gt;PureBasic&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1067&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/rbgirshick/py-faster-rcnn&quot;&gt;py-faster-rcnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Faster R-CNN (Python implementation) ‚Äì see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4536&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kpzhang93/MTCNN_face_detection_alignment&quot;&gt;MTCNN_face_detection_alignment&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks&lt;/td&gt;
      &lt;td&gt;Matlab&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/tensorflow/tfjs-core&quot;&gt;tfjs-core&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;WebGL-accelerated ML // linear algebra // automatic differentiation for JavaScript.&lt;/td&gt;
      &lt;td&gt;TypeScript&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;7755&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/data-science-ipython-notebooks&quot;&gt;data-science-ipython-notebooks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;13220&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/apple/turicreate&quot;&gt;turicreate&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Turi Create simplifies the development of custom machine learning models.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;7109&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/ShaoqingRen/faster_rcnn&quot;&gt;faster_rcnn&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Faster R-CNN&lt;/td&gt;
      &lt;td&gt;Matlab&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1740&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;87&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/plaidml/plaidml&quot;&gt;plaidml&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;PlaidML is a framework for making deep learning work everywhere.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1085&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;88&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/charlesq34/pointnet&quot;&gt;pointnet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/chenyuntc/pytorch-book&quot;&gt;pytorch-book&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1762&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/MorvanZhou/PyTorch-Tutorial&quot;&gt;PyTorch-Tutorial&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Build your neural network easy and fast&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1436&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;91&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/rguthrie3/DeepLearningForNLPInPytorch&quot;&gt;DeepLearningForNLPInPytorch&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;An IPython Notebook tutorial on deep learning for natural language processing, including structure prediction.&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;92&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/pkmital/CADL&quot;&gt;CADL&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Course materials/Homework materials for the FREE MOOC course on ‚ÄúCreative Applications of Deep Learning w/ Tensorflow‚Äù #CADL&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;93&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/wadhwasahil/Video-Classification-2-Stream-CNN&quot;&gt;Video-Classification-2-Stream-CNN&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Video Classification using 2 stream CNN&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;94&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/wkentaro/labelme&quot;&gt;labelme&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Image Polygonal Annotation with Python (polygon, rectangle, line, point and image-level flag annotation).&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;911&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/dmlc/tvm&quot;&gt;tvm&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Open deep learning compiler stack for cpu, gpu and specialized accelerators&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1728&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/NELSONZHAO/zhihu&quot;&gt;zhihu&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;This repo contains the source code in my personal column (https://zhuanlan.zhihu.com/zhaoyeyu), implemented using Python 3.6. Including Natural Language Processing and Computer Vision projects, such as text generation, machine translation, deep convolution GAN and other actual combat code.&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1144&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;97&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/roboticcam/machine-learning-notes&quot;&gt;machine-learning-notes&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;My continuously updated Machine Learning, Probabilistic Models and Deep Learning notes and demos (1000+ slides)  Êàë‰∏çÈó¥Êñ≠Êõ¥Êñ∞ÁöÑÊú∫Âô®Â≠¶‰π†ÔºåÊ¶ÇÁéáÊ®°ÂûãÂíåÊ∑±Â∫¶Â≠¶‰π†ÁöÑËÆ≤‰πâ(1000+È°µ)ÂíåËßÜÈ¢ëÈìæÊé•&lt;/td&gt;
      &lt;td&gt;Jupyter Notebook&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1064&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/philipperemy/tensorflow-1.4-billion-password-analysis&quot;&gt;tensorflow-1.4-billion-password-analysis&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Deep Learning model to analyze a large corpus of clear text passwords.&lt;/td&gt;
      &lt;td&gt;Python&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;888&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;99&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/dmlc/mshadow&quot;&gt;mshadow&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Matrix Shadow:Lightweight CPU/GPU Matrix and Tensor  Template Library in C++/CUDA for (Deep) Machine Learning&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;887&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/Dobiasd/frugally-deep&quot;&gt;frugally-deep&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Header-only library for using Keras models in C++.&lt;/td&gt;
      &lt;td&gt;C++&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;247&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/DeepLearning-Github%E6%8E%92%E8%A1%8C/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/DeepLearning-Github%E6%8E%92%E8%A1%8C/</guid>
        
        <category>Ê∑±Â∫¶Â≠¶‰π†</category>
        
        <category>Deep Learning</category>
        
        <category>Github</category>
        
        <category>Rank</category>
        
        <category>CNN</category>
        
        
      </item>
    
      <item>
        <title>Distilled News</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://data-flair.training/blogs/artificial-neural-network-model&quot;&gt;&lt;strong&gt;Introduction to Artificial Neural Network Model&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this Machine Learning tutorial, we will take you through the introduction of Artificial Neural network Model. First of all, we will discuss the multilayer Perceptron network next with the Radial Basis Function Network, they both are supervised learning model. At last, we will cover the Kohonen Model which follows Unsupervised learning and the difference between Multilayer Perceptron network and Radial Basis Function Network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561&quot;&gt;&lt;strong&gt;A Brief Overview of Outlier Detection Techniques&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Outliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jozefhajnala.gitlab.io/r/r104-unit-testing-coverage&quot;&gt;&lt;strong&gt;RStudio:addins part 4 ‚Äì Unit testing coverage investigation and improvement, made easy&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A developer always pays his technical debts! And we have a debt to pay to the gods of coding best practices, as we did not present many unit tests for our functions yet. Today we will show how to efficiently investigate and improve unit test coverage for our R code, with focus on functions governing our RStudio addins, which have their own specifics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://adamleerich.com/2018/07/21/miktex-install.html&quot;&gt;&lt;strong&gt;MiKTeX Behind a Windows Firewall&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I¬¥ve always had problems with MiKTeX on my work computer. I can install it just fine, or get IT to install it, but then the package manager doesn¬¥t work because of our firewall. You can set up a local repository to get around this problem, and I will show you how. I¬¥m just doing a basic setup here, just enough to compile the RStudio Rmd template to PDF. ‚ÄòWhy do I need MiKTeX ‚Äò, you might ask. Well, because if you want to create a PDF from an RMarkdown file in RStudio it is required. Otherwise you get this polite error message.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://privefl.github.io/blog/whether-to-use-a-data-frame-in-r&quot;&gt;&lt;strong&gt;Whether to use a data frame in R?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this post, I try to show you in which situations using a data frame is appropriate, and in which it¬¥s not.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://rviews.rstudio.com/2018/07/20/cvxr-a-direct-standardization-example&quot;&gt;&lt;strong&gt;CVXR: A Direct Standardization Example&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In our first blog post, we introduced CVXR, an R package for disciplined convex optimization, and showed how to model and solve a non-negative least squares problem using its interface. This time, we will tackle a non-parametric estimation example, which features new atoms as well as more complex constraints.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007%2Fs42081-018-0008-4&quot;&gt;&lt;strong&gt;Divide and recombine (D) data science projects for deep analysis of big data and high computational complexity&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The focus of data science is data analysis. This article begins with a categorization of the data science technical areas that play a direct role in data analysis. Next, big data are addressed, which create computational challenges due to the data size, as does the computational complexity of many analytic methods. Divide and recombine (D) is a statistical approach whose goal is to meet the challenges. In D, the data are divided into subsets, an analytic method is applied independently to each subset, and the outputs are recombined. This enables a large component of embarrassingly-parallel computation, the fastest parallel computation. DeltaRho open-source software implements D&amp;amp;R.; At the front end, the analyst programs in R. The back end is the Hadoop distributed file system and parallel compute engine. The goals of D are the following: access to thousands of methods of machine learning, statistics, and data visualization; deep analysis of the data, which means analysis of the detailed data at their finest granularity; easy programming of analyses; and high computational performance. To succeed, D requires research in all of the technical areas of data science. Network cybersecurity and climate science are two subject-matter areas with big, complex data benefiting from D&amp;amp;R.; We illustrate this by discussing two datasets, one from each area. The first is the measurements of 13 variables for each of 10,615,054,608 queries to the Spamhaus IP address blacklisting service. The second has 50,632 3-hourly satellite rainfall estimates at 576,000 locations.&lt;/p&gt;

&lt;h3 id=&quot;like-this&quot;&gt;Like this:&lt;/h3&gt;

&lt;p&gt;Like Loading‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/447fcaeff28e3c6c9baa72bbd3219b9b/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/447fcaeff28e3c6c9baa72bbd3219b9b/</guid>
        
        <category>data</category>
        
        <category>computational</category>
        
        <category>compute</category>
        
        <category>learning</category>
        
        <category>network</category>
        
        <category>rstudio</category>
        
        <category>science</category>
        
        <category>miktex</category>
        
        <category>complexity</category>
        
        <category>model</category>
        
        <category>blog</category>
        
        <category>experimental errors</category>
        
        <category>parallel</category>
        
        <category>convex</category>
        
        <category>article</category>
        
        <category>functions</category>
        
        <category>technical</category>
        
        <category>estimation</category>
        
        <category>estimates</category>
        
        <category>loading</category>
        
        <category>distributed</category>
        
        <category>file</category>
        
        <category>easy</category>
        
        <category>analysis</category>
        
        <category>blacklisting</category>
        
        <category>ip</category>
        
        <category>unit</category>
        
        
      </item>
    
      <item>
        <title>R Packages worth a look</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Double-Gap Life Expectancy Forecasting Model&lt;/em&gt;&lt;/strong&gt; (&lt;a href=&quot;https://github.com/mpascariu/MortalityGaps&quot;&gt;&lt;strong&gt;MortalityGaps&lt;/strong&gt;&lt;/a&gt;)Life expectancy is highly correlated over time among countries and between males and females. These associations can be used to improve forecasts. Here ‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Penalized Partial Least Squares&lt;/em&gt;&lt;/strong&gt; (&lt;a href=&quot;https://cran.r-project.org/package=ppls&quot;&gt;&lt;strong&gt;ppls&lt;/strong&gt;&lt;/a&gt;)Contains linear and nonlinear regression methods based on Partial Least Squares and Penalization Techniques. Model parameters are selected via cross-va ‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Statistical Inference on Lineup Fairness&lt;/em&gt;&lt;/strong&gt; (&lt;a href=&quot;https://cran.r-project.org/package=r4lineups&quot;&gt;&lt;strong&gt;r4lineups&lt;/strong&gt;&lt;/a&gt;)Since the early 1970s eyewitness testimony researchers have recognised the importance of estimating properties such as lineup bias (is the lineup biase ‚Ä¶&lt;/p&gt;

&lt;h3 id=&quot;like-this&quot;&gt;Like this:&lt;/h3&gt;

&lt;p&gt;Like Loading‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/4206204815b33c96834ca91e68c5173a/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/4206204815b33c96834ca91e68c5173a/</guid>
        
        <category>model</category>
        
        <category>ppls</category>
        
        <category>mortalitygaps</category>
        
        <category>testimony</category>
        
        <category>lineup</category>
        
        <category>forecasting</category>
        
        <category>forecasts</category>
        
        <category>methods</category>
        
        
      </item>
    
      <item>
        <title>Of statistics class and judo classÔºö  Beyond the paradigm of sequential education</title>
        <description>&lt;p&gt;In judo class they kinda do the same thing every time: you warm up and then work on different moves. Different moves in different classes, and there are different levels, but within any level the classes don‚Äôt really have a sequence. You just start where you start, practice over and over, and gradually improve. Different students in the class are at different levels, both when it comes to specific judo expertise and also general strength, endurance, and flexibility, so it wouldn‚Äôt make sense to set up the class sequentially. Even during the semester, some people show up at the dojo once a week, others twice or three times a week.&lt;/p&gt;

&lt;p&gt;Now compare that with how we organize our statistics classes. Each course has a sequence, everyone starts on week 1 and ends on week 15 (or less for a short course), and the assumption is that everyone starts at the same level, has the same abilities, will put in the same level of effort, and can learn at the same pace. We know this isn‚Äôt so, and we try our best to adapt to the levels of all the students in the class, but the baseline is uniformity.&lt;/p&gt;

&lt;p&gt;The main way in which we adapt to different levels of students is to offer many different courses, so each student can jump in at his or her own level. But this still doesn‚Äôt account for different abilities, different amounts of time spent during the semester, and so forth.&lt;/p&gt;

&lt;p&gt;Also, and relatedly, I don‚Äôt think the sequential model works so well within a course, even setting aside the differences between students. There is typically only a weak ordering of the different topics within a statistics course, and to really learn the material you have to keep going back and practicing what you‚Äôve learned.&lt;/p&gt;

&lt;p&gt;The sequential model works well in a textbook‚Äîit‚Äôs good to be able to find what you need, and see how it relates to the other material you‚Äôll be learning. But in a course, I‚Äôm thinking we‚Äôd be better off moving toward the judo model, in which we have a bunch of classes with regular hours and students can drop in and practice, setting their schedules as they see fit. We could then assess progress using standardized tests instead of course grades.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; I‚Äôm not an expert on judo, so please take the above description as approximate. This post is really about statistics teaching, not judo.&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/1eed6a1f736f3a73a8fcf48dcfbb547f/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/1eed6a1f736f3a73a8fcf48dcfbb547f/</guid>
        
        <category>differences</category>
        
        <category>judo class</category>
        
        <category>sequential model works</category>
        
        <category>courses</category>
        
        <category>sequentially</category>
        
        <category>classes</category>
        
        <category>improve</category>
        
        <category>grades</category>
        
        
      </item>
    
      <item>
        <title>If you did not already know</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Belief_propagation&quot;&gt;&lt;strong&gt;Belief Propagation&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;https://www.google.de/search?q=Belief Propagation&quot;&gt;&lt;img src=&quot;https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529&quot; alt=&quot;&quot; /&gt;
&lt;/a&gt;Belief propagation, also known as sum-product message passing is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability. The algorithm was first proposed by Judea Pearl in 1982, who formulated this algorithm on trees, and was later extended to polytrees. It has since been shown to be a useful approximate algorithm on general graphs. ‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1807.03646v1&quot;&gt;&lt;strong&gt;DSS-MAS&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;https://www.google.de/search?q=DSS-MAS&quot;&gt;&lt;img src=&quot;https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529&quot; alt=&quot;&quot; /&gt;
&lt;/a&gt;For some decision processes a significant added value is achieved when enterprises‚Äô internal Data Warehouse (DW) can be integrated and combined with external data gained from web sites of competitors and other relevant Web sources. In this paper we discuss the agent-based integration approach using ontologies (DSS-MAS). In this approach data from internal DW and external sources are scanned by coordinated group of agents, while semantically integrated and relevant data is reported to business users according to business rules. After data from internal DW, Web sources and business rules are acquired, agents using these data and rules can infer new knowledge and therefore facilitate decision making process. Knowledge represented in enterprises‚Äô ontologies is acquired from business users without extensive technical knowledge using user friendly user interface based on constraints and predefined templates. The approach presented in the paper was verified using the case study from the domain of mobile communications with the emphasis on supply and demand of mobile phones. ‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1709.08374v1&quot;&gt;&lt;strong&gt;Deep Sparse Subspace Clustering&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;https://www.google.de/search?q=Deep Sparse Subspace Clustering&quot;&gt;&lt;img src=&quot;https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529&quot; alt=&quot;&quot; /&gt;
&lt;/a&gt;In this paper, we present a deep extension of Sparse Subspace Clustering, termed Deep Sparse Subspace Clustering (DSSC). Regularized by the unit sphere distribution assumption for the learned deep features, DSSC can infer a new data affinity matrix by simultaneously satisfying the sparsity principle of SSC and the nonlinearity given by neural networks. One of the appealing advantages brought by DSSC is: when original real-world data do not meet the class-specific linear subspace distribution assumption, DSSC can employ neural networks to make the assumption valid with its hierarchical nonlinear transformations. To the best of our knowledge, this is among the first deep learning based subspace clustering methods. Extensive experiments are conducted on four real-world datasets to show the proposed DSSC is significantly superior to 12 existing methods for subspace clustering. ‚Ä¶&lt;/p&gt;

&lt;h3 id=&quot;like-this&quot;&gt;Like this:&lt;/h3&gt;

&lt;p&gt;Like Loading‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/22/087f689b4ebf34b19980dc142ed8ea22/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/22/087f689b4ebf34b19980dc142ed8ea22/</guid>
        
        <category>data</category>
        
        <category>deep</category>
        
        <category>clustering</category>
        
        <category>dssc</category>
        
        <category>based</category>
        
        <category>https</category>
        
        <category>nodes</category>
        
        <category>nonlinearity</category>
        
        <category>subspace</category>
        
        <category>distribution</category>
        
        <category>knowledge</category>
        
        <category>web</category>
        
        <category>extensive</category>
        
        <category>extension</category>
        
        <category>approximation</category>
        
        <category>approximate</category>
        
        <category>codes</category>
        
        <category>advantages</category>
        
        <category>empirical</category>
        
        <category>random</category>
        
        <category>decision</category>
        
        <category>networks</category>
        
        <category>neural</category>
        
        <category>significant added</category>
        
        <category>assumption</category>
        
        <category>integrated</category>
        
        <category>integration</category>
        
        
      </item>
    
      <item>
        <title>RStudioÔºöaddins part 4 ‚Äì Unit testing coverage investigation and improvement, made easy</title>
        <description>&lt;p&gt;A developer always pays his technical debts! And we have a debt to pay to the gods of coding best practices, as we did not present many unit tests for our functions yet. Today we will show how to efficiently investigate and improve unit test coverage for our R code, with focus on functions governing our RStudio addins, which have their own specifics.&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;As a practical example, we will do a simple resctructuring of one of our functions to increase its test coverage from a mere 34% to over 90%.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The pretty rewards for your tests&lt;/p&gt;

&lt;p&gt;Much has been written on the importance of unit testing, so we will not spend more time on convincing the readers, but rather very quickly provide a few references in case the reader is new to unit testing with R. In the later parts of the article we assume that these basics are known.&lt;/p&gt;

&lt;p&gt;In a few words&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/r-lib/devtools&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;devtools&lt;/code&gt;&lt;/a&gt; ‚Äì Makes package development easier by providing R functions that simplify common tasks&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/r-lib/testthat&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;testthat&lt;/code&gt;&lt;/a&gt;‚Äì Is the most popular unit testing package for R&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/r-lib/covr&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;covr&lt;/code&gt;&lt;/a&gt;‚Äì Helps track test coverage for R packages and view reports locally or (optionally) upload the results&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a start guide to use &lt;code class=&quot;highlighter-rouge&quot;&gt;testthat&lt;/code&gt; within a package, visit the &lt;a href=&quot;http://r-pkgs.had.co.nz/tests.html&quot;&gt;Testing section&lt;/a&gt; of R packages by Hadley Wickham. I would also recommend checking out the &lt;a href=&quot;https://www.tidyverse.org/articles/2017/12/testthat-2-0-0&quot;&gt;showcase on the 2.0.0&lt;/a&gt; release of the testthat itself.&lt;/p&gt;

&lt;p&gt;For the purpose of investigating the test coverage of a package we can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;covr&lt;/code&gt; package. Within an R project, we can call the &lt;code class=&quot;highlighter-rouge&quot;&gt;package_coverage()&lt;/code&gt; function to get a nicely printed high-level overview, or we can provide a specific path to a package root directory and call it as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# This looks much prettier in the R console ;)
covr::package_coverage(pkgPath)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## jhaddins Coverage: 59.05%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## R/viewSelection.R: 34.15%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## R/addRoxytag.R: 40.91%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## R/makeCmd.R: 92.86%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For a deeper investigation, converting the results to a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.frame&lt;/code&gt; might be very useful. The below shows the count of number of times that given expression was called during the running of our tests for each group of code lines:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;covResults &amp;lt;- covr::package_coverage(pkgPath)
as.data.frame(covResults)[, c(1:3, 5, 11)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## filename functions first_line last_line value
## 1 R/addRoxytag.R roxyfy 10 12 6
## 2 R/addRoxytag.R roxyfy 11 11 2
## 3 R/addRoxytag.R roxyfy 13 15 4
## 4 R/addRoxytag.R roxyfy 14 14 2
## 5 R/addRoxytag.R roxyfy 16 16 2
## 6 R/addRoxytag.R roxyfy 17 17 2
## 7 R/addRoxytag.R roxyfy 18 18 2
## 8 R/addRoxytag.R addRoxytag 29 29 0
## 9 R/addRoxytag.R addRoxytag 30 37 0
## 10 R/addRoxytag.R addRoxytag 32 34 0
## 11 R/addRoxytag.R addRoxytag 38 38 0
## 12 R/addRoxytag.R addRoxytagCode 44 44 0
## 13 R/addRoxytag.R addRoxytagLink 50 50 0
## 14 R/addRoxytag.R addRoxytagEqn 56 56 0
## 15 R/makeCmd.R makeCmd 20 24 5
## 16 R/makeCmd.R makeCmd 21 21 0
## 17 R/makeCmd.R makeCmd 23 23 5
## 18 R/makeCmd.R makeCmd 25 27 5
## 19 R/makeCmd.R makeCmd 26 26 4
## 20 R/makeCmd.R makeCmd 28 32 5
## 21 R/makeCmd.R makeCmd 33 35 5
## 22 R/makeCmd.R makeCmd 34 34 2
## 23 R/makeCmd.R makeCmd 36 38 5
## 24 R/makeCmd.R makeCmd 37 37 1
## 25 R/makeCmd.R makeCmd 39 39 5
## 26 R/makeCmd.R replaceTilde 48 50 1
## 27 R/makeCmd.R replaceTilde 49 49 1
## 28 R/makeCmd.R replaceTilde 51 51 1
## 29 R/makeCmd.R executeCmd 61 61 5
## 30 R/makeCmd.R executeCmd 62 66 5
## 31 R/makeCmd.R executeCmd 68 72 3
## 32 R/makeCmd.R executeCmd 69 69 0
## 33 R/makeCmd.R executeCmd 71 71 3
## 34 R/makeCmd.R runCurrentRscript 90 90 1
## 35 R/makeCmd.R runCurrentRscript 91 91 1
## 36 R/makeCmd.R runCurrentRscript 92 96 1
## 37 R/makeCmd.R runCurrentRscript 93 95 1
## 38 R/makeCmd.R runCurrentRscript 94 94 0
## 39 R/viewSelection.R viewSelection 7 7 0
## 40 R/viewSelection.R viewSelection 8 12 0
## 41 R/viewSelection.R viewSelection 10 10 0
## 42 R/viewSelection.R viewSelection 13 13 0
## 43 R/viewSelection.R getFromSysframes 24 24 6
## 44 R/viewSelection.R getFromSysframes 25 25 3
## 45 R/viewSelection.R getFromSysframes 26 26 3
## 46 R/viewSelection.R getFromSysframes 28 28 3
## 47 R/viewSelection.R getFromSysframes 29 29 3
## 48 R/viewSelection.R getFromSysframes 30 30 3
## 49 R/viewSelection.R getFromSysframes 31 31 92
## 50 R/viewSelection.R getFromSysframes 32 32 92
## 51 R/viewSelection.R getFromSysframes 33 33 92
## 52 R/viewSelection.R getFromSysframes 34 34 2
## 53 R/viewSelection.R getFromSysframes 37 37 1
## 54 R/viewSelection.R viewObject 56 56 3
## 55 R/viewSelection.R viewObject 57 57 3
## 56 R/viewSelection.R viewObject 58 58 3
## 57 R/viewSelection.R viewObject 61 61 0
## 58 R/viewSelection.R viewObject 64 64 0
## 59 R/viewSelection.R viewObject 65 65 0
## 60 R/viewSelection.R viewObject 66 66 0
## 61 R/viewSelection.R viewObject 69 69 0
## 62 R/viewSelection.R viewObject 70 70 0
## 63 R/viewSelection.R viewObject 71 71 0
## 64 R/viewSelection.R viewObject 74 74 0
## 65 R/viewSelection.R viewObject 76 76 0
## 66 R/viewSelection.R viewObject 77 77 0
## 67 R/viewSelection.R viewObject 79 79 0
## 68 R/viewSelection.R viewObject 81 81 0
## 69 R/viewSelection.R viewObject 82 82 0
## 70 R/viewSelection.R viewObject 83 83 0
## 71 R/viewSelection.R viewObject 88 88 0
## 72 R/viewSelection.R viewObject 89 89 0
## 73 R/viewSelection.R viewObject 91 91 0
## 74 R/viewSelection.R viewObject 92 92 0
## 75 R/viewSelection.R viewObject 93 93 0
## 76 R/viewSelection.R viewObject 96 96 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Calling &lt;code class=&quot;highlighter-rouge&quot;&gt;covr::zero_coverage&lt;/code&gt; with a overage object returned by &lt;code class=&quot;highlighter-rouge&quot;&gt;package_coverage&lt;/code&gt; will provide a data.frame with locations that have 0 test coverage. The nice thing about running it within RStudio is that it outputs the results on the Markers tab in RStudio, where we can easily investigate:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zeroCov &amp;lt;- covr::zero_coverage(covResults)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;zero_coverage markers&lt;/p&gt;

&lt;p&gt;Investigating our code, let us focus on the results for the &lt;code class=&quot;highlighter-rouge&quot;&gt;viewSelection.R&lt;/code&gt;, which has a very weak 34% test coverage. We can analyze exactly which lines have no test coverage in a specific file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zeroCov[zeroCov$filename == &quot;R/viewSelection.R&quot;, &quot;line&quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 7 8 9 10 11 12 13 61 64 65 66 69 70 71 74 76 77 79 81 82 83 88 89
## [24] 91 92 93 96
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://gitlab.com/jozefhajnala/jhaddins/blob/bd5979cecfbf4b81a6f8c02e5593fe109baed172/R/viewSelection.R&quot;&gt;Looking at the code&lt;/a&gt;, we can see that the first chuck of lines ‚Äì 7:13 represent the &lt;code class=&quot;highlighter-rouge&quot;&gt;viewSelection&lt;/code&gt; function, which just calls &lt;code class=&quot;highlighter-rouge&quot;&gt;lapply&lt;/code&gt; and invisibly returns &lt;code class=&quot;highlighter-rouge&quot;&gt;NULL&lt;/code&gt;.The main weak spot however is the function &lt;code class=&quot;highlighter-rouge&quot;&gt;viewObject&lt;/code&gt;, out of which we only test the early return in case of invalid &lt;code class=&quot;highlighter-rouge&quot;&gt;chr&lt;/code&gt; argument provided. None of the other functionality is tested.&lt;/p&gt;

&lt;p&gt;The reason behind this is that when running the tests, RStudio functionality is not available and therefore we would not be able to test even the not-so-well designed return values, as they are almost always preceded by a call to &lt;code class=&quot;highlighter-rouge&quot;&gt;rstudioapi&lt;/code&gt; or other RStudio-related functionality such as the object viewer, because that is what they are designed to do. This means we must restructure the code in such a way that we contain the RStudio-dependent functionality to a necessary minimum, keeping a big majority of the code testable ‚Äì only calling the side-effecting &lt;code class=&quot;highlighter-rouge&quot;&gt;rstudioapi&lt;/code&gt; when actually executing the addin functionality itself.&lt;/p&gt;

&lt;p&gt;We will now show one potential way to solve this issue for the particular case of our &lt;code class=&quot;highlighter-rouge&quot;&gt;viewObject&lt;/code&gt; function.&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;The idea behind the solution is to only return the arguments for the call to the RStudio API related functionality, instead of executing them in the function itself ‚Äì hence the rename to &lt;code class=&quot;highlighter-rouge&quot;&gt;getViewArgs&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This way we can test the function‚Äôs return value against the expected arguments and only execute them with &lt;code class=&quot;highlighter-rouge&quot;&gt;do.call&lt;/code&gt; in the addin execution wrapper itself. A picture may be worth a thousand words, so here is the diff with relevant changes:&lt;/p&gt;

&lt;p&gt;Refactoring for testability&lt;/p&gt;

&lt;p&gt;Now that our return values are testable across the entire &lt;code class=&quot;highlighter-rouge&quot;&gt;getViewArgs&lt;/code&gt; function, we can easily write tests to cover the entire function, a couple examples:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test_that(&quot;getViewArgs for function&quot;
 , expect_equal(
 getViewArgs(&quot;reshape&quot;)
 , list(what = &quot;View&quot;, args = list(x = reshape, title = &quot;reshape&quot;))
 )
 )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test_that(&quot;getViewArgs for data.frame&quot;
 , expect_equal(
 getViewArgs(&quot;datasets::women&quot;)
 , list(what = &quot;View&quot;,
 args = list(x = data.frame(
 height = c(58, 59, 60, 61, 62, 63, 64, 65,
 66, 67, 68, 69, 70, 71, 72),
 weight = c(115, 117, 120, 123, 126, 129, 132, 135,
 139, 142, 146, 150, 154, 159, 164)
 ),
 title = &quot;datasets::women&quot;
 )
 )
 )
 )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looking at the test coverage provided after our changes, we can see that we are at more than 90% percent coverage for &lt;code class=&quot;highlighter-rouge&quot;&gt;viewSelection.R&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# This looks much prettier in the R console ;)
covResults &amp;lt;- covr::package_coverage(pkgPath)
covResults
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## jhaddins Coverage: 82.05%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## R/addRoxytag.R: 40.91%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## R/viewSelection.R: 90.57%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## R/makeCmd.R: 92.86%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And looking at the lines that not covered for &lt;code class=&quot;highlighter-rouge&quot;&gt;viewSelection.R&lt;/code&gt;, we can indeed see that the only uncovered lines left are in fact those with the &lt;code class=&quot;highlighter-rouge&quot;&gt;viewSelection&lt;/code&gt; function, which is responsible only for executing the addin itself:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;covResults &amp;lt;- as.data.frame(covResults)
covResults[covResults$filename == &quot;R/viewSelection.R&quot; &amp;amp;
 covResults$value == 0, c(1:3, 5, 11)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## filename functions first_line last_line value
## 39 R/viewSelection.R viewSelection 13 17 0
## 40 R/viewSelection.R viewSelection 15 15 0
## 41 R/viewSelection.R viewSelection 16 16 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the ideal world we would of course want to also automate the testing of our addin execution itself by examining if their effects in the RStudio IDE are as expected, however this is far beyond the scope of this post. For some of our addin functionality we can however even directly test the side-effects, such as when the addin should produce a file with certain content.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/21/e1454fb99c3232629f42aef9acc49045/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/21/e1454fb99c3232629f42aef9acc49045/</guid>
        
        <category>testing</category>
        
        <category>tested</category>
        
        <category>test_that</category>
        
        <category>functions</category>
        
        <category>functionality</category>
        
        <category>test coverage</category>
        
        <category>unit tests</category>
        
        <category>covresults</category>
        
        <category>returns</category>
        
        <category>lines</category>
        
        <category>packages</category>
        
        <category>testthat</category>
        
        <category>investigate</category>
        
        <category>investigating</category>
        
        <category>investigation</category>
        
        <category>first_line last_line value</category>
        
        <category>https</category>
        
        <category>rstudio</category>
        
        <category>object returned</category>
        
        <category>coding</category>
        
        <category>code</category>
        
        <category>getviewargs</category>
        
        <category>jhaddins</category>
        
        <category>reshape</category>
        
        <category>minimum</category>
        
        <category>values</category>
        
        <category>devtools</category>
        
        <category>arguments</category>
        
        <category>executing</category>
        
        <category>execute</category>
        
        <category>execution</category>
        
        <category>weak</category>
        
        <category>list</category>
        
        <category>filename</category>
        
        <category>zerocov</category>
        
        <category>examples</category>
        
        <category>nicely printed</category>
        
        
      </item>
    
      <item>
        <title>Magister Dixit</title>
        <description>&lt;p&gt;‚ÄúIt‚Äôs not enough to tell someone, ‚ÄòThis is done by boosted decision trees, and that‚Äôs the best classification algorithm, so just trust me, it works.‚Äô As a builder of these applications, you need to understand what the algorithm is doing in order to make it better. As a user who ultimately consumes the results, it can be really frustrating to not understand how they were produced. When we worked with analysts in Windows or in Bing, we were analyzing computer system logs. That‚Äôs very difficult for a human being to understand. We definitely had to work with the experts who understood the semantics of the logs in order to make progress. They had to understand what the machine learning algorithms were doing in order to provide useful feedback. ‚Ä¶ It really comes back to this big divide, this bottleneck, between the domain expert and the machine learning expert. I saw that as the most challenging problem facing us when we try to really make machine learning widely applied in the world. I saw both machine learning experts and domain experts as being difficult to scale up. There‚Äôs only a few of each kind of expert produced every year. I thought, how can I scale up machine learning expertise? I thought the best thing that I could do is to build software that doesn‚Äôt take a machine learning expert to use, so that the domain experts can use them to build their own applications. That‚Äôs what prompted me to do research in automating machine learning while at MSR [Microsoft Research].‚Äù &lt;a href=&quot;http://radar.oreilly.com/2015/08/bridging-the-divide-business-users-and-machine-learning-experts.html&quot;&gt;Alice Zheng ( 2015 )&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;like-this&quot;&gt;Like this:&lt;/h3&gt;

&lt;p&gt;Like Loading‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/21/c63a034726e138cfb0ab20509d71e5e6/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/21/c63a034726e138cfb0ab20509d71e5e6/</guid>
        
        <category>boosted decision</category>
        
        <category>trees</category>
        
        <category>challenging problem facing</category>
        
        <category>divide</category>
        
        <category>learning</category>
        
        
      </item>
    
      <item>
        <title>The Real Problems with Neural Machine Translation</title>
        <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;¬†No! Your Machine Translation Model is not ‚Äúprophesying‚Äù, but let‚Äôs look at the six major issues with neural machine translation (NMT).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So I saw a &lt;a href=&quot;https://twitter.com/jason_koebler/status/1020360057282867200&quot;&gt;Twitter thread&lt;/a&gt; today with the editor-in-chief of Motherboard tweeting, ‚Äú&lt;em&gt;Google Translate is popping out bizarre religious texts and no one is sure why&lt;/em&gt;‚Äú. This post in response to that. I am going to spend a little time on the ‚Äúwhy‚Äù part (folks who work in MT know why), but mostly focus on actual problems with neural machine translation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i1.wp.com/deliprao.com/wp-content/uploads/2018/07/nmt-1.png?resize=604%2C424&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But first, a small digression on responsible AI reporting. The choice of headlines, the promotion tweet, and the tone of &lt;a href=&quot;https://motherboard.vice.com/en_us/article/j5npeg/why-is-google-translate-spitting-out-sinister-religious-prophecies&quot;&gt;the article&lt;/a&gt; reminds me of all the irresponsible writing that went around the famous ‚ÄúFacebook Frankenstein‚Äù experiment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i1.wp.com/deliprao.com/wp-content/uploads/2018/07/nmt-2.png?resize=662%2C296&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would not be surprised if other media outlets picked up this Motherboard piece and ran ridiculous stories about machine translation conspiracy theories. If you are writing about AI as a journalist, using a sensational lede to bring in engagement to your website is the worst thing you can do as most people just read the ledes and share on social networks with ‚ÄúZOMG! AI is going kill us!‚Äù This distracts us from the actual harms of current-day machine learning like biases, discrimination, and weaponization of AI. Ok. Enough rant; back to technical programming ‚Ä¶&lt;/p&gt;

&lt;p&gt;So why religious texts in the outputs? When it comes to parallel corpora, religious texts are the lowest common denominator resource. Major religious texts like the Bible and the Koran exist in many (all?) languages. Back in graduate school, I used a ‚ÄúDianetics parallel corpus‚Äù for a project, simply because, like the Bible, all the proselytizing has it translated to more than 50 languages, including some low-resource ones. Say, if you are bootstrapping a machine translation system for Urdu-to-English for the government, it is easy to put together a bunch of religious texts already translated to Urdu. So one can safely assume Google‚Äôs parallel corpus collection contains all religious texts, and for many low-resource languages, they are not an insignificant portion of the training corpus.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i2.wp.com/deliprao.com/wp-content/uploads/2018/07/nmt-3.png?resize=1024%2C356&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One explanation of why we see religious text outputs in particular for garbage inputs, esp. in low-resource language pairs (somali-english in the figure above): Religious texts¬†contain a lot of rare words that don‚Äôt occur anywhere else but in the context of religious¬†texts. So, rare words could trigger religious contexts in the decoder, esp. when the proportion of such texts are large.¬†Another explanation is the model does not have much statistical support for the input and the output is simply a gobbledygook¬†sampling from the decoder model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So what are some &lt;em&gt;real&lt;/em&gt; problems in neural machine translation today?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Philipp Koehn and Rebecca Knowles wrote a fantastic¬†&lt;a href=&quot;http://www.aclweb.org/anthology/W/W17/W17-3204.pdf&quot;&gt;WNMT paper&lt;/a&gt;¬†on this topic in 2017 that‚Äôs still relevant now. To summarize:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;1. NMT sucks with out-of-domain data:&lt;/em&gt;&lt;/strong&gt;¬†Current machine translation systems produce very fluent outputs unrelated to the input for out-of-domain data. So a general MT system like Google Translate will do particularly bad in specialized domains like legal or finance. Compared to traditional methods like phrase-based systems, NMT systems do especially worse. How bad? See the figure below from the paper. The off-diagonal elements are out-of-domain results, and the green bars are NMT and the blue bars are phrase-based systems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i2.wp.com/deliprao.com/wp-content/uploads/2018/07/nmt-ood.png?resize=628%2C365&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;MT systems trained on one domain (rows) and tested on another domain (cols). Blue: Phrase-based, Green: NMT.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;2. NMT sucks with small datasets:&lt;/em&gt;&lt;/strong&gt; While this can be said about most machine learning in general, this problem is especially pronounced in NMT. The promise of NMT is it generalizes better (than phrase-based MT) with increasing data, but under low data situations, NMT does significantly worse. In fact, to quote the authors, ‚ÄúUnder low resource conditions, NMT produces fluent output unrelated to the input.‚Äù This could be another reason for some of the NMT weirdness explored in the Motherboard article.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;3. Subword NMT sucks for rare words:&lt;/em&gt;&lt;/strong&gt; While still better than phrase-based translation, NMT does poorly for rare or unseen words. This can become a problem for highly-inflected languages and domains with a lot of named entities where the inflected forms and named entity occurrences respectively can be very rare.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://i2.wp.com/deliprao.com/wp-content/uploads/2018/07/turkish-ch2.png&quot;&gt;&lt;img src=&quot;https://i2.wp.com/deliprao.com/wp-content/uploads/2018/07/turkish-ch2.png?resize=1024%2C596&quot; alt=&quot;&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure excerpt from Chapter 2 of our &lt;a href=&quot;https://www.amazon.com/Natural-Language-Processing-PyTorch-Applications/dp/1491978236&quot;&gt;upcoming book&lt;/a&gt;. Click on image to zoom. In a language like Turkish, for example, it is quite common to encounter an inflected form infrequently.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Words that were observed only once simply get dropped. Techniques like byte-pair encoding help, but more detailed work on this is warranted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Long sentences:&lt;/strong&gt; Encoding long sentences and generating long sentences is still a wide-open problem. MT systems degrade with sentence length, but NMT systems especially so. Using attention helps but the problem is far from ‚Äúsolved‚Äù. In many domains, such as legal, it is quite common to have long, complex sentences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Attention != Alignments:&lt;/strong&gt; This is a very subtle but important issue. In traditional SMT systems, like Phrase-based MT, alignments provide useful debugging information to inspect the model. But the attention mechanism cannot be viewed as alignments in the traditional sense, even if papers have often sold attention as ‚Äúsoft alignments‚Äù. In an NMT system, a verb in the target could be attending over the subject and object in addition to the verb in the source.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;6. Difficult to control quality:&lt;/em&gt;&lt;/strong&gt; Every word has multiple translations and typical MT systems score over a lattice of possible translations for a source sentence. To keep this lattice reasonable in size, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;&gt;beam search&lt;/a&gt;. By varying the beam width, it is possible to find low probability but correct translations. For NMT systems, tuning beam sizes seem to have no-to-adverse effects.&lt;/p&gt;

&lt;p&gt;NMT systems are still hard to beat when you have a ton of data, and they are here to stay. There is also something to be said about the blackboxness¬†of neural network models in general, and today‚Äôs NMT models (both LSTM and Transformer based) suffer from that. This is an active area of research, and I look forward to attending the &lt;a href=&quot;https://blackboxnlp.github.io/&quot;&gt;EMNLP workshop on that topic&lt;/a&gt; if my calendar cooperates.&lt;/p&gt;
</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/07/21/a827670dc7543ba4b0c5b54d626f7af1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/21/a827670dc7543ba4b0c5b54d626f7af1/</guid>
        
        <category>https</category>
        
        <category>translated</category>
        
        <category>translations</category>
        
        <category>systems</category>
        
        <category>languages</category>
        
        <category>religious texts</category>
        
        <category>based</category>
        
        <category>domains</category>
        
        <category>sentences</category>
        
        <category>resource</category>
        
        <category>beam</category>
        
        <category>language pairs</category>
        
        <category>attention</category>
        
        <category>data</category>
        
        <category>motherboard</category>
        
        <category>machine</category>
        
        <category>article</category>
        
        <category>alignments</category>
        
        <category>ridiculous</category>
        
        <category>knowles</category>
        
        <category>outlets</category>
        
        <category>encoding</category>
        
        <category>google</category>
        
        <category>corpus</category>
        
        <category>word</category>
        
        <category>entities</category>
        
        <category>entity occurrences</category>
        
        <category>network models</category>
        
        <category>inflected</category>
        
        <category>parallel</category>
        
        <category>traditional</category>
        
        <category>major</category>
        
        <category>outputs</category>
        
        <category>networks</category>
        
        <category>tweeting</category>
        
        <category>wnmt</category>
        
        
      </item>
    
  </channel>
</rss>
