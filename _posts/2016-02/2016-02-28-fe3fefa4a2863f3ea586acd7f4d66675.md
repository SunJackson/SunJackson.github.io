---
layout:     post
title:      Discovering and understanding patterns in highly dimensional data
subtitle:   转载自：http://rocketdatascience.org/?p=567
date:       2016-02-28
author:     Kirk Borne
header-img: img/background3.jpg
catalog: true
tags:
    - data
    - variables
    - news
    - human
    - visualizing
---

***[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)*** is a critical component of any solution dealing with massive data collections. Being able to sift through a mountain of data efficiently in order to find the key descriptive, predictive, and explanatory features of the collection is a fundamental required capability for coping with the Big Data avalanche. Identifying the most interesting dimensions of data is especially valuable when visualizing high-dimensional (high-variety) big data.

There is a “good news, bad news” angle here. First, *the bad news*: the human capacity for seeing multiple dimensions is very limited: 3 or 4 dimensions are manageable; [5 or 6 dimensions are possible](http://spdf.gsfc.nasa.gov/research/visualization/visbard/about.html); but more dimensions are difficult-to-impossible to assimilate. Now for *the good news*: the human cognitive ability to detect patterns, anomalies, changes, or other “features” in a large complex “scene” surpasses most computer algorithms for speed and effectiveness. In this case, a “scene” refers to any small-*n* projection of a larger-N parameter space of variables.

In data visualization, a systematic ordered parameter sweep through an ensemble of small-*n* projections (scenes) is often referred to as a “[grand tour](https://en.wikipedia.org/wiki/Grand_Tour_(data_visualisation))”, which allows a human viewer of the visualization sequence to see quickly any patterns or trends or anomalies in the large-N parameter space. Even such “grand tours” can miss salient (explanatory) features of the data, especially when the ratio N/*n *is large.

Consequently, a data analytics approach that combines the best of both worlds (machine algorithms and human perception) will enable efficient and effective exploration of large high-dimensional data. One such approach is to apply [Computer Vision](https://en.wikipedia.org/wiki/Computer_vision) algorithms, which are designed to emulate human perception and cognitive abilities. Another approach is to generate “[interestingness metrics](http://www.columbia.edu/~rsb2162/DRS_EDM-2014.pdf)” that signal to the data end-user the most interesting and informative features (or combinations of features) in high-dimensional data. A specific example of the latter is [latent (hidden) variable discovery](https://en.wikipedia.org/wiki/Latent_variable).

Latent variables are not explicitly observed but are inferred from the observed features, specifically because they are the variables that deliver the all-important (but sometimes hidden) descriptive, predictive, and explanatory power of the data set. Latent variables can also be concepts that are implicitly represented by the data (*e.g.,* the “[sentiment](https://en.wikipedia.org/wiki/Sentiment_analysis)” of the author of a social media posting).  

Because some latent variables are “observable” in the sense that they can be generated through a “yet to be discovered” mathematical combination of several of the measured variables, these are therefore an obvious example of *dimension reduction* for visual exploration of large high-dimensional data.

***[Latent (Hidden) Variable Models](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)*** are used in statistics to infer variables that are not observed but are inferred from the variables that are observed. Latent variables are widely used in social science, psychology, economics, life sciences and machine learning. In machine learning, many problems involve collection of high-dimensional multivariate observations and then hypothesizing a model that explains them. In such models, the role of the latent variables is to represent properties that have not been directly observed.

After inferring the existence of latent variables, the next challenge is to understand them. This can be achieved by exploring their relationship with the observed variables (e.g., using Bayesian methods) . Several correlation measures and dimensionality reduction methods such as PCA can be used to measure those relationships. Since we don’t know in advance what relationships exist between the latent variables and the observed variables, more generalized nonparametric measures like the [Maximal Information Coefficient](https://en.wikipedia.org/wiki/Maximal_information_coefficient) ([MIC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3325791)) can be used.

MIC has become [popular recently](http://www.i-programmer.info/news/84-database/3501-mine-finding-patterns-in-big-data.html), to some extent because it provides a straightforward R-squared type of estimate to measure dependency among variables in a high-dimensional data set.  Since we don’t know in advance what a latent variable actually represents, it is not possible to predict the type of relationship that it might possess with the observed variables. Consequently, a nonparametric approach makes sense in the case of large high-dimensional data, for which the interrelationships among the many variables is a mystery. Exploring variables that possess the largest values of MIC can help us to understand the type of relationships that the latent variables have with the existing variables, thereby achieving both dimension reduction and a parameter space in which to conduct visual exploration of high-dimensional data.

The techniques described here can help data end-users to discover and understand data patterns that may lead to interesting insights within their massive data collections.

Follow Kirk Borne on Twitter [@KirkDBorne](https://twitter.com/KirkDBorne)
