---
layout:     post
title:      Weekly Review： 11/11/2017
subtitle:   转载自：https://codesachin.wordpress.com/2017/11/12/weekly-review-11-04-2017/
date:       2017-11-11
author:     srjoglekar246
header-img: img/background0.jpg
catalog: true
tags:
    - learning
    - tangent
    - graphcore
    - cnn
    - computing
---

The Motion Planning course is going faster than I expected. I completed 2 weeks within 5 days. Thats good I guess, since it means I might get to the Capstone project before I take a vacation to India.

Heres the stuff from this week:

**Graphcore and the Intelligent Processing Unit (IPU)**

[Graphcore](https://www.graphcore.ai/.) aims to disrupt the world of ML-focussed computing devices. In an [interesting blog post](https://www.graphcore.ai/posts/what-does-machine-learning-look-like), they visualize neuron connections in different CNN architectures, and talk about how they compare to the human brain.

If you are curious about how IPUs differ from CPUs and GPUs, [this NextPlatform article](https://www.nextplatform.com/2017/03/09/early-look-startup-graphcores-deep-learning-chip) gives a few hints: mind you, IPUs are yet to be ‘released’, so theres no concrete information out yet. If you want to brush up on why memory is so important for neural network training (more than inference), [this](https://www.graphcore.ai/posts/why-is-so-much-memory-needed-for-deep-neural-networks) is a good place to start.

**Overview of Different CNN architectures**

[This article](http://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception) on the CV-Tricks blog gives a high-level overview of the major CNN architectures so far: AlexNet, VGG, Inception, ResNets, etc. Its a good place to go for reference if you ever happen to forget what one of them did differently.

On that note, [this blog post](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html) by Adit Deshpande goes into the ‘Brief History of Deep Learning’, marking out all the main research papers of importance.

**Meta-learning and AutoML**

The New York Times posted an [article](https://www.nytimes.com/2017/11/05/technology/machine-learning-artificial-intelligence-ai.html) about AI systems that can build other AI systems, thus leading to what they call ‘Meta-learning’ (Learning how to learn/build systems that learn).

Google has been dabbling in meta-learning with a project called AutoML. [AutoML](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html) basically consists of a ‘Generator’ network that comes up with various NN architectures, which are then evaluated by a ‘Scorer’ that trains them and computes their accuracy. The gradients with respect to these scores are passed back to the Generator, in order to improve the output architectures. [This](https://arxiv.org/abs/1611.01578) is their original paper, in case you want to take a look.

The AutoML team recently wrote [another post](https://research.googleblog.com/2017/11/automl-for-large-scale-image.html) about large-scale object detection using their algorithms.

**Tangent**

People from Google [recently open-sourced](https://research.googleblog.com/2017/11/tangent-source-to-source-debuggable.html) their library for computing gradients of Python functions. Tangent works directly on your Python code(rather than view it as a black-box), and comes up with a derivative function to compute its gradient. This is useful in cases where you might want to debug how/why some NN architecture is not getting trained the way it’s supposed to. Here’s their [Github repo.](https://github.com/google/tangent)

**Reconstructing films with Neural Network**

[This blog post](https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe) talks about the use of [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder) and [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_network) to reconstruct films using NNs trained on them. They also venture into reconstructing films using NNs trained on other stylish films (like *A Scanner Darkly*). The results are pretty interesting.





### Like this:

Like Loading...
