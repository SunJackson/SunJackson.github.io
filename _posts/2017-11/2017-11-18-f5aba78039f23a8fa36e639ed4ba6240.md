---
layout:     post
title:      Weekly Review： 11/18/2017
subtitle:   转载自：https://codesachin.wordpress.com/2017/11/19/weekly-review-11-18-2017/
date:       2017-11-18
author:     srjoglekar246
header-img: img/background1.jpg
catalog: true
tags:
    - data
    - flatbuffers
    - robotics
    - google
    - articles
---

I finished the **Motion Planning** course from [Robotics](https://www.coursera.org/specializations/robotics) this week. It was expected, since the material was quite in line with data structures and algorithms that I have studied during my undergrad. The next one, [Mobility](https://www.coursera.org/learn/robotics-mobility/home/welcome), seems to be a notch tougher than Aerial Robotics, mainly because of the focus on calculus and physics (neither of which I have touched heavily in years).

Heres the articles this week:

**[Neural Networks: Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35)**

In this article from Medium, the Director of AI at Tesla gives a fresh perspective on NNs. He refers to the set of *weights* in a Neural Network as a *program* which is learnt, as opposed to coded in by a human. This line of thought is justified by the fact that many decisions in Robotics, Search, etc. are taken by parametric ML systems. He also compares it to traditional ‘Software 1.0’, and points out the benefits of each.

**[Baselines in Machine Learning](http://smerity.com/articles/2017/baselines_need_love.html)**

In this article, a senior Research Scientist from Salesforce points out that we need to pay greater attention to baselines in Machine Learning. A *baseline* is any meaningful ‘benchmark’ algorithm that you would compare your algorithm against. The actual reference point would depend on your task – random/stratified systems for classification, state-of-the-art CNNs for image processing, etc. Read Neal’s answer to [this Quora question](https://www.quora.com/What-does-baseline-mean-in-machine-learning) for a deeper understanding.

The article ends with a couple of helpful tips, such as:

1. Use meaningful baselines, instead of using very crude code. The better your baseline, the more meaningful your results.

1. Start off with optimizing the baseline itself. Tune the weights, etc. if you have to – this gives you a good base to start your work on.


[TensorFlow Lite](https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html?m=1)

TensorFlow Lite is now in the Developer Preview mode. It is a light-weight platform for inference (not training) using ML models on mobile/embedded devices. Google calls it an ‘evolution of TensorFlow mobile’. While the latter is still the system you should use in production, TensorFlow lite appears to perform better on many benchmarks ([Differences here](https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html?m=1)). Some of the major plus-points of this new platform are smaller binaries, and support for custom ML-focussed hardware accelerators via the [Android Neural Networks API](https://developer.android.com/ndk/guides/neuralnetworks/index.html).

**[Flatbuffers](https://google.github.io/flatbuffers)**

Reading up on Tensorflow Lite also brought me to Flatbuffers, which are a ‘liter’ version of Protobufs. Flatbuffer is a data serialization library  for performance-critical applications. Flatbuffers provide the benefits of a smaller memory footprint and lesser generated code, mainly due to skipping of the parsing/unpacking step. Heres the [Github repo](https://github.com/google/flatbuffers).

**[Adversarial Attacks](https://blog.ycombinator.com/how-adversarial-attacks-work)**

This YCombinator article gives a nice overview of Adversarial attacks on ML models – attacks that provide ‘noisy’ data inputs to intelligent systems, in order to get a ‘wrong’ output. The author points out how Gradient descent can be used to sort-of reverse engineer spurious noise, in order to get data ‘misclassified’ by a neural network. The article also shows examples of such faulty inputs, and they are surprisingly indistinguishable from the original data!

 





### Like this:

Like Loading...
