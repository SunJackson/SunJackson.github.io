---
layout:     post
title:      Building a Neo4j Reporting Service Part II
subtitle:   转载自：http://www.kennybastani.com/2014/04/building-graph-based-analytics-platform-part-2.html
date:       2014-04-30
author:     noreply@blogger.com (Kenny Bastani)
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - nodes
    - data
    - apis
    - queried
    - query
    - https
    - reporting
    - restful
    - compelling
    - subgraphs
    - meetup
    - neo
    - web
    - building
    - questions
    - importing
    - imports
    - platform
    - polling
    - polls
    - polled
    - holmes stories
    - formatted
---














## 
Pages






![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)
















### 
Building a Neo4j Reporting Service Part II


> 
It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.

Sir Arthur Conan Doyle, Author of Sherlock Holmes stories

Continuing on from [Building a Graph-based Reporting Platform: Part I](http://www.kennybastani.com/2014/04/building-graph-based-analytics-platform-part-1.html), I posed some questions related to understanding how to build great community experiences around Neo4j using Meetup.com for local events. I presented an idea to use Neo4j to build a platform that could help us understand the demand for presenting compelling content at events.

Compelling content is at the core of great community experiences. That content fuels the conversations between people, ideas begin to flow, and innovation is born.

My idea was to build an open-source platform that would poll public APIs, translate collected data into a graph, and store it in a graph database to be analyzed, queried, and visualized over time. The first component of this architecture is the [Data Import Scheduler](https://github.com/neo4j-meetups-reporting/wiki#data-import-scheduler), which this post describes in detail.


Polling Data From Public APIs

Let's start out by answering a basic question.




> 
What does the data import scheduler do?

As illustrated in the diagram below, the Node.js application wakes up once a day and checks in with the Meetup.com REST API.


The scheduler process polls Meetup.com's REST API daily. An HTTP GET request is dispatched for each city we're tracking, returning a JSON formatted response for groups in those cities. The JSON data for each group is then translated into a subgraph, formatted as [Neo4j's Cypher query language](http://docs.neo4j.org/chunked/stable/cypher-query-lang.html). The Cypher query is then sent as a transaction to Neo4j and updates a snapshot of the group's stats for that day.


Importing a Meetup Group's Subgraph
The image below is a visualization of a Meetup group's subgraph, translated from JSON data polled on an arbitrary date.


We see that the group has a set of topic nodes, which may already exist within the database. The subgraph must be merged into the larger graph without duplicating any nodes. Using Cypher's [MERGE](http://docs.neo4j.org/chunked/stable/query-merge.html) clause we can get or create nodes, which is useful for expanding our graph's connected data. Each topic will collect more groups as new subgraphs are merged for daily imports. The same is also true for both day and location nodes.

After a few days of scheduled imports, a group's subgraph begins to take shape. As day nodes are connected to the previous day's node, membership statistics are connected.




All analysis on the temporal stats collected from the data import scheduler is performed within the [REST API module of the reporting platform](https://github.com/neo4j-contrib/neo4j-meetups-reporting/wiki#rest-api). It also safely exposes the graph database to a front-end web dashboard, consumed from client-side JavaScript. The REST API uses [Swagger](https://github.com/neo4j-contrib/neo4j-meetups-reporting/wiki#the-swagger-project), which is a specification and complete framework for describing, producing, consuming, and visualizing RESTful web services.





