---
layout:     post
title:      Introduction to Random forest
subtitle:   转载自：https://dimensionless.in/introduction-to-random-forest/
date:       2017-02-28
author:     Raghav Aggiwal
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - make separating
    - decisive factor
    - decision tree
    - modelling
    - rules
    - stair
    - final result
    - purity
    - probabilities
---

Decision tree is a simple, deterministic data structure for modelling decision rules for a specific classification problem. At each node, one feature is selected to make separating decision. We can stop splitting once the leaf node has optimally less data points. Such leaf node then gives us insight into the final result (Probabilities for different classes in case of classfication).Refer the figure below for a clearer understanding:

> 
How does it split?


The most decisive factor for the efficiency of a decision tree is the efficiency of its splitting process. We split at each node in such a way that the resulting **purity** is maximum. Well, purity just refers to how well we can segregate the classes and increase our knowledge by the split performed. An image is worth a thousand words. Have a look at the image below for some intuition:

Two popular methods for splitting are:

1. [Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)

1. [Information Gain](https://www.youtube.com/watch?v=nodQ2s0CUbI)


Explaining each of these methods in detail is beyond the scope of this post, but I highly recommend you to go through the given links for an in-depth understanding.

> 
Visualization:


Each split leads to a straight line classifying the dataset into two parts. Thus, the final decision boundary will consist of straight lines (boxes).

- Each split leads to a straight line classifying the dataset into two parts. Thus, the final decision boundary will consist of straight lines (or boxes).


- In comparison to regression, a decision tree can fit a stair case boundary to classify data.


---

