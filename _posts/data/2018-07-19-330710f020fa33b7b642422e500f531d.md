---
layout:     post
title:      simple tensorboard visualisation for gradient norms
subtitle:   转载自：http://matpalm.com/blog/viz_gradient_norms
date:       2018-07-19
author:     未知
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - gradients
    - summaries
    - gradient norms
    - http
    - explicitly
    - python
    - layer
    - regularisation
    - image
    - zero norm
    - variables
    - means learning
    - conv
    - post
    - pil
---

( i've had three people recently ask me about how i was visualising gradient norms in tensorboard so, according to
my three strikes rule, i now have to "automate" it by writing a blog post about it )

one really useful visualisation you can do while training a network is visualise the norms of the variables and gradients.

how are they useful? some random things that immediately come to mind include the fact that...

- diverging norms of variables might mean you haven't got enough regularisation.

- zero norm gradient means learning has somehow stopped.

- exploding gradient norms means learning is unstable and you might need to clip (hellloooo deep reinforcement learning).


let's consider a simple bounding box regression conv net (the specifics aren't important, i just grabbed this from another project, just needed something for illustration) ...

a simple training loop using feed_dict would be something along the lines of ...

but if we want to get access to gradients we need to do things a little differently and call `compute_gradients` and `apply_gradients` ourselves ...

with access to the gradients we can inspect them and create tensorboard summaries for them ...

( though we may only want to run the expensive `summaries_op` once in awhile... )

with logging like this we get 8 histogram summaries per variable; the cross product of

- layer weights vs layer biases

- variable vs gradients

- norms vs values


e.g. for conv layer c3 in the above model we get the summaries shown below.
note: nothing terribly interesting in this example, but a couple of things

- red : very large magnitude of gradient very early in training; this is classic variable rescaling.

- blue: non zero gradients at end of training, so stuff still happening at this layer in terms of the balance of l2 regularisation vs loss. (note: no bias regularisation means it'll continue to drift)


![](http://matpalm.com/blog/imgs/2017/c3_summaries.png)


## gradient norms with ggplot

sometimes the histograms aren't enough and you need to do some more serious plotting. 
in these cases i hackily wrap the gradient calc in [tf.Print](https://www.tensorflow.org/api_docs/python/tf/Print) 
and plot with [ggplot](http://ggplot2.org/)

e.g. here's some gradient norms from an old actor / critic model 
[(cartpole++)](https://github.com/matpalm/cartpoleplusplus)

![](http://matpalm.com/blog/imgs/2017/cartpole_gradient_norms.png)


## related: explicit simple_value and image summaries

on a related note you can also explicitly write summaries which is sometimes easier to do than generating the summaries through the graph. 

i find this especially true for image summaries where there are many pure python options for post processing with, say, [PIL](http://www.effbot.org/imagingbook/image.htm)

e.g. explicit scalar values

e.g. explicit image summaries using PIL post processing
