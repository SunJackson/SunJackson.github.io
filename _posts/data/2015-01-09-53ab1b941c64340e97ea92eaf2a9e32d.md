---
layout:     post
title:      Anomaly Detection with Wikipedia Page View Data
subtitle:   转载自：http://beautifuldata.net/2015/01/anomaly-detection-with-wikipedia-page-view-data/
date:       2015-01-09
author:     Benedikt Koehler
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - anomalies
    - wikipedia
    - http
    - packages
    - anomalydetection
    - function
    - values
    - twitter
    - datetime
    - esd
    - seasonal
    - blog
---

Today, the Twitter engineering team released another very interesting [Open Source R](https://github.com/twitter/AnomalyDetection) package for working with time series data: “[AnomalyDetection](https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series)“. This package uses the Seasonal Hybrid ESD (S-H-[ESD](http://vsp.pnnl.gov/help/Vsample/Rosners_Outlier_Test.htm)) algorithm to identify local anomalies (= variations inside seasonal patterns) and global anomalies (= variations that cannot be explained with seasonal patterns).

As a kind of warm up and practical exploration of the new package, here’s a short example on how to download Wikipedia PageView statistics and mine them for anomalies (inspired by [this blog post](http://beautifuldata.net/2012/11/wikipedia-attention-and-the-us-elections), where this package wasn’t available yet):

First, we install and load the necessary packages:

Then we choose an interesting Wikipedia page and download the last 90 days of PageView statistics:

I also did some pre-processing and transformation of the dates in POSIX datetime format. A first plot shows this pattern:

![](http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_views_USA.png)


Now, let’s look for anomalies. The usual way would be to feed a dataframe with a date-time and a value column into the AnomalyDetection function AnomalyDetectionTs(). But in this case, this doesn’t work because our data is much too coarse. It doesn’t seem to work with data on days. So, we use the more generic function AnomalyDetectionVec() that just needs the values and some definition of a period. In this case, the period is 7 (= 7 days for one week):

![](http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_anomalies_usa.png)


In our case, the algorithm has discovered 4 anomalies. The first on October 30 2014 being an exceptionally high value overall, the second is a very high Sunday, the third a high value overall and the forth a high Saturday (normally, this day is also quite weak).
