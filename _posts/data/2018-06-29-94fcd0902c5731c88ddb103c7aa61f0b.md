---
layout:     post
title:      Decoding Data Pipelines for Startups
subtitle:   转载自：https://www.data-blogger.com/2018/06/29/decoding-data-pipelines-for-startups/
date:       2018-06-29
author:     Natasha
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - tracking events
    - schema
    - steps
    - operational
    - pipelines
    - business
    - database
    - interactive
    - pipeline tools
    - systems
    - processed
    - files
    - testing
    - labels
    - querying managed
    - session
    - google
    - redshift
    - looker
    - endpoint
    - running
    - understand
    - terms
---

In this era of Big Data, an issue that has been escalating off- late relates to data fragmentation across organisations. This makes the process of analytics and reporting to become even more complex. This is where data pipeline tools come into play. To define it, a data pipeline denotes a set of actions carried out to extract data from different sources. For a startup, building a data pipeline is an important aspect of data science. They need to gather data points from all users and process it in real- time for developing data products.



## The Evolution of Data Pipelines

The landscape of data collection and analysis has changed significantly in recent years. With the advent of modern systems, people can now track activity and apply the principles of ML in real-time rather than storing data locally through log files. There are basically four approaches to data pipelines. They are:
1. Flat File Era: In this, the data is saved locally in game servers
1. Database Era: In this, the data is staged in flat files, after which it is loaded into a database. 
1. Data Lake Era: In this the data is kept in Hadoop/s3 after which which it is loaded into a database.
1. Serverless Era : Here, for storage & querying, managed services are utilized. 

All the steps in this evolution support the collection of big data sets; however, it might introduce certain operational complexity.  The main challenge for a startup is to scale data collection without scaling operational resources. 

## Choosing Your Data Pipeline

Let us now look briefly into the various steps to choose a data pipeline for your business

Step 1: First decide the data warehouse that you are going to employ. Some examples of popular data warehouses are Amazon Redshift, Google Bigquery and Snowflake. 

Step 2: Select your tool for data visualization and business intelligence. These tools are important as they give analysts and other stakeholders access to data in a more meaningful way. Some tools trending in the market are Tableau, Looker,Chartio, Periscope and Mode. Mode is particularly data analyst friendly as it is able to run both Python and SQL. 

Step 3: The last step is to determine the data sources that your company uses. It is also important to identify the data source that contains the data you need for extracting meaning insights.  Another important step is to familiarize yourself with the different labels given to the data. Let’s have a brief look into it now.

 

## Types of Data In Your Data Pipeline

The data in the pipeline is known by various terms according to the number of changes it has undergone. Normally it is known with the labels mentioned below:
- Raw Data: This refers to the data that is kept in the format of message encoding that is utilized for sending tracking events like JSON. This kind of data does not have any schema used. It is a usual practice to send each and every tracking event as raw data mainly due to two factors: schema can be applied anytime in the pipeline and the fact that every event can be sent to a lone endpoint.
- Processed Data: This refers to the raw data that is decoded into formats. It is specific to each event and has schema applied. In a data pipeline, processed data is stored in different destinations.
- Cooked Data: Cooked data is the term given to processed data that has been condensed or clustered together. For example, processed data can include session start & end events. It can also be utilized as an input to cooked data that provides a daily summary of activities for the user including the session numbers and the time taken on site for a webpage.

## Finally… Know A Good Data Pipeline

A good data pipeline should show the following attributes:
- Minimum Event Latency: It should be possible for data scientists to enquire about the latest event data in the pipeline within seconds after the data is sent to the data collection endpoint . This would benefit while developing data products that needs to be updated in real-time. It is also helpful for testing purposes.
- Scalable: A good data pipeline should possess the ability to scale to an end- number of data points. This data can be stored as well as made available for enquiry by any high performing system.
- Interactive Querying: A data pipeline is deemed as high functioning if it can deliver to both long- running batch enquiries as well as smaller interactive enquiries. This would enable data experts to examine the tables and also comprehend schema rapidly during data sampling.
- Versioning: The user should be allowed to alter their event definition and data pipeline without causing any data loss or bringing it down. Therefore an ideal data pipeline pipeline should support varying event definitions, during an instance of change in event schema.
- Monitoring: The data pipeline must produce alerts through tools such as PagerDuty if an event or tracking data for for a specific region is not being received.
- Testing: An ideal data pipeline should yield itself to test events. These test events should measure components in the data pipeline. 

Do you fully understand the implication of Big Data, Data Science & Analytics for your business? Understand these concepts better and use it to drive your growth by getting certified. It’s never too early to enter the world of Data. 

 
