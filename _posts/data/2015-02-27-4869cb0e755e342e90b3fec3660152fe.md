---
layout:     post
title:      Comparing supervised learning algorithms
subtitle:   转载自：https://www.dataschool.io/comparing-supervised-learning-algorithms/
date:       2015-02-27
author:     Kevin Markham
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - learning
    - learned
    - table
    - evaluations
    - kaggle
    - science
    - mental
    - teaching model evaluation
    - algorithms
    - task
    - compare
    - comparing
    - force approach
    - dimensions
---

In the [data science course](https://github.com/justmarkham/DAT3) that I instruct, we cover most of the data science pipeline but focus especially on **machine learning**. Besides teaching model evaluation procedures and metrics, we obviously teach the algorithms themselves, primarily for supervised learning.

Near the end of this 11-week course, we spend a few hours reviewing the material that has been covered throughout the course, with the hope that students will start to construct mental connections between all of the different things they have learned. One of the skills that I want students to be able to take away from this course is the ability to **intelligently choose between supervised learning algorithms** when working a machine learning problem. Although there is some value in the "brute force" approach (try everything and see what works best), there is a lot more value in being able to **understand the trade-offs you're making** when choosing one algorithm over another.

I decided to create a game for the students, in which I gave them a blank table listing the supervised learning algorithms we covered and asked them to **compare the algorithms across a dozen different dimensions**. I couldn't find a table like this on the Internet, so I decided to construct one myself! Here's what I came up with:

I wanted to share this table for two reasons: First, I thought it might be useful to others as a **teaching or learning tool**. (You're welcome to [open it in Google Sheets](https://docs.google.com/spreadsheets/d/16i47Wmjpj8k-mFRk-NnXXU5tmSQz8h37YxluDV8Zy9U/edit?usp=sharing) and make a copy.) Second, **I want to make it better**, and one way to do that is to ask people more knowledgeable than me to tell me what I got wrong! :)

This table is a product of my own experience and research, but I'm not an expert in any one of these algorithms. **If you have a suggestion for how this table can be improved, I'd love to hear it in the comments!**

- Are any of my evaluations misleading or incorrect? (Of course, some of these dimensions are inherently subjective.)

- Are there any other "important" dimensions for comparison that should be added to this table?

- Are there any other algorithms that you would like me to add to this table? (Currently, it only includes algorithms that were taught in my course.)


I realize that the characteristics and relative performance of each algorithm can vary based upon the particulars of the data (and how well it is tuned), and thus some may argue that attempting to construct an "objective" comparison is an ill-advised task. However, I would argue that there is still value in providing this table as a **set of general guidelines** and as a **starting point for comparing algorithms** for your own supervised learning task.

Happy (machine) learning!

P.S. There are other discussions about this post on [Kaggle](https://www.kaggle.com/forums/f/15/kaggle-forum/t/12630/table-comparing-supervised-learning-algorithms/65094) and [DataTau](http://www.datatau.com/item?id=6354). P.P.S. I teach an online course about [Machine Learning with Text in Python](http://www.dataschool.io/learn).
