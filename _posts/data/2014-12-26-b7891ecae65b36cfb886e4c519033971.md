---
layout:     post
title:      Building Language Detector via Scikit-Learn
subtitle:   转载自：http://bugra.github.io/work/notes/2014-12-26/language-detector-via-scikit-learn/
date:       2014-12-26
author:     Bugra Akyildiz
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - char analyzer
    - features
    - use bag
    - create sufficient
---

I am using char analyzer which extract features on the "character" level rather than word level. This is important for many reasons, generally languages have various prefixes, suffixes and other small word parts that are specific to them. For suffixes, "-able", "-ness", "-s/tion", "-ist" are very common. For prefixes, "a-", "co-", "counter-", "ex-", "dis-", "mal-", "pre-", "under-", "up-", "re-" and "pro-". If I had used a word based tokenization and then use bag of words of tfidf, I would not be able to get these small language specific parts which are quite important for language detection.

If I step back for a second; it would be overly optimistic to expect the training set would cover all of the words and their various forms in a langauge detection problem. More ofthen than not, the things that features depend on not the words themselves but the parts that are specific to the languages. Those parts would create sufficient enough discriminative features and would make the classifier much better. Character ngrams to pick up those parts would be perfect.

Of course, you could try bag of words in the dataset and fail miserably. It is free to try, give it a shot!
