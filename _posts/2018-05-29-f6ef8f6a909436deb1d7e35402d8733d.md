---
layout:     post
title:      Some updates
subtitle:   转载自：https://blogs.princeton.edu/imabandit/2018/05/29/some-updates/
date:       2018-05-29
author:     Sebastien Bubeck
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - adversarial
    - mathematical
    - program
    - learning journal
    - videos
    - computationally
    - registration
    - deep
    - youtube
    - quality
    - selective
---

The blog has been eerily silent for most of 2018, here is why:

- The main culprit is definitely the COLT 2018 chairing. This year we received a surprise 50% increase in number of submissions. This is great news for the community, and it led to [a truly fantastic program](https://easychair.org/smart-program/COLT2018). We are looking forward to see many of you in Stockholm! Note that the early registration deadline is tomorrow (May 30th).

- [The first volume](https://www.ems-ph.org/journals/show_issue.php?issn=2520-2316&vol=1&iss=1) of our new Mathematical Statistics and Learning journal just got published. If you care about being evaluated by top experts, and/or get the visibility of being in a highly selective mathematical journal, you should consider submitting your highest quality work there!

- The first set of video lectures on [the youtube channel](https://www.youtube.com/c/SebastienBubeck) is now complete. I realize that the quality is not perfect and that it is hard to read the blackboard. I will try to improve this for the future videos (which should be posted some time during the summer).

- Finally I am quite excited to have [a first preprint](https://arxiv.org/abs/1805.10204) (joint work with [Eric Price](http://www.cs.utexas.edu/~ecprice) and [Ilya Razenshteyn](https://www.ilyaraz.org/)) at least loosely related to [deep learning](https://blogs.princeton.edu/imabandit/2015/03/20/deep-stuff-about-deep-learning). It is titled “Adversarial examples from computational constraints”, which sums up the paper pretty well. In a nutshell, we prove that avoiding adversarial examples is computationally hard, even in situations where there exists a very robust classifier. We are looking forward to get the deep learning community’s feedback on this work!

