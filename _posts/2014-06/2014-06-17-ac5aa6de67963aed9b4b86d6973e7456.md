---
layout:     post
title:      Hierarchical Pattern Recognition
subtitle:   转载自：http://www.kennybastani.com/2014/06/hierchical-pattern-recognition.html
date:       2014-06-17
author:     noreply@blogger.com (Kenny Bastani)
header-img: img/background3.jpg
catalog: true
tags:
    - learning
    - articles
    - patterns
    - graphs
    - selected based
---


















![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)
















### 
Hierarchical Pattern Recognition






How this worked is if a phrase was mentioned repeatedly in a collection of about 50 sentences, then I could extract that phrase as a node and link it back to the pieces of content it belonged to. Every now and then you'll get a reference to another article's name, which can then be verified against Wikipedia's site index, which would provide more sentences to find repeated phrases within.

I struggled with persistency because I knew how ugly my problem was for a relational database. I created some [entity-relationship models](http://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model), and implemented them using [Entity Framework](http://msdn.microsoft.com/en-us/data/ef.aspx) over [Microsoft SQL Server](http://en.wikipedia.org/wiki/Microsoft_sql_server). It worked, kind of. I waited patiently to happen upon a better solution. Thankfully I did, and using a [graph database](http://www.neo4j.com/download) I was able to take my cool little algorithms and solve my persistency problem at scale.








The image to the left shows article names surrounded by regular phrases that are shared by the content of many different articles on Wikipedia. The algorithm starts with a seed article, which in this case I chose the article "Dark Matter" as the seed.

As phrases are discovered within the content of the article titled "Dark Matter", they are compared to the Wikipedia site index. If a positive match is found from a phrase in one article to another's name, that article's content is then parsed using the same process.

After around 25 articles were parsed, linked, and stored as a graph, a fairly interesting image of the data emerges. Using PageRank in combination with a force directed layout, a wonderful thing appeared, a map of knowledge.

Naturally, I got pretty excited. I was reading a lot of [James Gleick's](http://en.wikipedia.org/wiki/James_Gleick) books on the history of [Chaos Theory](http://en.wikipedia.org/wiki/Chaos_theory) and [Information Theory](http://en.wikipedia.org/wiki/Information_theory). I started anew with a different article name, this time "[Artificial Intelligence](http://en.wikipedia.org/wiki/Artificial_intelligence)". I was feeling lucky. I started up the process, and let it be for about a day. I came back the next morning, imported the data to Gephi, did the PageRank and layout. The image below resulted.


The pattern started to emerge. Countries were situated around countries, mathematical topics next to mathematical topics, science fiction bordered topics about artificial intelligence. It was a genuine kind of discovery, the kind of thing that makes you tweet James Gleick with a presumptuous explanation that the universe is a network of information.




I was far off point, because what I had stumbled upon didn't require any inflation or grand explanation. The image is compelling because it looks a lot like a geographical map of human knowledge. Much like Google maps, you could zoom into this graph and observe how things are connected, you could find causality at the finest granularity.

Since then I have been on a pursuit to learn more about the constraints of theoretical computer science. Mainly, how the heuristics I developed could be improved upon and made scalable for other kinds of unsupervised machine learning. Late last year I joined up with the [company](http://www.neotechnology.com/) who created the database technology that allowed me to move past my graph persistency issues. This move allowed me to make graphs my life and ride the wave of an entirely new technology [as it disrupts and bites off a piece of its own market](http://db-engines.com/en/blog_post/26).

A part of this journey has been honing my skills as a Java developer and getting into the Neo4j internals that provide the extreme performance needed to do high-volume pattern recognition at scale.

Let's assume that each node is not just data but a code block. Each block has a very specific purpose, to take an input and produce an output. Each node could therefore be thought of as a [finite-state machine](http://en.wikipedia.org/wiki/Finite-state_machine), which performs a bit of computation, which causes its state to change according to the result of the computation. Consider the graph diagram below.




The new leaf nodes inherit the template of the parent node, but replace each ∆ with the bit selected based on the statistical probability measured over all previous matches for the parent. New ∆ wildcards encapsulate the new templates that are produced. This natural selection of patterns produces an algorithmic data structure from input data. Labels can be connected to nodes from labeled training data, like faces in photos or images of text.

This is largely inspired by Ray Kurzweil's [PRTM](http://en.wikipedia.org/wiki/How_to_Create_a_Mind#Pattern_Recognition_Theory_of_Mind) and Jeff Hawkin's [Hierarchical Temporal Memory](http://en.wikipedia.org/wiki/Hierarchical_Temporal_Memory). Both approaches teeter on the cusp of some kind of deep learning.

Quote from the Wikipedia article about [Deep Learning](http://deep_learning_in_the_human_brain/):
...These models share the interesting property that various proposed learning dynamics in the brain (e.g., a wave of neurotrophic growth factor) conspire to support the self-organization of just the sort of inter-related neural networks utilized in the later, purely computational deep learning models, and which appear to be analogous to one way of understanding the neocortex of the brain as a hierarchy of filters where each layer captures some of the information in the operating environment, and then passes the remainder, as well as modified base signal, to other layers further up the hierarchy.
That's all for now. Stay tuned for updates. 









