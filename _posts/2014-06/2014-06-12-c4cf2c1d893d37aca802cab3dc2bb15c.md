---
layout:     post
title:      Frequentism and Bayesianism III： Confidence, Credibility, and why Frequentism and Science do not Mix
subtitle:   转载自：http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/
date:       2014-06-12
author:     Jake VanderPlas
header-img: img/background3.jpg
catalog: true
tags:
    - frequentism
    - estimators
    - bayesianism
    - observed
    - confidence
---

This might be a harsh conclusion for some to swallow, but I want to emphasize that it is not simply a matter of opinion or idealogy; it's an undeniable fact based on the very philosophical stance underlying frequentism and the very definition of the confidence interval. If what you're interested in are conclusions drawn from the particular data you observed, frequentism's standard answers (i.e. the confidence interval and the closely-related $p$-values) are entirely useless.

Unfortunately, most people using frequentist principles in practice don't seem to realize this. I'd point out specific examples from the astronomical literature, but I'm not in the business of embarassing people directly (and they're easy enough to find now that you know what to look for). Many scientists operate as if the confidence interval is a Bayesian credible region, **but it demonstrably is not.** This oversight can perhaps be forgiven for the statistical layperson, as even trained statisticians will often mistake the interpretation of the confidence interval.

I think the reason this mistake is so common is that in many simple cases (as I showed in the first example above) the confidence interval and the credible region happen to coincide. Frequentism, in this case, correctly answers the question you ask, **but only because of the happy accident that Bayesianism gives the same result for that problem.**

Now, I should point out that I am certainly not the first person to state things this way, or even this strongly. The Physicist [E.T. Jaynes](http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes) was known as an ardent defender of Bayesianism in science; one of my primary inspirations for this post was his 1976 paper, *Confidence Intervals vs. Bayesian Intervals* ([pdf](http://bayes.wustl.edu/etj/articles/confidence.pdf)). More recently, statistician and blogger [W.M. Briggs](http://wmbriggs.com/) posted a diatribe on arXiv called [*It's Time To Stop Teaching Frequentism to Non-Statisticians*](http://arxiv.org/abs/1201.2590) which brings up this same point. It's in the same vein of argument that [Savage](http://en.wikipedia.org/wiki/Leonard_Jimmie_Savage), [Cornfield](http://en.wikipedia.org/wiki/Jerome_Cornfield), and other outspoken 20th-century Bayesian practitioners made throughout their writings, talks, and correspondance.

So should you ever use confidence intervals at all? Perhaps in situations (such as analyzing gambling odds) where multiple data realizations are the reality, frequentism makes sense. But in most scientific applications where you're concerned with what one particular observed set of data is telling you, **frequentism simply answers the wrong question**.

*Edit, November 2014: to appease several commentors, I'll add a caveat here. The unbiased estimator $\bar{x}$ that we used above is just one of many possible estimators, and it can be argued that such estimators [are not always the best choice](http://arxiv.org/abs/math/0206006). Had we used, say, the Maximum Likelihood estimator or a sufficient estimator like $\min(x)$, our initial misinterpretation of the confidence interval would not have been as obviously wrong, and may even have fooled us into thinking we were right. But this does not change our central argument, which involves the question frequentism asks. Regardless of the estimator, **if we try to use frequentism to ask about parameter values given observed data, we are making a mistake**. For some choices of estimator this mistaken interpretation may not be as manifestly apparent, but it is mistaken nonetheless.*
