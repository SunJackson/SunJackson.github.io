---
layout:     post
catalog: true
title:      Simulating the bias-variance tradeoff in R
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/YtIMsDL01f0/
date:      2019-06-26
author:      Robin Kraft
tags:
    - n_sim
    - data
    - figure
    - n_sample
    - simulating
---

In my last blog post, I have elaborated on the Bagging algorithm and showed its prediction performance via simulation. Here, I want to go into the details on how to simulate the bias and variance of a nonparametric regression fitting method using `R`. These kinds of questions arise here at STATWORX when developing, for example, new machine learning algorithms or testing established ones which shall generalize well to new unseen data.

## Decomposing the mean squared error

We consider the basic regression setup where we observe a real-valued sample ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-e67ee8612b8290448500f6f9eeb2d7a0_l3.png?resize=97%2C18&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-e67ee8612b8290448500f6f9eeb2d7a0_l3.png?resize=97%2C18&ssl=1)
 and our aim is to predict an outcome ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-5544a41a8ef848c514ca7413bbf0c961_l3.png?resize=89%2C16&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-5544a41a8ef848c514ca7413bbf0c961_l3.png?resize=89%2C16&ssl=1)
 based on some predictor variables („covariates“) ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-c8700e0258243116de0d4f288e2e3b44_l3.png?resize=15%2C11&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-c8700e0258243116de0d4f288e2e3b44_l3.png?resize=15%2C11&ssl=1)
 via a regression fitting method. Usually, we can measure our target only with noise ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f0aa232b2f53bc9466aa901bd42d2ed6_l3.png?resize=12%2C11&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f0aa232b2f53bc9466aa901bd42d2ed6_l3.png?resize=12%2C11&ssl=1)
,

![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-180b1af34bf9bbeb3faefe1f2c340438_l3.png?resize=231%2C18&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-180b1af34bf9bbeb3faefe1f2c340438_l3.png?resize=231%2C18&ssl=1)


To measure our prediction accuracy, we will use the mean squared error (MSE) which can be decomposed as follows:

![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-b519fdbfe88d3338f5830d843bcd5d28_l3.png?resize=450%2C24&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-b519fdbfe88d3338f5830d843bcd5d28_l3.png?resize=450%2C24&ssl=1)


![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-601b2737015729b3dac6e1e70f1ddcec_l3.png?resize=210%2C17&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-601b2737015729b3dac6e1e70f1ddcec_l3.png?resize=210%2C17&ssl=1)


![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-05208067dd0e54e5279f93d8863837c5_l3.png?resize=39%2C22&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-05208067dd0e54e5279f93d8863837c5_l3.png?resize=39%2C22&ssl=1)
 is our prediction obtained by the regression method at hand.

From the above expression, we observe that the MSE consists of two parts:

![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-bb7ac666ee5c87caa2290464d5c91d85_l3.png?resize=44%2C15&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-bb7ac666ee5c87caa2290464d5c91d85_l3.png?resize=44%2C15&ssl=1)
: measures the (squared) difference between the true underlying process and the mean of our predictions, i.e. ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-85b1393377bae37e5c7d56b3cb2094e8_l3.png?resize=149%2C23&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-85b1393377bae37e5c7d56b3cb2094e8_l3.png?resize=149%2C23&ssl=1)

![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-958f54392550e438ff49da8ceb2fafbd_l3.png?resize=74%2C12&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-958f54392550e438ff49da8ceb2fafbd_l3.png?resize=74%2C12&ssl=1)
: measures the variation of our predictions around its mean, i.e. ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-877078542b3c7285d9269984bb94b09d_l3.png?resize=90%2C22&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-877078542b3c7285d9269984bb94b09d_l3.png?resize=90%2C22&ssl=1)


In general, it will not be possible to minimize both expressions as they are competing with each other. This is what is called the bias-variance tradeoff. More complex models (e.g. higher order polynomials in polynomial regression) will result in low bias while yielding high variance as we fit characteristic features of the data that are not necessary to predict the true outcome (and vice versa, cf. Figure 1).

## Monte Carlo Setup & Simulation Code

To illustrate this, we consider a simple toy model.

![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f04b5b71291cc3584a826be2fa893337_l3.png?resize=310%2C18&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f04b5b71291cc3584a826be2fa893337_l3.png?resize=310%2C18&ssl=1)


where ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-e74f0d674ba4baa42227623eedbe4508_l3.png?resize=61%2C13&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-e74f0d674ba4baa42227623eedbe4508_l3.png?resize=61%2C13&ssl=1)
, ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-8e56576aae46a4fcbc9367b571693e60_l3.png?resize=102%2C19&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-8e56576aae46a4fcbc9367b571693e60_l3.png?resize=102%2C19&ssl=1)
 and ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-3b99fb75a90f39ba22fcf40c705f9868_l3.png?resize=97%2C18&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-3b99fb75a90f39ba22fcf40c705f9868_l3.png?resize=97%2C18&ssl=1)


As a fitting procedure, we will use a cubic smoothing spline (`smooth.spline`). For the purpose of this blog post, we only need to know that a smoothing spline divides our univariate predictor space into ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-d72f4e3699652cfc70b8880515893d7c_l3.png?resize=40%2C14&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-d72f4e3699652cfc70b8880515893d7c_l3.png?resize=40%2C14&ssl=1)
 intervals and fits each one to a cubic polynomial to approximate our target. The complexity of our smoothing spline can be controlled via the degrees of freedom (function argument `df`). If you are interested in the (impressive) mathematical details of smoothing splines, check out this script by Ryan Tibshirani.

Figure 1 shows the bias-variance tradeoff from above. For relatively low degrees of freedom we obtain a model (red line) which is too simple and does not approximate the true data generating process well (black dashed line) – Note: for ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f41d5664f21d0858c6402e34e187a9e2_l3.png?resize=57%2C17&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f41d5664f21d0858c6402e34e187a9e2_l3.png?resize=57%2C17&ssl=1)
 we obtain the least-squares fit.Here, the bias will be relatively large while the variance will remain low. On the other hand, choosing relatively high degrees of freedoms leads to a model which overfits the data (green line). In this case, we clearly observe that the model is fitting characteristic features of the data which are not relevant for approximating the true data generating process.
![](https://i2.wp.com/www.statworx.com/wp-content/uploads/spline-df-variation-1024x819.png?resize=456%2C365&is-pending-load=1#038;ssl=1)

![](https://i2.wp.com/www.statworx.com/wp-content/uploads/spline-df-variation-1024x819.png?resize=456%2C365&ssl=1)


To make this tradeoff more rigorous, we explicitly plot the bias and variance. For this, we conduct a Monte Carlo simulation. As a side note, to run the code snippets below, you only need the `stats` module which is contained in the standard `R` module scope.

For validation purposes, we use a training and test dataset (`*_test` and `*_train`, respectively). On the training set, we construct the algorithm (obtain coefficient estimates, etc.) while on the test set, we make our predictions.

```
# set a random seed to make the results reproducible
set.seed(123)

n_sim <- 200
n_df <- 40
n_sample <- 100

# setup containers to store the results
prediction_matrix <- matrix(NA, nrow = n_sim, ncol = n_sample)
mse_temp <- matrix(NA, nrow = n_sim, ncol = n_df)
results <- matrix(NA, nrow = 3, ncol = n_df)

# Train data -----
x_train <- runif(n_sample, -0.5, 0.5)
f_train <- 0.8*x_train+sin(6*x_train)

epsilon_train <- replicate(n_sim, rnorm(n_sample, 0, sqrt(2)))
y_train <- replicate(n_sim,f_train) + epsilon_train

# Test data -----
x_test <- runif(n_sample, -0.5, 0.5)
f_test <- 0.8*x_test+sin(6*x_test)
```

The bias-variance tradeoff can be modelled in `R` using two `for`-loops. The outer one will control the complexity of the smoothing splines (counter: `df_iter`). The Monte Carlo Simulation with 200 iterations (`n_sim`) to obtain the prediction matrix for the variance and bias is run in the inner loop. 

```
# outer for-loop
for (df_iter in seq(n_df)){
 # inner for-loop
 for (mc_iter in seq(n_sim)){
 cspline <- smooth.spline(x_train, y_train[, mc_iter], df=df_iter+1)
 cspline_predict <- predict(cspline, x_test)
 prediction_matrix[mc_iter, 1:n_sample] <- cspline_predict$y 
 mse_temp[mc_iter, df_iter] <- mean((cspline_predict$y - f_test)^2)
 }

 var_matrix <- apply(prediction_matrix, 2, FUN = var)
 bias_matrix <- apply(prediction_matrix, 2, FUN = mean)

 squared_bias <- (bias_matrix - f_test)^2

 results[1, df_iter] <- mean(var_matrix)
 results[2, df_iter] <- mean(squared_bias)
}

results[3,1:n_df] <- apply(mse_temp, 2, FUN = mean)
```

To model ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-cb6cc86a9c919ab798aef2582516002d_l3.png?resize=32%2C18&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-cb6cc86a9c919ab798aef2582516002d_l3.png?resize=32%2C18&ssl=1)
 and ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-9f302cffb9c30b1eaf5918257f498e5c_l3.png?resize=50%2C18&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-9f302cffb9c30b1eaf5918257f498e5c_l3.png?resize=50%2C18&ssl=1)
 from the above MSE-equation, we have to approximate those theoretical (population) terms by means of a Monte Carlo simulation (inner `for`-loop). We run ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-63cdec10a9d374929fb4d720b572a32f_l3.png?resize=56%2C13&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-63cdec10a9d374929fb4d720b572a32f_l3.png?resize=56%2C13&ssl=1)
 (`n_sim`) Monte Carlo iterations and save the predictions obtained by the `smooth.spline`-object in a ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-ed49b0e4351d1cc5cb4f8a4604d00714_l3.png?resize=59%2C18&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-ed49b0e4351d1cc5cb4f8a4604d00714_l3.png?resize=59%2C18&ssl=1)
 prediction matrix. To approximate ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-632c6742b431b982ffb59f22fd06d86c_l3.png?resize=67%2C22&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-632c6742b431b982ffb59f22fd06d86c_l3.png?resize=67%2C22&ssl=1)
 at each test sample point ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-27952bf115c2161770961258b46704de_l3.png?resize=131%2C18&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-27952bf115c2161770961258b46704de_l3.png?resize=131%2C18&ssl=1)
, by ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-2fdef2ec6351197cdcd0b45f8dd23221_l3.png?resize=106%2C24&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-2fdef2ec6351197cdcd0b45f8dd23221_l3.png?resize=106%2C24&ssl=1)
, we take the average of each column. ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-0970311315c13e4185ae418568817129_l3.png?resize=44%2C22&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-0970311315c13e4185ae418568817129_l3.png?resize=44%2C22&ssl=1)
 denotes the prediction of the algorithm obtained at some sample point ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-c8700e0258243116de0d4f288e2e3b44_l3.png?resize=15%2C11&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-c8700e0258243116de0d4f288e2e3b44_l3.png?resize=15%2C11&ssl=1)
 in iteration ![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f56d50c26583f9a035ff6b4e3c0ca5c0_l3.png?resize=8%2C13&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-f56d50c26583f9a035ff6b4e3c0ca5c0_l3.png?resize=8%2C13&ssl=1)
. Similar considerations can be made to obtain an approximation for ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-30097ecfa45da8e1957a339262df2139_l3.png?resize=85%2C22&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-30097ecfa45da8e1957a339262df2139_l3.png?resize=85%2C22&ssl=1)
.

## Bias-variance tradeoff as a function of the degrees of freedom

Figure 2 shows the simulated bias-variance tradeoff (as a function of the degrees of freedom). We clearly observe the complexity considerations of Figure 1. Here, the bias is quickly decreasing to zero while the variance exhibits linear increments with increasing degrees of freedoms. Hence, for higher degrees of freedom, the MSE is driven mostly by the variance.

Comparing this Figure with Figure 1, we note that for ![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-c335f6fa165ee78ba21c327d5ce6aaea_l3.png?resize=49%2C17&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-c335f6fa165ee78ba21c327d5ce6aaea_l3.png?resize=49%2C17&ssl=1)
, the bias contributes substantially more than the variance to the MSE. If we increase the degrees of freedom to ![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-117974d5ebf7137ecf421e667fda366f_l3.png?resize=57%2C17&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.statworx.com/wp-content/ql-cache/quicklatex.com-117974d5ebf7137ecf421e667fda366f_l3.png?resize=57%2C17&ssl=1)
 the bias tends to zero, characteristic features of the data are fitted and the MSE consists mostly of the variance.
![](https://i2.wp.com/www.statworx.com/wp-content/uploads/bv-tradeoff-1024x819.png?resize=456%2C365&is-pending-load=1#038;ssl=1)

![](https://i2.wp.com/www.statworx.com/wp-content/uploads/bv-tradeoff-1024x819.png?resize=456%2C365&ssl=1)


With small modifications, you can use this code to explore the bias-variance tradeoff of other regression fitting and also Machine Learning methods such as Boosting or Random Forest. I leave it to you to find out which hyperparameters induce the bias-variance tradeoff in these algorithms.

You can find the full code on my Github Account. If you spot any mistakes or if you are interested in discussing applied statistic and econometrics topics, feel free to contact me.

The simulation setup in this blog post follows closely the one from Buhlmann.

- Buhlmann, P. and Yu, B. (2003). Boosting with the L2-loss: Regression and classification. J. Amer. Statist.Assoc. 98, 324–339.


###### Über den Autor

![](https://secure.gravatar.com/avatar/24186ff61b3f1f2fb9f4631631c6550c?s=180&d=mm&r=g&is-pending-load=1)
![](https://secure.gravatar.com/avatar/24186ff61b3f1f2fb9f4631631c6550c?s=180&d=mm&%23038;r=g)


#### Robin Kraft

I am a working student at STATWORX and currently writing my Master thesis about componentwise boosting.

---

STATWORXis a consulting company for data science, statistics, machine learning and artificial intelligence located in Frankfurt, Zurich and Vienna. Sign up for our NEWSLETTER and receive reads and treats from the world of data science and AI. If you have questions or suggestions, please write us an e-mail addressed to blog(at)statworx.com.  



 



**

 Sign Up Now! 


Der Beitrag Simulating the bias-variance tradeoff in R erschien zuerst auf STATWORX.


*Related*







---
