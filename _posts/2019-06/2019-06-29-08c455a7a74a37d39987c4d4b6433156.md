---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/06/29/distilled-news-1114/
date:      2019-06-29
author:      Michael Laux
tags:
    - data
    - learning
    - computing
    - compute
    - businesses
---

**Math Behind Reinforcement Learning, the Easy Way**

This article is not about teaching Reinforcement Learning (RL) but about explaining the math behind it. So it assumes that you already know what is RL but have some difficulty grasping the mathematical equations. If you don’t know RL, it is better that you read about it before returning to this article. We will go step by step into how and why the above equation came into being.

**PsyToolkit’s experiment library**

One of the main features of PsyToolkit is that you can create and run cognitive psychological experiments in your browser. Here are a number of ready-to-use demos using this technology.

**The Next Generation of Deep Learning Hardware: Analog Computing**

Initially developed for gaming and 3-D rendering, graphics processing units (GPUs) were recognized to be a good fit to accelerate deep learning training. Its simple mathematical structure can easily be parallelized and can therefore take advantage of GPUs in a natural way. Further progress in compute efficiency for deep learning training can be made by exploiting the more random and approximate nature of deep learning work flows. In the digital space that means to trade off numerical precision for accuracy at the benefit of compute efficiency. It also opens the possibility to revisit analog computing, which is intrinsically noisy, to execute the matrix operations for deep learning in constant time on arrays of nonvolatile memories. To take full advantage of this in-memory compute paradigm, current nonvolatile memory materials are of limited use. A detailed analysis and design guidelines how these materials need to be reengineered for optimal performance in the deep learning space shows a strong deviation from the materials used in memory applications.

**R Plumber API in a Docker container? Of course, but security matters…**

This guide is for you, if:• You are a data scientist and want to quickly publish a training or scoring function to your peers with a plumber API.• Your IT demands, that connections should be encrypted and password protected.• You are familiar with docker containers.• You are lazy and do not want to dive in the technical details of dealing with nginx or another reverse proxyThe open-source deployment framework AHUB has undergone a major rework, making it now even easier to setup a secured R based API deployment in seconds. The only pre-requisite is, that you have built a docker container image with your plumber app listening on port 8000.

**Causation doesn’t imply Correlation either**

You may have misread the title as the old correlation does not imply causation mantra, but the opposite is also true! If you don’t believe me, read on…

**What is a Confounding Variable?**

Confounding variable is one of those statistical term that confuses a lot of people. Not because it represents a confusing concept, but because of how it’s used. (Well, it’s a bit of a confusing concept, but that’s not the worst part). First, it has slightly different meanings to different types of researchers. The definition is essentially the same, but the research context can have specific implications for how that definition plays out. If the person you’re talking to has a different understanding of what it means, you’re going to have a confusing conversation. Let’s take a look at some examples to unpack this.

**Monte Carlo Simulation in R with focus on Option Pricing**

In this blog, I will cover the basics of Monte Carlo Simulation, Random Number Distributions and the algorithms to generate them. Finally I will also cover an application of Monte Carlo Simulation in the field of Option Pricing. The whole blog focuses on writing the codes in R, so that you can also implement your own applications of Monte Carlo Simulation in R.

**AI to P: Navigating the mystical forest of production**

I often take solace in the idea that some wicked AI, hellbent on destruction and wreaking havoc on the world as we know it will most likely fail at the crucial point of moving from a notebook to production. Some of the coolest data-driven projects stumble at the point where their concept meets the intended implementation, but with cloud computing, expert domain spaces and infinite resources why is this the case? For those readers who were hoping that this would be some magical tutorial on bringing AI to production using ‘xyz’ technology, unfortunately, I must disappoint you. I chose instead to walk you through some of the common problems encountered when it comes to bringing algorithms into production, and possibly impart some wisdom.

**Topological Data Analysis – Unpacking the Buzzword**

In this article, I’ll specifically break down what Topological Data Analysis is and how to think about it. I aim to answer the following questions:• What is Topological Data Analysis?• How is Topology related to Topological Data Analysis?• How is Data Analysis related to Topological Data Analysis?

**Monte Carlo Simulations: The Intersection of Probabilistic and Deterministic**

Hi welcome to my blog, I hope you find this helpful as an introduction to Monte Carlo Simulations. What this is, a minimal mathematical approach to Monte Carlo Simulations with simple graphics to illustrate my various points. What this is not, a deep dive into Monte Carlo simulations for very specific and well defined industry problems. This data and premise of this Monte Carlo Simulation comes from Coursera’s Advance Business Analytics course taught by the faculty at the University of Colorado. Originally, the calculation was performed on Analytic Solver, but I decided to bring the process into an environment that I’m (and hopefully you) more comfortable in, Python. First, I used the words probabilistic and deterministic, I should define them in the context of this article.

**What is Robustness in Statistics? A Brief Intro to Robust Estimators**

Robust statistics are the statistics that are resistant to outliers. In other words, if there are low or high number of outliers in your samples, non-robust estimators provide you poor estimates of the population parameters. For example, if your experimental data includes the repeated measurements as 10, 10.3, 10.2, 10.1 and 100 and your last data is obviously wrong due to a systematic error. The mean will give you the location estimate as 28.12 since it is susceptible to even one outlier, while the median is not affected by only one outlier and will give you the result as 10.2 since it is a robust estimation methods.

**6 Steps: Getting Ready for CCPA**

Within the past few years, there has been growing concern about the need to manage and protect an individual’s personal data. While dozens of countries have enacted some sort of law intended to protect individuals’ personal data, as of yet there are no United States federal laws on the books. That being said, states such as California are enacting legislation for data protection, and the California Consumer Protection Act of 2018 (CCPA), which goes into effect on Jan. 1, 2020, is one of the first, but certainly not the last. The CCPA specifies steep penalties for unauthorized exposure of personal information. Intentional violations of the CCPA are fined at $7,500 per capita, while those lacking intent are subject to a maximum of $2,500 per violation. However, realize that violations are likely to be associated with selling large numbers of protected records, or mass exposure due to security breaches.

**How to make a success story of your data science team.**

Data science resounds throughout every industry and has reached the mainstream media. I no longer have to explain what I do for a living as long as I call it AI – we are peak data science hype! As a consequence, more and more companies are looking towards data science with big expectations, ready to invest into a team of their own. Unfortunately, the realities of data science in the enterprise are far from a success story. NewVantage published a survey in January 2019 which found that 77% of businesses report challenges with business adaptation. This translates into ¾ of all data projects collecting dust rather than providing a return on the investment. Gartner has always been very critical of the data science success and they haven’t gotten more cheerful as of late: According to Gartner January 2019, even analytics insights will not deliver business outcomes through 2022, what’s the hope then for data science? It’s apparent that for some reasons making data science a success is really hard!

**Simple Web Scraping with Python’s Selenium**

Summer to me means two things, spending time at the beach and playing football. With the weather improving and the 2019 FIFA Women’s World Cup underway, I now need to buy a beach football. This seems like a good opportunity to flex our web scraping muscles and write a simple web scraper that will gather data on beach balls from Walmart. This piece is intended to serve as an introduction to Web Scraping using the Selenium module. Furthermore, to add an extra challenge, lets scrape data, not just from one web page, but many! For easy data interpretation, I will conclude by writing the output to a CSV file.

**On Average, You’re Using the Wrong Average: Geometric & Harmonic Means in Data Analysis**

You have a bunch of numbers. You want to summarize them with fewer numbers, preferably a single number. So you add up all the numbers then divide the sum by the total number of numbers. Boom: behold the ‘average’, right?

**3 Stages of an AI Project in Plain English**

This article talks about the technical stages of an AI project. It shows an actionable process to setup teams for success, including who to hire and when. It assumes no prior knowledge and aims to be written in as plain of English as possible.

### Like this:

Like Loading...
