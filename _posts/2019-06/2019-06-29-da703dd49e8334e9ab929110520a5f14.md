---
layout:     post
catalog: true
title:      Text Parsing and Text Analysis of a Periodic Report (with R)
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/Obaju8UocsU/
date:      2019-06-29
author:      r on Tony ElHabr
tags:
    - figures
    - reports
    - data
    - sections
    - abcde
---





Some Context
Those of you non-academia folk who work in industry (like me)are probably conscious of any/all periodic reports that an independent entitypublishes for your companyâ€™s industry. For example, in the insurance industry inthe United States, theFederal Insurance Office of the U.S. Department of the Treasurypublishes several reports on an annualbasisdiscussing the industry at large, like this past yearâ€™s***Annual Report on the Insurance Industry***.(Admittedly, these kinds of reports are not always the most interesting thingsto read ğŸ˜›, but they are (usually) veryinformative.)

The same goes for the electricity grid and markets operated by theElectric Reliability Council of Texas (ERCOT) in Texas (whichjust so happens to be particularly relevant to me).Potomac Economics publishes an annual reportâ€œproviding an independent assessment of the competitive performance and operational efficiency of the marketâ€� operated byERCOT. This report tends to be pretty longâ€”over 160 pagesin every reports since 2016. A relatively large amount ofthe content is replication of language, figures, and tables from the prioryearsâ€™ report, substituting and updating numbers from the past year. Asan annual reader of said report, I wish it were easier to â€œparse outâ€� the mostimportant things from a given yearâ€™s report. 

With that in mind, I thought I would take a shot at using text analysis tocompare Potomac Economicsâ€™ annual reports on ERCOT for three yearsâ€”2016, 2017,and 2018â€”to, among other things,identify the most â€œuniqueâ€� things in each report. This kind ofanalysis can enhance oneâ€™s understanding ofcurrent grid and market trends, as well as make one aware ofthings to look out for in the future.

## A Brief Outline of the â€œProcessâ€�

While I initially planned on doing some kind of code tutorial with this project(primarily to demonstrate techniques for parsing and cleaning PDF text with `R`),I found that the code became more complex than I would have liked .(Letâ€™s just say that regular expressionswere my best friend ğŸ˜¢.)Instead, I decided to focus this post the visuals I created,providing some brief commentary where needed.

Nonetheless, I think a high-level description of my â€œprocessâ€� may beuseful to the reader, as I think it is probably generalizable to any project ofthis likeness (i.e. text analysis of similarly structured documents).(For those who areinterested in the code, it can all be found inthis GitHub repo.)My approach (using `R`, of course ğŸ˜›) can be simplified to the following set of steps.


Download the reports to a local directory and create a data frame with the year and file location of the report.


Import the raw text for each report to create a singular data frame with nested data frames storing the text for each yearâ€™s report.


â€œUnnestâ€� the singular data frame to create a â€œlongâ€� data frame where each row represents a page in the reports.


Split this long data frame (with pages as observations) into two data frames, one for the table of contents (TOC) for each report and another for the body of each report.


Clean the two data frames (for TOC and body content) separately.


Create â€œchildâ€� data frames as needed for analysis, and make plots for key insights from this analysis.


From this breakdown of the steps, it is evident that the effortthat goes into the data pre-processing (i.e. steps 1through 5) makes up most of the work!What most people perceive to be â€œdata analysisâ€� is all in step 6.In this case, the saying aboutâ€œ80% of data science is really data cleaningâ€�could not be any truer.

## The Analysis

So, what exactly does this report look like? Well, you could always open up oneof the reports yourself and peruse the 160+ pages , but maybe thefollowing screenshot of the 2018 report can provide some visual context. (Iffor nothing else, I thought this would be fun to look at.) I borrowed some codethat data visualization phenom Nathan Yaudemonstrated in a short write-upat his awesome website FlowingData.

![](https://i0.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report.png?w=456&ssl=1)


You can really get an idea of the vast amount of charts and tables that make upeach one of these reports just from this one figure!

### Table of Contents (TOC)

Next, letâ€™s begin to shape out this initial holisticperspective of the reports withsome analysis of the table of contents (TOC). This may seem trivial, but Iassure you that itâ€™s not! With a large document like this, we really ought tohave a solid understanding of the content that the document discusses.

Each documentâ€™s TOC is broken down into a couple of sections, including:


An outline for the executive summary (on the first page of the TOC);


An outline of the first- and second-level sections of thebody of the document (also starting on the first page of the TOC);


A list of figures (starting on the second page of the TOC);


A list of tables (on the fourth page).


Screenshots of parts of the TOC of the 2018 report are shown below.

![](https://i2.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report_3-cropped.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report_3-cropped.png?w=456&ssl=1)


![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report_4-cropped.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report_4-cropped.png?w=456&ssl=1)


![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report_6-cropped.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/2018-State-of-the-Market-Report_6-cropped.png?w=456&ssl=1)


A first question we might ask isâ€œHow do (counts of) the lists of figures and tables correspond to the sections of thetext?â€� (For the sake of readability, Iâ€™ve truncated some of the section labels,e.g. â€œReview of Real-Time Market Outcomesâ€� was truncated to just â€œRTMâ€�,which is an abbreviation for Real-Time Market.)

![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_toc_n_1yr_tree.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_toc_n_1yr_tree.png?w=456&ssl=1)


So we see that the Real-Time Market (RTM) and Day-Ahead Market (DAM)sections seem to make a larger-than-equal share of the reportsin terms of simple countsof figures and tables. We might hypothesize that these things are arguably theâ€œeasiestâ€� things to track in a graphical or quantitative manner among themany aspects of electricity markets and grid operations.Conversely, we see that the Analysis section (truncated from â€œAnalysis ofCompetitive Performanceâ€�) leverages plots and tables the least. We might saythat this supports our hypothesis that, in a few words, may be stated as**â€œeasy-to-quantify topics have more figures and tablesâ€�**.The section labels â€œAnalysisâ€�(shortend from â€œAnalysis of Competitive Performanceâ€� for visualization purposes)suggests that its content is â€œdeeperâ€� in nature, and that its may notbe quite as easy to illustrate via figures and tables.

Our initial hypothesisâ€”that easy-to-quantify topicshave more figures and tablesâ€”seems reasonable enough, but is it just adirect consequence of thesections having more pages? We can plot the number of pages per sectionagainst the count of sections to help us answer this question.

![](https://i0.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_content_section_n.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_content_section_n.png?w=456&ssl=1)


So it seems that those sections having more pages do **NOT** necessarilyhave more figures and tables. So our hypothesis still seems reasonable.

You might have noticed that the first plot(the â€œtreemapâ€�) only showed data forthe 2018 report. I didnâ€™t deem it necessary/helpful to make the same plot foreach of the three reports from 2016 through 2018 because, as it turns out,the TOC of the three reports are nearly identical in composition! (Really, thisis probably unsurprising ğŸ˜›.) That is, they haveidenticalâ€”or near identicalâ€”names and indexes for sections, figures, andtables. From an analysis point of view, this is goodâ€“the structure of the three reports facilitates direct comparison.

But note that I say that the TOCs as **nearly** identical, not **exactly**identical. What exactly are the differences between the three? More specifically,we might be curious to know which figures and tables were only in one of the threereports.

![](https://i0.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_toc_content_n1.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_toc_content_n1.png?w=456&ssl=1)


Aha! We see that the 2016 and 2018 reports had more than a handful offigures and tables that were unique to those reports. The table below lists exactlywhich figures and tables those are.

|Year|Section|Type|Label
|------
|2016|RTM|figure|implied heat rate duration curve â€“ top 2 percent of hours|
|2016|RTM|figure|aggregated peak hour generation offer stack|
|2016|RTM|figure|load, reserves and prices in august|
|2016|Supply/Demand|figure|load duration curve â€“ top five percent of hours|
|2016|Supply/Demand|figure|historic coal generation and capacity factor|
|2016|Supply/Demand|figure|top and bottom ten percent of net load|
|2016|Reliability|figure|frequency of reliability unit commitments|
|2016|Reliability|figure|average on-line summer reserves|
|2016|Reliability|figure|potential for combined cycle capacity available to ruc in houston|
|2016|Analysis|figure|surplus capacity|
|2016|RTM|table|15-minute price changes as a percentage of annual average prices|
|2016|Congestion|table|irresolvable elements|
|2017|Supply/Demand|figure|load duration curve â€“ top five percent of hours with highest load|
|2017|Supply/Demand|figure|energy transacted across dc ties in august|
|2018|RTM|figure|ercot rena analysis|
|2018|RTM|figure|average real-time energy price spikes|
|2018|RTM|figure|monthly load exposure|
|2018|DAM|figure|daily collateral|
|2018|DAM|figure|average costs of procured sasm ancillary services|
|2018|DAM|figure|ercot-wide net ancillary service shortages|
|2018|DAM|figure|qse-portfolio net ancillary service shortages|
|2018|Congestion|figure|[year] crr auction revenue|
|2018|Supply/Demand|figure|load duration curve â€“ top 5% of hours with highest load|
|2018|Supply/Demand|figure|annual energy transacted across dc ties|
|2018|Reliability|figure|capacity commitment timing â€“ july and august hour ending 17|
|2018|Reliability|figure|real-time to cop comparisons for renewable capacity|
|2018|Reliability|figure|real-time to cop comparisons for thermal capacity|
|2018|Reliability|figure|standard deviations of real-time to cop capacity differences|
|2018|Resource Adequacy|figure|west zone net revenues|
|2018|Resource Adequacy|figure|net revenues by generation resource type|
|2018|Analysis|figure|derating, planned outages and forced outages|
|2018|Reliability|table|ruc settlement|

Ok, enough about the TOC. Letâ€™s see what kind of things we can learn about thetext.

### Sentences

We saw that some sections have many more figures and tables than others and thatthis does not necessarily correlate with the number of pages in the section. Isthere some kind of correlation with the number of sentences of text in eachsection?

![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_sents_section_n.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_sents_section_n.png?w=456&ssl=1)


It may (or may not) be surprising to find a lack of a relationship here. Givenour previous finding that the number of figures and tables and the number of pagesin a given section are not really related. 

Next, I think it is interesting to look at how the counts of sentencesper section has changed over time.

![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_sents_section_n_yr.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_sents_section_n_yr.png?w=456&ssl=1)


The first thing we might notice from the plot is that thenumber of sentences has increased across all sections from their totals in 2016.Ok, so maybe thatâ€™s not so interestingâ€”I think itâ€™s reasonable to assume thatan annual report like this incrementally adds on its â€œfoundationâ€�from the prior year(s).

Perhaps the most interesting that we might observe from this graphis the â€œleapsâ€� in the sentencecounts from 2017 to 2018 for the Reliability and Resource Adequacy sections. Onemight draw a connection between this and what we observed earlier when lookingat the figures and tables that were unique to a single report. There were more thana handful of observations for Resource Adequacy and Reliability (2 and 5respectively) exclusive to the 2018 report.

A final takeaway that I have from this chart is the â€œevennessâ€� ofthe counts across the sections (which wasnâ€™t quite as obvious in the previousplot depicting sentence counts). Thedifference between the maximum and the minimum number of sentences persection in a given year is always below 50. Personally, I might have expectedgreater variation. Either way, it doesnâ€™t really say anything about theâ€œgoodnessâ€� of the content; this is just somethingâ€”the count of sentences ina section of a long documentâ€”for which I donâ€™t really have a strong priorknowledge. 

Ok, so the prior plot wasnâ€™t so complex, so letâ€™s nowlook at something more complicated and which deserves some explanation.

One of the things that I really wanted to investigate when I startedthis analysis was â€œtext similarityâ€�â€”just how much of each reportis â€œboiler-plate, copy-pasteâ€� text?There are lots of ways of going about quantifying this. (Just do an Internet searchfor Natural Language Processing (NLP)and word embeddings andsee what kind of rabbit hole that takes you down.)I decided to use cosine similarity ofsentences. 

To provide a kind of â€œbaselineâ€� expectation of what these cosine similarityvalues might be, see the table below. It provides a handful ofsimple examples of the cosine similarity for singular strings of characters. 

|Case|Description|String 1|String 2|Cosine Similarity
|------
|1|Identical strings.|abcde|abcde|1.00|
|2|One different character (â€œzâ€� in String 2 instead of â€œaâ€�).|abcde|zbcde|0.80|
|3|Two different characters (â€œzâ€� and â€œyâ€� in String 2 instead of â€œaâ€� and â€œbâ€�).|abcde|zycde|0.60|
|4|All different characters.|abcde|fghij|0.00|
|5|Different ordering of characters, but identical characters.|abcde|baecd|1.00|
|6|Repeated characters (â€œaâ€� in String 2), one-character difference in string lengths.|abcde|aabcde|0.95|
|7|Repeated characters (â€œaâ€� twice in String 2), two-character difference in string lengths.|abcde|aaabcde|0.87|
|8|Same characters, one additional character (â€œzâ€� in String 2) in one string.|abcde|abcdef|0.91|
|9|Same characters, one additional character (â€œeâ€� in String 1) in one string.|abcde|abcd|0.89|

Cases 2 and 3 in the table above are probably the most illustrative of the cosinesimilarity calculation. In case 2, a one-character difference in twostrings having five characters total results in a value of 0.8.(i.e. Four of five characters matched.)In case 3, a two-character difference (given the same setup) resultsin a value of 0.6. (i.e. Three of five characters matched.)

Note that the cosine similarity calculation is identical for different typesof text tokens (i.e. sentences, paragraphs, etc.).In general, the calculated values are likely to be smaller for longer tokens.

Ok, so given the above examples and explanation,we now have some context for understanding(and appreciating) the high degree of sentencesimilarity demonstrated across the reports, as illustratedin the figure below.

![](https://i2.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_sents_section_sim.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_sents_section_sim.png?w=456&ssl=1)


If you have read the reports, youâ€™ll realize that there are lots ofâ€œrepeatedâ€� sentences across the documents.For example, take the description of the first figure in 2018:*â€œFigure 1 summarizes changes in energy prices and other market costs by showingthe all-in price of electricity, which is a measure of the total cost of servingload in ERCOT for 2016 through 2018.â€�* The â€œsameâ€� sentence in the 2017 report is*â€œFigure 1 summarizes changes in energy prices and other market costs by showingthe all-in price of electricity, which is a measure of the total cost of servingload in ERCOT for 2015 through 2017.â€�* Just by inspection, you can tellthat these two sentences would have a cosine similarity almost equal to 1.In fact, I did some extra text-processingâ€”most notably, replacingyear and months with generic labels, e.g. â€œ[year]â€� and â€œ[month]â€œâ€”thatwould make these two sentencesappear exactly identical (and, consequently, have a cosine similarity of exactly 1).This whole effort to quantify of sentence similarity was probably the most interestingthing to me out of this entire analysis! 

I think thatâ€™s enough about sentences. Letâ€™s next explore what we mightlearn from just the words.

### Words

One of the best ways to gauge â€œimportanceâ€� of words (or any kind of text token)across multiple documents isterm-frequency, inverse-document-frequency (TF-IDF). 

First, letâ€™s use TFIDF to identify which sections have the most â€œuniqueâ€� words(aggregating across the three reports).

![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_words_section_tfidf.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_words_section_tfidf.png?w=456&ssl=1)


So we see that the Analysis section appears to be the most uniquefrom this perspective. This deduction can be interpreted as anextension of the hypothesisdeveloped earlierâ€”that easy-to-quantify topics have more figures and tables. The extension of this notioncan be stated explicitly as **â€œeasy-to-quantify topicsâ€� have less unique words**.

What exactly are some of the words that were identified as the most unique?

![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_words_tfidf.png?w=456&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/tonyelhabr.rbind.io/post/text-parsing-analysis-periodic-report/viz_words_tfidf.png?w=456&ssl=1)


The â€œchatterâ€� plot above depicts the most unique wordsidentified by TFIDF (after applying common text processing techniques such asâ€œstemmingâ€� and droppingâ€œstop wordsâ€�).Several of the words shown happen to be words associated withsources of high electric transmission congestion in the Texas grid during the year for the which the report was made.This set of words could be potentially leveraged as astarting point for an ambitious reader who is curiousto learn more about the Texas electricity grid.

## The End

I had a really fun time exploring this topic. I encourage any readers todo something similar with periodic or standardized reports that arerelevant to you in some way. If they are boring to you (yet you must readthem for whatever reason), then a text analysis like that demonstrated herecan really spark some life into your consumption of the report!


*Related*







---
