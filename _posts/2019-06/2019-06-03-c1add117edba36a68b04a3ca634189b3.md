---
layout:     post
catalog: true
title:      Fine-tuning with Keras and Deep Learning
subtitle:      转载自：https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/
date:      2019-06-03
author:      Adrian Rosebrock
tags:
    - trained
    - training
    - lines
    - learning
    - learned
---

![](https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/fine-tuning-keras/fine_tuning_keras_animation.gif)


In this tutorial, you will learn how to perform fine-tuning with Keras and Deep Learning.

We will take a CNN pre-trained on the ImageNet dataset and fine-tune it to perform image classification and recognize classes it was never trained on.

Today is the final post in our three-part series on fine-tuning:

1. **Part #1:** Transfer learning with Keras and Deep Learning

1. **Part #2:** Feature extraction with on large datasets with Keras and Deep Learning

1. **Part #3:** Fine-tuning with Keras and Deep Learning (today’s post)


I would *strongly encourage* you to read the previous two tutorials in the series if you haven’t yet — understanding the concept of **transfer learning**, including performing **feature extraction** via a pre-trained CNN, will better enable you to understand (and appreciate) **fine-tuning**.

When performing feature extraction we *did not* re-train the original CNN. Instead, we treated the CNN as an arbitrary feature extractor and then trained a simple machine learning model on top of the extracted features.

**Fine-tuning, on the other hand, requires that we not only *update* the CNN architecture but also *re-train* it to learn new object classes.**

Fine-tuning is a multi-step process:

1. Remove the fully connected nodes at the end of the network (i.e., where the actual class label predictions are made).

1. Replace the fully connected nodes with freshly initialized ones.

1. Freeze earlier CONV layers earlier in the network (ensuring that any previous robust features learned by the CNN are not destroyed).

1. Start training, *but only train the FC layer heads*.

1. Optionally unfreeze some/all of the CONV layers in the network and perform a second pass of training.


If you are new to deep learning and CNNs, I would recommend you stop here and learn how to train your first CNN.

Fine-tuning with Keras is a more advanced technique with plenty of gotchas and pitfalls that will trip you up along the way (for example, it tends to be *very easy to overfit a network* when performing fine-tuning if you are not careful).

**To learn how to perform fine-tuning with Keras and deep learning, *just keep reading.***

## Fine-tuning with Keras and Deep Learning

***Note:** Many of the fine-tuning concepts I’ll be covering in this post also appear in my book, Deep Learning for Computer Vision with Python. **Inside the book, I go into considerably more detail (and include more of my tips, suggestions, and best practices).** If you would like more detail on fine-tuning with Keras after going through this guide, definitely take a look at my book.*

In the first part of this tutorial, we’ll discuss the concept of fine-tuning and how we can re-train a neural network to recognize classes it was not originally trained to recognize.

From there we’ll review the dataset we are using for fine-tuning.

I’ll then discuss our project directory structure.

Once we have a good handle on the dataset we’ll then switch to implementing fine-tuning with Keras.

After you have finished going through this tutorial you will be able to:

1. Fine-tune networks with Keras.

1. Make predictions using the fine-tuned network.


Let’s get started!

### What is fine-tuning?
![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_knob.jpg)


**Figure 1:** Fine-tuning with Keras and deep learning using Python involves retraining the head of a network to recognize classes it was not originally intended for.

***Note:** The following section has been adapted from my book, Deep Learning for Computer Vision with Python. For the full set of chapters on transfer learning and fine-tuning, please refer to the text.*

Earlier in this series of posts on transfer learning, we learned how to treat a pre-trained Convolutional Neural Network as a feature extractor.

Using this feature extractor, we forward propagated our dataset of images through the network, extracted the activations at a given layer (treating the activations as a feature vector), and then saved the values to disk.

A standard machine learning classifier (in our case, Logistic Regression), was trained on top of the CNN features, exactly as we would do with hand-engineered features such as SIFT, HOG, LBPs, etc.

This approach to transfer learning is called ***feature extraction***.

But there is another type of transfer learning, one that can actually outperform the feature extraction method. This method is called ***fine-tuning*** and requires us to perform “network surgery”.

First, we take a scalpel and cut off the final set of fully connected layers (i.e., the “head” of the network where the class label predictions are returned) from a pre-trained CNN (typically VGG, ResNet, or Inception).

We then replace the head with a *new* set of fully connected layers with random initializations.

From there, *all layers below the head* are frozen so their weights cannot be updated (i.e., the backward pass in back propagation does not reach them).

We then train the network using a very small learning rate so the new set of fully connected layers can learn patterns from the *previously learned* CONV layers earlier in the network — **this process is called allowing the FC layers to “warm up”.**

Optionally, we may unfreeze the rest of the network and continue training. Applying fine-tuning allows us to utilize pre-trained networks to recognize classes *they were not originally trained on*.

And furthermore, this method can lead to higher accuracy than transfer learning via feature extraction.

### Fine-tuning and network surgery

***Note:** The following section has been adapted from my book, Deep Learning for Computer Vision with Python. For the full set of chapters on transfer learning and fine-tuning, please refer to the text.*

As we discussed earlier in this series on transfer learning via feature extraction, pre-trained networks (such as ones trained on the ImageNet dataset) contain rich, discriminative filters. The filters can be used on datasets to predict class labels outside the ones the network has already been trained on.

However, instead of simply applying feature extraction, we are going to perform network surgery and *modify the actual architecture* so we can re-train parts of the network.

If this sounds like something out of a bad horror movie; don’t worry, there won’t be any blood and gore — but we’ll have some fun and learn a lot about transfer learning via our Dr. Frankenstien-esque network experiments.

To understand how fine-tuning works, consider the following figure:
![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_network_surgery.png)


**Figure 2:** **Left:** The original VGG16 network architecture. **Middle:** Removing the FC layers from VGG16 and treating the final POOL layer as a feature extractor. **Right:** Removing the original FC Layers and replacing them with a brand new FC head. These FC layers can then be fine-tuned to a specific dataset (the old FC Layers are no longer used).

On the *left* we have the layers of the VGG116 network.

As we know, the final set of layers (i.e., the “head”) are our fully connected layers along with our softmax classifier.

When performing fine-tuning, we actually *sever* the head of the network, just as in feature extraction (**Figure 2**, *middle*).

However, unlike feature extraction, when we perform *fine-tuning* we actually **build a new fully connected head** and **place it on top of the original architecture** (**Figure 2**, *right*).

The new FC layer head is randomly initialized (just like any other layer in a new network) and connected to the body of the original network.

However, there is a problem:

Our CONV layers have already learned rich, discriminative filters while our FC layers are brand new and totally random.

If we allow the gradient to backpropagate from these random values all the way through the network, we risk destroying these powerful features.

To circumvent this problem, we instead let our FC head “warm up” by (ironically) “freezing” all layers in the body of the network (I told you the horror/cadaver analogy works well here) as depicted in **Figure 2** (*left*).
![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_freeze_unfreeze.png)


**Figure 3:** **Left:** When we start the fine-tuning process, we freeze all CONV layers in the network and only allow the gradient to backpropagate through the FC layers. Doing this allows our network to “warm up”. **Right:** After the FC layers have had a chance to warm up, we may choose to unfreeze all or some of the layers earlier in the network and allow each of them to be fine-tuned as well.

Training data is forward propagated through the network as we usually would; however, the backpropagation is stopped after the FC layers, which allows these layers to start to learn patterns from the highly discriminative CONV layers.

In some cases, we may decide to never unfreeze the body of the network as our new FC head may obtain sufficient accuracy.

However, for some datasets it is often advantageous to allow the original CONV layers to be modified during the fine-tuning process as well (**Figure 3**, *right*).

After the FC head has started to learn patterns in our dataset, we can pause training, unfreeze the body, and continue training, *but with a very small learning rate* — we do not want to alter our CONV filters dramatically.

Training is then allowed to continue until sufficient accuracy is obtained.

Fine-tuning is a *super-powerful method* to obtain image classifiers on your own custom datasets from pre-trained CNNs (and is even more powerful than transfer learning via feature extraction).

**If you’d like to learn more about transfer learning via deep learning, including:**

- Deep learning-based feature extraction

- Training models on top of extracted features

- Fine-tuning networks on your own custom datasets

- My personal tips, suggestions, and best practices for transfer learning


**…then you’ll want to take a look at my book, *Deep Learning for Computer Vision with Python*, where I cover these algorithms and techniques in detail.**

### The Food-11 Dataset
![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_food11.jpg)


**Figure 4:** The Food-11 dataset is curated by the Multimedia Signal Processing Group (MSPG) of the Swiss Federal Institute of Technology. (image source)

The dataset we’ll be using for fine-tuning is the **Food-11 dataset** from the Multimedia Signal Processing Group (MSPG) of the Swiss Federal Institute of Technology.

**The dataset consists of 16,643 images belonging to 11 major food categories:**

1. **Bread** (1724 images)

1. **Dairy product** (721 images)

1. **Dessert** (2,500 images)

1. **Egg** (1,648 images)

1. **Fried food** (1,461images)

1. **Meat** (2,206 images)

1. **Noodles/pasta** (734 images)

1. **Rice** (472 images)

1. **Seafood** (1,505 images)

1. **Soup** (2,500 images)

1. **Vegetable/fruit** (1,172 images)


Using the Food-11 dataset we can train a deep learning model capable of recognizing each major food group — such a model could be used, for example, in a mobile fitness application that automatically tracks estimated food group and caloric intake.

To train such a model, we’ll be utilizing fine-tuning with the Keras deep learning library.

#### Downloading the Food-11 dataset

Go ahead and grab the zip from the ***“Downloads”*** section of this blog post.

Once you’ve downloaded the source code, change directory into 
fine-tuning-keras :



||$ unzip fine-tuning-keras.zip$ cd fine-tuning-keras|

$ cd fine-tuning-keras

Now let’s create a 
Food-11/  directory to house our unaltered dataset:



||$ mkdir Food-11$ cd Food-11|

$ cd Food-11

In my experience, I’ve found that **downloading the Food-11 dataset is unreliable.**

Therefore I’m presenting **two options** to download the dataset:

**Option 1:**Use 
wget  in your terminal

The 
wget  application comes pre-installed on Ubuntu and other Linux distros. On macOS, you must install it:



||

To download the Food-11 dataset, let’s use 
wget  in our terminal:



||$ wget --passive-ftp --ftp-user FoodImage@grebvm2.epfl.ch \ --ftp-password Cahc1moo ftp://tremplin.epfl.ch/Food-11.zip|

 --ftp-password Cahc1moo ftp://tremplin.epfl.ch/Food-11.zip

**Note:** At least on macOS, I’ve found that if the 
wget  command fails once, just run it again and then the download will start.

**Option 2:** Use FileZilla

FileZilla is a GUI application for FTP and SCP connections. You may download it for your OS here.

Once you’ve installed and launched the application, enter the credentials:

- **Host:** tremplin.epfl.ch

- **Username:** FoodImage@grebvm2.epfl.ch

- **Password:** Cahc1moo


You can then connect and download the file into the appropriate destination.
![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_filezilla.jpg)


**Figure 5:** Downloading the Food-11 dataset with FileZilla.

The username and password combination was obtained from the official Food-11 dataset website. If the username/password combination stops working for you, check to see if the dataset curators changed the login credentials.

Once downloaded (hopefully with no issues), we can go ahead and unzip the dataset inside of the 
Food-11/  directory:



||


Project structure
Now that we’ve downloaded the project and dataset, go ahead and navigate back to the project root. From there let’s analyze the project structure:



|1234567891011121314151617181920|$ cd ..$ tree --dirsfirst --filelimit 10.├── Food-11│   ├── evaluation [3347 entries]│   ├── training [9866 entries]│   ├── validation [3430 entries]│   └── Food-11.zip├── dataset├── output│   ├── unfrozen.png│   └── warmup.png├── pyimagesearch│   ├── __init__.py│   └── config.py├── build_dataset.py├── predict.py└── train.py 7 directories, 8 files|

2


4


6


8


10


12


14


16


18


20


$ tree --dirsfirst --filelimit 10

├── Food-11

│   ├── training [9866 entries]

│   └── Food-11.zip

├── output

│   └── warmup.png

│   ├── __init__.py

├── build_dataset.py

└── train.py

7 directories, 8 files

Our project structure is similar to last week’s.

Our original dataset is in the 
Food-11/  directory.

Executing 
build_dataset.py  enables us to organize the Food-11 images into the 
dataset/  directory.

From there, we’ll use 
train.py  to **perform fine tuning.**

Finally, we’ll use 
predict.py  to make predictions on sample images using our fine-tuned network.

Each of the aforementioned scripts takes advantage of a configuration file named 
config.py . Let’s go ahead and learn more about the configuration script now.

### Understanding our configuration file

Before we can actually fine-tune our network, we first need to create our configuration file to store important variables, including:

- Paths to the input dataset

- Class labels

- Batch size/training parameters

- Output paths, including model files, label encoders, plot histories, etc.


Since there are so many parameters that we need, I’ve opted to use a configuration file to keep our code nice and organized (versus having to utilize many command line arguments).

Our configuration file, 
config.py, lives in a Python module named 
pyimagesearch .

We keep the 
config.py  file there for two reasons:

1. To ensure we can import the configuration into our own Python scripts

1. To keep our code tidy and organized


***Note:*** *This config file is similar to the one in last week’s and the prior week’s tutorials.*

Let’s fill our 
config.py  file now — open it up in your favorite code editor and insert the following lines:



||# import the necessary packagesimport os # initialize the path to the *original* input directory of imagesORIG_INPUT_DATASET = "Food-11" # initialize the base path to the *new* directory that will contain# our images after computing the training and testing splitBASE_PATH = "dataset"|

import os

# initialize the path to the *original* input directory of images

 

# our images after computing the training and testing split

First, we import 
os , enabling us to build file/directory paths directly in this config.

The original dataset path where we extracted the Food-11 dataset is contained in 
ORIG_INPUT_DATASET .

Then we specify the 
BASE_PATH  where our organized dataset will soon reside.

From there we’ll define the names of our 
TRAIN , 
TEST , and 
VAL  directories:



||# define the names of the training, testing, and validation# directoriesTRAIN = "training"TEST = "evaluation"VAL = "validation"|

# directories

TEST = "evaluation"

Followed by listing the eleven 
CLASSES  of our Food-11 dataset:



||# initialize the list of class label namesCLASSES = ["Bread", "Dairy product", "Dessert", "Egg", "Fried food", "Meat", "Noodles/Pasta", "Rice", "Seafood", "Soup", "Vegetable/Fruit"]|

CLASSES = ["Bread", "Dairy product", "Dessert", "Egg", "Fried food",

 "Vegetable/Fruit"]

Finally, we’ll specify our batch size and model + plot paths:



||# set the batch size when fine-tuningBATCH_SIZE = 32 # set the path to the serialized model after trainingMODEL_PATH = os.path.sep.join(["output", "food11.model"]) # define the path to the output training history plotsUNFROZEN_PLOT_PATH = os.path.sep.join(["output", "unfrozen.png"])WARMUP_PLOT_PATH = os.path.sep.join(["output", "warmup.png"])|

BATCH_SIZE = 32

# set the path to the serialized model after training

 

UNFROZEN_PLOT_PATH = os.path.sep.join(["output", "unfrozen.png"])

Our 
BATCH_SIZE  of 
32  represents the size of the chunks of data that will flow through our CNN.

We’ll store our fine-tuned serialized Keras model in the 
MODEL_PATH .

Similarly, we specify the paths where our warmup and unfrozen plot images will be stored.

### Building our image dataset for fine-tuning

If we were to store the entire Food-11 dataset in memory, it would occupy **~10GB of RAM**.

Most deep learning rigs should be able to handle that amount of data, but nevertheless, I’ll be showing you how to use the 
.flow_from_directory  function with Keras to only load small batches of data from disk at a time.

However, before we can actually get to fine-tuning and re-training a network, **we first must (correctly) organize our dataset of images on disk.**

In order to use the 
.flow_from_directory  function, Keras requires that we have our dataset organized using the following template:


dataset_name/class_label/example_of_class_label.jpg

And since the Food-11 dataset *also provides pre-supplied data splits*, our final directory structure will have the form:


dataset_name/split_name/class_label/example_of_class_label.jpg

**Having the above directory structure ensures that:**

The 
.flow_from_directory  function will properly work.
1. Our dataset is organized into a neat, easy to follow directory structure.


In order to take the *original* Food-11 images and then copy them into our desired directory structure, we need the 
build_dataset.py  script.

Let’s review that script now:



|1234567891011121314151617181920212223242526272829|# import the necessary packagesfrom pyimagesearch import configfrom imutils import pathsimport shutilimport os # loop over the data splitsfor split in (config.TRAIN, config.TEST, config.VAL): # grab all image paths in the current split print("[INFO] processing '{} split'...".format(split)) p = os.path.sep.join([config.ORIG_INPUT_DATASET, split]) imagePaths = list(paths.list_images(p))  # loop over the image paths for imagePath in imagePaths: # extract class label from the filename filename = imagePath.split(os.path.sep)[-1] label = config.CLASSES[int(filename.split("_")[0])]  # construct the path to the output directory dirPath = os.path.sep.join([config.BASE_PATH, split, label])  # if the output directory does not exist, create it if not os.path.exists(dirPath): os.makedirs(dirPath)  # construct the path to the output image file and copy it p = os.path.sep.join([dirPath, filename]) shutil.copy2(imagePath, p)|

2


4


6


8


10


12


14


16


18


20


22


24


26


28


from pyimagesearch import config

import shutil

 

for split in (config.TRAIN, config.TEST, config.VAL):

 print("[INFO] processing '{} split'...".format(split))

 imagePaths = list(paths.list_images(p))

 # loop over the image paths

 # extract class label from the filename

 label = config.CLASSES[int(filename.split("_")[0])]

 # construct the path to the output directory

 

 if not os.path.exists(dirPath):

 

 p = os.path.sep.join([dirPath, filename])

**Lines 2-5** import our necessary packages, in particular, our 
config .

From there we loop over data splits beginning on **Line 8**. Inside, we:

Extract 
imagePaths  and each class 
label  (**Lines 11-18**).
- Create a directory structure for our organized image files (**Lines 21-25**).

- Copy the image files into the appropriate destination (**Lines 28 and 29**).


This script has been reviewed in more detail inside the *Transfer learning with Keras and deep learning* post. If you would like more detail on the inner-workings of 
build_dataset.py , please refer to the previous tutorial.

---

Before continuing, make sure you have used the ***“Downloads”*** section of the tutorial to download the source code associated with this blog post.

From there, open up a terminal and execute the following command:



||$ python build_dataset.py [INFO] processing 'training split'...[INFO] processing 'evaluation split'...[INFO] processing 'validation split'...|

[INFO] processing 'training split'...

[INFO] processing 'validation split'...

If you investigate the 
dataset/  directory you’ll see three directories, one for each of our respective data splits:



||$ ls dataset/evaluationtrainingvalidation|

evaluationtrainingvalidation

Inside each of the data split directories you’ll also find class label subdirectories:



||$ ls -l dataset/training/BreadDairy productDessertEggFried foodMeatNoodlesRiceSeafoodSoupVegetable|

Bread

Dessert

Fried food

Noodles

Seafood

Vegetable

And inside each of the class label subdirectories you’ll find images associated with that label:



||$ ls -l dataset/training/Bread/*.jpg | head -n 5dataset/training/Bread/0_0.jpgdataset/training/Bread/0_1.jpgdataset/training/Bread/0_10.jpgdataset/training/Bread/0_100.jpgdataset/training/Bread/0_101.jpg|

dataset/training/Bread/0_0.jpg

dataset/training/Bread/0_10.jpg

dataset/training/Bread/0_101.jpg


Implementing fine-tuning with Keras
Now that our images are in the proper directory structure, we can perform fine-tuning with Keras.

Let’s implement the fine-tuning script inside 
train.py :



|1234567891011121314151617181920|# set the matplotlib backend so figures can be saved in the backgroundimport matplotlibmatplotlib.use("Agg") # import the necessary packagesfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.applications import VGG16from keras.layers.core import Dropoutfrom keras.layers.core import Flattenfrom keras.layers.core import Densefrom keras.layers import Inputfrom keras.models import Modelfrom keras.optimizers import SGDfrom sklearn.metrics import classification_reportfrom pyimagesearch import configfrom imutils import pathsimport matplotlib.pyplot as pltimport numpy as npimport pickleimport os|

2


4


6


8


10


12


14


16


18


20


import matplotlib

 

from keras.preprocessing.image import ImageDataGenerator

from keras.layers.core import Dropout

from keras.layers.core import Dense

from keras.models import Model

from sklearn.metrics import classification_report

from imutils import paths

import numpy as np

import os

**Lines 2-20** import required packages. Let’s briefly review those that are most important to the fine-tuning concepts in today’s post:


matplotlib : We’ll be plotting our frozen and unfrozen training efforts. **Line 3** sets the backend ensuring that we can save our plots to disk as image files.

ImageDataGenerator : Allows for data augmentation. Be sure to refer to DL4CV and this blog post for more information on this class.

VGG16 : The seminal network trained on ImageNet that we’ll be slicing and dicing with our scalpel for the purposes of fine-tuning.

classification_report : Calculates basic statistical information upon evaluation of our model.

config : Our custom configuration file which we reviewed in the *“Understanding our configuration file”* section.

Be sure to familiarize yourself with the rest of the imports as well.

With the packages at our fingertips, we’re now ready to move on. Let’s start by defining a function for plotting training history:



|22232425262728293031323334|def plot_training(H, N, plotPath): # construct a plot that plots and saves the training history plt.style.use("ggplot") plt.figure() plt.plot(np.arange(0, N), H.history["loss"], label="train_loss") plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss") plt.plot(np.arange(0, N), H.history["acc"], label="train_acc") plt.plot(np.arange(0, N), H.history["val_acc"], label="val_acc") plt.title("Training Loss and Accuracy") plt.xlabel("Epoch #") plt.ylabel("Loss/Accuracy") plt.legend(loc="lower left") plt.savefig(plotPath)|

23


25


27


29


31


33


 # construct a plot that plots and saves the training history

 plt.figure()

 plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")

 plt.plot(np.arange(0, N), H.history["val_acc"], label="val_acc")

 plt.xlabel("Epoch #")

 plt.legend(loc="lower left")

The 
plot_training  function is defined on **Lines 22-34**. This helper function will be used to construct and save a plot of our training history.

Let’s determine the total number of images in each of our splits:



||# derive the paths to the training, validation, and testing# directoriestrainPath = os.path.sep.join([config.BASE_PATH, config.TRAIN])valPath = os.path.sep.join([config.BASE_PATH, config.VAL])testPath = os.path.sep.join([config.BASE_PATH, config.TEST]) # determine the total number of image paths in training, validation,# and testing directoriestotalTrain = len(list(paths.list_images(trainPath)))totalVal = len(list(paths.list_images(valPath)))totalTest = len(list(paths.list_images(testPath)))|

# directories

valPath = os.path.sep.join([config.BASE_PATH, config.VAL])

 

# and testing directories

totalVal = len(list(paths.list_images(valPath)))

**Lines 38-40** define paths to training, validation, and testing directories, respectively.

Then, we determine the total number of images for each split via **Lines 44-46** — these values will enable us to calculate the steps per epoch.

Let’s initialize our data augmentation object and establish our mean subtraction value:



|4849505152535455565758596061626364656667|# initialize the training data augmentation objecttrainAug = ImageDataGenerator( rotation_range=30, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode="nearest") # initialize the validation/testing data augmentation object (which# we'll be adding mean subtraction to)valAug = ImageDataGenerator() # define the ImageNet mean subtraction (in RGB order) and set the# the mean subtraction value for each of the data augmentation# objectsmean = np.array([123.68, 116.779, 103.939], dtype="float32")trainAug.mean = meanvalAug.mean = mean|

49


51


53


55


57


59


61


63


65


67


trainAug = ImageDataGenerator(

 zoom_range=0.15,

 height_shift_range=0.2,

 horizontal_flip=True,

 

# we'll be adding mean subtraction to)

 

# the mean subtraction value for each of the data augmentation

mean = np.array([123.68, 116.779, 103.939], dtype="float32")

valAug.mean = mean

The process of data augmentation is important for small datasets. In fact, it is nearly always recommended. **Lines 49-56** define our training data augmentation object. The parameters specify random rotations, zooms, translations, shears, and flips to the training data as we train.

***Note:** A common misconception I see about data augmentation is that the random transforms of the images are then *added* to the *original* training data — **that’s not the case.** The random transformations performed by data augmentation are performed **in-place**, implying that the dataset size **does not increase.** These transforms are performed in-place, on the fly, during training.*

Although our validation data augmentation object (**Line 60**) uses the same class, we do not supply any parameters (we don’t apply data augmentation to validation or testing data). The validation 
ImageDataGenerator  will only be used for mean subtraction which is why no parameters are needed.

Next, we set the ImageNet mean subtraction values on **Line 65**. In this pre-processing technique, we perform a pixel-wise subtraction for all images. Mean subtraction is one of several scaling techniques I explain in the *Practitioner Bundle* of *Deep Learning for Computer Vision with Python*. In the text, we’ll even build a custom preprocessor to more efficiently accomplish mean subtraction.

Given the pixel-wise subtraction values, we prepare each of our data augmentation objects for mean subtraction (**Lines 66 and 67**).

Our data augmentation generators will generate data directly from their respective directories:



|6970717273747576777879808182838485868788899091929394|# initialize the training generatortrainGen = trainAug.flow_from_directory( trainPath, class_mode="categorical", target_size=(224, 224), color_mode="rgb", shuffle=True, batch_size=config.BATCH_SIZE) # initialize the validation generatorvalGen = valAug.flow_from_directory( valPath, class_mode="categorical", target_size=(224, 224), color_mode="rgb", shuffle=False, batch_size=config.BATCH_SIZE) # initialize the testing generatortestGen = valAug.flow_from_directory( testPath, class_mode="categorical", target_size=(224, 224), color_mode="rgb", shuffle=False, batch_size=config.BATCH_SIZE)|

70


72


74


76


78


80


82


84


86


88


90


92


94


trainGen = trainAug.flow_from_directory(

 class_mode="categorical",

 color_mode="rgb",

 batch_size=config.BATCH_SIZE)

# initialize the validation generator

 valPath,

 target_size=(224, 224),

 shuffle=False,

 

testGen = valAug.flow_from_directory(

 class_mode="categorical",

 color_mode="rgb",

 batch_size=config.BATCH_SIZE)

**Lines 70-94** define generators that will load batches of images from their respective, training, validation, and testing splits.

Using these generators ensures that our machine will not run out of RAM by trying to load all of the data at once.

Let’s go ahead and **perform network surgery:**



|96979899100101102103104105106107108109110111|# load the VGG16 network, ensuring the head FC layer sets are left# offbaseModel = VGG16(weights="imagenet", include_top=False, input_tensor=Input(shape=(224, 224, 3))) # construct the head of the model that will be placed on top of the# the base modelheadModel = baseModel.outputheadModel = Flatten(name="flatten")(headModel)headModel = Dense(512, activation="relu")(headModel)headModel = Dropout(0.5)(headModel)headModel = Dense(len(config.CLASSES), activation="softmax")(headModel) # place the head FC model on top of the base model (this will become# the actual model we will train)model = Model(inputs=baseModel.input, outputs=headModel)|

97


99


101


103


105


107


109


111


# off

 input_tensor=Input(shape=(224, 224, 3)))

# construct the head of the model that will be placed on top of the

headModel = baseModel.output

headModel = Dense(512, activation="relu")(headModel)

headModel = Dense(len(config.CLASSES), activation="softmax")(headModel)

# place the head FC model on top of the base model (this will become

model = Model(inputs=baseModel.input, outputs=headModel)

First, we’ll load the VGG16 architecture (with pre-trained ImageNet weights) from disk, leaving off the fully connected layers (**Lines 98 and 99**). By omitting the fully connected layers, we have effectively put the network in a guillotine to behead our network as in **Figure 2**.

From there, we define a new fully connected layer head (**Lines 103-107**).

***Note:** If you are unfamiliar with the contents on **Lines 103-107**, I recommend that you read my Keras tutorial or CNN tutorial. And if you would like to immerse yourself completely into the world of deep learning, be sure to check out my highly rated deep learning book.*

On **Line 111** we place the new FC layer head on top of the VGG16 base network. You can think of this as adding sutures to sew the head back on to the network body after surgery.

Take the time to review the above code block carefully as it is where the heart of **fine-tuning with Keras** begins.

Continuing on with fine-tuning, let’s **freeze all of the CONV layers in the body of VGG16:**



||# loop over all layers in the base model and freeze them so they will# *not* be updated during the first training processfor layer in baseModel.layers: layer.trainable = False|

# *not* be updated during the first training process

 layer.trainable = False

**Lines 115-116** freeze all CONV layers in the VGG16 base model.

**Given that the *base is now frozen*, we’ll go ahead and train our network *(only the head weights will be updated):***



|118119120121122123124125126127128129130131132133134135136137138139140141142143144145|# compile our model (this needs to be done after our setting our# layers to being non-trainableprint("[INFO] compiling model...")opt = SGD(lr=1e-4, momentum=0.9)model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"]) # train the head of the network for a few epochs (all other layers# are frozen) -- this will allow the new FC layers to start to become# initialized with actual "learned" values versus pure randomprint("[INFO] training head...")H = model.fit_generator( trainGen, steps_per_epoch=totalTrain // config.BATCH_SIZE, validation_data=valGen, validation_steps=totalVal // config.BATCH_SIZE, epochs=50) # reset the testing generator and evaluate the network after# fine-tuning just the network headprint("[INFO] evaluating after fine-tuning network head...")testGen.reset()predIdxs = model.predict_generator(testGen, steps=(totalTest // config.BATCH_SIZE) + 1)predIdxs = np.argmax(predIdxs, axis=1)print(classification_report(testGen.classes, predIdxs, target_names=testGen.class_indices.keys()))plot_training(H, 50, config.WARMUP_PLOT_PATH)|

119


121


123


125


127


129


131


133


135


137


139


141


143


145


# layers to being non-trainable

opt = SGD(lr=1e-4, momentum=0.9)

 metrics=["accuracy"])

# train the head of the network for a few epochs (all other layers

# initialized with actual "learned" values versus pure random

H = model.fit_generator(

 steps_per_epoch=totalTrain // config.BATCH_SIZE,

 validation_steps=totalVal // config.BATCH_SIZE,

 

# fine-tuning just the network head

testGen.reset()

 steps=(totalTest // config.BATCH_SIZE) + 1)

print(classification_report(testGen.classes, predIdxs,

plot_training(H, 50, config.WARMUP_PLOT_PATH)

In this block, we train our 
model , keeping in mind that **no weight updates will occur in the base. *Only the head of the network will be tuned at this point.***

In this code block, we:

Compile the 
model  (**Lines 121-123**). We use 
"categorical_crossentropy"  for our 
loss  function. If you are performing classification with only two classes, be sure to use 
"binary_crossentropy" .
- Train our network while applying data augmentation, only updating the weights for the head of the network (**Lines 129-134**)

- Reset our testing generator (**Line 139**).

- Evaluate our network on our testing data (**Lines 140-142**). We’ll print classification statistics in our terminal via **Lines 143 and 144**.

Plot the training history via our 
plot_training  function (**Line 145**).

Now let’s proceed to ***unfreeze* the final set of CONV layers in the base model layers:**



|147148149150151152153154155156157158159|# reset our data generatorstrainGen.reset()valGen.reset() # now that the head FC layers have been trained/initialized, lets# unfreeze the final set of CONV layers and make them trainablefor layer in baseModel.layers[15:]: layer.trainable = True # loop over the layers in the model and show which ones are trainable# or notfor layer in baseModel.layers: print("{}: {}".format(layer, layer.trainable))|

148


150


152


154


156


158


trainGen.reset()

 

# unfreeze the final set of CONV layers and make them trainable

 layer.trainable = True

# loop over the layers in the model and show which ones are trainable

for layer in baseModel.layers:

We start by reseting our training and validation generators (**Lines 148 and 149**).

We then ***unfreeze*** the final CONV layer block in VGG16 (**Lines 153 and 154**). Again, only the *final* CONV block of VGG16 is unfrozen (not the rest of the network).

Just so there is no confusion about what is going on in our network, **Lines 158 and 159** will show us which layers are frozen/not frozen (i.e., trainable). The information will print out in our terminal.

Continuing on, let’s**fine-tune *both* the final set of CONV layers *and* our set of FC layers:**



|161162163164165166167168169170171172173174175|# for the changes to the model to take affect we need to recompile# the model, this time using SGD with a *very* small learning rateprint("[INFO] re-compiling model...")opt = SGD(lr=1e-4, momentum=0.9)model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"]) # train the model again, this time fine-tuning *both* the final set# of CONV layers along with our set of FC layersH = model.fit_generator( trainGen, steps_per_epoch=totalTrain // config.BATCH_SIZE, validation_data=valGen, validation_steps=totalVal // config.BATCH_SIZE, epochs=20)|

162


164


166


168


170


172


174


# the model, this time using SGD with a *very* small learning rate

opt = SGD(lr=1e-4, momentum=0.9)

 metrics=["accuracy"])

# train the model again, this time fine-tuning *both* the final set

H = model.fit_generator(

 steps_per_epoch=totalTrain // config.BATCH_SIZE,

 validation_steps=totalVal // config.BATCH_SIZE,

Since we’ve unfrozen additional layers, we must re-compile the model (**Lines 164-166**).

**We then train the model again, this time fine-tuning *both* the FC layer head *and* the final CONV block (Lines 170-175).**

Wrapping up, let’s evaluate the network once more:



|177178179180181182183184185186187188189190|# reset the testing generator and then use our trained model to# make predictions on the dataprint("[INFO] evaluating after fine-tuning network...")testGen.reset()predIdxs = model.predict_generator(testGen, steps=(totalTest // config.BATCH_SIZE) + 1)predIdxs = np.argmax(predIdxs, axis=1)print(classification_report(testGen.classes, predIdxs, target_names=testGen.class_indices.keys()))plot_training(H, 20, config.UNFROZEN_PLOT_PATH) # serialize the model to diskprint("[INFO] serializing network...")model.save(config.MODEL_PATH)|

178


180


182


184


186


188


190


# make predictions on the data

testGen.reset()

 steps=(totalTest // config.BATCH_SIZE) + 1)

print(classification_report(testGen.classes, predIdxs,

plot_training(H, 20, config.UNFROZEN_PLOT_PATH)

# serialize the model to disk

model.save(config.MODEL_PATH)

Here we:

- Make predictions on the testing data (**Lines 180-183**).

- Print a new classification report (**Lines 184 and 185**).

- Save the unfrozen training plot to disk (**Line 186**).

And serialize the model to disk, allowing us to recall the model in our 
predict.py  script (**Line 190**).

Great job sticking with me on our fine-tuning journey. We’re going to put our script to work next!

### Training a network via fine-tuning with Keras

Now that we’ve implemented our Python script to perform fine-tuning, let’s give it a try and see what happens.

Make sure you’ve used the ***“Downloads”*** section of this tutorial to download the source code to this post, and from there, execute the following command:



|123456789101112131415161718192021222324252627282930313233343536373839|$ python train.pyUsing TensorFlow backend.Found 9866 images belonging to 11 classes.Found 3430 images belonging to 11 classes.Found 3347 images belonging to 11 classes.[INFO] compiling model...[INFO] training head...Epoch 1/50308/308 [==============================] - 246s 799ms/step - loss: 10.7644 - acc: 0.2883 - val_loss: 8.0234 - val_acc: 0.4614Epoch 2/50308/308 [==============================] - 237s 768ms/step - loss: 8.3090 - acc: 0.4336 - val_loss: 6.3494 - val_acc: 0.5556Epoch 3/50308/308 [==============================] - 233s 757ms/step - loss: 7.0419 - acc: 0.4963 - val_loss: 5.2425 - val_acc: 0.6071...Epoch 48/50308/308 [==============================] - 238s 771ms/step - loss: 0.8755 - acc: 0.7085 - val_loss: 0.8004 - val_acc: 0.7663Epoch 49/50308/308 [==============================] - 236s 765ms/step - loss: 0.8473 - acc: 0.7127 - val_loss: 0.7725 - val_acc: 0.7743Epoch 50/50308/308 [==============================] - 235s 763ms/step - loss: 0.8434 - acc: 0.7169 - val_loss: 0.7893 - val_acc: 0.7599[INFO] evaluating after fine-tuning network head...               precision    recall  f1-score   support         Bread       0.79      0.52      0.62       368Dairy product       0.75      0.55      0.64       148      Dessert       0.71      0.68      0.69       500          Egg       0.68      0.78      0.72       335   Fried food       0.64      0.74      0.68       287         Meat       0.73      0.88      0.79       432      Noodles       0.94      0.95      0.95       147         Rice       0.92      0.89      0.90        96      Seafood       0.80      0.82      0.81       303         Soup       0.92      0.94      0.93       500    Vegetable       0.89      0.84      0.86       231     micro avg       0.78      0.78      0.78      3347    macro avg       0.80      0.78      0.78      3347 weighted avg       0.78      0.78      0.77      3347|

2


4


6


8


10


12


14


16


18


20


22


24


26


28


30


32


34


36


38


Using TensorFlow backend.

Found 3430 images belonging to 11 classes.

[INFO] compiling model...

Epoch 1/50

4

308/308 [==============================] - 237s 768ms/step - loss: 8.3090 - acc: 0.4336 - val_loss: 6.3494 - val_acc: 0.5556

308/308 [==============================] - 233s 757ms/step - loss: 7.0419 - acc: 0.4963 - val_loss: 5.2425 - val_acc: 0.6071

Epoch 48/50

Epoch 49/50

Epoch 50/50

[INFO] evaluating after fine-tuning network head...

 

Dairy product       0.75      0.55      0.64       148

          Egg       0.68      0.78      0.72       335

         Meat       0.73      0.88      0.79       432

         Rice       0.92      0.89      0.90        96

         Soup       0.92      0.94      0.93       500

 

    macro avg       0.80      0.78      0.78      3347

![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/warmup.png)
**Figure 6:** Our Keras fine-tuning network is allowed to “warm up” prior to unfreezing only the final block of CONV layers in VGG16.

After fine-tuning just our newly initialized FC layer head and allowing the FC Layers to warm up, we are obtaining **~78% accuracy** which is quite respectable.

Next, we see that we have unfrozen the final block of CONV layers in VGG16 while leaving the rest of the network weights frozen:



|12345678910111213141516171819|<keras.engine.input_layer.InputLayer object at 0x7f95da8baf60>: False<keras.layers.convolutional.Conv2D object at 0x7f95da880128>: False<keras.layers.convolutional.Conv2D object at 0x7f95da87ac18>: False<keras.layers.pooling.MaxPooling2D object at 0x7f95da87c588>: False<keras.layers.convolutional.Conv2D object at 0x7f95da87c438>: False<keras.layers.convolutional.Conv2D object at 0x7f95d84e0da0>: False<keras.layers.pooling.MaxPooling2D object at 0x7f95da5c0080>: False<keras.layers.convolutional.Conv2D object at 0x7f95da5c00b8>: False<keras.layers.convolutional.Conv2D object at 0x7f95da5cd470>: False<keras.layers.convolutional.Conv2D object at 0x7f95da5dd048>: False<keras.layers.pooling.MaxPooling2D object at 0x7f95da57c080>: False<keras.layers.convolutional.Conv2D object at 0x7f95da57c0b8>: False<keras.layers.convolutional.Conv2D object at 0x7f95da58b4a8>: False<keras.layers.convolutional.Conv2D object at 0x7f95da59b780>: False<keras.layers.pooling.MaxPooling2D object at 0x7f95da53a0f0>: False<keras.layers.convolutional.Conv2D object at 0x7f95da53a128>: True<keras.layers.convolutional.Conv2D object at 0x7f95da548518>: True<keras.layers.convolutional.Conv2D object at 0x7f95da5590f0>: True<keras.layers.pooling.MaxPooling2D object at 0x7f95da4f6198>: True|

2


4


6


8


10


12


14


16


18


<keras.layers.convolutional.Conv2D object at 0x7f95da880128>: False

<keras.layers.pooling.MaxPooling2D object at 0x7f95da87c588>: False

<keras.layers.convolutional.Conv2D object at 0x7f95d84e0da0>: False

<keras.layers.convolutional.Conv2D object at 0x7f95da5c00b8>: False

<keras.layers.convolutional.Conv2D object at 0x7f95da5dd048>: False

<keras.layers.convolutional.Conv2D object at 0x7f95da57c0b8>: False

<keras.layers.convolutional.Conv2D object at 0x7f95da59b780>: False

<keras.layers.convolutional.Conv2D object at 0x7f95da53a128>: True

<keras.layers.convolutional.Conv2D object at 0x7f95da5590f0>: True

Once we’ve unfrozen the final CONV block, we resume fine-tuning:



|123456789101112131415161718192021222324252627282930313233343536373839404142|[INFO] re-compiling model...poch 1/20308/308 [==============================] - 245s 795ms/step - loss: 0.8553 - acc: 0.7201 - val_loss: 0.7468 - val_acc: 0.7766Epoch 2/20308/308 [==============================] - 234s 759ms/step - loss: 0.7736 - acc: 0.7461 - val_loss: 0.7006 - val_acc: 0.8031Epoch 3/20308/308 [==============================] - 233s 756ms/step - loss: 0.7246 - acc: 0.7680 - val_loss: 0.7132 - val_acc: 0.8034Epoch 4/20308/308 [==============================] - 232s 753ms/step - loss: 0.6738 - acc: 0.7820 - val_loss: 0.6806 - val_acc: 0.8072Epoch 5/20308/308 [==============================] - 230s 746ms/step - loss: 0.6533 - acc: 0.7905 - val_loss: 0.6465 - val_acc: 0.8096...Epoch 16/20308/308 [==============================] - 231s 749ms/step - loss: 0.3888 - acc: 0.8703 - val_loss: 0.6178 - val_acc: 0.8434Epoch 17/20308/308 [==============================] - 232s 753ms/step - loss: 0.3993 - acc: 0.8671 - val_loss: 0.6077 - val_acc: 0.8434Epoch 18/20308/308 [==============================] - 233s 755ms/step - loss: 0.3665 - acc: 0.8758 - val_loss: 0.6093 - val_acc: 0.8405Epoch 19/20308/308 [==============================] - 233s 756ms/step - loss: 0.3575 - acc: 0.8801 - val_loss: 0.5789 - val_acc: 0.8508Epoch 20/20308/308 [==============================] - 236s 766ms/step - loss: 0.3536 - acc: 0.8840 - val_loss: 0.6020 - val_acc: 0.8464[INFO] evaluating after fine-tuning network...               precision    recall  f1-score   support         Bread       0.86      0.78      0.82       368Dairy product       0.85      0.65      0.74       148      Dessert       0.83      0.79      0.81       500          Egg       0.84      0.84      0.84       335   Fried food       0.75      0.92      0.82       287         Meat       0.89      0.88      0.88       432      Noodles       0.99      0.95      0.97       147         Rice       0.88      0.95      0.91        96      Seafood       0.86      0.91      0.88       303         Soup       0.97      0.95      0.96       500    Vegetable       0.86      0.96      0.91       231     micro avg       0.87      0.87      0.87      3347    macro avg       0.87      0.87      0.87      3347 weighted avg       0.87      0.87      0.87      3347 [INFO] serializing network...|

2


4


6


8


10


12


14


16


18


20


22


24


26


28


30


32


34


36


38


40


42


poch 1/20

Epoch 2/20

Epoch 3/20

Epoch 4/20

Epoch 5/20

...

308/308 [==============================] - 231s 749ms/step - loss: 0.3888 - acc: 0.8703 - val_loss: 0.6178 - val_acc: 0.8434

308/308 [==============================] - 232s 753ms/step - loss: 0.3993 - acc: 0.8671 - val_loss: 0.6077 - val_acc: 0.8434

308/308 [==============================] - 233s 755ms/step - loss: 0.3665 - acc: 0.8758 - val_loss: 0.6093 - val_acc: 0.8405

308/308 [==============================] - 233s 756ms/step - loss: 0.3575 - acc: 0.8801 - val_loss: 0.5789 - val_acc: 0.8508

308/308 [==============================] - 236s 766ms/step - loss: 0.3536 - acc: 0.8840 - val_loss: 0.6020 - val_acc: 0.8464

               precision    recall  f1-score   support

        Bread       0.86      0.78      0.82       368

      Dessert       0.83      0.79      0.81       500

   Fried food       0.75      0.92      0.82       287

      Noodles       0.99      0.95      0.97       147

      Seafood       0.86      0.91      0.88       303

    Vegetable       0.86      0.96      0.91       231

    micro avg       0.87      0.87      0.87      3347

 weighted avg       0.87      0.87      0.87      3347

[INFO] serializing network...

![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/unfrozen.png)
**Figure 7:** We have unfrozen the final CONV block and resumed fine-tuning with Keras and deep learning. Training and validation loss are starting to divide indicating the start of overfitting, so fine-tuning stops at epoch 20.

I decided to not train past epoch 20 for *fear of overfitting.* If you take a look at **Figure 7** you can start to see our training and validation loss start to rapidly divide. When you see training loss falling quickly while validation loss stagnates or even increases, you know you are overfitting.

That said, at the end of our fine-tuning process, we are now obtaining **87% accuracy**, a significant increase from just fine-tuning the FC layer heads alone!

### Making predictions with fine-tuning and Keras

Now that we’ve fine-tuned our Keras model, let’s see how we can use it to make predictions on images *outside* the training/testing set (i.e., our own custom images).

Open up 
predict.py  and insert the following code:



||# import the necessary packagesfrom keras.models import load_modelfrom pyimagesearch import configimport numpy as npimport argparseimport imutilsimport cv2 # construct the argument parser and parse the argumentsap = argparse.ArgumentParser()ap.add_argument("-i", "--image", type=str, required=True, help="path to our input image")args = vars(ap.parse_args())|

from keras.models import load_model

import numpy as np

import imutils

 

ap = argparse.ArgumentParser()

 help="path to our input image")

**Lines 2-7** import our required packages. We’re going to use 
load_model  to recall our Keras fine-tuned model from disk and make predictions. This is also the first time today that we will use OpenCV (
cv2 ).

On **Lines 10-13** we parse our command line argument. The 
--image  argument allows us to supply any image from our terminal at runtime with no modifications to the code. It makes sense to take advantage of a command line argument rather than hard-coding the value here or in our config.

Let’s go ahead and load that image from disk and preprocess it:



|15161718192021222324252627282930|# load the input image and then clone it so we can draw on it laterimage = cv2.imread(args["image"])output = image.copy()output = imutils.resize(output, width=400) # our model was trained on RGB ordered images but OpenCV represents# images in BGR order, so swap the channels, and then resize to# 224x224 (the input dimensions for VGG16)image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)image = cv2.resize(image, (224, 224)) # convert the image to a floating point data type and perform mean# subtractionimage = image.astype("float32")mean = np.array([123.68, 116.779, 103.939][::1], dtype="float32")image -= mean|

16


18


20


22


24


26


28


30


image = cv2.imread(args["image"])

output = imutils.resize(output, width=400)

# our model was trained on RGB ordered images but OpenCV represents

# 224x224 (the input dimensions for VGG16)

image = cv2.resize(image, (224, 224))

# convert the image to a floating point data type and perform mean

image = image.astype("float32")

image -= mean

**Lines 16-30** load and preprocess our 
image . The preprocessing steps are identical to training and include:

Making a 
copy  of the image and resizing it for 
output  purposes (**Lines 17 and 18**).
Swapping color channels since we trained with RGB images and OpenCV loaded this 
image  in BGR order (**Line 23**).
Resizing the 
image  to *224×224* pixels for inference (**Line 24**).
Converting the 
image  to floating point (**Line 28**).
- Performing mean subtraction (**Lines 29 and 30**).


***Note:** When we perform inference using a custom prediction script, if the results are unsatisfactory nine times out of ten it is due to improper preprocessing. Typically having color channels in the wrong order or forgetting to perform mean subtraction altogether will lead to unfavorable results. Keep this in mind when writing your own scripts.*

Now that our image is ready, let’s predict its class label:



|3233343536373839404142434445464748|# load the trained model from diskprint("[INFO] loading model...")model = load_model(config.MODEL_PATH) # pass the image through the network to obtain our predictionspreds = model.predict(np.expand_dims(image, axis=0))[0]i = np.argmax(preds)label = config.CLASSES[i] # draw the prediction on the output imagetext = "{}: {:.2f}%".format(label, preds[i] * 100)cv2.putText(output, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) # show the output imagecv2.imshow("Output", output)cv2.waitKey(0)|

33


35


37


39


41


43


45


47


print("[INFO] loading model...")

 

preds = model.predict(np.expand_dims(image, axis=0))[0]

label = config.CLASSES[i]

# draw the prediction on the output image

cv2.putText(output, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,

 

cv2.imshow("Output", output)

We load our fine-tuned 
model  via **Line 34** and then perform inference. The top prediction class 
label  is extracted on **Lines 37-39**.

Finally, we annotate the 
output  image and display it on screen (**Lines 42-48**). The 
text  annotation contains the highest prediction along with its associated confidence.

On to the fun part — testing our script on food! I’m hungry just thinking about it and I bet you may be too.

### Keras fine-tuning results

To see our fine-tuned Keras model in action, make sure you use the ***“Downloads”*** section of this tutorial to download the source code and example images.

From there, open up a terminal and execute the following command:



||$ python predict.py --image dataset/evaluation/Seafood/8_186.jpg|

![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_result01.jpg)
**Figure 8:** Our fine-tuned Keras deep learning network correctly recognizes oysters as “seafood”.

As you can see from **Figure 7**, we have correctly classified the input image as “Seafood”.

Let’s try another example:



||$ python predict.py --image dataset/evaluation/Meat/5_293.jpg|

![](https://www.pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_result02.jpg)
**Figure 9:** With 64% accuracy this image of chicken wings is classified as “fried food”. We have applied the process fine-tuning to a pre-trained model to recognize new classes with Keras and deep learning.

Our fine-tuned network has labeled the image as “Fried food” ***despite it being in the “Meat” class in our dataset***.

Chicken wings are typically fried and these ones clearly are. They are both “Meat” and “Fried food” which is why we are pulled in two directions. Therefore, I’m still declaring it as a “correct” classification. A fun experiment would be to apply fine-tuning with multi-label classification. I’ll leave that as an exercise to you to implement.

Below I have included a few additional results from my fine-tuning experiments:
![](https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/fine-tuning-keras/fine_tuning_keras_animation.gif)


**Figure 10:** Fine-tuning with Keras and deep learning on the Food-11 dataset.

## What’s next — where do I learn more about transfer learning, feature extraction, and fine-tuning?

![](https://www.pyimagesearch.com/wp-content/uploads/2019/03/pi_ks_dl4cv_addon_post.jpg)


Over the past few weeks since we started this series on transfer learning with Keras, I’ve received a number of emails and comments that are some variation of the following:

- *“How can I determine the number of nodes to put in my fully connected layer head when fine-tuning?”*

- *“What optimizer and learning rate should I use for fine-tuning?”*

- *“Which CONV layers (and when) should I freeze and unfreeze?”*

- *“How do I classify images outside my training/testing set?”*

- *“How do I load an image from disk, extract features from it using a CNN, and then classify it using the neural network?”*

- *“How do I correctly preprocess my input image before classification?”*


Today’s tutorial is long enough as it is, so I can’t include those sections of ***Deep Learning for Computer Vision with Python*** inside this post.

**If you’d like to learn more about transfer learning, including:**

1. More details on the concept of transfer learning

1. How to perform feature extraction

1. How to fine-tune networks

1. How to classify images outside your training/testing set using both feature extraction and fine-tuning


…then you’ll definitely want to refer to *Deep Learning for Computer Vision with Python*.

Besides chapters on transfer learning, you’ll also find:

- **Super practical walkthroughs** that present solutions to actual, real-world image classification, object detection, and instance segmentation problems.

- **Hands-on tutorials (with lots of code)** that not only show you the *algorithms* behind deep learning for computer vision but their *implementations* as well.

- **A no-nonsense teaching style** that is guaranteed to help you master deep learning for image understanding and visual recognition.


To learn more about the book, and grab the table of contents + free sample chapters, ***just click here!***

## Summary

In this tutorial, you learned how to perform fine-tuning with Keras and deep learning.

To perform fine-tuning, we:

1. Loaded the VGG16 network architecture from disk with weights pre-trained on ImageNet.

1. Ensured the original fully connected layer heads were removed (i.e., where the output predictions from the network are made).

1. Replaced the originally fully connected layers with brand new, freshly initialized ones.

1. Froze all CONV layers in VGG16.

1. Trained only the fully connected layer heads.

1. Unfroze the final set of CONV layer blocks in VGG16.

1. Continued training.


**Overall, we were able to obtain *87% accuracy* on the Food-11 dataset.**

Further accuracy can be obtained by applying additional data augmentation and adjusting the parameters to our optimizer and number of FC layer nodes.

If you’re interested in learning more about fine-tuning with Keras, including my tips, suggestions, and best practices, **be sure to take a look at *Deep Learning for Computer Vision with Python* where I cover fine-tuning in more detail.**

I hope you enjoyed today’s tutorial on fine-tuning!

**To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), *just enter your email address in the form below!***

## Downloads:
