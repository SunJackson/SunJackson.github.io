---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/06/30/distilled-news-1115/
date:      2019-06-30
author:      Michael Laux
tags:
    - data
    - learning
    - models
    - model computes
    - objects
---

**Autocorrelation test for independence**

Autocorrelation is a characteristic of data in which the correlation between the values of the same variables is based on related objects. It violates the assumption of instance independence, which underlies most of the conventional models. It generally exists in those types of data-sets in which the data, instead of being randomly selected, is from the same source.

**Monte Carlo in Reinforcement Learning, the Easy Way**

In Dynamic Programming (DP) we have seen that in order to compute the value function on each state, we need to know the transition matrix as well as the reward system. But this is not always a realistic condition. Probably it is possible to have such thing in some board games, but in video games and real life problems like self-driving car there is no way to know these information before hand.

**Facebook’s AI System Can Speak With Bill Gates’s Voice**

The company’s AI researchers have developed a speech synthesizer capable of copying anybody’s voice with uncanny accuracy.

**Understanding Multiple Linear Regression**

Hello World, ever wondered how your favorite Machine Learning algorithms actually work? Let’s see how a Multiple Linear Regression(MLR) model computes the ideal parameters, given the features matrix (X) and target variable(y), using Linear Algebra.

**Running Peta-Scale Spark Jobs on Object Storage Using S3 Select**

When one looks at the amazing roster of talks for most data science conferences what you don’t see is a lot of discussion on how to leverage object storage. On some level you would expect to – ultimately if you want to run your Spark or Presto job on peta-scale data sets and have it be available to your applications in the public or private cloud – this would be the logical storage architecture. While logical, there has been a catch, at least historically, and that is object storage wasn’t performant enough to actually make running Spark jobs feasible. With the advent of a modern, cloud native approaches that changes and the implications for the Apache Spark community are pretty significant. At the heart of this change is the extension of the S3 API to include SQL query capabilities, S3 Select. With S3 Select, users can execute queries directly on their objects, returning just the relevant subset, instead of having to download the whole object – significantly more efficient than the regular method of retrieving the entire object store. MinIO has pioneered S3 compatible object storage. MinIO’s implementation of the S3 Select API matches the native features while offering better resource utilization when it comes to executing Spark jobs. These advancements deliver orders of magnitude performance improvements across a range of frequently used queries.

**Innovations in Graph Representation Learning**

Relational data representing relationships between entities is ubiquitous on the Web (e.g., online social networks) and in the physical world (e.g., in protein interaction networks). Such data can be represented as a graph with nodes (e.g., users, proteins), and edges connecting them (e.g., friendship relations, protein interactions). Given the widespread prevalence of graphs, graph analysis plays a fundamental role in machine learning, with applications in clustering, link prediction, privacy, and others. To apply machine learning methods to graphs (e.g., predicting new friendships, or discovering unknown protein interactions) one needs to learn a representation of the graph that is amenable to be used in ML algorithms.

**Graph Mining**

Our mission is to build the most scalable library for graph algorithms and analysis and apply it to a multitude of Google products. We formalize data mining and machine learning challenges as graph problems and perform fundamental research in those fields leading to publications in top venues. Our algorithms and systems are used in a wide array of Google products such as Search, YouTube, AdWords, Play, Maps, and Social.

**Do Conv-nets Dream of Psychedelic Sheep?**

The modern successes of deep learning are in part due to the universal approximation theorem developed by George Cybenko, Kurt Hornik, and others. The theorem in essence states that a neural network with at least one hidden layer and a non-linear activation function can generally approximate arbitrary continuous functions. In the past decade, the re-purposing of powerful GPUs for training deep networks unleashed the potential of the universal approximation theorem, fueling many new areas of business and research. In a deep learning model, many hidden layers can be stacked one on top of another like a skyward-reaching Tower of Babel, and internal representations can come to represent complicated abstractions and feature hierarchies. These representations can be part of a model predicting anything from the seemingly inconsequential such as nonsensical astrophysics acronyms, streaming video preferences, and dating matches; to the potentially grave: credit risk ratings, medical diagnoses, and dating matches. Given their pervasiveness, it is more important than ever to understand how deep neural network models form internal representations of input data and make decisions. In practice, back-propagation through variants of stochastic gradient descent do learn to fit the training data pretty well, but there are no guarantees for global convergence and the input data and learning process can often interact in surprising ways.

**Data Quality Case Studies: How We Saved Clients Real Money Thanks to Data Validation**

Machine learning models grow more powerful every week, but the earliest models and the most recent state-of-the-art models share the exact same dependency: data quality. The maxim ‘garbage in – garbage out’ coined decades ago, continues to apply today. Recent examples of data verification shortcomings abound, including JP Morgan/Chase’s 2013 fiasco and this lovely list of Excel snafus. Brilliant people make data collection and entry errors all of the time, and that isn’t just our opinion (although we have plenty of personal experience with it); Kaggle did a survey of data scientists and found that ‘dirty data’ is the number one barrier for data scientists.

**Welcome to the Augmented Age**

The next economic revolution is a revolution of the mind. We’ve had these revolutions before. The invention of agriculture more than 10,000 years ago set a new benchmark for modernity. The invention of steam power triggered the Industrial Revolution, forever changing human labor between the 18th and 19th centuries. Science matured during this time, and the development of the assembly line made short work of manufacturing complicated products. That brings us to the digital revolution of the 1990s, which saw the proliferation of the modern internet and the severe mainstreaming of personal computers. Fast forward to the present day, and we are on the cusp of an Augmented Age that will see people work in conjunction with artificial intelligence technologies to maximize the quality of their work and personal lives. It will soon be so normal to enhance our cognitive capabilities with AI technology that it will define a new era of life and business!

**Quantifying Chatroom Toxicity**

The other night I was at an SF data science meetup hosted by Twitch (beautiful office and great food!) and I struck up a conversation with some software engineers there. It turns out they were on a ‘Safety’ team entirely dedicated to keeping chats clean and protecting streamers and viewers in the chatroom, which makes sense because the chat interactivity is arguably the core driving experience of Twitch. With this in mind, I set out to see if I could build my own version of a chat toxicity classifier (with the help of my good friend Randy Macaraeg). Today I’d like to walk you through that process!

**7 Tips for Dealing With Small Data**

We often hear that Big Data is the key to building successful machine learning projects. This is a major problem: Many organizations won’t have the data you need. How can we prototype and validate machine learning ideas without the most essential raw material? How can we efficiently obtain and create value with data when resources are sparse? At my workplace, we produce a lot of functional prototypes for our clients. Because of this, I often need to make Small Data go a long way. In this article, I’ll share 7 tips to improve your results when prototyping with small datasets.1. Realize that your model won’t generalize that well.2. Build good data infrastructure.3. Do some data augmentation.4. Generate some synthetic data.5. Beware of lucky splits.6. Use transfer learning.7. Try an ensemble of ‘weak learners’.

**Political Python**

The 2020 Democratic candidates for President will face off in debates starting Wednesday. Many of them are current or former members of Congress. All of them are vying to lead the country. As voters, wouldn’t it be extraordinary if we had a record of everything that they had said on the floor of the Senate or the House, over the course of their careers as politicians? And as data scientists, wouldn’t we like to extract their words, analyze them, and use them to make judgments or predictions about these Congresspeople as Presidential candidates?

**Neural Networks with push button, AI for all**

Neural Architecture Search with NASBench from Google Research – Can we design network architectures automatically, instead of relying on expert experience and knowledge?

**Ear Biometrics – Machine Learning a little further…**

Like other biometric using face, iris and finger, ear as a biometric contains a large amount of specific and unique features that allow for human identification. The ear morphology changes slightly after the age of 10 years and the medical studies have shown that significant changes in the shape of the ear happen only before the age of 8 years and after the age of 70 years. It does grow symmetrically in size and begins to bulge downwards as the person ages, but that is a measurable effect. Studies suggest that ear changes only 1.22 mm per year. Also, the color distribution of ear, unlike face, is almost uniform. The position of the ear is almost in the middle of the profile face. Ear data can be captured even without the awareness of the subject from a distance. Ear biometrics can stand as an excellent example for passive biometrics and does not need much cooperation from the subject, which meets the demand of the secrecy of authentication system present in the environment.

**Collaborative filtering to “predict” the efficacy of a drug**

In drug development, screening is a significant part of the early discovery process. The purpose of this screening is to find molecules that bind the target strong and specific enough to have clinical benefit. Scientists have tolled in the lab to synthesize all kinds of molecules, hoping to improve their binding strength. This process is oftentimes guided by an empirical understanding of the drug-target interaction. It is thus natural to ask whether it is possible to ‘design’ a drug judiciously just as how a smartphone is designed to achieve a predefined set of performance goals. With the advent of quantum mechanics and molecular dynamics, scientists have been able to predict the interaction between molecules in silico. However, it requires advanced domain knowledge and expensive computational resources. Advancement of high-throughput screening also contributed to the accumulation of a large repertoire of experimentally determined binding strengths over the past few decades. Still people wish to have a quicker and more accurate way of obtaining binding strengths.

**Everyday Life and Microprediction**

Do we have the right as individuals to use artificial intelligence to attempt to predict behaviour in everyday life? As an example should you be able to predict the risk of hiring a specific babysitter? In 2018 Predictim advertised a service that promised to vet possible babysitters by scanning their presence on the web, social media and online criminal databases. AI safety for personal use so to speak.

### Like this:

Like Loading...
