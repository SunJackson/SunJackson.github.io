---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://advanceddataanalytics.net/2018/09/22/if-you-did-not-already-know-490/
date:      2018-09-21
author:      Michael Laux
tags:
    - pooling
    - experiments
    - model
    - posterior
    - based
---

**Mini-batch Tempered MCMC (MINT-MCMC)** ![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
In this paper we propose a general framework of performing MCMC with only a mini-batch of data. We show by estimating the Metropolis-Hasting ratio with only a mini-batch of data, one is essentially sampling from the true posterior raised to a known temperature. We show by experiments that our method, Mini-batch Tempered MCMC (MINT-MCMC), can efficiently explore multiple modes of a posterior distribution. As an application, we demonstrate one application of MINT-MCMC as an inference tool for Bayesian neural networks. We also show an cyclic version of our algorithm can be applied to build an ensemble of neural networks with little additional training cost. … 

**Datmo** ![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
Workflow tools to help you experiment, deploy, and scale. By data scientists, for data scientists. Datmo is an open source model tracking and reproducibility tool for developers. Features• One command environment setup (languages, frameworks, packages, etc)• Tracking and logging for model config and results• Project versioning (model state tracking)• Experiment reproducibility (re-run tasks)• Visualize + export experiment history … 

**Ordinal Pooling Network (OPN)** ![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
In the framework of convolutional neural networks that lie at the heart of deep learning, downsampling is often performed with a max-pooling operation that however completely discards the information from other activations in a pooling region. To address this issue, a novel pooling scheme, Ordinal Pooling Network (OPN), is introduced in this work. OPN rearranges all the elements of a pooling region in a sequence and assigns different weights to all the elements based upon their orders in the sequence, where the weights are learned via the gradient-based optimisation. The results of our small-scale experiments on image classification task on MNIST database demonstrate that this scheme leads to a consistent improvement in the accuracy over max-pooling operation. This improvement is expected to increase in the deep networks, where several layers of pooling become necessary. … 





### Like this:

Like Loading...


*Related*

