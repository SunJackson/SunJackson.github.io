---
layout:     post
catalog: true
title:      Whats new on arXiv
subtitle:      转载自：https://advanceddataanalytics.net/2018/09/12/whats-new-on-arxiv-760/
date:      2018-09-12
author:      Michael Laux
tags:
    - models
    - modeling
    - modelling
    - learning
    - learned
---

**Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: Properties and Typology**

Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set.

**An iterative method for classification of binary data**

In today’s data driven world, storing, processing, and gleaning insights from large-scale data are major challenges. Data compression is often required in order to store large amounts of high-dimensional data, and thus, efficient inference methods for analyzing compressed data are necessary. Building on a recently designed simple framework for classification using binary data, we demonstrate that one can improve classification accuracy of this approach through iterative applications whose output serves as input to the next application. As a side consequence, we show that the original framework can be used as a data preprocessing step to improve the performance of other methods, such as support vector machines. For several simple settings, we showcase the ability to obtain theoretical guarantees for the accuracy of the iterative classification method. The simplicity of the underlying classification framework makes it amenable to theoretical analysis and studying this approach will hopefully serve as a step toward developing theory for more sophisticated deep learning technologies.

**Attentional Multi-Reading Sarcasm Detection**

Recognizing sarcasm often requires a deep understanding of multiple sources of information, including the utterance, the conversational context, and real world facts. Most of the current sarcasm detection systems consider only the utterance in isolation. There are some limited attempts toward taking into account the conversational context. In this paper, we propose an interpretable end-to-end model that combines information from both the utterance and the conversational context to detect sarcasm, and demonstrate its effectiveness through empirical evaluations. We also study the behavior of the proposed model to provide explanations for the model’s decisions. Importantly, our model is capable of determining the impact of utterance and conversational context on the model’s decisions. Finally, we provide an ablation study to illustrate the impact of different components of the proposed model.

**LDW-SCSA: Logistic Dynamic Weight based Sine Cosine Search Algorithm for Numerical Functions Optimization**

Particle swarm optimization (PSO) and Sine Cosine algorithm (SCA) have been widely used optimization methods but these methods have some disadvantages such as trapped local optimum point. In order to solve this problem and obtain more successful results than others, a novel logistic dynamic weight based sine cosine search algorithm (LDW-SCSA) is presented in this paper. In the LDW-SCSA method, logistic map is used as dynamic weight generator. Logistic map is one of the famous and widely used chaotic map in the literature. Search process of SCA is modified in the LDW-SCSA. To evaluate performance of the LDW-SCSA, the widely used numerical benchmark functions were utilized as test suite and other swarm optimization methods were used to obtain the comparison results. Superior performances of the LDW-SCSA are proved success of this method.

**The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure**
![](https://s0.wp.com/latex.php?latex=O%28%5Csqrt+n%29&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=O%28%5Csqrt+m%29&bg=ffffff&fg=000&s=0)


**Mobile Collaborative Spectrum Sensing for Heterogeneous Networks: A Bayesian Machine Learning Approach**

Spectrum sensing in a large-scale heterogeneous network is very challenging as it usually requires a large number of static secondary users (SUs) to obtain the global spectrum states. To tackle this problem, in this paper, we propose a new framework based on Bayesian machine learning. We exploit the mobility of multiple SUs to simultaneously collect spectrum sensing data, and cooperatively derive the global spectrum states. We first develop a novel non-parametric Bayesian learning model, referred to as beta process sticky hidden Markov model (BP-SHMM), to capture the spatial-temporal correlation in the collected spectrum data, where SHMM models the latent statistical correlation within each mobile SU’s time series data, while BP realizes the cooperation among multiple SUs. Bayesian inference is then carried out to automatically infer the heterogeneous spectrum states. Based on the inference results, we also develop a new algorithm with a refinement mechanism to predict the spectrum availability, which enables a newly joining SU to immediately access the unoccupied frequency band without sensing. Simulation results show that the proposed framework can significantly improve spectrum sensing performance compared with the existing spectrum sensing techniques.

**Approximation and Estimation for High-Dimensional Deep Learning Networks**
![](https://s0.wp.com/latex.php?latex=%5Cell%5E1&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5B%28L%5E3+%5Clog+d%29%2Fn%5D%5E%7B1%2F2%7D&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=d&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=L&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Cell%5E1&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=L%5E3%5Clog+d&bg=ffffff&fg=000&s=0)


**Cloud Index Tracking: Enabling Predictable Costs in Cloud Spot Markets**

Cloud spot markets rent VMs for a variable price that is typically much lower than the price of on-demand VMs, which makes them attractive for a wide range of large-scale applications. However, applications that run on spot VMs suffer from cost uncertainty, since spot prices fluctuate, in part, based on supply, demand, or both. The difficulty in predicting spot prices affects users and applications: the former cannot effectively plan their IT expenditures, while the latter cannot infer the availability and performance of spot VMs, which are a function of their variable price. To address the problem, we use properties of cloud infrastructure and workloads to show that prices become more stable and predictable as they are aggregated together. We leverage this observation to define an aggregate index price for spot VMs that serves as a reference for what users should expect to pay. We show that, even when the spot prices for individual VMs are volatile, the index price remains stable and predictable. We then introduce cloud index tracking: a migration policy that tracks the index price to ensure applications running on spot VMs incur a predictable cost by migrating to a new spot VM if the current VM’s price significantly deviates from the index price.

**A Deep Reinforced Sequence-to-Set Model for Multi-Label Text Classification**

Multi-label text classification (MLTC) aims to assign multiple labels to each sample in the dataset. The labels usually have internal correlations. However, traditional methods tend to ignore the correlations between labels. In order to capture the correlations between labels, the sequence-to-sequence (Seq2Seq) model views the MLTC task as a sequence generation problem, which achieves excellent performance on this task. However, the Seq2Seq model is not suitable for the MLTC task in essence. The reason is that it requires humans to predefine the order of the output labels, while some of the output labels in the MLTC task are essentially an unordered set rather than an ordered sequence. This conflicts with the strict requirement of the Seq2Seq model for the label order. In this paper, we propose a novel sequence-to-set framework utilizing deep reinforcement learning, which not only captures the correlations between labels, but also reduces the dependence on the label order. Extensive experimental results show that our proposed method outperforms the competitive baselines by a large margin.

**Memristive LSTM network hardware architecture for time-series predictive modeling problem**

Analysis of time-series data allows to identify long-term trends and make predictions that can help to improve our lives. With the rapid development of artificial neural networks, long short-term memory (LSTM) recurrent neural network (RNN) configuration is found to be capable in dealing with time-series forecasting problems where data points are time-dependent and possess seasonality trends. Gated structure of LSTM cell and flexibility in network topology (one-to-many, many-to-one, etc.) allows to model systems with multiple input variables and control several parameters such as the size of the look-back window to make a prediction and number of time steps to be predicted. These make LSTM attractive tool over conventional methods such as autoregression models, the simple average, moving average, naive approach, ARIMA, Holt’s linear trend method, Holt’s Winter seasonal method, and others. In this paper, we propose a hardware implementation of LSTM network architecture for time-series forecasting problem. All simulations were performed using TSMC 0.18um CMOS technology and HP memristor model.

**Studying Confirmation Bias in Hashtag Usage on Twitter**

The micro-blogging platform Twitter allows its nearly 320 million monthly active users to build a network of follower connections to other Twitter users (i.e., followees) in order to subscribe to content posted by these users. With this feature, Twitter has become one of the most popular social networks on the Web and was also the first platform that offered the concept of hashtags. Hashtags are freely-chosen keywords, which start with the hash character, to annotate, categorize and contextualize Twitter posts (i.e., tweets). Although hashtags are widely accepted and used by the Twitter community, the heavy reuse of hashtags that are popular in the personal Twitter networks (i.e., own hashtags and hashtags used by followees) can lead to filter bubble effects and thus, to situations, in which only content associated with these hashtags are presented to the user. These filter bubble effects are also highly associated with the concept of confirmation bias, which is the tendency to favor and reuse information that confirms personal preferences. One example would be a Twitter user who is interested in political tweets of US president Donald Trump. Depending on the hashtags used, the user could either be stuck in a pro-Trump (e.g., #MAGA) or contra-Trump (e.g., #fakepresident) filter bubble. Therefore, the goal of this paper is to study confirmation bias and filter bubble effects in hashtag usage on Twitter by treating the reuse of hashtags as a phenomenon that fosters confirmation bias.

**Classification by Re-generation: Towards Classification Based on Variational Inference**

As Deep Neural Networks (DNNs) are considered the state-of-the-art in many classification tasks, the question of their semantic generalizations has been raised. To address semantic interpretability of learned features, we introduce a novel idea of classification by re-generation based on variational autoencoder (VAE) in which a separate encoder-decoder pair of VAE is trained for each class. Moreover, the proposed architecture overcomes the scalability issue in current DNN networks as there is no need to re-train the whole network with the addition of new classes and it can be done for each class separately. We also introduce a criterion based on Kullback-Leibler divergence to reject doubtful examples. This rejection criterion should improve the trust in the obtained results and can be further exploited to reject adversarial examples.

**Automated Test Generation to Detect Individual Discrimination in AI Models**

Dependability on AI models is of utmost importance to ensure full acceptance of the AI systems. One of the key aspects of the dependable AI system is to ensure that all its decisions are fair and not biased towards any individual. In this paper, we address the problem of detecting whether a model has an individual discrimination. Such a discrimination exists when two individuals who differ only in the values of their protected attributes (such as, gender/race) while the values of their non-protected ones are exactly the same, get different decisions. Measuring individual discrimination requires an exhaustive testing, which is infeasible for a non-trivial system. In this paper, we present an automated technique to generate test inputs, which is geared towards finding individual discrimination. Our technique combines the well-known technique called symbolic execution along with the local explainability for generation of effective test cases. Our experimental results clearly demonstrate that our technique produces 3.72 times more successful test cases than the existing state-of-the-art across all our chosen benchmarks.

**Privacy-Preserving Deep Learning for any Activation Function**

This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from existing systems in the following features: {\bf (1)} any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; {\bf (2)} gradients computed by SGD are not shared but the weight parameters are shared instead; and {\bf (3)} robustness against colluding parties even in the extreme case that only one honest party exists. We prove that our systems, while privacy-preserving, achieve the same learning accuracy as SGD and hence retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets, and show that our systems outperform previous system in terms of learning accuracies.

**Torchbearer: A Model Fitting Library for PyTorch**

**Probabilistic Binary Neural Networks**

Low bit-width weights and activations are an effective way of combating the increasing need for both memory and compute power of Deep Neural Networks. In this work, we present a probabilistic training method for Neural Network with both binary weights and activations, called BLRNet. By embracing stochasticity during training, we circumvent the need to approximate the gradient of non-differentiable functions such as sign(), while still obtaining a fully Binary Neural Network at test time. Moreover, it allows for anytime ensemble predictions for improved performance and uncertainty estimates by sampling from the weight distribution. Since all operations in a layer of the BLRNet operate on random variables, we introduce stochastic versions of Batch Normalization and max pooling, which transfer well to a deterministic network at test time. We evaluate the BLRNet on multiple standardized benchmarks.

**A Moral Framework for Understanding of Fair ML through Economic Models of Equality of Opportunity**

Equality of opportunity (EOP) is an extensively studied conception of fairness in political philosophy. In this work, we map recently proposed notions of algorithmic fairness to economic models of EOP. We formally show that through our proposed mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and also interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new, more general family of measures for algorithmic fairness. We empirically show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences on the subjects’ welfare.

**Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space**

Capturing the semantic relations of words in a vector space contributes to many natural language processing tasks. One promising approach exploits lexico-syntactic patterns as features of word pairs. In this paper, we propose a novel model of this pattern-based approach, neural latent relational analysis (NLRA). NLRA can generalize co-occurrences of word pairs and lexico-syntactic patterns, and obtain embeddings of the word pairs that do not co-occur. This overcomes the critical data sparseness problem encountered in previous pattern-based models. Our experimental results on measuring relational similarity demonstrate that NLRA outperforms the previous pattern-based models. In addition, when combined with a vector offset model, NLRA achieves a performance comparable to that of the state-of-the-art model that exploits additional semantic relational data.

**Keep it stupid simple**

Deep reinforcement learning can match and exceed human performance, but if even minor changes are introduced to the environment artificial networks often can’t adapt. Humans meanwhile are quite adaptable. We hypothesize that this is partly because of how humans use heuristics, and partly because humans can imagine new and more challenging environments to learn from. We’ve developed a model of hierarchical reinforcement learning that combines both these elements into a stumbler-strategist network. We test transfer performance of this network using Wythoff’s game, a gridworld environment with a known optimal strategy. We show that combining imagined play with a heuristic–labeling each position as ‘good’ or ‘bad”–both accelerates learning and promotes transfer to novel games, while also improving model interpretability.

**Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud**

The increasing demand for on-device deep learning services calls for a highly efficient manner to deploy deep neural networks (DNNs) on mobile devices with limited capacity. The cloud-based solution is a promising approach to enabling deep learning applications on mobile devices where the large portions of a DNN are offloaded to the cloud. However, revealing data to the cloud leads to potential privacy risk. To benefit from the cloud data center without the privacy risk, we design, evaluate, and implement a cloud-based framework ARDEN which partitions the DNN across mobile devices and cloud data centers. A simple data transformation is performed on the mobile device, while the resource-hungry training and the complex inference rely on the cloud data center. To protect the sensitive information, a lightweight privacy-preserving mechanism consisting of arbitrary data nullification and random noise addition is introduced, which provides strong privacy guarantee. A rigorous privacy budget analysis is given. Nonetheless, the private perturbation to the original data inevitably has a negative impact on the performance of further inference on the cloud side. To mitigate this influence, we propose a noisy training method to enhance the cloud-side network robustness to perturbed data. Through the sophisticated design, ARDEN can not only preserve privacy but also improve the inference performance. To validate the proposed ARDEN, a series of experiments based on three image datasets and a real mobile application are conducted. The experimental results demonstrate the effectiveness of ARDEN. Finally, we implement ARDEN on a demo system to verify its practicality.

**A collection of database industrial techniques and optimization approaches of database operations**

Databases play an essential role in our society today. Databases are embedded in sectors like corporations, institutions, and government organizations, among others. These databases are used for our video and audio streaming platforms, social gaming, finances, cloud storage, e-commerce, healthcare, economy, etc. It is therefore imperative that we learn how to properly execute database operations and efficiently implement methodologies so that we may optimize the performance of databases.

**Event Graphs: Advances and Applications of Second-Order Time-Unfolded Temporal Network Models**

Recent advances in data collection and storage have allowed both researchers and industry alike to collect data in real time. Much of this data comes in the form of ‘events’, or timestamped interactions, such as email and social media posts, website clickstreams, or protein-protein interactions. This of type data poses new challenges for modelling, especially if we wish to preserve all temporal features and structure. We propose a generalised framework to explore temporal networks using second-order time-unfolded models, called event graphs. Through examples we demonstrate how event graphs can be used to understand the higher-order topological-temporal structure of temporal networks and capture properties of the network that are unobserved when considering either a static (or time-aggregated) model. Furthermore, we show that by modelling a temporal network as an event graph our analysis extends easily to consider non-dyadic interactions, known as hyper-events.

**Physics-Informed Kriging: A Physics-Informed Gaussian Process Regression Method for Data-Model Convergence**

In this work, we propose a new Gaussian process regression (GPR) method: physics-informed Kriging (PhIK). In the standard data-driven Kriging, the unknown function of interest is usually treated as a Gaussian process with assumed stationary covariance with hyperparameters estimated from data. In PhIK, we compute the mean and covariance function from realizations of available stochastic models, e.g., from realizations of governing stochastic partial differential equations solutions. Such a constructed Gaussian process generally is non-stationary, and does not assume a specific form of the covariance function. Our approach avoids the costly optimization step in data-driven GPR methods to identify the hyperparameters. More importantly, we prove that the physical constraints in the form of a deterministic linear operator are guaranteed in the resulting prediction. We also provide an error estimate in preserving the physical constraints when errors are included in the stochastic model realizations. To reduce the computational cost of obtaining stochastic model realizations, we propose a multilevel Monte Carlo estimate of the mean and covariance functions. Further, we present an active learning algorithm that guides the selection of additional observation locations. The efficiency and accuracy of PhIK are demonstrated for reconstructing a partially known modified Branin function and learning a conservative tracer distribution from sparse concentration measurements.

• On Solving Linear Systems in Sublinear Time• Evidence-based lean logic profiles for conceptual data modelling languages• Distribution-aware Block-sparse Recovery via Convex Optimization• Cyclotomy, difference sets, sequences with low correlation, strongly regular graphs, and related geometric substructures• Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability• Universal Barrier is $n$-Self-Concordant• Online Learning for Position-Aided Millimeter Wave Beam Training• Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?• Fingertip Detection and Tracking for Recognition of Air-Writing in Videos• Leveraging Elastic Demand for Forecasting• Stochastic Gradient Descent Learns State Equations with Nonlinear Activations• Development of a Social Network for Research Support and Individual Well-being Improvement• A Public-Key Cryptosystem from Interleaved Goppa Codes• Variational Bayes inference in high-dimensional time-varying parameter models• Simulated Annealing for Optimal Resource Allocation in Wireless Networks with ImperfectCommunications• Variational Inference for Stochastic Control of Infinite Dimensional Systems• A Neural Temporal Model for Human Motion Prediction• Fairness-Aware Recommendation of Information Curators• How clever is the FiLM model, and how clever can it be?• Randomized Iterative Algorithms for Fisher Discriminant Analysis• A Social Recommender System based on Bhattacharyya Coefficient• Clustering of graph vertex subset via Krylov subspace model reduction• Comparing the power of cops to zombies in pursuit-evasion games• TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade• Large-Scale Spectrum Allocation for Cellular Networks via Sparse Optimization• SEGA: Variance Reduction via Gradient Sketching• SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation• Variance Reduction in Monte Carlo Counterfactual Regret Minimization (VR-MCCFR) for Extensive Form Games using Baselines• Active Inverse Reward Design• Analysis of the generalization error: Empirical risk minimization over deep artificial neural networks overcomes the curse of dimensionality in the numerical approximation of Black-Scholes partial differential equations• Learning in time-varying games• A case for deep learning in semantics• Sample Complexity of Nonparametric Semi-Supervised Learning• Online Convex Optimization for Sequential Decision Processes and Extensive-Form Games• An Optimization-Based Generative Model of Power Laws Using a New Information Theory Based Metric• Fluctuation-induced Distributed Resonances in Oscillatory Networks• Sensitivity of the frog model to initial conditions• Stabilization of regime-switching processes by feedback control based on discrete time observations II: state-dependent case• Efficient Counterfactual Learning from Bandit Feedback• Weak convergence of path-dependent SDEs with irregular coefficients• FKN theorem for the multislice, with applications• Parameterized Games and Parameterized Automata• A Simplicial Complex Model for Dynamic Epistemic Logic to study Distributed Task Computability• Tester versus Bug: A Generic Framework for Model-Based Testing via Games• Parameterized Verification of Coverability in Well-Structured Broadcast Networks• Complexity of Timeline-Based Planning over Dense Temporal Domains: Exploring the Middle Ground• On Computing the Measures of First-Order Definable Sets of Trees• Maximum Pairwise Bayes Factors for Covariance Structure Testing• Regular omega-Languages with an Informative Right Congruence• Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction• Second-Order Adversarial Attack and Certifiable Robustness• Mixed-ADC/DAC Multipair Massive MIMO Relaying Systems: Performance Analysis and Power Optimization• Comparison of Sobol’ sequences in financial applications• Stack-Sorting Preimages of Permutation Classes• The LKPY Package for Recommender Systems Experiments: Next-Generation Tools and Lessons Learned from the LensKit Project• M-convex Function Minimization Under L1-Distance Constraint• Monitoring data quality for telehealth systems in the presence of missing data• Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation• On Privacy of Quantized Sensor Measurements through Additive Noise• A Linear Approach to Fault Analysis and Intervention in Boolean Systems• Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers• Deep MR Image Super-Resolution Using Structural Priors• Computational Aspects of Optimal Strategic Network Diffusion• Fast and Efficient Information Transmission with Burst Spikes in Deep Spiking Neural Networks• A Stochastic Game Framework for Analyzing Computational Investment Strategies in Distributed Computing with Application to Blockchain Mining• Optimal variable selection and adaptive noisy Compressed Sensing• Learning to Advertise with Adaptive Exposure via Constrained Two-Level Reinforcement Learning• A Multi-Agent Reinforcement Learning Method for Impression Allocation in Online Display Advertising• Learning Time Dependent Choice• Extension and Application of Deleting Items and Disturbing Mesh Theorem of Riemann Integral• Resilience Bounds of Sensing-Based Network Clock Synchronization• Dynamic interpolation for obstacle avoidance on Riemannian manifolds• Short-term meaning shift: an exploratory distributional analysis• The AAU Multimodal Annotation Toolboxes: Annotating Objects in Images and Videos• Heart Rate Estimation from Ballistocardiography Based on Hilbert Transform and Phase Vocoder• Geoseg: A Computer Vision Package for Automatic Building Segmentation and Outline Extraction• Adaptive Approximation Error Models for Efficient Uncertainty Quantification with Application to Multiphase Subsurface Fluid Flow• Inverse zero-sum problems for certain groups of rank three• A subgeometric convergence formula for finite-level M/G/1-type Markov chains via the Poisson equation of the deviation matrix• Towards one-shot learning for rare-word translation with external experts• Shallow vs deep learning architectures for white matter lesion segmentation in the early stages of multiple sclerosis• Off-line vs. On-line Evaluation of Recommender Systems in Small E-commerce• A note on concentration for polynomials in the Ising model• OpenMP Loop Scheduling Revisited: Making a Case for More Schedules• Zero-Crossing Waveform Interferometry: an Alternative to Correlation in Signal Processing• Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks• Improving Response Selection in Multi-turn Dialogue Systems• Learning to Generate Structured Queries from Natural Language with Indirect Supervision• Decentralized Cooperative Planning for Automated Vehicles with Continuous Monte Carlo Tree Search• Learning Sequence Encoders for Temporal Knowledge Graph Completion• Beyond the Selected Completely At Random Assumption for Learning from Positive and Unlabeled Data• Confinement of Brownian Polymers under Geometric Area Tilts• Towards JointUD: Part-of-speech Tagging and Lemmatization using Recurrent Neural Networks• Adaptive Behavior Generation for Autonomous Driving using Deep Reinforcement Learning with Compact Semantic States• Delivery-Secrecy Tradeoff for Cache-Enabled Stochastic Networks: Content Placement Optimization• Hand-tremor frequency estimation in videos• Gait learning for soft microrobots controlled by light fields• Revisiting asymptotic periodicity in networks of degrade-and-fire oscillators• Dynamical transition in the TASEP with Langmuir kinetics: mean-field theory• Multi-Context Deep Network for Angle-Closure Glaucoma Screening in Anterior Segment OCT• Finding Better Topologies for Deep Convolutional Neural Networks by Evolution• Hyperbolic polynomials and the Kadison-Singer problem• Diffusion and superdiffusion in lattice models of colliding particles with stored momentum• Using phase instead of optical flow for action recognition• The Skiplist-Based LSM Tree• Proceedings of the 26th International Symposium on Graph Drawing and Network Visualization (GD 2018)• Fair splittings by independent sets in sparse graphs• Multilingual Extractive Reading Comprehension by Runtime Machine Translation• DistdichoR a R Package for the distributional dichotomisation of continuous outcomes• Unified spectral hamiltonian results of balanced bipartite graphs and complementary graphs• Properties of stepwise irregular graphs• A Brief Review of Real-World Color Image Denoising• Classification of grasping tasks based on EEG-EMG coherence• On the Optimality of Treating Inter-Cell Interference as Noise in Uplink Cellular Networks• Interactive Binary Image Segmentation with Edge Preservation• A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications• xSense: Learning Sense-Separated Sparse Representations and Textual Definitions for Explainable Word Sense Networks• Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks• Improving Optimization Bounds using Machine Learning: Decision Diagrams meet Deep Reinforcement Learning• The ancestral matrix of a rooted tree• Wideband Hybrid Precoding for Next-Generation Backhaul/Fronthaul Based on mmWave FD-MIMO• Estimating Formation Mechanisms and Degree Distributions in Mixed Attachment Networks• Machine Learning Based Hybrid Precoding for MmWave MIMO-OFDM with Dynamic Subarray• The discrete Gaussian free field on a compact manifold• SPASS: Scientific Prominence Active Search System with Deep Image Captioning Network• Large deviation analysis for classes of interacting Bosonic cycle counts• The Coordinate Sampler: A Non-Reversible Gibbs-like MCMC Sampler• Joint Beamforming and Association Design for MIMO Radar• Toward a Standardized and More Accurate Indonesian Part-of-Speech Tagging• Optimal Partition of a Tree with Social Distance• A Markov-Switching Model Approach to Heart Sound Segmentation and Classification• Does Your Phone Know Your Touch?• Jointly Learning to See, Ask, and GuessWhat• Exact lower tail large deviations of the KPZ equation• Filling Missing Paths: Modeling Co-occurrences of Word Pairs and Dependency Paths for Recognizing Lexical Semantic Relations• Monocular Object and Plane SLAM in Structured Environments• Identifying Relationships Among Sentences in Court Case Transcripts Using Discourse Relations• The smallest strictly Neumaier graph and its generalisations• Strongly regular graphs from weakly regular plateaued functions• An extension of Stanley’s chromatic symmetric function to binary delta-matroids• Inferring Influence Networks from Longitudinal Bipartite Relational Data• Periodic structure of translational multi-tilings in the plane• Inverse-Consistent Deep Networks for Unsupervised Deformable Image Registration• Expert-augmented actor-critic for ViZDoom and Montezumas Revenge• CLT for fluctuations of linear statistics in the Sine-beta process• Exploring Machine Reading Comprehension with Explicit Knowledge• Deep Single-View 3D Object Reconstruction with Visual Hull Embedding• The Pareto Frontier of Inefficiency in Mechanism Design• A stochastic game and stochastic free boundary problem• Steady state clusters and the Rath-Toth mean field forest fire model• ViZDoom Competitions: Playing Doom from Pixels• Multi-party Poisoning through Generalized $p$-Tampering• SpRRAM: A Predefined Sparsity Based Memristive Neuromorphic Circuit for Low Power Application• Towards a Fatality-Aware Benchmark of Probabilistic Reaction Prediction in Highly Interactive Driving Scenarios• Relay-Aided Secure Broadcasting for Visible Light Communications• Longitudinal Safety Analysis For Heterogeneous Platoon Of Automated And Human Vehicles• Multi-view Models for Political Ideology Detection of News Articles• The HyperKron Graph Model for higher-order features





### Like this:

Like Loading...


*Related*

