---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://advanceddataanalytics.net/2018/09/11/distilled-news-856/
date:      2018-09-11
author:      Michael Laux
tags:
    - training
    - trained
    - learning
    - data
    - models
---

**BaiDu Machine Learning Platform: EZDL Custom Image Recognition**

BaiDu announced a new Machine Learning Platform: Build custom machine learning models without having to code.Steps to Use the Platform1. Create a model2. Upload and label images/objects3. Train and test the model4. Get the model deployed with cloud API or offline SDK

**Graph-based intelligence analysis**

For decades, the intelligence community has been collecting and analyzing information to produce timely and actionable insights for intelligence consumers. But as the amount of information collected increases, analysts are facing new challenges in terms of data processing and analysis. In this article, we explore the possibilities that graph technology is offering for intelligence analysis.

**A step-by-step guide to build a data catalog**

A data catalog helps companies organize and find data that´s stored in their many systems. It works a lot like a fashion catalog. But instead of detailing swimsuits or shoes, it has information about tables, files, and databases from a company´s ERP, HR, Finance, and E-commerce systems (as well as social media feeds). The catalog also shows where all the data entities are located. A data catalog contains lots of critical information about each piece of data, such as the data´s profile (statistics or informative summaries about the data), lineage (how the data is generated), and what others say about it. The catalog is the go-to spot for analysts and others, who are trying to find data to build insights, discover trends, and identify new products for the company.

**Simple Experiments with Smoothed Scatterplots**

Scatter plots reveal relationships between two variables X, Y when both are numeric variables. Traditional scatter plots suffer from datapoints overlapping as the number of (Xi, Yi) pairs increases. Datapoints overlapping makes the relationship between the two variables difficult to discern.

**Text-to-Speech for Low-Resource Languages (Episode 4): One Down, 299 to Go**

This is the fourth episode in the series of posts reporting on the work we are doing to build text-to-speech (TTS) systems for low resource languages. In the first episode, we described the crowdsourced acoustic data collection effort for Project Unison. In the second episode, we described how we built parametric voices based on that data. In the third episode, we described the compilation of a pronunciation lexicon for a TTS system. In this episode, we describe how to make a single TTS system speak many languages.

**Training with Keras-MXNet on Amazon SageMaker**

In this post, you will learn how to train Keras-MXNet jobs on Amazon SageMaker. I´ll show you how to:• build custom Docker containers for CPU and GPU training,• configure multi-GPU training,• pass parameters to a Keras script,• save the trained models in Keras and MXNet formats.

**5 Things to Know About A/B Testing**

1. Don’t make conclusions based on small sample sizes2. Don’t overlook the psychology of A/B testing3. Beware the local minima; A/B testing isn’t for everything4. It’s all about the buckets5. Only include people in your analysis who could have been affected by the change

**Power Laws in Deep Learning**

Why Does Deep Learning Work ? If we could get a better handle on this, we could solve some very hard problems in Deep Learning. A new class at Stanford is trying to address this. Here, I review some very new results from my own research (w/UC Berkeley) on the topic which has direct practical application.

**Training, evaluating, and interpreting topic models**

At the beginning of this year, I wrote a blog post about how to get started with the stm and tidytext packages for topic modeling. I have been doing more topic modeling in various projects, so I wanted to share some workflows I have found useful for• training many topic models at one time,• evaluating topic models and understanding model diagnostics, and• exploring and interpreting the content of topic models.I’ve been doing all my topic modeling with Structural Topic Models and the stm package lately, and it has been GREAT. One thing I am not going to cover in this blog post is how to use document-level covariates in topic modeling, i.e., how to train a model with topics that can vary with some continuous or categorical characteristic of your documents. I hope to build up some posts about that, but in the meantime, you can check out the stm vignette and perhaps Carsten Schwemmer´s Shiny app for more details on this.

**Accelerating Deep Learning Using Distributed SGD — An Overview**

Training neural networks is hard and time-consuming. Often, training runs take days or even weeks until convergence. This hinders innovation and costs a lot of money for the companies involved in research and development of AI technologies. For this reason, a lot of brainpower recently went into improving neural network training speeds, in particular using paralellization. Stochastic Gradient Descent (SGD) and its multiple variants (such as RMSProp or Adam) are the most popular training algorithms for Deep Learning. These algorithms are inherently serial due to their iterative nature. Usually, the only paralellization to occur is the parallelization over pixels in Convolutional Neural Networks (CNNs) or parallelization over mini-batches. However, the size of the mini-batches is also a crucial parameter for the network’s accuracy and therefore cannot be altered at will. Furthermore, the size of the mini-batches is strongly limited by comuter memory (GPU memory if the algorithms run on GPUs). Due to these reasons, we need a fast and stable solution for parallelizing training over multiple independent nodes (computers) to achieve higher speedups. Being able to do this would also have a dramatic impact on the cost of development, as large commodity clusters (bunch of ordinary PCs) are typically much cheaper than a small number of highly specialized computers with custom hardware (such as GPUs or FPGAs). Additionally, scalability would no longer be an issue, and on-demand training with dynamic hardware scaling would become a real possibility.

**Another Machine Learning Walk-Through and a Challenge**

After spending considerable time and money on courses, books, and videos, I’ve arrived at one conclusion: the most effective way to learn data science is by doing data science projects. Reading, listening, and taking notes is valuable, but it’s not until you work through a problem that concepts solidify from abstractions into tools you feel confident using. In this article, I’ll present another machine learning walk-through in Python and also leave you with a challenge: try to develop a better solution (some helpful tips are included)! The complete Jupyter Notebook for this project can be run on Kaggle?-?no download required?-?or accessed on GitHub.

**An Overview of Monte Carlo Methods**

Monte Carlo (MC) methods are a subset of computational algorithms that use the process of repeated random sampling to make numerical estimations of unknown parameters. They allow for the modeling of complex situations where many random variables are involved, and assessing the impact of risk. The uses of MC are incredibly wide-ranging, and have led to a number of groundbreaking discoveries in the fields of physics, game theory, and finance. There are a broad spectrum of Monte Carlo methods, but they all share the commonality that they rely on random number generation to solve deterministic problems. I hope to outline some of the basic principles of MC, and perhaps infect you with a bit of the excitement that I have about their possible applications.

**Python Environment Setup for Deep Learning on Windows 10**

A detailed introduction on how to get started with Deep Learning starting with enabling an environment suited to it on the Microsoft Windows 10. The frameworks to be installed will be Keras API with Google’s TensorFlow GPU version as the back end engine. This guide is the same procedure I had utilized during my own deep learning project and it has served me well. The purpose of this guide is to accumulate all necessary and updated information in one place rather than searching all over Google. Let’s get started.

**Regularization in Machine Learning: Connect the dots**

Following are the various steps we will walk together and try gaining an understanding.1. Context2. Prerequisites3. Problem of Overfitting4. Objective5. What is Regularization?6. L2 Norm or Ridge Regularization7. L1 Norm or Lasso Regularization8. Lasso (L1) vs Ridge (L2) Regularizations9. Conclusion

**Neural networks and back-propagation explained in a simple way**

Any complex system can be abstracted in a simple way, or at least dissected to its basic abstract components. Complexity arises by the accumulation of several simple layers. The goal of this post, is to explain how neural networks work with the most simple abstraction. We will try to reduce the machine learning mechanism in NN to its basic abstract components. Unlike other posts that explain neural networks, we will try to use the least possible amount of mathematical equations and programming code, and focus only on the abstract concepts.





### Like this:

Like Loading...


*Related*

