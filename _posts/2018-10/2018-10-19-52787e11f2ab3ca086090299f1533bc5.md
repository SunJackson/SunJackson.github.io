---
layout:     post
catalog: true
title:      Holy Grail of AI for Enterprise — Explainable AI
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/DpV7hVarIwc/enterprise-explainable-ai.html
date:      2018-10-19
author:      Dan Clark
tags:
    - xai
    - ais
    - models
    - business
    - techs
---

**By Saurabh Kaushik**.

Having deployed about 20+ AI Solutions in past 10 years from building Intelligent Audience Measurement System for a Media Company in 2009 to Intelligent Financial Compliance System for large CPG customer in 2018, one skepticism stayed constant throughout with Enterprise customers — *Trustworthy Production Deployment of an AI System*. Yes, it is Holi Grail of AI and for the right reason; whether it about losing a High-Value customer due to wrong Churn Prediction or losing dollars due to incorrect classification of a financial transaction. In reality, Customers are the less bothered accuracy of AI model, but their concerns are about Cluelessness of Data Scientist to explain “How do I trust its decision making?”

![](https://cdn-images-1.medium.com/max/1600/1*7L8UdU5I18ARdGaDkbtK3w.png)


**AI Systems — How do I trust them?**In most AI enabled Digital Transformation, most customers are fascinated about having AI capability in their systems for certain Business Value Proposition. On other hands, Most Data Scientists are fascinated by applying most fashionable Algo (DNN/GAINS/DRN, etc). Sadly, both of these stakeholders forget one key consideration of Accountability and Trust factors. In real life, every decision made, either by Machine or by low-rank Employee or by CEO, all are subjected to due regular Scrutiny to explain their actions in order to improve overall Business systems/processes. This gives the rise of the Emerging Branch of AI, called “Explainable AI” (XAI).

#### What is Explainable AI (XAI)?

XAI is an emerging branch of AI where AI systems are made to explain the reasoning behind every decision made by them. Following is a simple depiction of the full circle of AI.

![](https://cdn-images-1.medium.com/max/1600/1*szl70qMtCfOrY36q3W5pqQ.png)


#### Additional Business Benefits :

Apart from a solution of the above scenarios, XAI offers deeper Business benefits, such as:

![](https://cdn-images-1.medium.com/max/1600/1*pRHaTfsf718UkC5-KXSxNQ.png)


**XAI — Business Benefits for Enterprise**

- Improves AI Model **performance**as explanation help pinpoint issues in data and feature behaviors.

- Better **Decision**Making as explanation provides added info and confidence for Man-in-Middle to act wisely and decisively.

- Gives a sense of **Control**as an AI system owner clearly knows levers for its AI system’s behavior and boundary.

- Gives a sense of **Safety**as each decision can be subjected to pass through safety guidelines and alerts on its violation.

- Build **Trust**with stakeholders who can see through all the reasoning of each and every decision made.

- Monitor for **Ethical**issues and violation due to bias in training data.Better mechanism to comply with Accountability requirements within the organization for auditing and other purposes.

- Better adherence to **Regulatory**requirements (like GDPR) where ‘Right to Explain’ is must-have for a system.


#### Roles in AI System Deployment:

Just like Dev-Ops, ML-Ops has another AI emerging field where dreaded deployment scenarios are being solved using tools and technology. But it is not about tool & tech rather about roles human plays around these AI systems. Broadly, we can define them in three buckets.

- **Trainer******— Trains the system to an expected functional behavior.

- **Explainer******— Explains the decision made by the system.

- **Sustainer******— Sustains the performance of AI system to business KPI.


![](https://cdn-images-1.medium.com/max/1600/1*ZYYsd2O9jZUl3LFcVILz7w.png)


**ML Ops — Roles for AI System**For both Trainer and Sustainer roles, there are plenty of tool-kit available in Data Scientist’s bag. But for Explainer, the situation is not as rosy. Here is the reason.

#### Black Box AI:

AI/ML algorithms are notoriously Black Box in nature due to its learning mechanism of their Weights and Bias from the large sum of non-linear nature of training data.

![](https://cdn-images-1.medium.com/max/1600/1*ls9QKeEbT0Xa3PuXcYN7Jw.png)


**Black Box AI and Problems**

#### XAI — Key Dimensions :

There are three key dimensions of XAI - Responsible, Traceable and Understandable AIs.

![](https://cdn-images-1.medium.com/max/1600/1*1iu6oudVrzZ9reDaY5JMYw.png)


**Three Dimensions of XAI**

#### XAI — Design Principals:

There are 8 general principals to adapt XAI from Conceptualization to Deployment of AI solutions.

![](https://cdn-images-1.medium.com/max/1600/1*2uabXjTk4LEJTPtu4Tz7hA.png)


**XAI — Design Principals**

#### XAI — Major Techniques:

There are two major techniques for XAI.

![](https://cdn-images-1.medium.com/max/1600/1*jauUmD9tjbG9TEx-B5AmMA.png)


**Model Specific Techniques:** In this, there are two sets, First where existing ML Algorithms are partially explainable and Second ones which are being researched & developed to have complete explainability., aka White Box Model. SPINE is particularly making news, but yet to make landfall in enterprise production.**Model Agnostic Techniques:** This one works outside the Operational Model by hacking into its working. A technique called LIME which basically try to estimate local boundaries for decision making.

I have used LIME library extensively in my past experiences coupled with Natural Language Generation tech to narrate for Sustainer and Operators.

Following shows the current state of Algorithms in Accuracy vs Explainability space along with approach directions.

![](https://cdn-images-1.medium.com/max/1600/1*c_57a1KNHRK5dkudFLA-FQ.png)


**XAI Approach — Model Specific vs Model Agnostic**

#### Future of XAI:

XAI field has a promising future to help enterprise deal with AI shortcomings. Here are a few directions for it.

- **Accurate Models:**XAI will both Sustainer and Trainer to improve their model and keep it going on a continuous basis.

- **Trustworthy Models:**XAI will help build confidence in Enterprise and other Regulated industries when XAI shows transparency into AI inner workings.

- **Natural Language Explanation:**XAI will have to explain its decision not through Analytical Chart or complex Dashboard, instead it has to communicate via Natural Language (Voice or Text). This would require NLG usage coupled with heuristics driven by XAI tech.

- **Adversarial Use (misuse):**XAI techs, like LIME and Deep Learning Tech like GAIN, can potentially decipher a given AI model externally. This will give rise to situations where the organization needs to protect their AI models using both Policy & Regulation as well as multi-layer Security measures.

- **Collaboration with Machine:**XAI will build bridges for trust so that we humans could adapt, work and excel with machine intelligence in our various endeavors.


Original. Reposted with permission.

**Bio**: Saurabh Kaushik is passionate about leading Digital Product Engineering using Artificial Intelligence and other digital tech, deploying them at global scale to make our clients more successful and their customers more delightful.

**Related:**



 
