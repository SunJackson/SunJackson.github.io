---
layout:     post
title:      Online Hard Example Mining on PyTorch
subtitle:   转载自：http://www.erogol.com/online-hard-example-mining-pytorch/
date:       2017-10-22
author:     erogol
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - hard
    - ohem
    - losses
    - computationally
    - computes
    - computed
    - instances
    - performance
    - performing
    - optimization
    - performs regular
    - detection
    - datasets
    - assumption
    - batch
    - negatives
    - examples
---

Online Hard Example Mining (OHEM) is a way to pick hard examples with reduced computation cost to improve your network performance on borderline cases which generalize to the general performance. It is mostly used for Object Detection. Suppose you like to train a car detector and you have positive (with car) and negative images (with no car). Now you like to train your network. In practice, you find yourself in many negatives as oppose to relatively much small positives. To this end, it is clever to pick a subset of negatives that are the most informative for your network. Hard Example Mining is the way to go to this.
![](http://www.erogol.com/wp-content/uploads/2017/10/DNIPj7iW0AAUKTV.jpg)


In general, to pick a subset of negatives, first you train your network for couple of iterations, then you run your network all along your negative instances then you pick the ones with the greater loss values. However, it is very computationally toilsome since you have possibly millions of images to process, and sub-optimal for your optimization since you freeze your network while picking your hard instances that are not all being used for the next couple of iterations. That is, you assume here all hard negatives you pick are useful for all the next iterations until the next selection. Which is an imperfect assumption especially for large datasets.

Okay, what Online means in this regard. OHEM solves these two aforementioned problems by performing hard example selection batch-wise. Given a batch sized K, it performs regular forward propagation and computes per instance losses. Then, it finds M<K hard examples in the batch with high loss values and it only back-propagates the loss computed over the selected instances. Smart hah ? ðŸ™‚

It reduces computation by running hand to hand with your regular optimization cycle. It also unties the assumption of the foreseen usefulness by picking hard examples per iteration so thus we now really pick the hard examples for each iteration.

If you like to test yourself, here is PyTorch OHEM implementation that I offer you to use a bit of grain of salt.
