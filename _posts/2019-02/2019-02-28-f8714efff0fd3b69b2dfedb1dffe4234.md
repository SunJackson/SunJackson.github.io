---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/02/28/distilled-news-988/
date:      2019-02-28
author:      Michael Laux
tags:
    - networks
    - ai reinforcement learning
    - planning
    - customers
    - data
---

**Docker for python development?**

In this article, we’ll be talking about how to start using Docker for python development.

**Convolutional Neural Networks**

Researchers came up with the concept of CNN or Convolutional Neural Network while working on image processing algorithms. Traditional fully connected networks were kind of a black box – that took in all of the inputs and passed through each value to a dense network that followed into a one hot output. That seemed to work with small set of inputs. But, when we work on a image of 1024×768 pixels, we have an input of 3x1024x768 = 2359296 numbers (RGB values per pixel). A dense multi layer neural network that consumes an input vector of 2359296 numbers would have at least 2359296 weights per neuron in the first layer itself?-?2MB of weights per neuron of the first layer. That would be crazy! For the processor as well as the RAM. Back in 1990’s and early 2000’s, this was almost impossible.

**Exploring PlaNet**

Planning has been long considered one of the cognitive abilities of the human mind that is nearly impossible to replicate by artificial intelligence(AI). Some neuroscientists even relate to future planning as one of the key characteristics of human consciousness. Planning does not only requires understanding a specific objective but also projecting that objective onto an environment whose characteristics are unknown in the present. Humans are able to plan not only because we are able to understand a specific task in detail but because our ability to understand our surrounding environment enough that we can project the outcome of that task in the future. In the context of AI, reinforcement learning is the discipline that has been trying to build long-term planning capabilities in AI agents. Recently, AI researchers from Google and DeepMind joined forces to work on a Deep Planning Network(PlaNet), a new reinforcement learning model that can learn about the world using images and utilize that knowledge for long-term planning.

**How to PyTorch in Production**

ML is fun, ML is popular, ML is everywhere. Most of the companies use either TensorFlow or PyTorch. There are some oldfags who prefer caffe, for instance. Mostly it’s all about Google vs Facebook battle. Most of my experience goes to PyTorch, even though most of the tutorials and online tutorials use TensofFlow (or hopefully bare numpy). Currently, at Lalafo (AI-driven classified), we are having fun with PyTorch. No, really, we tried caffe, it is awesome unless you have not spent a few days on installing it already. Even better, PyTorch is 1.0 now, we were using it from 0.3 and it was dead simple and robust. Ahhh.. maybe a few tweaks here, a few tweaks there. Most of the issues were easy to fix and did not cause any problems for us. Walk in the park, really. Here, I want to share the most common 5 mistakes for using PyTorch in production. Thinking about using a CPU? Multithreading? Using more GPU memory? We’ve gone through it. Now let me guide you too.

**Time Series Forecasting with Prophet**

Producing high quality forecasts is hard for many machine learning engineers. It requires a substantial amount of experience and and very specific skills. Also, other forecasting tools were too inflexible to incorporate useful assumptions. For those reasons, Facebook open sourced Prophet, a forecasting tool available in both Python and R. This tool allows both experts and non-experts to produce high quality forecasts with minimal efforts. Here, we will use Prophet to help us predict air quality!

**From Optimization to Prescriptive Analytics**

True prescriptive analytics requires the use of real optimization techniques that very few applications actually use. Here’s a refresher on optimization with examples of where and how they’re best used.

**What Should be the Analytics Organization Structure?**

Analytics organizations tend to be setup in very different ways dependent on the specific companies that set them up, be it in terms of reporting lines, segmented or central teams, or in terms of overall focus (project vs. product/business based). Each of these attributes brings its own set of tradeoff that need to be understood and managed.

**Advances in Generative Adversarial Networks (GANs)**

A summary of the latest advances in Generative Adversarial Networks

**Customer Segmentation Analysis with Python**

In this article I’ll explore a data set on mall customers to try to see if there are any discernible segments and patterns. Customer segmentation is useful in understanding what demographic and psychographic sub-populations there are within your customers in a business case. By understanding this, you can better understand how to market and serve them. This is similar and related but slightly different from the UX methodology of creating user personas: creating your ideal customers, their pain points, a defining quote, and so on, to understand their perspective.

**Sentiment Analysis using LSTM Step-by-Step**

I think this result from google dictionary gives a very succinct definition. I don’t have to re-emphasize how important sentiment analysis has become. So, here we will build a classifier on IMDB movie dataset using a Deep Learning technique called RNN.

**Cleaner R Code with Functional Programming**

Due to a job switch, I am a recent R-to-Python convert. However, a few side-projects keep me switching between the two languages daily. In addition to giving me a headache, this daily back-and-forth has given me a lot of thought about programming paradigms. Specifically, I’ve really become an evangelist for the functional programming paradigm in R. I want to give a little insight as to what functional programming (FP) is and how it can give you superpowers (sort of).

**A Data Scientific Method**

How to take a pragmatic and goal-driven approach to data science The main aim of data science is simple: it is to extract value from data. This value could be of different forms in different contexts – but, usually, it comes in the form of better decisions.

**Animate intermediate results of your algorithm**

The outline of this post is as follows: We will first generate some artificial data to work with. This allows to visualize the behavior of the algorithm. The k-means criterion and an algorithm to optimize it is then presented and implemented in R in order to store the intermediate results in a dataframe. Last, the content of the dataframe is ploted dynamically with gganimate.

**Running R and Python in Jupyter**

This is to get you started with R and Python in Jupyter. You can work in R as you normally would. There are more to explore in Jupyter. An upcoming tutorial will focus on how to report with Jupyter using Markdown and nb-extensions.

**Automated Machine Learning is broken**

Automated Machine Learning (AutoML) according to Wikipedia: is automating the end-to-end process of applying machine learning to real-world problems.Typical machine learning application contains:• Data preprocessing• Feature extraction, engineering and selection• Algorithm selection• Hyperparameters optimizationA lot of steps! AutoML aims to automate them so you can spend your precious time on more challenging tasks. Sounds cool, but is it? Let’s look closer …

**People don’t trust AI. We need to change that.**

This past week, I had the pleasure of attending and speaking at THINK, IBM’s annual customer and developer conference that brought over 25,000 attendees to San Francisco. Of course, it’s next to impossible to sum up an event of that scale in a simple blog post?-?but I’d like to share a few key ideas that stuck out to me from the conversations and sessions in which I took part. One theme, in particular, was the focus of my time at THINK, and that was the question of our trust in artificially intelligent systems. To put it bluntly, most people don’t trust AI?-?at least, not enough to put it into production. A 2018 study conducted by The Economist found that 94% of business executives believe that adopting AI is important to solving strategic challenges; however, the MIT Sloan Management Review found in 2018 that only 18% of organizations are true AI ‘pioneers,’ having extensively adopted AI into their offerings and processes. This gap illustrates a very real usability problem that we have in the AI community: people want our technology, but it isn’t working for them in its current state. And I believe that lack of trust lies is one of the chief causes of this problem.

**Hypothesis Testing**

The main purpose of statistics is to test a hypothesis. For example, you might run an experiment and find that a certain drug is effective at treating headaches. But if you can’t repeat that experiment, no one will take your results seriously. A good example of this was the cold fusion discovery, which petered into obscurity because no one was able to duplicate the results.





### Like this:

Like Loading...


*Related*

