---
layout:     post
catalog: true
title:      Investigating words distribution with R – Zipf’s law
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/OU0etIXyEAU/
date:      2019-02-07
author:      Michal Maj
tags:
    - word_count
    - rank
    - count alpha
    - law
    - data
---





![](https://i0.wp.com/appsilon.com/assets/uploads/2019/02/patrick-tomasso-71909-unsplash-600x400.jpg?w=450&ssl=1)
![](https://i0.wp.com/appsilon.com/assets/uploads/2019/02/patrick-tomasso-71909-unsplash-600x400.jpg?w=450&ssl=1)

![](https://www.google-analytics.com/collect?v=1&tid=UA-46680230-1&cid=*%7CUNIQID%7C*&t=event&ec=repost&ea=open&cm=reposting&cn=Investigating%20words%20distribution%20with%20R%20-%20Zipf%E2%80%99s%20law)
![](https://www.google-analytics.com/collect?v=1&tid=UA-46680230-1&cid=*%7CUNIQID%7C*&t=event&ec=repost&ea=open&cm=reposting&cn=Investigating%20words%20distribution%20with%20R%20-%20Zipf%E2%80%99s%20law)

Hello again! Typically I would start by describing a complicated problem that can be solved using machine or deep learning methods, but today I want to do something different, I want to show you some interesting probabilistic phenomena!

Have you heard of **Zipfâ€™s law**? I hadnâ€™t until recently. Zipfâ€™s law is an empirical law that states that many different datasets found in nature can be described using Zipfâ€™s distribution. Most notably, word frequencies in books, documents and even languages can be described in this way. Simplified, Zipfâ€™s law states that if we take a document, book or any collection of words and then the how many times each word occurs, their frequencies will be very similar to Zipfâ€™s distribution. Letâ€™s say that the number of occurrences of the most frequently occurring word is:

X

Zipfâ€™s law states that the number of occurrences of the second most frequently occurring word will be equal to:

X/2

So basically this word will occur half of the number of times the most frequent word did. The number of occurrences of the third most frequently occurring word would be:

X/3

And so on â€¦ So the number of occurrences of the Nth most frequent word would be:

X/N

Most recent studies of this phenomena show that in the case of words, typically there is the same value of ğ��ª, and the frequency on Nth word is described as:

X/Nğ��ª

To check the theory I downloaded a set of the 50,000 most frequent Polish words in subtitles (https://github.com/hermitdave/FrequencyWords/blob/master/content/2016/pl/pl_50k.txt) from *OpenSubtitles.org*. Hereâ€™s a visualization of real and theoretical frequencies.

![](https://i2.wp.com/appsilon.com/assets/uploads/2019/02/zipfs_law.gif?w=450&ssl=1)
![](https://i2.wp.com/appsilon.com/assets/uploads/2019/02/zipfs_law.gif?w=450&ssl=1)


To see it more clearly we can use logarithmic scales.

![](https://i0.wp.com/appsilon.com/assets/uploads/2019/02/zipfs_law_log.gif?w=450&ssl=1)
![](https://i0.wp.com/appsilon.com/assets/uploads/2019/02/zipfs_law_log.gif?w=450&ssl=1)


Try it out yourself: a list of example datasets can be found here: https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists

You can use this example code to create a similar visualization:

```

library(ggplot2)
library(dplyr)
library(themes)
library(gganimate)

word_count <- # Data frame containing words and their frequency 
colnames(word_count) <- c("word", "count")
alpha <- 1 # Change it needed
word_count <- word_count %>%
 mutate(word = factor(word, levels = word),
 rank = row_number(),
 zipfs_freq = ifelse(rank == 1, count, dplyr::first(count) / rank^alpha))

zipfs_plot <- ggplot(word_count, aes(x = rank, y = count)) + 
geom_point(aes(color = "observed")) +
 theme_bw() + 
geom_point(aes(y = zipfs_freq, color = "theoretical")) +
 transition_reveal(count, rank) + 
labs(x = "rank", y = "count", title = "Zipf's law visualization") +
 scale_colour_manual(name = "Word count", values=c("theoretical" = "red", "observed" = "black")) +
 theme(legend.position = "top")
zipfs_animation <- animate(p)

```

This experiment is amazing, because language is very complicated: words in text are not random in any sense, and they depend on the previous ones. Thatâ€™s why itâ€™s so surprising to see such patterns here. We should always remember that the world can astonish us in many different ways! See you next time ğŸ™‚

Article Investigating words distribution with R â€“ Zipfâ€™s law comes from Appsilon Data Science | EndÂ­ toÂ­ End Data Science Solutions.


*Related*








---
