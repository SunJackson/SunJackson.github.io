---
layout:     post
catalog: true
title:      Introducing Rank Data Analysis with Arkham Horror Data
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/D6BeyCPlX8E/
date:      2019-02-25
author:      ntguardian
tags:
    - ranked
    - ranks
    - clusters
    - cards
    - mystics
---





Introduction
Last week I analyzed player rankings of the Arkham Horror LCG classes. This week I explain what I did in the data analysis. As I mentioned, this is the first time that I attempted inference with rank data, and I discovered how rich the subject is. A lot of the tools for the analysis I had to write myself, so you now have the code I didnâ€™t have access to when I started.



This post will *not* discuss rank data modelling. Instead, it will cover what one may consider basic statistics and inference. The primary reference for what I did here is *Analyzing and Modeling Rank Data*, by John Marden. So far Iâ€™ve enjoyed his book and I may even buy a personal copy.

## What is Rank Data?

Suppose we have ![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
 objects we ask our study participants (also known as â€œjudgesâ€�) to rank. For example, suppose we asked people to rank apples, oranges, and bananas. What we then get is a prioritization of these objects according to our judges. This could come in the form

![](https://s0.wp.com/latex.php?latex=%283%2C+1%2C+2%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%283%2C+1%2C+2%29&bg=ffffff&%23038;fg=444444&%23038;s=0)


and we interpret the number in the ![](https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 position as the ranking of the ![](https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 item. In this case, if the tuple is in the order of apples, oranges, and bananas, then oranges recieved the highest ranking, bananas the second-highest, and apples the last position.

An alternative view of this data may be

![](https://s0.wp.com/latex.php?latex=%28%5Ctext%7Boranges%7D%2C+%5Ctext%7Bbananas%7D%2C+%5Ctext%7Bapples%7D%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%28%5Ctext%7Boranges%7D%2C+%5Ctext%7Bbananas%7D%2C+%5Ctext%7Bapples%7D%29&bg=ffffff&%23038;fg=444444&%23038;s=0)


where the items are arranged in order of preference. This form of describing a ranking has its uses, but we will consider only the first form in this introduction.

Ranking data has the following distinguishing characteristics from other data: first, the data is ordinal. All that matters is the order in which items were placed, not necessarily the numbers themselves. We could insist on writing rank data as ![](https://s0.wp.com/latex.php?latex=%28100%2C+1%2C+50%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%28100%2C+1%2C+50%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and the information content would not have changed. (But of course we would never do this.) Second, every item gets a ranking. This excludes â€œChoose your top 3 out of 50â€�-type questions, since not every item would receive a ranking (this is called an incomplete ranking and requires special care; I wonâ€™t discuss this type of data in this article). Finally, every itemâ€™s ranking is distinct; no ties are allowed.

Thus ranking data is distinct even from just ordinal data since data comes from judges in the form of a tuple, not just a single ordinal value. (Thus we would not consider, say, Likert scale responses as automatically being an instance of rank data.) An ideal method for rank data would account for this unique nature and exploit its features.

## Basic Descriptive Statistics

From this point on I will be working with the Arkham Horror player class ranking data. I made the `Timestamp` column nonsense to anonymize the data. You can download a CSV file of the data from here, then convert it to a `.Rda` file with the script below (which is intended to be run as an executable):

(The script with all the code for the actual analysis appears at the end of this article.)

The first statistic we will compute for this data is the marginals matrix. This matrix simply records the proportion of times an item received a particular ranking in the sample. If we want to get mathematical, if ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is a ranking tuple and ![](https://s0.wp.com/latex.php?latex=%5Comega_i&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega_i&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is the ranking of the ![](https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 option and the sample is ![](https://s0.wp.com/latex.php?latex=%5Comega%5E%7B%281%29%7D%2C+%5Cldots%2C+%5Comega%5E%7B%28n%29%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega%5E%7B%281%29%7D%2C+%5Cldots%2C+%5Comega%5E%7B%28n%29%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
, then the ![](https://s0.wp.com/latex.php?latex=ij%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=ij%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 entry of the marginalâ€™s matrix is

![](https://s0.wp.com/latex.php?latex=%5Chat%7BM%7D_%7Bij%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bl+%3D+1%7D%5En+I_%7B%5C%7B%5Comega%5E%7B%28l%29%7D_i+%3D+j%5C%7D%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7BM%7D_%7Bij%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bl+%3D+1%7D%5En+I_%7B%5C%7B%5Comega%5E%7B%28l%29%7D_i+%3D+j%5C%7D%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)


{% raw %}
where the function $\latex I_{{A}}$ is 1 if ![](https://s0.wp.com/latex.php?latex=A&bg=ffffff&%23038;fg=444444&%23038;s=0)
{% endraw %}
![](https://s0.wp.com/latex.php?latex=A&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is true and 0 otherwise. (Thus the sum above simply counts how many times ![](https://s0.wp.com/latex.php?latex=%5Comega%5E%7B%28l%29%7D_i&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega%5E%7B%28l%29%7D_i&bg=ffffff&%23038;fg=444444&%23038;s=0)
 was equal to ![](https://s0.wp.com/latex.php?latex=j&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=j&bg=ffffff&%23038;fg=444444&%23038;s=0)
.)

The marginals matrix for the Arkham Horror data is given below

Below is a visual representation of the marginals matrix.

![](https://ntguardian.files.wordpress.com/2019/02/marginal_plot.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/02/marginal_plot.png?w=456)


From the marginals matrix you could compute the vector representing the â€œmeanâ€� ranking of the data. For instance, the mean ranking of the Guardian class is the sum of the ranking numbers (column headers) times their respective proportions (in the Guardian row); here, thatâ€™s about 2.9 for Guardians. Repeat this process for every other group to get the mean ranking vector; here, the mean rank vector is ![](https://s0.wp.com/latex.php?latex=%282.92%2C+3.10%2C+3.16%2C+2.60%2C+3.22%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%282.92%2C+3.10%2C+3.16%2C+2.60%2C+3.22%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 (keeping the ordering of the classes suggested by the rows above, which is alphabetical order; this will always be the ordering I use unless otherwise stated.) Of couse this is not a ranking vectors; rankings are integers. The corresponding ranking vector would be to rank the means themselves; this gives a ranking vector of ![](https://s0.wp.com/latex.php?latex=%282%2C+3%2C+4%2C+1%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%282%2C+3%2C+4%2C+1%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
.

I donâ€™t like inference using the mean ranking vector. As mentioned above, this data is ordinal; that means the magnitude of the numbers themselves should not matter. We could replace 1, 2, 3, 4, 5 with 1, 10, 100, 1000, 10000 and the data would mean the same thing. That is *not* the case if youâ€™re using the mean rank unless you first apply a transformation to the rankings. In short, I donâ€™t think that the mean ranking vector appreciates the nature of the data well. And since the marginals matrix is closely tied to this notion of â€œmeanâ€�, I donâ€™t think the matrix is fully informative.

Another matrix providing descriptive statistics is the pairs matrix. The matrix records the proportion of respondents who preferred one option to the other (specifically, the row option to the column option). Mathematically, the ![](https://s0.wp.com/latex.php?latex=ij%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=ij%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 entry of the pairs matrix is

![](https://ntguardian.files.wordpress.com/2019/02/pairs_mat_formula.gif?w=456)
![](https://ntguardian.files.wordpress.com/2019/02/pairs_mat_formula.gif?w=456)


The pairs matrix for the Arkham Horror data is below:

First, notice that the diagonal entries are all zero; this will always be the case. Second, the pairs matrix is essentially completely determined by the entries above the diagonal of the matrix. Other forms of interence use these upper-diagonal entries and donâ€™t use the lower-diagonal entries since they give no new information. The number of upper-diagonal entries is ![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D+%3D+m%28m+-+1%29%2F2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D+%3D+m%28m+-+1%29%2F2&bg=ffffff&%23038;fg=444444&%23038;s=0)
, which is the number of ways to pick pairs of ![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
 classes.

The pairs matrix for the Arkham Horror data is visualized below.

![](https://ntguardian.files.wordpress.com/2019/02/pair_plot.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/02/pair_plot.png?w=456)


With the pairs matrix, crossing above or below 50% of the sample being in the bin is a significant event; it indicates which classes are preferred to the other. In fact, by counting how many times this threshold was crossed, we can estimate that the overall favorite class is the Seeker class, followed by Guardians, then Mystics, then Rogues, and finally Survivors. This is another estimate of the â€œcentralâ€�, â€œmodalâ€�, or â€œconsensusâ€� ranking. (This agrees with the â€œmeanâ€� ranking, but thatâ€™s not always going to be the case; the metrics can disagree with each other.)

While I did not like the marginals matrix I do like the pairs matrix; I feel as if it accounts for the features of rank data I want any measures or inference to take account of. It turns out that the pairs matrix is also related to my favorite distance metric for analyzing rank data.

## Distance Metrics for Rank Data

A *distance metric* is a generalized notion of distance, or â€œhow far awayâ€� two objects ![](https://s0.wp.com/latex.php?latex=x&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=x&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=444444&%23038;s=0)
 are. In order for a function ![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 to be a metric, it must have the following properties:

![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%5Cgeq+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%5Cgeq+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
 for all ![](https://s0.wp.com/latex.php?latex=x&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=x&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=444444&%23038;s=0)
.
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%3D+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%3D+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
 if and only if ![](https://s0.wp.com/latex.php?latex=x+%3D+y&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=x+%3D+y&bg=ffffff&%23038;fg=444444&%23038;s=0)
.
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%3D+d%28y%2C+x%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%3D+d%28y%2C+x%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 for all ![](https://s0.wp.com/latex.php?latex=x&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=x&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=444444&%23038;s=0)
.
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%5Cleq+d%28x%2C+z%29+%2B+d%28y%2C+z%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d%28x%2C+y%29+%5Cleq+d%28x%2C+z%29+%2B+d%28y%2C+z%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 for all ![](https://s0.wp.com/latex.php?latex=x%2C+y%2C+z&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=x%2C+y%2C+z&bg=ffffff&%23038;fg=444444&%23038;s=0)
 (the â€œtriangleinequalityâ€�)

The notion of distance you use in every-day life, the one taught in middle-school geometry and computed whenever you use a ruler, is known as Euclidean distance. Itâ€™s not the only notion of distance, though, and may not be the only distance function you use in real-life. For instance, Manhattan or taxi cab distance is the distance from one point to another when you can only make 90-degree turns and is the distance that makes the most sense when travelling in the city.

There are many distance metrics we could consider when working with rank data. The Spearman distance is the square of the Euclidean distance, while the footrule distance corresponds to the Manhattan distance. It turns out that the mean rank vector above minimizes the sum of Spearman distances. The distance metric I based my analysis on, though, was the Kendall distance. I like this distance metric since it is not connected to the mean and considers the distance between the rankings ![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%285%2C+2%2C+3%2C+4%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%285%2C+2%2C+3%2C+4%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 to be greater than the distance between ![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%282%2C+1%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%282%2C+1%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 (unlike, say, the Hamming distance, which gives the same distance in either case).

Kendallâ€™s distance even has an interpretation. Suppose that two ranking tuples are seen as the ordering of books on a bookshelf. We want to go from one ordering of books to another ordering of books. The Kendall distance is how many times we would need to switch adjacent pairs of books (chosen well, so as not to waste time and energy) to go from one ordering to the other. Thus the Kendall distance between ![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%282%2C+1%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%282%2C+1%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is one; we only need to make one swap. The distance between ![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%281%2C+2%2C+3%2C+4%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%285%2C+2%2C+3%2C+4%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%285%2C+2%2C+3%2C+4%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
, in comparison, is seven, since we need to make seven swaps.

It also turns out that the Kendall distance is related to the pairs matrix. The average Kendall distance of the judges from any chosen ranking ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is

![](https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%2C+j%3A+%5Comega_i+%3E+%5Comega_j%7D+%5Chat%7BK%7D_%7Bi%2Cj%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%2C+j%3A+%5Comega_i+%3E+%5Comega_j%7D+%5Chat%7BK%7D_%7Bi%2Cj%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)


(There is a similar expression relating the Spearman distance to the marginal matrix.)

## Central Ranking Estimator

Once we have a distance metric, we can define what the â€œbestâ€� estimate for the most central ranking is. The central ranking is the ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
 that minimizes

![](https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bl+%3D+1%7D%5E%7Bn%7D+d%28%5Comega%5E%7B%28l%29%7D%2C+%5Comega%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bl+%3D+1%7D%5E%7Bn%7D+d%28%5Comega%5E%7B%28l%29%7D%2C+%5Comega%29&bg=ffffff&%23038;fg=444444&%23038;s=0)


In other words, the most central ranking minimized the sum of distances of all the rankings in the data to that ranking.

Sometimes this ranking has already been determined. For instance, when using the Spearman distance, the central ranking emerges from the â€œmeanâ€� rankings. Otherwise, though, we may need to apply some search procedure to find this optimal ranking.

Since weâ€™re working with rank data, though, itâ€™s very tempting to not use any fancy optimization algorithms and simply compute the sum of distances for every possible ranking. This isnâ€™t a bad idea at all if the number of items being ranked is relatively small. Here, since there are five items being ranked, the number of possible rankings is ![](https://s0.wp.com/latex.php?latex=5%21+%3D+120&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=5%21+%3D+120&bg=ffffff&%23038;fg=444444&%23038;s=0)
, which is not too big for a modern computer to handle. It may take some time for the exhaustive search approach to yield and answer, but the answer produced by exhaustive search comes with the reassurance that it does, in fact, minimize the sum of distances.

This is in fact what I did for estimating the central ranking when minimizing the sum of Kendall distances from said ranking. The resulting ranking, again, was Seeker/Guardian/Mystic/Rogue/Survivor (which agrees with what we determined just by looking at the pairs matrix; this likely is not a coincidence).

## Statistical Inference

All of the above I consider falling into the category of descriptive statistics. It describes aspects of the sample without attempting to extrapolate to the rest of the population. With statistical inference we want to see what we can say about the population as a whole.

I should start by saying that the usual assumptions made in statistical inference are likely not satisfied by my sample. It was an opt-in sample; people *chose* to participate. That alone makes it a non-random sample. Additionally, only participants active on Facebook, Reddit, Twitter, Board Game Geek, and the Fantasy Flight forums were targeted by my advertising of the poll. Thus the Arkham players were likely those active on the Internet, likely at a particular time of day and day of the week (given how these websites try to push older content off the main page). They were likely young, male, and engaged enough in the game to be in the community (and unlikely to be a â€œcasualâ€� player). Thus the participants are likely to be more homogenous than the population of Arkham Horror players overall.

Just as a thought experiment, what would be a better study, one where we could feel confident in the inferential ability of our sample? Well, we would grab randomly selected people from the population (perhaps from pulling random names from the phone book), have them join our study, teach them how to play the game, make them play the game for many hours until they could form an educated opinion of the game (probably at least 100 hours), then ask them to rate the classes. This would be high-quality data and we could believe the data is reliable, but *damn* would it be expensive! No one at FFG would consider data of that quality worth the price, and frankly neither would I.

Having said that, while the sample I have is certainly flawed in how it was collected, I actually believe we can get good results from it. The opinions of the participants are likely educated ones, so we probably still have a good idea how the Arkham Horror classes compare to one another.

In rank data analysis there is a probability model called the *uniform distribution* that serves as a starting point for inference. Under the uniform distribution, every ranking vector is equally likely to be observed; in short, there is no preference among the judges among the choices. The marginals matrix should have all entries be ![](https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
, all off-diagonal entries of the pairs matrix should be ![](https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
, and any â€œcentralâ€� ranking is meaningless since every ranking is equally likely to be seen. According to the uniform distribution, ![](https://s0.wp.com/latex.php?latex=P%28%5C%7B%5Comega%5C%7D%29+%3D+%5Cfrac%7B1%7D%7Bm%21%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=P%28%5C%7B%5Comega%5C%7D%29+%3D+%5Cfrac%7B1%7D%7Bm%21%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
. If we cannot distinguish our data from data drawn from the uniform distribution, our work is done; we basically say there is no â€œcommonâ€� ranking scheme and go about our day.

There are many tests for checking for the uniform distribution, and they are often based on the statistics weâ€™ve already seen, such as the mean rank vector, the marginals matrix, and the pairs matrix. If ![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is small enough relative to the sample size, we could even just base a test off of how frequently each particular ranking was seen. A test based off the latter could detect any form of non-uniformity in the data, while tests based off the marginals or pairs matrices or the mean vector cannot detect all forms of non-uniformity; that said, they often require much less data to be performed.

As mentioned, I like working with the pairs matrix/Kendall distance. The statistical test, though, involves a vector ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
, which is the aforementioned upper triangle of the pairs matrix (excluding the diagonal entries which are always zero). (More specifically, ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is a vector containing the upper-diagonal entries of the pairs matrix laid out in row-major form.)

The test decides between

![](https://s0.wp.com/latex.php?latex=H_0%3A+%5Ctext%7BData+was+drawn+from+uniform+distribution%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=H_0%3A+%5Ctext%7BData+was+drawn+from+uniform+distribution%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)


![](https://s0.wp.com/latex.php?latex=H_A%3A+H_0+%5Ctext%7B+is+false%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=H_A%3A+H_0+%5Ctext%7B+is+false%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)


The test statistic is

![](https://s0.wp.com/latex.php?latex=12n%28%5C%7C%5Chat%7B%5Ckappa%7D+-+%5Cfrac%7B1%7D%7B2%7D+1_%7B%5Cbar%7Bm%7D%7D%5C%7C%5E2+-+%5C%7C%5Cbar%7By%7D+-+%5Cfrac%7Bm+%2B+1%7D%7B2%7D+1_m%5C%7C%5E2+%2F+%28m+%2B+1%29%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=12n%28%5C%7C%5Chat%7B%5Ckappa%7D+-+%5Cfrac%7B1%7D%7B2%7D+1_%7B%5Cbar%7Bm%7D%7D%5C%7C%5E2+-+%5C%7C%5Cbar%7By%7D+-+%5Cfrac%7Bm+%2B+1%7D%7B2%7D+1_m%5C%7C%5E2+%2F+%28m+%2B+1%29%29&bg=ffffff&%23038;fg=444444&%23038;s=0)


If the null hypothesis is true, then the test statistic, for large ![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&%23038;fg=444444&%23038;s=0)
, a ![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2&bg=ffffff&%23038;fg=444444&%23038;s=0)
 distribution with ![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 degrees of freedom. (For the Arkham Horror classes case, ![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D+%3D+10&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D+%3D+10&bg=ffffff&%23038;fg=444444&%23038;s=0)
.) Large test statistics are evidence against the null hypothesis, so ![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=444444&%23038;s=0)
-values are the area underneath the ![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2_%7B%5Cbar%7Bm%7D%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2_%7B%5Cbar%7Bm%7D%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 curve to the right of the test statistic.

For our data set, the reported test statistic was 2309938376; not shockingly, the corresponding ![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=444444&%23038;s=0)
-value is near zero. So the data was not drawn from the uniform distribution. Arkham Horror players do have class preferences.

But what are plausible preferences players could have? We can answer this using a confidence interval. Specifically, we want to know what *rankings* are plausible, and thus what we want is a confidence set of rankings.

Finding a formula for a confidence set of the central ranking is extremely hard to do, but itâ€™s not as hard to form one for one of the statistics we can compute from the rankings, then use the possible values of that statistic to find corresponding plausible central rankings. For example, once could find a confidence set for the mean ranking vector, then translate those mean rankings into ranking vectors (this is what Marden did in his book).

As I said before, I like the pairs matrix/Kendall distance in the rank data context, so I want to form a confidence set for ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
, the population equivalent of ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
, the key entries of the pairs matrix. To do this, we cannot view the rank data the same way we did before; instead of seeing the ![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
-dimensional vector ![](https://s0.wp.com/latex.php?latex=%282%2C+1%2C+4%2C+3%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%282%2C+1%2C+4%2C+3%2C+5%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
, we need to see the equivalent ![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
-dimensional vector ![](https://s0.wp.com/latex.php?latex=%280%2C+1%2C+1%2C+1%2C+1%2C+1%2C+1%2C+0%2C+1%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%280%2C+1%2C+1%2C+1%2C+1%2C+1%2C+1%2C+0%2C+1%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 that consists only of ones and zeros and records the pair-wise relationships among the ranks, rather than the ranks themselves (the latter vector literally says that item one is not ranked higher than item two, item one is ranked higher than item three, same for four, same for five, then that item two is ranked higher than item three, same for four, same for five, and so on, finally saying in its last entry that item four is ranked higher than item five).

We first compute ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 by taking the means of these vectors. Then we compute the sample covariance matrix of the vectors; call it ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5CSigma%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5CSigma%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
. Then a ![](https://s0.wp.com/latex.php?latex=100%281+-+%5Calpha%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=100%281+-+%5Calpha%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
% confidence set for the true ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
, appropriate for large sample sizes, is:

![](https://s0.wp.com/latex.php?latex=%5C%7B%5Ckappa%3A+n%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29%5E%7BT%7D+%5Chat%7B%5CSigma%7D%5E%7B-1%7D+%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29+%5Cleq+%5Cchi%5E2_%7B%5Cbar%7Bm%7D%2C+1+-+%5Calpha%7D%5C%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5C%7B%5Ckappa%3A+n%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29%5E%7BT%7D+%5Chat%7B%5CSigma%7D%5E%7B-1%7D+%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29+%5Cleq+%5Cchi%5E2_%7B%5Cbar%7Bm%7D%2C+1+-+%5Calpha%7D%5C%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)


where ![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2_%7Bk%2C+1+-+%5Calpha%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2_%7Bk%2C+1+-+%5Calpha%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is the ![](https://s0.wp.com/latex.php?latex=%281+-+%5Calpha%29%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%281+-+%5Calpha%29%5E%7Bth%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 percentile of the ![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2&bg=ffffff&%23038;fg=444444&%23038;s=0)
 distribution with ![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=444444&%23038;s=0)
 degrees of freedom.

The region Iâ€™ve just described is a ![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bm%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
-dimensional ellipsoid, a football-like shape that lives in a space with (probably) more than three dimensions. It sounds daunting, but one can still figure out what rankings are plausible once this region is computed. The trick is to work with each of the coordinates of the vector ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and determine whether there is a ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 in the ellipsoid where that coordinate is 1/2. If the answer is no, then the value of that coordinate, for all ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 in the ellipsoid, is either always above or always below 1/2. You can then look to ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 (which is in the dead center of the ellipsoid) to determine which is the case.

Whatâ€™s the significance of this? Letâ€™s say that you listed all possible rankings in a table. Letâ€™s suppose you did this procedure for the coordinate of ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 corresponding to the Seeker/Rogue pair. If you determine that this coordinate is not 1/2 and that all ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 in the ellipsoid ranks Seekers above Rogues, then you would take your list of rankings and remove all rankings that Rogues before Seekers, since these rankings are not in the confidence set.

If you do find a $\latex \kappa$ in the ellipsoid where the selected coordinate is 1/2, then you would not eliminate any rows in your list of rankings since you know that your confidence set must include some rankings that rank the two items one way and some rankings where the items are ranked the opposite way.

Repeat this procedure with every coordinate of ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
â€”that is, every possible pairing of choicesâ€”and you then have a confidence set for central rankings.

Determining whether there is a ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 vector in the ellipsoid with a select coordinate valued at 1/2 can be done via optimization. That is, find a $\latex \kappa$ that minimizes ![](https://s0.wp.com/latex.php?latex=n%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29%5E%7BT%7D+%5Chat%7B+%5CSigma%7D%5E%7B-1%7D+%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29%5E%7BT%7D+%5Chat%7B+%5CSigma%7D%5E%7B-1%7D+%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 subject to the constraint that ![](https://s0.wp.com/latex.php?latex=%5Ckappa_j+%3D+1%2F2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa_j+%3D+1%2F2&bg=ffffff&%23038;fg=444444&%23038;s=0)
. You donâ€™t even need fancy minimization algorithms for doing this; the minimum can, in principle, be computed analytically with multivariate calculus. After you found a minimizing ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
, determine what the value of ![](https://s0.wp.com/latex.php?latex=n%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29%5E%7BT%7D+%5Chat%7B%5CSigma%7D%5E%7B-1%7D+%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29%5E%7BT%7D+%5Chat%7B%5CSigma%7D%5E%7B-1%7D+%28%5Chat%7B%5Ckappa%7D+-+%5Ckappa%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is at that ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
. If it is less than ![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2_%7B%5Cbar%7Bm%7D%2C+1+-+%5Calpha%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cchi%5E2_%7B%5Cbar%7Bm%7D%2C+1+-+%5Calpha%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
, then you found a ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 in the ellipsoid; otherwise, you know there is no such ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
.

This was the procedure I used on the Arkham Horror class ranking data. The 95% confidence interval so computed determined that Seekers were ranked higher than Rogues and Survivors. That means that Seekers cannot have a ranking worse than 3 and Rogues and Survivors could not have rankings better than 2. Any ranking consistent with these constraints, though, is a plausible population central ranking. In fact, this procedure suggested that all the rankings below are plausible central population rankings:

The confidence interval, by design, is much less bold than just an estimate of the most central ranking. Our interval suggests that thereâ€™s a lot we donâ€™t know about what the central ranking is; we only know that whatever it is, it ranks Seekers above Rogues and Survivors.

The confidence set here is at least conservative in that it could perhaps contain too many candidate central rankings. I donâ€™t know for sure whether we could improve on the set and eliminate more ranks from the plausible set by querying more from the confidence set for ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
. Perhaps there are certain combinations that cannot exist, like excluding rankings that give both Seekers and Guardians a high ranking at the same time. If I were a betting man, though, Iâ€™d bet that the confidence set found with this procedure could be improved, in that not every vector in the resulting set corresponds with a ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 in the original ellipsoidal confidence set. Improving this set, though, would take a lot of work as one would have to consider multiple coordinates of potential ![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ckappa&bg=ffffff&%23038;fg=444444&%23038;s=0)
 simultaneously, then find a rule for eliminating ranking vectors based on the results.

## Clustering

Matt Newman, the lead designer of Arkham Horror: The Card Game, does not believe all players are the same. Specifically, he believes that there are player types that determine how they like to play. In statistics we might say that Matt Newman believes that there are clusters of players within any sufficiently large and well-selected sample of players. This suggests we may want to perform cluster analysis to find these sub-populations.

If you havenâ€™t heard the term before, clustering is the practice of finding â€œsimilarâ€� data points, grouping them together, and identifying them as belonging to some sub-population for which no label was directly observed. Itâ€™s not unreasonable to believe that these sub-populations exist and so I sought to do clustering myself.

There are many ways to cluster. Prof. Malden said that a clustering of rank data into ![](https://s0.wp.com/latex.php?latex=L&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=L&bg=ffffff&%23038;fg=444444&%23038;s=0)
 clusters should minimize the sum of the distances of each observation from their assigned clusterâ€™s centers. However, he did not suggest a good algorithm for finding these clusters. He did suggest that for small samples, small ![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=m&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and for a small number of clusters, we could exhaustively search for optimal clusters, an impractical idea.

I initially attempted a k-means-type algorithm for finding good clusters, one that used the Kendall distance rather than the Euclidean distance, but unfortunately I could not get the algorithm to give good results. I donâ€™t know whether I have errors in my code (listed below) or whether the algorithm just doesnâ€™t work for Kendall distances, but it didnâ€™t work; in fact, it would take a good clustering and make it worse! I eventually abandoned my home-brewed k-centers algorithm (and the hours of work that went into it) and just used spectral clustering.

Spectral clustering isnâ€™t easily described, but the idea of spectral clustering is to find groups of data that a random walker, walking from point to point along a weighted graph, would spend a long time in before moving to another group. (Thatâ€™s the best simplification I can make; the rest is linear algebra.) In order to do spectral clustering, one must have a notion of â€œsimilarityâ€� of data points. â€œSimilarityâ€� roughly means the opposite of â€œdistanceâ€�; in fact, if you have a distance metric (and we do here), you can find a similarity measure by subtracting all distances from the maximum distance between any two objects. Similarity measures are not as strictly defined as distance metrics; any function that gives two â€œsimilarâ€� items a high score and two â€œdissimilarâ€� items a low score could be considered a similarity function.

Spectral clustering takes a matrix of similarity measures, computed for each pair of observations, and spits out cluster assignments. But in addition to the similarity measure, we need to decide how many clusters to find.

I find determining the â€œbestâ€� number of clusters to find the hardest part of clustering. We could have only one cluster, containing all our data; this is what we start with. We could also assign each data point to its own cluster; our aforementioned measure of cluster quality would then be zero, which would be great if it werenâ€™t for the fact that our clusters mean nothing!

One approach people use for determining how many clusters to pick is the so-called elbow method. You take a plot of, say, Maldenâ€™s metric, compared against the number of clusters, and see if you can spot the â€œelbowâ€� in the plot. The elbow corresponds to the â€œbestâ€� number of clusters.

Hereâ€™s the corresponding plot for the dataset here:

![](https://ntguardian.files.wordpress.com/2019/02/cluster_scores.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/02/cluster_scores.png?w=456)


If youâ€™re unsure where the â€œelbowâ€� of the plot is, thatâ€™s okay; Iâ€™m not sure either. My best guess is that itâ€™s at five clusters; hence my choice of five clusters.

Another plot that people use is the silhouette plot, explained quite well by the scikit-learn documentation. The silhouette plot for the clustering found by spectral clustering is shown below:

![](https://ntguardian.files.wordpress.com/2019/02/cluster_silhouette.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/02/cluster_silhouette.png?w=456)


Is this a good silhouette plot? Iâ€™m not sure. Itâ€™s not the worst silhouette plot I saw for this data set but itâ€™s not as good as examples shown in the **scikit-learn** documentation. There are observations that appear to be in the wrong cluster according to the silhouette analysis. Soâ€¦ inconclusive?

I also computed the Dunn index of the clusters. I never got a value greater than 0.125. All together, these methods lead me to suspect that there are no meaningful clusters in this data set, at least none that can be found with this approach.

But people like cluster analysis, so if youâ€™re one of those folks, I have results for you.

When computing confidence sets for clusters I ran into an interesting problem: what if, say, you never see Seekers ranked below Guardians? This will cause one of the entries of ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
 to be either 0 or 1, and there is no â€œvarianceâ€� in its value; itâ€™s always the same. This will cause the covariance matrix to be non-invertible since it has rows/columns that are zero. The solution to this is to eliminate those rows and work only with the non-constant entries of ![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Chat%7B%5Ckappa%7D&bg=ffffff&%23038;fg=444444&%23038;s=0)
. That said, I still treat the entries removed as if they were â€œstatisticall significantâ€� results and remove rankings from our confidence set that are inconsistent with what we saw in the data. In short, if Seekers are never ranked below Guardians, remove all rankings in the confidence set that rank Seekers below Guardians.

One usually isnâ€™t satisfied with just a clustering; it would be nice to determine what a clustering signifies about those who are in the cluster. For instance, what type of player gets assigned to Cluster 1? I feel that inspecting the data in a more thoughtful and manual way can give a sense to what characteristic individuals assigned to a cluster share. For instance, I read the comments submitted by poll participants to hypothesize what types of players were being assigned to particular clusters. You can read these comments at the bottom of this article, after the code section.

## Code

All source code used to do the rank analysis done here is listed below, in a `.R` file intended to be run as an executable from a command line. (I created and ran it on a Linux system.)

Several packages had useful functions specific for this type of analysis, such as **pmr** (meant for modelling rand data) and **rankdist** (which had a lot of tools for working with the Kendall distance). The confidence interval, central ranking estimator, and hypothesis testing tools, though, I wrote myself, and they may not exist elsewhere.

I at least feel that the script itself is well-documented and I no longer need to explain it. But I will warn others that it was tailored to my problem, and the methods employed may not work well with larger sample sizes or when more items need to be ranked.

## Conclusion

This is only the tip of the iceberg for rank data analysis. We have not even touched on modelling for rank data, which can provide even richer inference. If youâ€™re interested, Iâ€™ll refer you again to Maldenâ€™s book.

I enjoyed this analysis so much I asked a Reddit question about where else I could conduct surveys (while at the same time still being statistically sound) because Iâ€™d love to do it again. I feel like thereâ€™s much to learn from rank data; it has great potential. Hopefully this article sparked your interest too.

## R Script for Analysis

## Report

## Respondent Comments Groups By Cluster



![](https://i0.wp.com/s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png?w=456&ssl=1)


![](https://i0.wp.com/s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png?w=456&ssl=1)


---

Packt Publishing published a book for me entitled *Hands-On Data Analysis with NumPy and Pandas*, a book based on my video course *Unpacking NumPy and Pandas*. This book covers the basics of setting up a Python environment for data analysis with Anaconda, using Jupyter notebooks, and using NumPy and pandas. If you are starting out using Python for data analysis or know someone who is, please consider buying my book or at least spreading the word about it. You can buy the book directly or purchase a subscription to Mapt and read it there.

If you like my blog and would like to support it, spread the word (if not get a copy yourself)!


*Related*








---
