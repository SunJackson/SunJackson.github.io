---
layout:     post
catalog: true
title:      Black and white image colorization with OpenCV and Deep Learning
subtitle:      转载自：https://www.pyimagesearch.com/2019/02/25/black-and-white-image-colorization-with-opencv-and-deep-learning/
date:      2019-02-25
author:      Adrian Rosebrock
tags:
    - colorized
    - colors
    - colored
    - colorizing
    - coloration
---

![](https://www.pyimagesearch.com/wp-content/uploads/2019/02/bw_colorization_opencv_header.jpg)


In this tutorial, you will learn how to colorize black and white images using OpenCV, Deep Learning, and Python.

Image colorization is the process of taking an **input grayscale (black and white) image** and then producing an **output colorized image** that represents the semantic colors and tones of the input (for example, an ocean on a clear sunny day must be plausibly “blue” — it can’t be colored “hot pink” by the model).

Previous methods for image colorization either:

1. Relied on significant human interaction and annotation

1. Produced desaturated colorization


The novel approach we are going to use here today instead relies on deep learning. We will utilize a Convolutional Neural Network capable of colorizing black and white images with results that can even “fool” humans!

**To learn how to perform black and white image coloration with OpenCV, *just keep reading!***

## Black and white image colorization with OpenCV and Deep Learning

In the first part of this tutorial, we’ll discuss how deep learning can be utilized to colorize black and white images.

From there we’ll utilize OpenCV to colorize black and white images for both:

1. Images

1. Video streams


We’ll then explore some examples and demos of our work.

### How can we colorize black and white images with deep learning?
![](https://www.pyimagesearch.com/wp-content/uploads/2019/02/bw_colorization_opencv_arch.png)


**Figure 1:** Zhang et al.’s architecture for colorization of black and white images with deep learning.

The technique we’ll be covering here today is from Zhang et al.’s 2016 ECCV paper, *Colorful Image Colorization*.

Previous approaches to black and white image colorization relied on *manual human annotation* and often produced desaturated results that were not “believable” as true colorizations.

Zhang et al. decided to attack the problem of image colorization by using Convolutional Neural Networks to “hallucinate” what an input grayscale image would look like when colorized.

To train the network Zhang et al. started with the ImageNet dataset and converted all images from the RGB color space to the **Lab color space.**

Similar to the RGB color space, the Lab color space has *three channels*. But *unlike* the RGB color space, Lab encodes color information differently:

- The ***L* channel** encodes lightness intensity only

- The ***a* channel** encodes green-red.

- And the ***b* channel** encodes blue-yellow


A full review of the Lab color space is outside the scope of this post (see this guide for more information on Lab), but the gist here is that Lab does a better job representing how humans see color.

Since the *L* channel encodes only the intensity, **we can use the *L* channel as our grayscale input to the network.**

From there the network must **learn to predict the *a* and *b* channels.**Given the **input *L* channel** and the **predicted *ab* channels** we can then form our **final output image.**

**The entire (simplified) process can be summarized as:**

1. Convert all training images from the RGB color space to the Lab color space.

1. Use the ***L* channel** as the input to the network and train the network to predict the ***ab* channels.**

1. Combine the input ***L* channel** with the predicted ***ab* channels.**

1. Convert the Lab image back to RGB.


To produce more plausible black and white image colorizations the authors also utilize a few additional techniques including mean annealing and a specialized loss function for color rebalancing (both of which are outside the scope of this post).

For more details on the image colorization algorithm and deep learning model, be sure to refer to the official publication of Zhang et al.

### Project structure

Go ahead and download the source code, model, and example images using the ***“Downloads”*** section of this post.

Once you’ve extracted the zip, you should navigate into the project directory.

From there, let’s use the 
tree  command to inspect the project structure:



||$ tree --dirsfirst.├── images│   ├── adrian_and_janie.png│   ├── albert_einstein.jpg│   ├── mark_twain.jpg│   └── robin_williams.jpg├── model│   ├── colorization_deploy_v2.prototxt│   ├── colorization_release_v2.caffemodel│   └── pts_in_hull.npy├── bw2color_image.py└── bw2color_video.py 2 directories, 9 files|

.

│   ├── adrian_and_janie.png

│   ├── mark_twain.jpg

├── model

│   ├── colorization_release_v2.caffemodel

├── bw2color_image.py

 

We have four sample black and white images in the 
images/  directory.

Our Caffe model and prototxt are inside the 
model/  directory along with the cluster points NumPy file.

We’ll be reviewing two scripts today:


bw2color_image.py

bw2color_video.py

The **image** script can process any black and white (also known as grayscale) image you pass in.

Our **video** script will either use your webcam or accept an input video file and then perform colorization.

### Colorizing black and white images with OpenCV

Let’s go ahead and implement black and white image colorization script with OpenCV.

Open up the 
bw2color_image.py file and insert the following code:



||# import the necessary packagesimport numpy as npimport argparseimport cv2 # construct the argument parser and parse the argumentsap = argparse.ArgumentParser()ap.add_argument("-i", "--image", type=str, required=True, help="path to input black and white image")ap.add_argument("-p", "--prototxt", type=str, required=True, help="path to Caffe prototxt file")ap.add_argument("-m", "--model", type=str, required=True, help="path to Caffe pre-trained model")ap.add_argument("-c", "--points", type=str, required=True, help="path to cluster center points")args = vars(ap.parse_args())|

import numpy as np

import cv2

# construct the argument parser and parse the arguments

ap.add_argument("-i", "--image", type=str, required=True,

ap.add_argument("-p", "--prototxt", type=str, required=True,

ap.add_argument("-m", "--model", type=str, required=True,

ap.add_argument("-c", "--points", type=str, required=True,

args = vars(ap.parse_args())

Our colorizer script only requires three imports: NumPy, OpenCV, and 
argparse .

Let’s go ahead and use `argparse` to parse command line arguments. This script requires that these four arguments be passed to the script directly from the terminal:


--image : The path to our input black/white image.

--prototxt : Our path to the Caffe prototxt file.

--model . Our path to the Caffe pre-trained model.

--points : The path to a NumPy cluster center points file.

With the above four flags and corresponding arguments, the script will be able to run with different inputs without changing any code.

Let’s go ahead and load our model and cluster centers into memory:



||# load our serialized black and white colorizer model and cluster# center points from diskprint("[INFO] loading model...")net = cv2.dnn.readNetFromCaffe(args["prototxt"], args["model"])pts = np.load(args["points"]) # add the cluster centers as 1x1 convolutions to the modelclass8 = net.getLayerId("class8_ab")conv8 = net.getLayerId("conv8_313_rh")pts = pts.transpose().reshape(2, 313, 1, 1)net.getLayer(class8).blobs = [pts.astype("float32")]net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]|

# center points from disk

net = cv2.dnn.readNetFromCaffe(args["prototxt"], args["model"])

 

class8 = net.getLayerId("class8_ab")

pts = pts.transpose().reshape(2, 313, 1, 1)

net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]

**Line 21** loads our Caffe model directly from the command line argument values. OpenCV can read Caffe models via the 
cv2.dnn.readNetFromCaffe function.

**Line 22** then loads the cluster center points directly from the command line argument path to the points file. This file is in NumPy format so we’re using 
np.load .

From there, **Lines 25-29****:**

- Load centers for ***ab* channel** quantization used for rebalancing.

- Treat each of the points as *1×1* convolutions and add them to the model.


Now let’s load, scale, and convert our image:



||# load the input image from disk, scale the pixel intensities to the# range [0, 1], and then convert the image from the BGR to Lab color# spaceimage = cv2.imread(args["image"])scaled = image.astype("float32") / 255.0lab = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)|

# range [0, 1], and then convert the image from the BGR to Lab color

image = cv2.imread(args["image"])

lab = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)

To load our input image from the file path, we use 
cv2.imread  on **Line 34**.

Preprocessing steps include:

- Scaling pixel intensities to the range *[0, 1]* (**Line 35**).

- Converting from BGR to Lab color space (**Line 36**).


Let’s continue with our preprocessing:



||# resize the Lab image to 224x224 (the dimensions the colorization# network accepts), split channels, extract the 'L' channel, and then# perform mean centeringresized = cv2.resize(lab, (224, 224))L = cv2.split(resized)[0]L -= 50|

# network accepts), split channels, extract the 'L' channel, and then

resized = cv2.resize(lab, (224, 224))

L -= 50

We’ll go ahead and resize the input image to 22*4×2*24 (**Line 41**), the required input dimensions for the network.

Then we grab the 
L  channel only (i.e., the input) and perform mean subtraction (**Lines 42 and 43**).

Now we can pass the **input *L* channel** through the network to **predict the *ab* channels:**



||# pass the L channel through the network which will *predict* the 'a'# and 'b' channel values'print("[INFO] colorizing image...")'net.setInput(cv2.dnn.blobFromImage(L))ab = net.forward()[0, :, :, :].transpose((1, 2, 0)) # resize the predicted 'ab' volume to the same dimensions as our# input imageab = cv2.resize(ab, (image.shape[1], image.shape[0]))|

# and 'b' channel values

net.setInput(cv2.dnn.blobFromImage(L))

 

# input image

A forward pass of the 
L  channel through the network takes place on **Lines 48 and 49** (here is a refresher on OpenCV’s `blobFromImage` if you need it).

Notice that after we called 
net.forward , on the same line, we went ahead and extracted the predicted 
ab  volume. I make it look easy here, but refer to the Zhang et al. documentation and demo on GitHub if you would like more details.

From there, we resize the predicted 
ab  volume to be the same dimensions as our input image (**Line 53**).

Now comes the time for post-processing. Stay with me here as we essentially go in reverse for some of our previous steps:



|5556575859606162636465666768697071727374|# grab the 'L' channel from the *original* input image (not the# resized one) and concatenate the original 'L' channel with the# predicted 'ab' channelsL = cv2.split(lab)[0]colorized = np.concatenate((L[:, :, np.newaxis], ab), axis=2) # convert the output image from the Lab color space to RGB, then# clip any values that fall outside the range [0, 1]colorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR)colorized = np.clip(colorized, 0, 1) # the current colorized image is represented as a floating point# data type in the range [0, 1] -- let's convert to an unsigned# 8-bit integer representation in the range [0, 255]colorized = (255 * colorized).astype("uint8") # show the original and output colorized imagescv2.imshow("Original", image)cv2.imshow("Colorized", colorized)cv2.waitKey(0)|

56


58


60


62


64


66


68


70


72


74


# resized one) and concatenate the original 'L' channel with the

L = cv2.split(lab)[0]

 

# clip any values that fall outside the range [0, 1]

colorized = np.clip(colorized, 0, 1)

# the current colorized image is represented as a floating point

# 8-bit integer representation in the range [0, 255]

 

cv2.imshow("Original", image)

cv2.waitKey(0)

Post processing includes:

Grabbing the 
L  channel from the ***original*** input image (**Line 58**) and concatenating the original 
L  channel and ***predicted***
ab  channels together forming 
colorized  (**Line 59**).
Converting the 
colorized image from the Lab color space to RGB (**Line 63**).
- Clipping any pixel intensities that fall outside the range *[0, 1]* (**Line 64**).

Bringing the pixel intensities back into the range *[0, 255]* (**Line 69**). During the preprocessing steps (**Line 35**) we divided by 
255  and now we are multiplying by 
255 . I’ve also found that this scaling and 
"uint8"  conversion isn’t a requirement but that it helps the code work between **OpenCV 3.4.x** and **4.x** versions.

Finally, both our original 
image  and 
colorized  images are displayed on the screen!

### Image colorization results

Now that we’ve implemented our image colorization script, let’s give it a try.

Make sure you’ve used the ***“Downloads”*** section of this blog post to download the source code, colorization model, and example images.

From there, open up a terminal, navigate to where you downloaded the source code, and execute the following command:



||$ python bw2color_image.py \ --prototxt model/colorization_deploy_v2.prototxt \ --model model/colorization_release_v2.caffemodel \ --points model/pts_in_hull.npy \ --image images/robin_williams.jpg[INFO] loading model...|

 --prototxt model/colorization_deploy_v2.prototxt \

 --points model/pts_in_hull.npy \

[INFO] loading model...

![](https://www.pyimagesearch.com/wp-content/uploads/2019/02/bw_colorization_opencv_robin_williams.jpg)
**Figure 2:** Grayscale image colorization with OpenCV and deep learning. This is a picture of famous late actor, Robin Williams.

On the *left*, you can see the original input image of Robin Williams, a famous actor and comedian who passed away ~5 years ago.

On the *right*, you can see the output of the black and white colorization model.

Let’s try another image, this one of Albert Einstein:



||$ python bw2color_image.py \ --prototxt model/colorization_deploy_v2.prototxt \ --model model/colorization_release_v2.caffemodel \ --points model/pts_in_hull.npy --image images/albert_einstein.jpg[INFO] loading model...|

 --prototxt model/colorization_deploy_v2.prototxt \

 --points model/pts_in_hull.npy

[INFO] loading model...

![](https://www.pyimagesearch.com/wp-content/uploads/2019/02/bw_colorization_opencv_albert_einstein.jpg)
**Figure 3:** Image colorization using deep learning and OpenCV. This is an image of Albert Einstein.

I’m particularly impressed by this image colorization.

Notice how the water is an appropriate shade of blue while Einstein’s shirt is white and his pants are khaki — all of these are plausible colorizations.

Here is another example image, this one of Mark Twain, one of my all-time favorite authors:



||$ python bw2color_image.py \ --prototxt model/colorization_deploy_v2.prototxt \ --model model/colorization_release_v2.caffemodel \ --points model/pts_in_hull.npy --image images/mark_twain.jpg[INFO] loading model...|

 --prototxt model/colorization_deploy_v2.prototxt \

 --points model/pts_in_hull.npy

[INFO] loading model...

![](https://www.pyimagesearch.com/wp-content/uploads/2019/02/bw_colorization_opencv_mark_twain.jpg)
**Figure 4:** A black/white image of Mark Twain has undergone colorization via OpenCV and deep learning.

Here we can see that the grass and foliage are correctly colored a shade of green, although you can see these shades of green blending into Twain’s shoes and hands.

The final image demonstrates a not-so-great black and white image colorization with OpenCV:



||$ python bw2color_image.py \ --prototxt model/colorization_deploy_v2.prototxt \ --model model/colorization_release_v2.caffemodel \ --points model/pts_in_hull.npy --image images/adrian_and_janie.png[INFO] loading model...|

 --prototxt model/colorization_deploy_v2.prototxt \

 --points model/pts_in_hull.npy

[INFO] loading model...

![](https://www.pyimagesearch.com/wp-content/uploads/2019/02/bw_colorization_opencv_adrian_janie.jpg)
**Figure 5:** Janie is the puppers we recently adopted into our family. This is her first snow day. Black and white cameras/images are great for snow, but I wanted to see how image colorization would turn out with OpenCV and deep learning.

This photo is of myself and Janie, my beagle puppy, during a snowstorm a few weeks ago.

Here you can see that while the snow, Janie, my jacket, and even the gazebo in the background are correctly colored, my blue jeans are actually red.

Not all image colorizations will be perfect but the results here today do demonstrate the plausibility of the Zhang et al. approach.

### Real-time black and white video colorization with OpenCV

We’ve already seen how we can apply black and white image colorization to images — but can we do the same with **video streams?**

You bet we can.

This script follows the same process as above except we’ll be processing frames of a video stream. I’ll be reviewing it in less detail and focusing on the frame grabbing + processing aspects.

Open up the 
bw2color_video.py and insert the following code:



|123456789101112131415161718192021|# import the necessary packagesfrom imutils.video import VideoStreamimport numpy as npimport argparseimport imutilsimport timeimport cv2 # construct the argument parser and parse the argumentsap = argparse.ArgumentParser()ap.add_argument("-i", "--input", type=str, help="path to optional input video (webcam will be used otherwise)")ap.add_argument("-p", "--prototxt", type=str, required=True, help="path to Caffe prototxt file")ap.add_argument("-m", "--model", type=str, required=True, help="path to Caffe pre-trained model")ap.add_argument("-c", "--points", type=str, required=True, help="path to cluster center points")ap.add_argument("-w", "--width", type=int, default=500, help="input width dimension of frame")args = vars(ap.parse_args())|

2


4


6


8


10


12


14


16


18


20


from imutils.video import VideoStream

import argparse

import time

 

ap = argparse.ArgumentParser()

 help="path to optional input video (webcam will be used otherwise)")

 help="path to Caffe prototxt file")

 help="path to Caffe pre-trained model")

 help="path to cluster center points")

 help="input width dimension of frame")

Our video script requires two additional imports:


VideoStream allows us to grab frames from a webcam or video file

time  will be used to pause to allow a webcam to warm up

Let’s initialize our 
VideoStream  now:



|2324252627282930313233343536|# initialize a boolean used to indicate if either a webcam or input# video is being usedwebcam = not args.get("input", False) # if a video path was not supplied, grab a reference to the webcamif webcam: print("[INFO] starting video stream...") vs = VideoStream(src=0).start() time.sleep(2.0) # otherwise, grab a reference to the video fileelse: print("[INFO] opening video file...") vs = cv2.VideoCapture(args["input"])|

24


26


28


30


32


34


36


# video is being used

 

if webcam:

 vs = VideoStream(src=0).start()

 

else:

 vs = cv2.VideoCapture(args["input"])

Depending on whether we’re working with a 
webcam  or video file, we’ll create our 
vs  (i.e., “video stream”) object here.

From there, we’ll load the colorizer deep learning model and cluster centers (the same way we did in our previous script):



||# load our serialized black and white colorizer model and cluster# center points from diskprint("[INFO] loading model...")net = cv2.dnn.readNetFromCaffe(args["prototxt"], args["model"])pts = np.load(args["points"]) # add the cluster centers as 1x1 convolutions to the modelclass8 = net.getLayerId("class8_ab")conv8 = net.getLayerId("conv8_313_rh")pts = pts.transpose().reshape(2, 313, 1, 1)net.getLayer(class8).blobs = [pts.astype("float32")]net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]|

# center points from disk

net = cv2.dnn.readNetFromCaffe(args["prototxt"], args["model"])

 

class8 = net.getLayerId("class8_ab")

pts = pts.transpose().reshape(2, 313, 1, 1)

net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype="float32")]

Now we’ll start an infinite 
while  loop over incoming frames. We’ll process the frames directly in the loop:



|51525354555657585960616263646566676869707172737475|# loop over frames from the video streamwhile True: # grab the next frame and handle if we are reading from either # VideoCapture or VideoStream frame = vs.read() frame = frame if webcam else frame[1]  # if we are viewing a video and we did not grab a frame then we # have reached the end of the video if not webcam and frame is None: break  # resize the input frame, scale the pixel intensities to the # range [0, 1], and then convert the frame from the BGR to Lab # color space frame = imutils.resize(frame, width=args["width"]) scaled = frame.astype("float32") / 255.0 lab = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)  # resize the Lab frame to 224x224 (the dimensions the colorization # network accepts), split channels, extract the 'L' channel, and # then perform mean centering resized = cv2.resize(lab, (224, 224)) L = cv2.split(resized)[0] L -= 50|

52


54


56


58


60


62


64


66


68


70


72


74


while True:

 # VideoCapture or VideoStream

 frame = frame if webcam else frame[1]

 # if we are viewing a video and we did not grab a frame then we

 if not webcam and frame is None:

 

 # range [0, 1], and then convert the frame from the BGR to Lab

 frame = imutils.resize(frame, width=args["width"])

 lab = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)

 # resize the Lab frame to 224x224 (the dimensions the colorization

 # then perform mean centering

 L = cv2.split(resized)[0]

Each frame from our 
vs  is grabbed on **Lines 55 and 56**. A check is made for a 
None  type 
frame  — when this occurs, we’ve reached the end of a video file (if we’re processing a video file) and we can 
break  from the loop (**Lines 60 and 61**).

Preprocessing (just as before) is conducted on **Lines 66-75**. This is where we resize, scale, and convert to Lab. Then we grab the 
L  channel, and perform mean subtraction.

Let’s now apply deep learning colorization and post-process the result:



|77787980818283848586878889909192939495| # pass the L channel through the network which will *predict* the # 'a' and 'b' channel values net.setInput(cv2.dnn.blobFromImage(L)) ab = net.forward()[0, :, :, :].transpose((1, 2, 0))  # resize the predicted 'ab' volume to the same dimensions as our # input frame, then grab the 'L' channel from the *original* input # frame (not the resized one) and concatenate the original 'L' # channel with the predicted 'ab' channels ab = cv2.resize(ab, (frame.shape[1], frame.shape[0])) L = cv2.split(lab)[0] colorized = np.concatenate((L[:, :, np.newaxis], ab), axis=2)  # convert the output frame from the Lab color space to RGB, clip # any values that fall outside the range [0, 1], and then convert # to an 8-bit unsigned integer ([0, 255] range) colorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR) colorized = np.clip(colorized, 0, 1) colorized = (255 * colorized).astype("uint8")|

78


80


82


84


86


88


90


92


94


 # 'a' and 'b' channel values

 ab = net.forward()[0, :, :, :].transpose((1, 2, 0))

 # resize the predicted 'ab' volume to the same dimensions as our

 # frame (not the resized one) and concatenate the original 'L'

 ab = cv2.resize(ab, (frame.shape[1], frame.shape[0]))

 colorized = np.concatenate((L[:, :, np.newaxis], ab), axis=2)

 # convert the output frame from the Lab color space to RGB, clip

 # to an 8-bit unsigned integer ([0, 255] range)

 colorized = np.clip(colorized, 0, 1)

Our deep learning forward pass of 
L through the network results in the predicted 
ab  channel.

Then we’ll post-process the result to from our 
colorized  image (**Lines 86-95**). This is where we resize, grab our ***original***
L , and concatenate our predicted 
ab . From there, we convert from Lab to RGB, clip, and scale.

If you followed along closely above, you’ll remember that all we do next is display the results:



|979899100101102103104105106107108109110111112113114115116| # show the original and final colorized frames cv2.imshow("Original", frame) cv2.imshow("Grayscale", cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)) cv2.imshow("Colorized", colorized) key = cv2.waitKey(1) & 0xFF  # if the `q` key was pressed, break from the loop if key == ord("q"): break # if we are using a webcam, stop the camera video streamif webcam: vs.stop() # otherwise, release the video file pointerelse: vs.release() # close any open windowscv2.destroyAllWindows()|

98


100


102


104


106


108


110


112


114


116


 cv2.imshow("Original", frame)

 cv2.imshow("Colorized", colorized)

 

 if key == ord("q"):

 

if webcam:

 

else:

 

cv2.destroyAllWindows()

Our original webcam 
frame  is shown along with our grayscale image and 
colorized  result.

If the 
"q"  
key  is pressed, we’ll 
break  from the loop and cleanup.

That’s all there is to it!

### Video colorization results

Let’s go ahead and give our video black and white colorization script a try.

Make sure you use the ***“Downloads”*** section of this tutorial to download the source code and colorization model.

From there, open up a terminal and execute the following command to have the colorizer run on your webcam:



||$ python bw2color_video.py \ --prototxt model/colorization_deploy_v2.prototxt \ --model model/colorization_release_v2.caffemodel \ --points model/pts_in_hull.npy|

 --prototxt model/colorization_deploy_v2.prototxt \

 --points model/pts_in_hull.npy

![](https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/bw-image-colorization/bw_colorization_opencv_demo.gif)
**Figure 6:** Black and white image colorization in video with OpenCV and deep learning demo.

If you want to run the colorizer on a video file you can use the following command:



||$ python bw2color_video.py \ --prototxt model/colorization_deploy_v2.prototxt \ --model model/colorization_release_v2.caffemodel \ --points model/pts_in_hull.npy --video video/jurassic_park_intro.mp4|

 --prototxt model/colorization_deploy_v2.prototxt \

 --points model/pts_in_hull.npy

https://youtu.be/r3Mmh_k2F0E

Credits:

The model here is running in close to real-time on my 3Ghz Intel Xeon W.

With a GPU, real-time performance could certainly be obtained; however, keep in mind that GPU support for OpenCV’s “dnn” module is currently a bit limited and it, unfortunately, does not yet support NVIDIA GPUs.

## Summary

In today’s tutorial, you learned how to colorize black and white images using OpenCV and Deep Learning.

The image colorization model we used here today was first introduced by Zhang et al. in their 2016 publication, *Colorful Image Colorization*.

Using this model, we were able to colorize both:

1. Black and white images

1. Black and white videos


Our results, while not perfect, demonstrated the plausibility of automatically colorizing black and white images and videos.

According to Zhang et al., their approach was able to “fool” humans 32% of the time!

**To download the source code to this post, and be notified when future tutorials are published here on PyImageSearch, *just enter your email address in the form below!***

## Downloads:
