---
layout:     post
catalog: true
title:      Two Major Difficulties in AI and One Applied Solution
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/_MUbQREV0co/ai-chess-difficulties-solution.html
date:      2019-02-22
author:      Dan Clark
tags:
    - algorithms
    - humans use
    - mimic
    - explaining
    - explainable
---

**By Ofer Shamai (PHD), co-founder and head of R&D at ChesStories**

These are the heydays of AI. New and exciting applications are found on an almost daily basis, proving that the AI promise was not in vain. However, this success comes with a price, and I would like to highlight two such prices and one solution on the path to remedy.

**The black box**

In the past two years, AI is suffering from an increasing problem of trust. ML and NN algorithms’ lack of *transparency, fairness, and safety*[i] (AKA the *black box), *together with their weaknesses in accounting for specific situations impedes the desired adoption rate of AI algorithms and creates frustration in different domains of our modern life. Ironically, the more successful the algorithms, the more imminent is the problem of validating and explaining their results to a human user.

As a result, we’re witnessing the rise of explainable AI (XAI), with companies and agencies putting more resources and budgets[ii] to explain these algorithms and give decision makers more control over AI.

With all this fuss around the amazing capabilities of the NN algorithms, we might think of NN and ML algorithms as mysterious and smart. We think that they must be smart as they’re doing some really amazing things and they seem to represent the neural connection in our brains. However, this is far from being true. Actually, as Turing Award winner Judea Pearl correctly claims[iii] , they are more curve fitting techniques that are very useful but intellectually stupid: In order to perform reasonably, these algorithms need to digest an enormous amount of data. Much more than a human can or need to. We humans use just a few examples in order to get results that are almost as good as the algorithms. This is an amazing ability of human thinking. This is possible, as humans can think and abstract about the data and be much more efficient than the current NN algorithms.

Moreover, the patterns “learned” by NN statistical learning are a more useful tool for statistical correlations than a truly useful abstract. This is best seen in their humorous identification mistakes, for example identifying a toothbrush and a basketball as similar objects.[iv] These mistakes are a strong hint that the features “learned” by these algorithms do not represent meaningful human casual connections between actual entities in the world. Rather, they are just useful statistical correlations.

**The abandoned path**

Absurdly, in the quest to achieve smart results we gave up intelligence. What is sometime missed in the (justified) enthusiasm from ML results is that people do it much better, or at least could have done it better if had they been equipped with the ability to contain enormous amounts of data. The last few years’ breakthroughs are the result of the mechanical ability of computers and not an ability to mimic high cognitive functions.

This brings us to the more profound problem – we abandoned an important goal of AI which is to understand human thinking. We no longer put enough efforts trying to solve problems in the area of general intelligence, or trying to create algorithms that actually mimic human way of reasoning. This is the obligation and difficulty that we do not address nowadays. Here again, Pearl accurately describes one instance of this neglect. When asked about the meaning of his new book’s title *The Book of Why *he answered: “*It means to be a summary of the work I’ve been doing the past 25 years about cause and effect, what it means in one’s life, its applications, and how we go about coming up with answers to questions that are inherently causal. Oddly, those questions have been abandoned by science. So I’m here to make up for the neglect of science.*”

**An elegant solution, inspired by Chess! **

I would like to share an example in which the focus on modelling human reasoning has resulted in an impressive XAI feat.  As a co-founder at DecodeChess, my team develops algorithms that explain chess-playing program recommendations in a human way. Chess and AI have always been a hot topic, and we have been able to crack one of the most daunting riddles ever since the first AI-driven chess programs have been programmed: Why did the move the AI computer program (whose computation power far exceeds that of humans) decided to make is good and not another?

![](https://www.kdnuggets.com/wp-content/uploads/decode-chess-fig-1.jpg)


**Image 1: DecodeChess explaining why the next best move for black is 23…Qg3**

We use a subtle model of human thinking to model how a human would make a recommendation based on the vast amount of data the computer searches. That is, we look for the important factors that human expert would identify in order to find a solution equal to the one the computer found. We minimize the information to the what is relevant for us humans. This is a difficult project but very satisfying as it improves my understanding of what reasoning is and makes me amazed by our flexible and complicated thinking capabilities.

I’m not claiming that rule-obeying machines can mimic the whole of human thinking. In fact, in my PhD thesis I proved that rule-obeying devices will never be able to totally mimic mathematical reasoning. However, there is much we can learn and adopt from human thinking that can improve our AI algorithms and there is much we can learn about ourselves in this process. Our work at DecodeChess is a difficult and small step in that direction.

![](https://www.kdnuggets.com/wp-content/uploads/decode-chess-fig-2.jpg)


**Image 2: DecodeChess providing a human thinking model for how to find the best move**

[i] https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 and https://www.oii.ox.ac.uk/blog/could-counterfactuals-explain-algorithmic-decisions-without-opening-the-black-box/.

[ii] https://www.darpa.mil/news-events/2018-09-07, https://www.youtube.com/watch?v=-O01G3tSYpU&t=636s

[iii] https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/)

[iv] https://www.darpa.mil/attachments/AIFull.pdf  p.23

**Bio**: Ofer Shamai (PHD) is a co-founder and head of R&D at ChesStories, a company that specializes in developing cognitive computing and explainable AI algorithms. Its debut product is DecodeChess, the first and only XAI software in chess that explains the moves of a chess engine in rich, intuitive language.

**Resources:**

**Related:**



 
