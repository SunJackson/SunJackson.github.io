---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/02/27/distilled-news-987/
date:      2019-02-26
author:      Michael Laux
tags:
    - learned
    - deep learning
    - data mining
    - time debugging
    - neural networks
---

**Facebook’s chief AI scientist: Deep learning may need a new programming language**

Deep learning may need a new programming language that’s more flexible and easier to work with than Python, Facebook AI Research director Yann LeCun said today. It’s not yet clear if such a language is necessary, but the possibility runs against very entrenched desires from researchers and engineers, he said. LeCun has worked with neural networks since the 1980s. ‘There are several projects at Google, Facebook, and other places to kind of design such a compiled language that can be efficient for deep learning, but it’s not clear at all that the community will follow, because people just want to use Python,’ LeCun said in a phone call with VentureBeat.

**The evolution and expanding utility of Ray**

In a recent post, I listed some of the early use cases described in the first meetup dedicated to Ray – a distributed programming framework from UC Berkeley’s RISE Lab. A second meetup took place a few months later, and both events featured some of the first applications built with Ray. On the development front, the core API has stabilized and a lot of work has gone into improving Ray’s performance and stability. The project now has around 5,700 stars on GitHub and more than 100 contributors across many organizations. At this stage of the project, how does one describe Ray to those who aren’t familiar with the project? The RISE Lab team describes Ray as a ‘general framework for programming your cluster or cloud.’ To place the project into context, Ray and cloud functions (FaaS, serverless) currently sit somewhere in the middle between extremely flexible systems on one end or systems that are much more targeted and emphasize ease of use. More precisely, users currently can avail of extremely flexible cluster management and virtualization tools on one end (Docker, Kubernetes, Mesos, etc.), or domain specific systems on the other end of the flexibility spectrum (Spark, Kafka, Flink, PyTorch, TensorFlow, Redshift, etc.).

**Troubleshooting Deep Neural Networks**

When I started in deep learning, I felt frustrated that I was spending most of my time debugging instead of the ‘fun’ stuff.(Later, I discovered that debugging never goes away, and the best practitioners still spend most of their time on it.) As I learned more and began helping others train models, I realized that much of my advice consisted of walking people through a mental decision tree for how to improve their model’s performance. This guide is an attempt to codify that decision tree.

**Seven Myths in Machine Learning Research**

We present seven myths commonly believed to be true in machine learning research, circa Feb 2019.Myth 1: TensorFlow is a Tensor manipulation libraryMyth 2: Image datasets are representative of real images found in the wildMyth 3: Machine Learning researchers do not use the test set for validationMyth 4: Every datapoint is used in training a neural networkMyth 5: We need (batch) normalization to train very deep residual networksMyth 6: Attention > ConvolutionMyth 7: Saliency maps are robust ways to interpret neural networks

**Data mining tools**

The development and application of data mining algorithms requires the use of powerful software tools. With challenges such as big data encountered in economy or gene sequencing for life science, data mining is important for daily problems as well as specialized fields. However, the large variety of requirements and user groups lead to a huge number and diversity of software tools. We give an overview by discussing the historical development and presenting a range of existing state-of-the-art data mining and related tools. This paper is an update of our previous article from 2011 following the encyclopedic aspect of Wiley Interdisciplinary Reviews to include new findings or references and changing outdated information. However, since the paper should be able to stand alone, it includes many still valid elements of the previous article. Following the original paper, we propose criteria for the tool categorization based on different user groups, data structures, data mining tasks and methods, visualization and interaction styles, import and export options for data and models, platforms, and license policies. These criteria are then used to classify data mining tools into nine different categories. The typical characteristics of these types are explained and a selection of the most important tools is categorized.

**3 reasons to add deep learning to your time series toolkit**

The ability to accurately forecast a sequence into the future is critical in many industries: finance, supply chain, and manufacturing are just a few examples. Classical time series techniques have served this task for decades, but now deep learning methods – similar to those used in computer vision and automatic translation – have the potential to revolutionize time series forecasting as well. Due to their applicability to many real-life problems – such as fraud detection, spam email filtering, finance, and medical diagnosis – and their ability to produce actionable results, deep learning neural networks have gained a lot of attention in recent years. Generally, deep learning methods have been developed and applied to univariate time series forecasting scenarios, where the time series consists of single observations recorded sequentially over equal time increments. For this reason, they have often performed worse than naïve and classical forecasting methods, such as exponential smoothing (ETS) and autoregressive integrated moving average (ARIMA). This has led to a general misconception that deep learning models are inefficient in time series forecasting scenarios, and many data scientists wonder whether it’s really necessary to add another class of methods – such as convolutional neural networks or recurrent neural networks – to their time series toolkit. The Artificial Intelligence Conference The Artificial Intelligence Conference in New York, April 15-18, 2019 Early price ends March 1 In this post, I’ll discuss some of the practical reasons why data scientists may still want to think about deep learning when they build time series forecasting solutions.

**Manifold’s Decision Making Process**

One of the ways that we work to build velocity and agility at Manifold is to make it possible for people to make autonomous decisions – without having to convince a committee – while still aligning organizationally. The balance we strike is to require every decision maker to get advice, and then empower them to make their own decision. How much advice, from whom? Consider this chart.

**Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixel’s Camera!**

Us humans are supremely adept at glancing at any image and understanding what’s within it. In fact, it’s an almost imperceptible reaction from us. It takes us a fraction of a second to analyze. It’s a completely different ball game for machines. There have been numerous attempts over the last couple of decades to make machines smarter at this task – and we might finally have cracked it, thanks to deep learning (and computer vision) techniques!

**Code submission should be encouraged but not compulsory**

ICML, ICLR, and NeurIPS are all considering or experimenting with code and data submission as a part of the reviewer or publication process with the hypothesis that it aids reproducibility of results. Reproducibility has been a rising concern with discussions in paper, workshop, and invited talk. The fundamental driver is of course lack of reproducibility. Lack of reproducibility is an inherently serious and valid concern for any kind of publishing process where people rely on prior work to compare with and do new things. Lack of reproducibility (due to random initialization for example) was one of the things leading to a period of unpopularity for neural networks when I was a graduate student. That has proved nonviable (Surprise! Learning circuits is important!), but the reproducibility issue remains. Furthermore, there is always an opportunity and latent suspicion that authors ‘cheat’ in reporting results which could be allayed using a reproducible approach.

**Why feature weights in a machine learning model are meaningless**

As I see our customers fall in love with BigQuery ML, an old problem rises its head?-?I find that they can not resist the temptation to assign meaning to feature weights. ‘The largest weight in my model to predict customer lifetime value,’ they might remark, ‘is whether or not the customer received a thank you call from an executive.’ Or they might look at negative weights and draw a dire conclusion: ‘Stores located in urban areas lead to negative satisfaction scores.’ Please don’t do that. Don’t make your execs call every customer! Don’t close all your urban locations!

**Debugging a Machine Learning model written in TensorFlow and Keras**

In this article, you get to look over my shoulder as I go about debugging a TensorFlow model. I did a lot of dumb things, so please don’t judge.

**To all Data Scientists – The one Graph Algorithm you need to know**

Graphs provide us with a very useful data structure. They can help us to find structure within our data. With the advent of Machine learning and big data, we need to get as much information as possible about our data. Learning a little bit of graph theory can certainly help us with that. Here is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.

**Top takeaways from R Studio conf 2019**

Conferences1. Shiny in production: Principles, practices, and tools2. A guide to modern reproducible data science with R3. R in Production4. Databases using R: The latest5. Scaling R with Spark6. Democratizing R with Plumber APIs7. tidy time series analysis8. 3D mapping, plotting, and printing with rayshade9. Spatial Data Science in the TidyverseWorkshops1. Applied Machine Learning Workshop2. Advanced R Markdown Workshop3. Big Data with R Workshop4. Shiny in Production | Data Products at Scale Workshop5. Introduction to Deep Learning Workshop

**Hierarchical RNNs, training bottlenecks and the future.**

As we know, the standard backpropagation algorithm is the most efficient procedure to compute the exact gradients of a loss function in a neural network with respect to its parameters. By efficient, I mean that given a fixed network architecture, its computational cost remains always the same as that of computing the loss (for instance, given a feed forward network of 4 layers with i, j, k, and l nodes respectively, the total time complexity for one epoch with ‘t’ training instances will always be O(t*(ij + jk + kl))). The axiom of differentiability and continuity tells us that for a function to be differentiable at a point x_0, it must be continuous at every point in its domain. This further demands that all the parameters involved in the backprop. must possess continuous values. However, the case might not be the same every time; we might instead desire for a network composing neurons that take hard stochastic decisions about temporal events at different time scales?-?a 0/1 value denoting a word/phrase boundary in a text or the end of a story segment in a video. Such binary outputs further introduce sparse representations which could serve as a regularization technique, thus allowing to design gating units to select which part of the model actually needs to be computed for a given instance.[1]

**What Uncertainties tell you in Bayesian Neural Networks**

This time, we will examine what homoscedastic, heteroscedastic, epistemic, and aleatoric uncertainties actually tell you. In my opinion, this is an upcoming research field in Bayesian deep learning and has been greatly shaped by Yarin Gal’s contributions. Most illustrations here are taken from his publications. As a background, in Bayesian deep learning, we have probability distributions over weights. Since most of the times we assume these probability distributions are Gaussians, we have a mean ? and a variance ?². The mean ? is the most probable value we sample for the weight.

**https://towardsdatascience.com/grus-and-lstm-s-741709a9b9b1**

Recurrent Neural Networks are networks which persist information. They are useful for sequence related tasks like Speech Recognition, Music Generation, etc. However, RNN’s suffer from short-term memory. If a sequence is long enough, they will have a hard time carrying the information from the earlier timesteps to later ones. This is called the Vanishing Gradient Problem. In this post, we will look into Gated Recurrent Unit(GRU) and Long Short Term Memory(LSTM) Networks, which solve this issue. If you haven’t read about RNN’s, here’s a link to my post explaining what RNN is and how it works.

**How to Configure the Number of Layers and Nodes in a Neural Network**

Artificial neural networks have two main hyperparameters that control the architecture or topology of the network: the number of layers and the number of nodes in each hidden layer. You must specify values for these parameters when configuring your network. The most reliable way to configure these hyperparameters for your specific predictive modeling problem is via systematic experimentation with a robust test harness. This can be a tough pill to swallow for beginners to the field of machine learning, looking for an analytical way to calculate the optimal number of layers and nodes, or easy rules of thumb to follow. In this post, you will discover the roles of layers and nodes and how to approach the configuration of a multilayer perceptron neural network for your predictive modeling problem.





### Like this:

Like Loading...


*Related*

