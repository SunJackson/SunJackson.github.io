---
layout:     post
catalog: true
title:      I Just Wanted The Data ： Turning Tableau & Tidyverse Tears Into Smiles with Base R (An Encoding Detective Story)
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/a1_d_tY88JY/
date:      2019-02-20
author:      hrbrmstr
tags:
    - files
    - data
    - https tfil
    - wat
    - csv
---





Those outside the Colonies may not know that Paylessâ€”a national chain that made footwear affordable for millions of â€˜Muricans who canâ€™t spare $100.00 USD for a pair of shoes their 7 year old will outgrow in a yearâ€” is closing. CNBC also had a story that featured a choropleth with a tiny button at the bottom that indicated one could get the data:

![](https://i2.wp.com/rud.is/b/wp-content/uploads/2019/02/payless-get-data.png?resize=780%2C463&ssl=1)
![](https://i2.wp.com/rud.is/b/wp-content/uploads/2019/02/payless-get-data.png?resize=780%2C463&ssl=1)


I should have known this would turn out to be a chore since they used Tableauâ€”the platform of choice when you want to take advantage of all the free software libraries they use to power their premier platform which, in turn, locks up all the data for you so others canâ€™t adopt, adapt and improve. Go. Egregious. Predatory. Capitalism.

*Anyway.*

I wanted the data to do some real analysis vs produce a fairly unhelpful visualization (TLDR: layer in Census data for areas impacted, estimate job losses, compute nearest similar Payless stores to see impact on transportation-challenged homes, etc. Yâ€™now, citizen data journalism-y things) so I pressed the button and watched for the URL in Chrome (aye, for those that remember I moved to Firefox et al in 2018, I switched back; more on that in March) and copied it to try to make this post actually reproducible (a novel concept for Tableau fanbois):

```
library(tibble)
library(readr)

# https://www.cnbc.com/2019/02/19/heres-a-map-of-where-payless-shoesource-is-closing-2500-stores.html

tfil <- "~/Data/Sheet_3_data.csv"

download.file(
 "https://public.tableau.com/vizql/w/PAYLESSSTORECLOSINGS/v/Dashboard2/vud/sessions/6A678928620645FF99C7EF6353426CE8-0:0/views/10625182665948828489_7202546092381496425?csv=true&showall=true",
 tfil
)
## trying URL 'https://public.tableau.com/vizql/w/PAYLESSSTORECLOSINGS/v/Dashboard2/vud/sessions/6A678928620645FF99C7EF6353426CE8-0:0/views/10625182665948828489_7202546092381496425?csv=true&showall=true'
## Error in download.file("https://public.tableau.com/vizql/w/PAYLESSSTORECLOSINGS/v/Dashboard2/vud/sessions/6A678928620645FF99C7EF6353426CE8-0:0/views/10625182665948828489_7202546092381496425?csv=true&showall=true", : 
## cannot open URL 'https://public.tableau.com/vizql/w/PAYLESSSTORECLOSINGS/v/Dashboard2/vud/sessions/6A678928620645FF99C7EF6353426CE8-0:0/views/10625182665948828489_7202546092381496425?csv=true&showall=true'
## In addition: Warning message:
## In download.file("https://public.tableau.com/vizql/w/PAYLESSSTORECLOSINGS/v/Dashboard2/vud/sessions/6A678928620645FF99C7EF6353426CE8-0:0/views/10625182665948828489_7202546092381496425?csv=true&showall=true", :
## cannot open URL 'https://public.tableau.com/vizql/w/PAYLESSSTORECLOSINGS/v/Dashboard2/vud/sessions/6A678928620645FF99C7EF6353426CE8-0:0/views/10625182665948828489_7202546092381496425?csv=true&showall=true': HTTP status was '410 Gone'

```

WAT

Truth be told I expected a time-boxed URL of some sort (prior experience FTW). Selenium or Splash were potential alternatives but I didnâ€™t want to research the legality of more forceful scraping (I just wanted the data) so I *manually* downloaded the file (*the horror*) and proceeded to read it in. Well, *try* to read it in:

```
read_csv(tfil)
## Parsed with column specification:
## cols(
## A = col_logical()
## )
## Warning: 2092 parsing failures.
## row col expected actual file
## 1 A 1/0/T/F/TRUE/FALSE '~/Data/Sheet_3_data.csv'
## 2 A 1/0/T/F/TRUE/FALSE '~/Data/Sheet_3_data.csv'
## 3 A 1/0/T/F/TRUE/FALSE '~/Data/Sheet_3_data.csv'
## 4 A 1/0/T/F/TRUE/FALSE '~/Data/Sheet_3_data.csv'
## 5 A 1/0/T/F/TRUE/FALSE '~/Data/Sheet_3_data.csv'
## ... ... .................. ...... .........................
## See problems(...) for more details.
## 
## # A tibble: 2,090 x 1
## A 
## 
## 1 NA 
## 2 NA 
## 3 NA 
## 4 NA 
## 5 NA 
## 6 NA 
## 7 NA 
## 8 NA 
## 9 NA 
## 10 NA 
## # â€¦ with 2,080 more rows

```

WAT

Getting a single column back from `readr::read_[ct]sv()` is (generally) a tell-tale sign that the file format is amiss. Before donning a deerstalker (I just wanted the data!) I tried to just use good olâ€™ `read.csv()`:

```
read.csv(tfil, stringsAsFactors=FALSE)
## Error in make.names(col.names, unique = TRUE) : 
## invalid multibyte string at 'A'
## In addition: Warning messages:
## 1: In read.table(file = file, header = header, sep = sep, quote = quote, :
## line 1 appears to contain embedded nulls
## 2: In read.table(file = file, header = header, sep = sep, quote = quote, :
## line 2 appears to contain embedded nulls
## 3: In read.table(file = file, header = header, sep = sep, quote = quote, :
## line 3 appears to contain embedded nulls
## 4: In read.table(file = file, header = header, sep = sep, quote = quote, :
## line 4 appears to contain embedded nulls
## 5: In read.table(file = file, header = header, sep = sep, quote = quote, :
## line 5 appears to contain embedded nulls

```

WAT

Actually the â€œWATâ€� isnâ€™t *really* warranted since `read.csv()` gave us some super-valuable info via `invalid multibyte string at 'A'`. `FF FE` is a *big* signal1 2 weâ€™re working with a file in another encoding as thatâ€™s a common â€œmagicâ€� sequence at the start of such files.

But, I didnâ€™t want to delve into my Columbo personaâ€¦ *I. Just. Wanted. The. Data*. So, I tried the mind-bendingly fast and flexible helper from `data.table`:

```
data.table::fread(tfil)
## Error in data.table::fread(tfil) : 
## File is encoded in UTF-16, this encoding is not supported by fread(). Please recode the file to UTF-8.

```

*AHA*. UTF-16 (*maybe*). Letâ€™s poke at the raw file:

```
x <- readBin(tfil, "raw", file.size(tfil)) ## also: read_file_raw(tfil)

x[1:100]
## [1] ff fe 41 00 64 00 64 00 72 00 65 00 73 00 73 00 09 00 43 00
## [21] 69 00 74 00 79 00 09 00 43 00 6f 00 75 00 6e 00 74 00 72 00
## [41] 79 00 09 00 49 00 6e 00 64 00 65 00 78 00 09 00 4c 00 61 00
## [61] 62 00 65 00 6c 00 09 00 4c 00 61 00 74 00 69 00 74 00 75 00
## [81] 64 00 65 00 09 00 4c 00 6f 00 6e 00 67 00 69 00 74 00 75 00

```

Thereâ€™s our `ff fe` (which is the beginning of the possibility itâ€™s UTF-16) but that `41 00` harkens back to UTF-16â€™s older sibling UCS-2. The `0x00`â€˜s are embedded `nul`s (likely to get bytes aligned). And, there are *alot* of `09`s. Yâ€™know what *they* are? Theyâ€™re ``s. Thatâ€™s right. Tableau named file full of TSV records in an unnecessary elaborate encoding `CSV`. Perhaps they broke the â€œTâ€� on all their keyboards typing their product name so much.

### Living A Boyâ€™s [Data] Adventure Tale

At this point we have:

- no way to support an automated, reproducible workflow

- an ill-named file for what it contains

- an overly-encoded file for what it contains

- many wasted minutes (which is likely by design to have us give up and just use Tableau. No. Way.)


At this point Iâ€™m in full-on Rockford Files (pun intended) mode and delved down to the command line to use a old, trusted sidekick `enca`![](https://i0.wp.com/s.w.org/images/core/emoji/11/72x72/1f517.png?w=456&ssl=1)
![](https://i0.wp.com/s.w.org/images/core/emoji/11/72x72/1f517.png?w=456&ssl=1)
:

```
$ enca -L none Sheet_3_data.csv
## Universal character set 2 bytes; UCS-2; BMP
## LF line terminators
## Byte order reversed in pairs (1,2 -> 2,1)

```

Now, all we have to do is specify the encoding!

```
read_tsv(tfil, locale = locale(encoding = "UCS-2LE"))
## Error in guess_header_(datasource, tokenizer, locale) : 
## Incomplete multibyte sequence

```

WAT

Unlike the other 99% of the time (mebbe 99.9%) you use it, the tidyverse doesnâ€™t have your back in this situation (but it *does* have your backlog in that itâ€™s on the TODO).

Yâ€™know who *does* have your back? Base R!:

```
read.csv(tfil, sep="\t", fileEncoding = "UCS-2LE", stringsAsFactors=FALSE) %>% 
 as_tibble()
## # A tibble: 2,089 x 14
## Address City Country Index Label Latitude Longitude
## 
## 1 1627 Oâ€¦ Aubuâ€¦ Unitedâ€¦ 1 Paylâ€¦ 32.6 -85.4
## 2 900 Coâ€¦ Dothâ€¦ Unitedâ€¦ 2 Paylâ€¦ 31.3 -85.4
## 3 301 Coâ€¦ Florâ€¦ Unitedâ€¦ 3 Paylâ€¦ 34.8 -87.6
## 4 304 Oxâ€¦ Homeâ€¦ Unitedâ€¦ 4 Paylâ€¦ 33.5 -86.8
## 5 2000 Râ€¦ Hoovâ€¦ Unitedâ€¦ 5 Paylâ€¦ 33.4 -86.8
## 6 6140 Uâ€¦ Huntâ€¦ Unitedâ€¦ 6 Paylâ€¦ 34.7 -86.7
## 7 312 Scâ€¦ Mobiâ€¦ Unitedâ€¦ 7 Paylâ€¦ 30.7 -88.2
## 8 3402 Bâ€¦ Mobiâ€¦ Unitedâ€¦ 8 Paylâ€¦ 30.7 -88.1
## 9 5300 Hâ€¦ Mobiâ€¦ Unitedâ€¦ 9 Paylâ€¦ 30.6 -88.2
## 10 6641 Aâ€¦ Montâ€¦ Unitedâ€¦ 10 Paylâ€¦ 32.4 -86.2
## # â€¦ with 2,079 more rows, and 7 more variables:
## # Number.of.Records , State , Store.Number ,
## # Store.count , Zip.code , State.Usps ,
## # statename 

```

WAT WOOT!

Note that `read.csv(tfil, sep="\t", fileEncoding = "UTF-16LE", stringsAsFactors=FALSE)` would have worked equally as well.

### The Road Not [Originally] Taken

Since this activity decimated productivity, for giggles I turned to another trusted R sidekick, the `stringi` package, to see what it said:

```
library(stringi)

stri_enc_detect(x)
## [[1]]
## Encoding Language Confidence
## 1 UTF-16LE 1.00
## 2 ISO-8859-1 pt 0.61
## 3 ISO-8859-2 cs 0.39
## 4 UTF-16BE 0.10
## 5 Shift_JIS ja 0.10
## 6 GB18030 zh 0.10
## 7 EUC-JP ja 0.10
## 8 EUC-KR ko 0.10
## 9 Big5 zh 0.10
## 10 ISO-8859-9 tr 0.01

```

And, just so itâ€™s primed in the Google caches for future searchers, another way to get this data (and other data thatâ€™s even gnarlier but similar in form) into R would have been:

```
stri_read_lines(tfil) %>% 
 paste0(collapse="\n") %>% 
 read.csv(text=., sep="\t", stringsAsFactors=FALSE) %>% 
 as_tibble()
## # A tibble: 2,089 x 14
## Address City Country Index Label Latitude Longitude
## 
## 1 1627 Oâ€¦ Aubuâ€¦ Unitedâ€¦ 1 Paylâ€¦ 32.6 -85.4
## 2 900 Coâ€¦ Dothâ€¦ Unitedâ€¦ 2 Paylâ€¦ 31.3 -85.4
## 3 301 Coâ€¦ Florâ€¦ Unitedâ€¦ 3 Paylâ€¦ 34.8 -87.6
## 4 304 Oxâ€¦ Homeâ€¦ Unitedâ€¦ 4 Paylâ€¦ 33.5 -86.8
## 5 2000 Râ€¦ Hoovâ€¦ Unitedâ€¦ 5 Paylâ€¦ 33.4 -86.8
## 6 6140 Uâ€¦ Huntâ€¦ Unitedâ€¦ 6 Paylâ€¦ 34.7 -86.7
## 7 312 Scâ€¦ Mobiâ€¦ Unitedâ€¦ 7 Paylâ€¦ 30.7 -88.2
## 8 3402 Bâ€¦ Mobiâ€¦ Unitedâ€¦ 8 Paylâ€¦ 30.7 -88.1
## 9 5300 Hâ€¦ Mobiâ€¦ Unitedâ€¦ 9 Paylâ€¦ 30.6 -88.2
## 10 6641 Aâ€¦ Montâ€¦ Unitedâ€¦ 10 Paylâ€¦ 32.4 -86.2
## # â€¦ with 2,079 more rows, and 7 more variables: `Number of
## # Records` , State , `Store Number` , `Store
## # count` , `Zip code` , `State Usps` ,
## # statename 

```

(with similar dances to use `read_csv()` or `fread()`).

### FIN

The nightâ€™s quest to do some real work with the data was DoSâ€™d by what Iâ€™ll brazenly call a deliberate attempt to dissuade doing exactly that in anything but a commercial program. But, understanding the impact of yet-another massive retail store closing is super-important and it looks like it may be up to us (since the media is too distracted by incompetent leaders and inexperienced junior NY representatives) to do the work.

Folks whoâ€™d like to do the same can grab the UTF-8 encoded actual CSV from this site which has also been run through `janitor::clean_names()` so thereâ€™s proper column types and names to work with.

Speaking of which, hereâ€™s the `cols` spec for that CSV:

```
cols(
 address = col_character(),
 city = col_character(),
 country = col_character(),
 index = col_double(),
 label = col_character(),
 latitude = col_double(),
 longitude = col_double(),
 number_of_records = col_double(),
 state = col_character(),
 store_number = col_double(),
 store_count = col_double(),
 zip_code = col_character(),
 state_usps = col_character(),
 statename = col_character()
)

```

If you do anything with the data blog about it and post a link in the comments so I and others can learn from what youâ€™ve discovered! Itâ€™s already kinda scary that one doesnâ€™t even need a basemap to see just how much apart of â€˜Murica Payless was:

![](https://i0.wp.com/rud.is/b/wp-content/uploads/2019/02/payless-map-01.png?resize=780%2C521&ssl=1)
![](https://i0.wp.com/rud.is/b/wp-content/uploads/2019/02/payless-map-01.png?resize=780%2C521&ssl=1)



*Related*








---
