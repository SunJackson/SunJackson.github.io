---
layout:     post
catalog: true
title:      Building a shiny app to explore historical newspapers： a step-by-step guide
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/20KkwUFP5yU/
date:      2019-02-04
author:      Econometrics and Free Software
tags:
    - mutate
    - content_
    - files
    - library
    - french
---

### Step 2: Joining the data and the metadata

Now that I extracted the data from the ALTO files, and the metadata from the METS files, I stillneed to join both data sets and do some cleaning. What is the goal of joining these two sources?Remember, by doing this I will know which words come from which article, which will make thingsmuch easier later on. I explain how the code works as comments in the code block below:

Click if you want to see the code


```
library(tidyverse)
library(udpipe)
library(textrank)
library(tidytext)

# First, I need the path to each folder that contains the ALTO and METS files. Each newspaper
# data is inside its own folder, one folder per publication. Inside, there's `text` folder that
# contains the ALTO and METS files. This is also where I saved the .csv files from before.

pathdirs <- list.dirs(recursive = FALSE) %>%
 str_match(".*lunion.*") %>%
 discard(is.na)

# The following function imports the METS and the ALTO csv files, joins them, and does some 
# basic cleaning. I used a trick to detect German articles (even though L'Union is a French publication
# some articles are in German) and then remove them.

tidy_papers <- function(path){
 mets_path <- paste0(path, "/", list.files(path, ".*.xml.csv"))
 mets_csv <- data.table::fread(mets_path)

 alto_path <- paste0(path, "/text/", list.files(paste0(path, "/text/"), ".*.csv"))
 alto_csv <- map_dfr(alto_path, data.table::fread)

 final <- full_join(alto_csv, mets_csv, by = "begins") %>%
 mutate(content = tolower(content)) %>%
 mutate(content = if_else(str_detect(content, "hyppart1"), str_extract_all(content, "(?<=CONTENT_).*", simplify = TRUE), content)) %>%
 mutate(content = if_else(str_detect(content, "hyppart2"), NA_character_, content)) %>%
 # When words are separated by a hyphen and split over two lines, it looks like this in the data.
 # ex SUBS_TYPEHypPart1 SUBS_CONTENTexceptée
 # ceptée SUBS_TYPEHypPart2 SUBS_CONTENTexceptée
 # Here, the word `exceptée` is split over two lines, so using a regular expression, I keep
 # the string `exceptée`, which comes after the string `CONTENT`, from the first line and 
 # replace the second line by an NA_character_
 mutate(content = if_else(str_detect(content, "superscript"), NA_character_, content)) %>%
 mutate(content = if_else(str_detect(content, "subscript"), NA_character_, content)) %>%
 filter(!is.na(content)) %>%
 filter(type == "article") %>%
 group_by(id) %>%
 nest %>%
 # Below I create a list column with all the content of the article in a single string.
 mutate(article_text = map(data, ~paste(.$content, collapse = " "))) %>%
 mutate(article_text = as.character(article_text)) %>%
 # Detecting and removing german articles
 mutate(german = str_detect(article_text, "wenn|wird|und")) %>%
 filter(german == FALSE) %>%
 select(-german) %>%
 # Finally, creating the label of the article (the title), and removing things that are 
 # not articles, such as the daily feuilleton.
 mutate(label = map(data, ~`[`(.$label, 1))) %>%
 filter(!str_detect(label, "embranchement|ligne|bourse|abonnés|feuilleton")) %>%
 filter(label != "na")

 # Save the data in the rds format, as it is not a flat file
 saveRDS(final, paste0(path, "/", str_sub(path, 11, -1), ".rds"))
}

# Here again, I do this in parallel

library(furrr)

plan(multiprocess, workers = 8)

future_map(pathdirs, tidy_papers)
```

This is how one of these files looks like, after passing through this function:

![](https://i0.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/articles_rds.png?w=456)
![](https://i0.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/articles_rds.png?w=456)


One line is one article. The first column is the id of the article, the second column containsa data frame, the text of the article and finally the title of the article.Let’s take a look at the content of the first element of the *data* column:

![](https://i2.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/merged_alto_mets.png?w=456)
![](https://i2.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/merged_alto_mets.png?w=456)


This is the result of the merger of the METS and ALTO csv files. The first column is the id of thearticle, the second column contains each individual word of the article, the *label* column thelabel, or title of the article.

### Step 3: Part-of-speech annotation

Part-of-speech annotation is a technique with the aim of assigning to each word its part of speech.Basically, Pos annotation tells us whether a word is a verb, a noun, an adjective… This willbe quite useful for the analysis. To perform Pos annotation, you need to install the `{udpipe}`package, and download the pre-trained model for the language you want to annotate, in my case French:

Click if you want to see the code


```
# Only run this once. This downloads the model for French
udpipe_download_model(language = "french")

# Load the model
udmodel_french <- udpipe_load_model(file = 'french-gsd-ud-2.3-181115.udpipe')

# Save the path of the files to annotate in a list:
pathrds <- list.files(path = "./", all.files = TRUE, recursive = TRUE) %>% 
 str_match(".*.rds") %>%
 discard(is.na)

annotate_rds <- function(path, udmodel){

 newspaper <- readRDS(path)

 s <- udpipe_annotate(udmodel, newspaper$article_text, doc_id = newspaper$label)
 x <- data.frame(s)

 saveRDS(x, str_replace(path, ".rds", "_annotated.rds"))
}

library(furrr)
plan(multiprocess, workers = 8)
tic <- Sys.time()
future_map(pathrds, annotate_rds, udmodel = udmodel_french)
toc <- Sys.time()
toc - tic
```

And here is the result:

![](https://i0.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/pos_article.png?w=456)
![](https://i0.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/pos_article.png?w=456)


The *upos* column contains the tags. Now I know which words are nouns, verbs, adjectives, stopwords…Meaning that I can easily focus on the type of words that interest me. Plus, as an added benefit, Ican focus on the lemma of the words. For example, the word *viennent*, is theconjugated form of the verb *venir*. *venir* isthus the lemma of *viennent*. This means that I can focus my analysis on lemmata. This is useful,because if I compute the frequency of words, *viennent* would be different from *venir*, which isnot really what we want.

### Step 4: tf-idf

Just like what I did in my first blog post,I compute the tf-idf of words. The difference, is that here the “document” is the article. This meansthat I will get the most frequent words inside each article, but who are at the same time rarein the other articles. Doing this ensures that I will only get very relevant words for each article.

In the lines below, I prepare the data to then make the plots. The files that are created usingthe code below are available in the following Github link.

In the Shiny app, I read the data directly from the repo. This way, I can keep the app small in size.

Click if you want to see the code


```
path_annotatedrds <- list.files(path = "./", all.files = TRUE, recursive = TRUE) %>% str_match(".*_annotated.rds") %>%
 discard(is.na)

prepare_tf_idf <- function(path){

 annotated_newspaper <- readRDS(path)

 tf_idf_data <- annotated_newspaper %>%
 filter(upos %in% c("NOUN", "VERB", "ADJ", "PROPN")) %>%
 filter(nchar(lemma) > 3) %>%
 count(doc_id, lemma) %>%
 bind_tf_idf(lemma, doc_id, n) %>%
 arrange(desc(tf_idf)) %>%
 group_by(doc_id)

 name_tf_idf_data <- str_split(path, "/", simplify = 1)[1] %>%
 paste0("_tf_idf_data.rds") %>%
 str_sub(start = 9, -1)

 saveRDS(tf_idf_data, paste0("tf_idf_data/", name_tf_idf_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_tf_idf)
```

### Step 5: Summarizing articles by extracting the most relevant sentences, using `{textrank}`

The last step in data preparation is to extract the most relevant sentences of each articles, usingthe `{textrank}` package. This packages implements the *PageRank* algorithm developed by Larry Pageand Sergey Brin in 1995. This algorithm ranks pages by the number of links that point to the pages;the most popular and important pages are also the ones with more links to them. A similar approachis used by the implementation of `{textrank}`. The algorithm is explained in detail in the followingpaper.

However, I cannot simply apply `{textrank}` to the annotated data frame as it is. Because I haveseveral articles, I have to run the `textrank_sentences()` function, which extracts the relevantsentences, article by article. For this I still need to transform the data set and also need toprepare the data in a way that makes it digestible by the function. I will not explain the codebelow line by line, since the documentation of the package is quite straightforward. However,keep in mind that I have to run the `textrank_sentences()` function for each article, which explainsthat as some point I use the following:

```
group_by(doc_id) %>%
 nest() %>%
```

which then makes it easy to work by article (*doc_id* is the id of the articles). This part isdefinitely the most complex, so if you’re interested in the methodology described here, reallytake your time to understand this function. Let me know if I can clarify things!

Click if you want to see the code


```
library(textrank)
library(brotools)

path_annotatedrds <- list.files(path = "./", all.files = TRUE, recursive = TRUE) %>% str_match(".*_annotated.rds") %>%
 discard(is.na)

prepare_textrank <- function(path){

 annotated_newspaper <- readRDS(path)

 # sentences summary
 x_text_rank <- annotated_newspaper %>%
 group_by(doc_id) %>%
 nest() %>%
 mutate(textrank_id = map(data, ~unique_identifier(., c("paragraph_id", "sentence_id")))) %>%
 mutate(cleaned = map2(.x = data, .y = textrank_id, ~cbind(.x, "textrank_id" = .y))) %>%
 select(doc_id, cleaned)

 x_text_rank2 <- x_text_rank %>%
 mutate(sentences = map(cleaned, ~select(., textrank_id, sentence))) %>%
 # one_row() is a function from my own package, which eliminates duplicates rows
 # from a data frame
 mutate(sentences = map(sentences, ~one_row(., c("textrank_id", "sentence"))))

 x_terminology <- x_text_rank %>%
 mutate(terminology = map(cleaned, ~filter(., upos %in% c("NOUN", "ADJ")))) %>%
 mutate(terminology = map(terminology, ~select(., textrank_id, "lemma"))) %>%
 select(terminology)

 x_final <- bind_cols(x_text_rank2, x_terminology)

 possibly_textrank_sentences <- possibly(textrank_sentences, otherwise = NULL)

 x_final <- x_final %>%
 mutate(summary = map2(sentences, terminology, possibly_textrank_sentences)) %>%
 select(doc_id, summary)

 name_textrank_data <- str_split(path, "/", simplify = 1)[1] %>%
 paste0("_textrank_data.rds") %>%
 str_sub(start = 9, -1)

 saveRDS(x_final, paste0("textrank_data/", name_textrank_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_textrank)
```

You can download the annotated data sets from the followinglink. This is howthe data looks like:

![](https://i0.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/textrank_df.png?w=456)
![](https://i0.wp.com/www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/img/textrank_df.png?w=456)


Using the `summary()` function on an element of the *summary* column returns the 5 most relevantsentences as extracted by `{textrank}`.
