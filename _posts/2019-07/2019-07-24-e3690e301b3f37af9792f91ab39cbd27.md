---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/07/24/distilled-news-1141/
date:      2019-07-24
author:      Michael Laux
tags:
    - data
    - predicts
    - predictive
    - predictions
    - models
---

**The Curse of Dimensionality**

Have you ever been in the middle of telling someone a story or struggling through a long explanation of something complicated when the other person looks at you and asks, ‘What’s the point?’ First, your friend is so rude! But we have also been in your friend’s shoes too – we are all busy people with places to go and folks to see. We want our information quick and to the point. That is the essence of dimensionality reduction. When confronted with a ton of data, we can use dimensionality reduction algorithms to make the data ‘get to the point’. In a previous post, I covered PCA, a powerful and versatile dimensionality reduction algorithm that unearths the underlying trends in our data. There are other options as well – such as non-negative matrix factorization, linear discriminant analysis and more (I promise to cover these in the future). However, today our topic is not about a specific algorithm, but rather about why we need dimensionality reduction algorithms in the first place – The Curse of Dimensionality.

**Visualizing and Animating Optimization Algorithms with Matplotlib**

In this series of notebooks, we demonstrate some useful patterns and recipes for visualizing animating optimization algorithms using Matplotlib.

**AI: Almost Immortal**

Healthcare’s AI revolution is changing the way we think about age-related diseases, even aging itself We are in the midst of an epidemic. Regardless of your family history, race, or geography, there is a disease that will befall each and every one of us. You can hide in the mountains of Siberia, but the disease will still reach you because it’s not contagious. It’s followed humanity throughout time, and will continue to do so into the foreseeable future despite our recent attempts to forestall it. That disease is called aging.

**An Indian research university has assembled 73 million journal articles (without permission) and is offering the archive for unfettered scientific text-mining**

The JNU Data Depot is a joint project between rogue archivist Carl Malamud (previously), bioinformatician Andrew Lynn, and a research team from New Delhi’s Jawaharlal Nehru University: together, they have assembled 73 million journal articles from 1847 to the present day and put them into an airgapped respository that they’re offering to noncommercial third parties who want to perform textual analysis on them to ‘pull out insights without actually reading the text.’ This text-mining process is already well-developed and has produced startling scientific insights, including ‘databases of genes and chemicals, map[s of] associations between proteins and diseases, and [automatically] generate[d] useful scientific hypotheses.’ But the hard limit of this kind of text mining is the paywalls that academic and scholarly publishers put around their archives, which both limit who can access the collections and what kinds of queries they can run against them.

**Talk**

An open-source commenting platform focused on better conversation

**Time series forecast cross-validation**

Time series cross-validation is important part of the toolkit for good evaluation of forecasting models. forecast::tsCV makes it straightforward to implement, even with different combinations of explanatory regressors in the different candidate models for evaluation. Suprious correlation between time series is a well documented and mocked problem, with Tyler Vigen’s educational website on the topic (‘per capita cheese consumption correlated with number of people dying by becoming entangled in their bedsheets’) even spawning a whole book of humourous examples. Identifying genuinely-correlated series can be immensely helpful for time series forecasting. Forecasting is hard, and experience generally shows that complex causal models don’t do as well as much simpler methods. However, a well chosen small set of ‘x regressors’ can improve forecasting performance in many situations. I have been investigating one of those situations for a future blog post on forecasting unemployment rates. One method I’ll be using is time series cross-validation, as well described in this Rob Hyndman post. The implementation was sufficiently non-trivial in my real-data case that I’m writing today a separate post with simulated data to be sure I’m doing it right.

**How to put your machine learning in a Docker**

You are a data scientist, you understand the business issue, you collect data, perform some feature engineering, and came up with an amazing machine learning model that predicts stuff very well on your local IDE.

**Do NLP Entailment Benchmarks Measure Faithfully?**

This post may be read by itself or as the second installment of ‘The Emperor’s New Benchmarks’. That post looked at a benchmark of passing curiosity (pun detection), but now we consider a problem that commands the community’s ongoing veneration.

**Detecting the Onset of Machine Failure using Anomaly Detection Techniques**

Numerous factors can contribute to the quality of a product, and not every one of these factors is under manufacturers control. One of the most common sources of quality problems is faulty equipment which has not been properly maintained. Hence, monitoring the condition of machines and components such as cooling fans, bearings, turbines, gears, belts and maintaining a desirable working state becomes very crucial. When a machine or a component fails, corrective maintenance is performed to identify the cause of failure and decide on repair procedures required to maintain and re-initiate the machine to its normal working conditions. However, because the machine has already failed without any prior warnings, time is required for procuring and repairing of the failed component. Therefore, a maintenance strategy needs to be considered to minimize the downtime of service. But machines and their components degrade through time, and the time of failure is not known in advance. Hence, time-based maintenance strategies are predominantly used for maintaining the conditions of machines and equipment. In this article, I will discuss various techniques that can be used to detect the onset of failure occurring in machines.

**Why Deep Learning Works: solving a farmer’s problem**

In the beginning was the neuron: understanding gradient descent, back propagation, linear regression, logistic regression, autoencoders, convolutional neural networks and VGG16. With visual aids and practical hands-on coding in Python & Keras.

**How to tune hyperparameters of tSNE**

This is the second post of the column Mathematical Statistics and Machine Learning for Life Sciences. In the first post we discussed whether and where in Life Sciences we have Big Data suitable for Machine / Deep Learning, and emphasized that Single Cell is one of the most promising Big Data resources. t-distributed stochastic neighbor embedding (tSNE) is a Machine Learning non-linear dimensionality reduction technique which is absolutely central for Single Cell data analysis. However, the choice of hyperparameters for the tSNE might be confusing for beginners. In this post, I will share my recommendations on selecting optimal values of hyperparameters such as perplexity, number of principal components to keep, and number of iterations for running tSNE.

**Streamlining Predictive Modeling Workflow with Sagemaker and Essentia**

Ecommerce sites generate tons of web server log data which can provide valuable insights through analysis. For example, if we know which users are more likely to buy a product, we can perform targeted marketing, improve relevant product placement on our site and lift conversion rates. However, raw web logs are often enormous and messy so preparing the data to train a predictive model is time consuming for data scientists. According to a recent Forbes article, ‘Data preparation accounts for about 80% of the work of data scientists’, but ‘76% of data scientists view data preparation as the least enjoyable part of their work’. But most data scientists know that if the preparation steps are not properly done, then the rest of their efforts will be wasted. This blog post demonstrates how to streamline the process of data preparation for predictive modeling by combining Amazon SageMaker with AuriQ’s Essentia for a large corporate client.

**Introducing ILI Nearby**

ILI Nearby is a data visualization web application that uses machine learning, data analytics, and crowd-sourcing methods to generate geographically detailed real-time estimates (nowcasts) of influenza-like-illness in the United States.

**How to Choose Between Multiple Models**

The fundamental challenge discussed in this article is that a model’s performance against a specific data set doesn’t guarantee that it will perform well against other data sets. Forcing a mathematical model to match a given data set only ensures that it matches that specific data set, and doesn’t say anything about it’s predictive power. This challenge can be overcome through rounds of development, validation, and testing using different data sets. The first step is developing the model against a training data set, and ensuring that the model can accurately predict the results in that easiest case. The second step is comparing the best performing models against a second data set, the validation data set. The validation data set should include points spanning the range of values included in the training data set, but should not include any identical points. This second check goes a long way to ensuring that a model will have reasonable predictive power. The final step is to choose the model which performed best against the validation data set and compare it against a third data set, the testing data set. If the model performs well against that data set, then it can reasonably be used to create predictions.

### Like this:

Like Loading...
