---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/07/23/distilled-news-1140/
date:      2019-07-23
author:      Michael Laux
tags:
    - data
    - modeled
    - models
    - modeling
    - learning
---

**Artificial Neural Network and it’s contribution to Machine Learning – A beginner’s hand-book**

Artificial Neural Networks(ANN) is a computational nonlinear model which is widely used in Machine Learning and is considered to be a prominent component of futuristic Artificial Intelligence.

**Inferential Statistics: Hypothesis Testing Using Normal Deviate Z -Test**

In data engineering you will always find an instance where you need to establish whether the data sample which you have got from population data, is reliable enough to build a model around it. There can be an instance where you may have got the data from the old archive, which may not represent the true behavior of process modeled around it in a production environment, with time behavior changes and so the process on which model was built. So if we go ahead and build our new model around such old sample data, we may end up with a faulty process and the model will not be effective or useful.So what we do is to perform certain inferential statistical test to ensure data is reliable.

**Chi-square Test of Independence**

In an independent-samples t-test, we compare the means of twogroups. The variable on which we are comparing the two groups must be aninterval or ratio-level variable. What if the variable is a nominalvariable? For example, are women more likely to graduate from high school than men? Herewe are comparing two groups: men and women. However, graduationstatus (graduated/not graduated) is a nominal variable. Nominal variables have no meansand thus an independent-samples t-test is impossible. Nominalvariables do have frequencies, though, and these frequencies can be compared.Specifically, we can ask whether the ratio of graduates to non-graduates ishigher for women than it is for men. Another way of thinking about this is whether two nominal variables havea relationship. If they do, the ratio of one variable’s frequenciesdepends on the value of the other variable. If sex and graduation status are related, the proportion of men whohave graduated from high school is different from that of women.

**Don’t Trust a Model Because it “Works”**

People often try to convince others that their predictions should be believed because they resulted from some statistical model that ‘works’. What this typically means is that the model predicted well in the past, and should therefore be trusted to do so in the future. It sounds intuitive: just as we are more likely to trust a machine (for instance a car) that has rarely malfunctioned, we might believe that a model that worked well in the past will continue doing so. When considering different models, why not ‘backtest’ them to see which one did the best? I will argue that this is an important category error: statistical models are not machines, and should not be treated as such.

**Targeted Sentiment analysis vs Traditional Sentiment analysis**

Want to find the sentiment of a sentence towards an entity within that sentence? Then Sentiment Analysis might not be enough for you…Read on to find on about Targeted Sentiment Analysis!

**How To Analyze Survey Data With Python**

How To Read SPSS-/SAV Data With Python, Using Weights And Performing Typical SPSS-Commands As Frequencies And Crosstabs.

**Unified Language Model Pre-training for Natural Language Understanding and Generation**

Recent state-of-the-art NLP pre-trained models also use a language model to learn contextualized text representation. From ELMo (Peter et al., 2018), GPT (Radford et al., 2018) to BERT (Devlin et al., 2018), all of them use language model (LM) to achieve a better result. Dong et al. present a new model, Unified Language Model (UNILM), to tackle natural language understanding (NLU) and natural language generation (NLG) which is trained by English Wikipedia and BookCorpus. Different from ELMo (Peter et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2018), UNILM implement unidirectional language model (LM), bidirectional language model (LM) and sequence-to-sequence language model (LM) for different tasks. The following figure shows the models’ architecture. Unidirectional (left-to-right and right-to-left) LSTM is applied by ELMo . A left-to-right transformer is leveraged by GPT while BERT uses a bidirectional Transformer to learn text representation.

**Facebook vs. EU Artificial Intelligence and Data Politics**

This article is a summary of the paper by the European Union Agency for Fundamental Rights (FRA) called Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights. I then proceed to look at the recent move by Facebook in its data politics; statements made by Zuckerberg; and their recent hiring of previous Deputy Prime Minister Nick Clegg as head of global policy and communications. It is my wish that this makes EU policy more comprehensible and give you an overview of a few actions taken by Facebook in this regard.

**Anatomy of an enterprise-scale AI strategy**

When it comes to AI, companies typically test the waters proof of concepts or small-scale use cases, taking advantage of vendor offerings, such as new features in their existing SaaS platforms. If things go well, they pursue another project, then another – and soon they’re relying on a sprawl of incompatible systems, competing data lakes, problems with cost overruns, duplication of efforts, and an inability to scale, not to mention privacy, compliance or ethics problems.

**Faster Neural Network Training with Data Echoing**

In the twilight of Moore’s law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce ‘data echoing,’ which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or ‘echoes’) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline’s predictive performance using less upstream computation. In some cases, data echoing can even compensate for a 4x slower input pipeline.

**Collective Transparency**

As our privacy continues to be challenged by the endless pursuit of data, does collective transparency offer a solution?

**Data Exploration with Adversarial Autoencoders**

Like many, I started my journey into deep learning with Autoencoders as they are a nice entry point for developing an intuition for artificial neural networks and to get your head around the deep learning framework of your choice. What started as an exercise with not many relevant use-cases already found many applications today, and turned out to be a deep learning swiss army knife. Autoencoders are often used for the detection of anomalies, to learn low dimensional representations that you can feed into other neural networks, to generate data and more. In this article, I want to introduce you to a special architecture called Adversarial Autoencoders, and with it, a new application for autoencoders, the unsupervised clustering of data, which I will mainly focus on in this article. I want to discuss the workings of such an encoder and show you how to easily build and train them with Keras and how to use them to cluster and explore time-series on an example involving foreign exchange market data.

**Creating An Explainable Machine Learning Algorithm**

A couple of years back I started learning Python and R with the goal of learning how to apply their optimization, statistics, data science, machine learning and data visualization packages and libraries.I also bought stock market data (horse racing data wasn’t available) with the idea that I would test the various models with several goals in mind:• Learn how the model works• Learn Python and R• Identify those stocks that might be good investment candidatesI have an Operations Research (prescriptive analytics for any data scientists reading this) background and have done a lot of modeling in PL1, Fortran and Fortran statistical and optimization libraries, C and SAS in a variety of industries to solve a variety of business problems. So goals (1) and (2) weren’t very hard to achieve.

**Automation in Data Science**

To summarize everything:• Talking to the client, and explaining results to the client, are the two aspects of work that you would least associate with data science, but they are also the hardest ones to automate.• Data preparation and cleaning are partially automated, but are hard to automate completely because there are thousands of special cases that need to be accounted for.• Data exploration and feature engineering are partially automated and it looks like feature engineering will become more heavily automated soon.• Model building is already fully automated and competitive with experts.• Deployment to production can be automated, but it’s up to the client to do so.

### Like this:

Like Loading...
