---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/07/04/distilled-news-1122/
date:      2019-07-04
author:      Michael Laux
tags:
    - modeling
    - data
    - models require
    - programs
    - learning
---

**NLP: Text Data To Numbers**

Working on a NLP project can be a tedious task, in particular when the data is in textual format and the models require numerical values. This article explains how we can convert text to numerical values.

**6 Powerful Open Source Machine Learning GitHub Repositories for Data Scientists**

• XLNet: The Next Big NLP Framework• Implementation of XLNet in PyTorch• Google Research Football – A Unique Reinforcement Learning Environment• Implementation of the CRAFT Text Detector• MMAction – Open Source Toolbox for Action Understanding in Videos• TRAINS – Auto-Magical Experiment Manager & Version Control for AI

**The power of unbiased recursive partitioning**

The significance tests underlying the unbiased tree algorithms CTree, MOB, and GUIDE are embedded into a unifying framework. This allows to assess relative strengths and weaknesses in a variety of setups, highlighting the advantages of score-based tests (as in CTree/MOB) vs. residual-based tests (as in GUIDE).

**Generating Beatles’ Lyrics with Machine Learning**

The Beatles were a huge cultural phenomenon. Their timeless music still resonates with people today, both young and old. Personally, I’m a big fan. In my humble opinion, they are the greatest band to have ever lived¹. Their songs are full of interesting lyrics and deep ideas. Take these bars for example: When you’ve seen beyond yourself Then you may find peace of mind is waiting there² Powerful stuff. However, the thing that made the Beatles great was their versatility. Some of their songs are deep and thoughtful while others are fun and lighthearted. Unsurprisingly, the biggest theme that weaves throughout their lyrics is love. Here is one such verse: My sweet little darling can’t you see, how wonderful it is when you’re mine. If only I could be your lover forever, I can never get my heart, I can’t get my mind. Actually, these lyrics weren’t written by any Beatles you may know. Not by Lennon, or McCartney, or Harrison, or even, God forbid, Ringo Starr (just kidding, Ringo’s alright). They were actually generated by a Machine Learning model, namely OpenAI’s GPT-2 [1]. Although this uses their smallest model, the results are quite astonishing. But before we get too ahead of ourselves, let’s take a step back and look at how this all works. As always, full working code is available on my Github.

**Illustrating Predictive Models with the ROC Curve**

Data science is a term that has really grown within the last few years, and it seems like everyone wants to get on board. One of the most attractive objectives is to use the power of a data asset to create machine learning models capable of predicting various outcomes. With a well-defined model, one can identify top influential factors capable of predicting outcomes, develop valuable insight for strategic hypotheses, or even implement the model’s logic into software applications with a friendly user interface. However, before any of this magic happens, we need to know if the predictions created by the model are any good! For example, we would all be furious if our email program’s spam classifier was only able to detect 50% of those unwanted email or solicitations. With this post I go over how to evaluate a predictive model with a classical tool that every data scientist should be familiar with: the Receiver Operating Characteristic (ROC) curve.

**FaceMath: An Algebra of Ideas in A.I.**

Close your eyes, and think in your head about a doll. Now image that same doll can talk. Watch the doll talking. What you just did there in your head – the thing you can see talking – is the merger of the idea of a doll with the idea of talking. As it turns out, we can use artificial intelligence to add concepts together using algebra, and that’s what this article is about. I like to call it ‘an algebra of ideas’, because we are using basic math to add concepts together, forming compound concepts. Let’s talk about ideas and turning them into numbers. Back in this cartoon AI article, we saw how blocks of text could be turned into special fixed-length lists of numbers (embedded into vectors), with the clustering of those vectors revealing information about the topic of groups of comics. Individual words can also be embedded into vectors, with their meaning given through their relationships to other words. The traditional example is that the vectors ‘king’ – ‘man’ + ‘woman’ = ‘queen’. This means that we can learn a representation such that the vector for king, man, and woman can be used to find the vector for queen. This is algebra on words, but we can do even more! It turns out you can do this with images as well!

**A quick run-through of Holt-Winters, Seasonal ARIMA and FB Prophet**

The purpose of this post is to show a quick run-through of Holt Winters, SARIMA and FB Prophet. I am skipping anything about parameter tuning as that could be multiple posts on its own.

**AI needs a new developer stack!**

In today’s world, data has played a huge role in the success of technology giants like Google, Amazon, and Facebook. All of these companies have built massively scalable infrastructure to process data and provide great product experiences for their users. In the last 5 years, we’ve seen a real emergence of AI as a new technology stack. For example, Facebook built an end-to-end platform called FBLearner that enables an ML Engineer or a Data Scientist build Machine Learning pipelines, run lots of experiments, share model architectures and datasets with team members, scale ML algorithms for billions of Facebook users worldwide. Since its inception, millions of models have been trained on FBLearner and every day these models answer billions of real-time queries to personalize News Feed, show relevant Ads, recommend Friend connections, etc.

**Testing Statistical Software**

Recently I’ve been implementing and attempting to extend some computationally intense methods. These methods are from papers published in the last several years, and haven’t made their way into mainstream software libraries yet. So I’ve been spending a lot of time reading research code, and I’d like to share what I’ve learned. In this post, I describe how I evaluate the trustworthiness of a modeling package, and in particular what I want from the test suite. If you use statistical software, this post will help you evaluate whether a package is worth using. If you write statistical software, this post will help you confirm the correctness of the code that you write.

**Abigail Echo-Hawk on the art and science of ‘decolonizing data’**

The chief research officer of the Seattle Indian Health Board is creating programs and databases that are not based on Western concepts to better serve indigenous communities.

**Gen – A general-purpose probabilistic programming system with programmable inference.**

Probabilistic modeling and inference are core tools in diverse fields including statistics, machine learning, computer vision, cognitive science, robotics, natural language processing, and artificial intelligence. To meet the functional requirements of applications, practitioners use a broad range of modeling techniques and approximate inference algorithms. However, implementing inference algorithms is often difficult and error prone. Gen simplifies the use of probabilistic modeling and inference, by providing modeling languages in which users express models, and high-level programming constructs that automate aspects of inference. Like some probabilistic programming research languages, Gen includes universal modeling languages that can represent any model, including models with stochastic structure, discrete and continuous random variables, and simulators. However, Gen is distinguished by the flexibility that it affords to users for customizing their inference algorithm. It is possible to use built-in algorithms that require only a couple lines of code, as well as develop custom algorithms that are more able to meet scalability and efficiency requirements. Gen’s flexible modeling and inference programming capabilities unify symbolic, neural, probabilistic, and simulation-based approaches to modeling and inference, including causal modeling, symbolic programming, deep learning, hierarchical Bayesiam modeling, graphics and physics engines, and planning and reinforcement learning. Gen is a package for the Julia programming language. Gen consists of multiple modeling languages that are implemented as DSLs in Julia and a Julia library for inference programming.

**XLNet outperforms BERT on several NLP Tasks**

XLNet is a new pretraining method for NLP that achieves state-of-the-art results on several NLP tasks. Two pretraining objectives that have been successful for pretraining neural networks used in transfer learning NLP are autoregressive (AR) language modeling and autoencoding (AE). Autoregressive language modeling is not able to model deep bidirectional context which has recently been found to be effective in several downstream NLP tasks such as sentiment analysis and question answering. On the other hand, autoencoding based pretraining aims to reconstruct original data from corrupted data. A popular example of such modeling is used in BERT, an effective state-of-the-art technique used to address several NLP tasks.

**Hubway Station Metrics**

Hubway, a bike sharing system in Boston, was launched in July of 2011. In the past 8 years, they have expanded to over 150 locations throughout the city.In 2014, as a part of a data science challenge, Hubway made 3 years of its data public. This reflected every time a user started or ended a trip. The main data features included of:• Start Time and Location• End Time and Location• User Type• User Gender (if user is a subscriber)From these data points we can extrapolate information on how Boston uses this new transportation service, and how Hubway can more effectively market to its customer base.

**NLP: Text Mining Algorithms**

Explaining N-Grams, Bag Of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) algorithms and their implementation in Python.

### Like this:

Like Loading...
