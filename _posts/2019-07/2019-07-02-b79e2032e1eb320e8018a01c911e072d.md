---
layout:     post
catalog: true
title:      Don't cite the No Free Lunch Theorem
subtitle:      转载自：https://peekaboo-vision.blogspot.com/2019/07/dont-cite-no-free-lunch-theorem.html
date:      2019-07-02
author:      Andreas Mueller (noreply@blogger.com)
tags:
    - wolpert
    - learning
    - assumptions
    - actual
    - citing
---

























### 
Don't cite the No Free Lunch Theorem


The paper on the “No Free Lunch Theorem”, actually called "*The Lack of A Priori Distinctions Between Learning Algorithms*" is one of these papers that are often cited and rarely read, and I hear many people in the ML community refer to it when supporting the claim that “one model can’t be the best at everything” or “one model won’t always be better than another model”.

The point of this post is to convince you that this is not what the paper or theorem says (at least not the one usually cited by Wolpert), and you should not cite this theorem in this context; and also that common versions cited of the "No Free Lunch" Theorem are not actually true.
Multiple Theorems, one Name
The first problem is that there are at least two theorems with the name “no free lunch” that I know about. One by Wolpert, first published in *The Lack of A Priori Distinctions Between Learning Algorithms* (there's actually multiple but they go in the same direction), and one in *Understanding Machine Learning* By Shalev-Shwarz and Ben-David (a very excellent book!).
Wolpert also published a “No Free Lunch in Optimization”, but I'm only concerned with the theorem for supervised learning. The Theorem in *Understanding Machine Learning* is actually quite different, and I’ll it below. I think the goal of Shalev-Shwarz and Ben-David was to formalize the folk wisdom of the “no free lunch” theorem in a different way than Wolpert did, and their theorem actually says “One model can’t always win”, though in a very specific way (and if you cite this one, there’s other caveats, see below). In a way, the theorem they present is much clearer in what we actually learn from it. But I’m not sure giving it a name that was already taken was a good idea.
So what does it say?
The main statement of the original paper can be summarized as “Under the assumptions of the theorem, any two prediction functions have the same ability to generalize”.
There are two crucial parts to this: the assumptions and the conclusions. Let’s start with trying to understand the conclusion. It is often read to mean something like “Gradient Boosting can’t always win”. Instead, what it actually says is that “Gradient boosting is as good as always predicting the most frequent class”. Or, “Neural networks are as good as predicting the **least** frequent class.”
Clearly these statements don’t correspond to our actual experience in machine learning practice. Always predicting the least frequent class is clearly a terrible strategy. But according to the theorem, it’s as good as the best state-of-the-art model you can find with respect to generalization properties. So what’s happening here?
The Assumptions
The key to understanding the theorem is in understanding the assumptions of the theorem. The theorem does not use one of the most commonly used assumptions in machine learning theory, which is that the data is drawn i.i.d. from some given distribution.
Instead, Wolpert assumes the data is a finite set, and training and test set are disjoint and drawn from a discrete distribution. This does sound reasonable; in practice our data is always finite, and we want to generalize to new data that we haven’t seen before.
Making these assumptions allow Wolpert to average over all possible datasets. The statement of the theorem is comparing two algorithms over all possible datasets generated using these assumptions.**While these assumptions sound reasonable for doing machine learning, they really are not. What these assumptions are saying is thata) the test data and the training data are statistically independent,** i.e. the test data has nothing to do with the training data, and**b) the labels have nothing to do with the features** (because we are averaging over all possible labelings).

Phrasing it this way, these assumptions are clearly not good for doing any predictive modelling.
So what does it mean?
Now that we revisited both conclusion and assumptions, let’s try to summarize, or maybe rephrase the No Free Lunch theorem by Wolpert. The conclusion includes “any model is as good as predicting the minority class”. What that really says is that “learning is impossible”.
Given our understanding of the assumptions, the full statement of the Theorem is something like:
“**If the training set has nothing to do with the test set, and the features have nothing to do with the labels, learning is impossible**”.

This statement makes intuitive sense, but it is very far from what I hear commonly associated with the theorem. The most sensible reading of the theorem is “**You need to make assumptions in order for learning to be possible**”. However, what it really shows is that if you make the specific assumption of the theorem, learning is not possible. So if you want to claim generally that “learning requires assumptions”, I don’t think you should cite this paper.
What (I think) Wolpert’s point was
I think the point of the paper is to challenge the i.i.d. assumption. Wolpert gives (good) reasons why he thinks it’s not appropriate, and why machine learning theory should explore other frameworks. In particular, there is a good case to be made for modeling datasets as being finite.
If this is the case, then the i.i.d. assumption would allow points to be both in the training and the test set. Clearly that’s not the point of generalization. So Wolpert requires training and test set to be disjoint, which also makes sense.
However, the consequence is that training and test set (and their labels) are completely independent now, which is strange.
I don’t know if he actually thought this was a good framework for machine learning. I assume his motivation was to challenge the field to revisit assumptions and find alternatives to the i.i.d. assumption that more closely resemble machine learning practice. Many years later, it seems to me there was an unfortunate lack of follow up from the rest of the community, and a clear misunderstanding of the theorem by many machine learning practitioners.
The other “No Free Lunch Theorem”
As I mentioned, there’s another “No Free Lunch Theorem”. It’s quite different in that it does use an i.i.d. assumption for evaluating a model. In other ways it’s quite similar in that it makes use of the fact that without additional assumptions, if you see part of the data, the remaining part could have any possible labeling. More concretely, the theorem says “**For any predictive algorithm there is a dataset on which it fails, i.e. a dataset on which a different learner would perform better**”. However, this does not prevent statements like “Algorithm X is always better than Algorithm Y”, because the algorithm that does better is not realizable (it's the algorithm that produces the true answer for this dataset without looking at the data). **Under this framework I’m sure you could easily prove that in an imbalanced dataset, it’s better (in expectation) to predict the majority class than to predict the minority class, a statement that is not true in Wolpert’s framework.**
What to Cite
I think there are very few cases when citing Wolpert supports whatever point you’re making.
**If your point is “No model can always be best”, I would suggest citing Shalev-Shwartz and Ben-David.**
If your point is “Learning is impossible without proper assumptions”, you might cite the whole chapter by Shalev-Shwartz and Ben-David. I’m not sure there’s a good way to make this statement in a formal way. You could cite Wolpert if you really want to, but I think that might be more confusing than helpful. If your point is “The i.i.d. Assumption is weird in the presence of finite data”, definitely cite Wolpert!**Finally, if your point is “Gradient boosting can’t always be better than neural networks because of the No Free Lunch Theorem”, then, as far as I know, you’re wrong, and there is nothing that would prohibit a statement of this form**. I don’t believe that there’s many strict “always worse” or “always better” relationships between commonly used ML algorithms, but I’m also not aware of any theoretical reason that would prevent such statements to exist (in a framework where learning is possible). And as I said above, predicting the majority class is always better than predicting the minority class, for a reasonable definition of “always”.
Learning More
If you’re interested in learning more about Machine Learning theory, I think the book by Shalev-Shwartz and Ben-David is actually quite excellent. I also really enjoyed “Foundations of Machine Learning” by Mehryar Mohri, Afshin Rostamizadeh and Ameet Talwalkar. Both books are available as PDFs on the author websites I linked to. I’m by no means a theory person, but I think having some background on machine learning theory helps provide a framework in reasoning about algorithms.
Of course you can also check out Wolpert’s paper, though I think that’s mostly interesting if you want to learn why he doesn’t like the i.i.d. Assumption - so it’s more about philosophy of machine learning theory than about standard machine learning theory.


### 
So what does it say?

### 
So what does it mean?

### 
The other “No Free Lunch Theorem”

### 
Learning More

As this is clearly impossible, I went to work straight away.

This is the result:

[edit2]**clarification: With ensemble classifiers and ensemble regressors I mean random forests**, **extremely randomized trees, gradient boosted trees**, and the soon-to-be-come weight boosted trees (adaboost).[/edit2]

Needless to say, this sheet is completely authoritative.








A Wordcloud in Python





Last week I was at Pycon DE, the German Python conference. After hacking on scikit-learn a lot last week, I decided to to something different on my way back, that I had planned for quite a while:doing a wordl-like word cloud.I know, word clouds are a bit out of style but I kind of like them any way. My motivation to think about word clouds was that I thought these could be combined with topic-models to give somewhat more interesting visualizations.

So I looked around to find a nice open-source implementation of word-clouds ... only to find none. (This has been a while, maybe it has changed since).

While I was bored in the train last week, I came up with this code.A little today-themed taste:








Kernel Approximations for Efficient SVMs (and other feature extraction methods) [update]





Recently we added another method for kernel approximation, the Nyström method, 
to scikit-learn, which will be featured in the upcoming 0.13 release.Kernel-approximations were my first somewhat bigger contribution to 
scikit-learn and I have been thinking about them for a while.To dive into kernel approximations, first recall the kernel-trick.






MNIST for ever....





**[update]**This post is a bit old, but many people still seem interested. So just a short update:
Nowadays I would use Python and scikit-learn to do this. Here is an example of how to do cross-validation for SVMs in scikit-learn.****Scikit-learn even downloads MNIST for you. [/update]
MNIST is, for better or worse, one of the standard benchmarks for machine learning and is also widely used in then neural networks community as a toy vision problem.

### MNIST for ever....

Just for the unlikely case that anyone is not familiar with it:It is a dataset of handwritten digits, 0-9, in black on white background.It looks something like this:

There are 60000 training and 10000 test images, each 28x28 gray scale.There are roughly the same number of examples of each category in the test and training datasets.I used it in some papers myself even though there are some reasons why it is a little weird.

Some not-so-obvious (or maybe they are) facts are:- The images actually contain a 20x20 patch of digit and where padded to …


### Graphcuts for Python: pygco (slight update)
