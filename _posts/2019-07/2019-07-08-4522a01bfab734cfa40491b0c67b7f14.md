---
layout:     post
catalog: true
title:      Classifying Heart Disease Using K-Nearest Neighbors
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/OvCQDZepJw0/classifying-heart-disease-using-k-nearest-neighbors.html
date:      2019-07-08
author:      Matt Mayo Editor
tags:
    - algorithms
    - distance
    - learning algorithm
    - similarity
    - neighbors
---

**By Nagesh Singh Chauhan, Big data developer at CirrusLabs**

Classification of objects is an important area of research and application in a variety of fields. In the presence of full knowledge of the underlying probabilities, Bayes decision theory gives optimal error rates. In those cases where this information is not present, many algorithms make use of distance or similarity among samples as a means of classification.

The article has been divided into 2 parts. In the first part, we’ll talk all about the K-NN machine learning algorithm and in the second part, we will implement K-NN in real life and classify Heart disease patients.

**Table of content**

What is a K-NN algorithm?
How does the K-NN algorithm work?
When to choose K-NN?
How to choose the optimal value of K?
What is Curse of dimensionality?
Building K-NN classifier using python sci-kit learn.
How to improve the performance of your classifier?

 

### What is a K-NN Algorithm?

 ![K-NN Algorithm representation](https://cdn-images-1.medium.com/max/800/1*ZSN4cdbDzufJcUBieGUp7A.png)


K-NN or K-Nearest Neighbors is one of the most famous classification algorithms as of now in the industry simply because of its simplicity and accuracy.

K-NN is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already at the beginning of the 1970s as a non-parametric technique.

The algorithm assumes that similar things exist in close proximity. In other words, entities which are similar exist together.

 

### How the K-NN algorithm works?

 In K-NN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. **K is generally an odd number if the number of classes is 2**. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case.

In the below figure, suppose yellow colored “**?**” let's say P is the point, for which label needs to predict. First, you find the one closest point to P and then the label of the nearest point assigned to P.

![](https://cdn-images-1.medium.com/max/800/0*uNbO79MrS7jvY4qp.png)


Second, you find the k closest point to P and then classify points by majority vote of its K neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, we find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance, and Minkowski distance. The algorithm has the following basic steps:

Calculate distance
Find closest neighbors
Vote for labels

![](https://cdn-images-1.medium.com/max/800/0*QmLAPLYUDcpJYwvo.png)


Three most commonly used distance measures used to calculate the distance between point P and its nearest neighbors are represented as :

![](https://cdn-images-1.medium.com/max/800/0*439t63Ui2lsu4eqU.png)


In this article we will go ahead with Euclidean distance, so let's understand it first.

**Euclidean distance:** It is the most commonly used distance measure also called simply distance. The usage of a Euclidean distance measure is highly recommended when the data is dense or continuous. Euclidean distance is the best proximity measure. The Euclidean distance between two points is the length of the path connecting them. The Pythagorean theorem gives this distance between two points.

Below figure shows how to calculate Euclidean distance between two points in a 2-dimensional plane.
![](https://cdn-images-1.medium.com/max/800/1*Tt6a01VofqCLd8YGJvgNKA.png)

