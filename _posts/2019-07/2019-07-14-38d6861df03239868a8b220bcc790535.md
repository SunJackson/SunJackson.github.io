---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/07/14/distilled-news-1131/
date:      2019-07-14
author:      Michael Laux
tags:
    - data
    - modeling
    - models
    - machine learning
    - parameters
---

**Attention in Neural Networks**

In an earlier post on ‘Introduction to Attention’ we saw some of the key challenges that were addressed by the attention architecture introduced there (and referred in Fig 1 below). While in the same spirit, there are other variants that you might come across as well. Among other aspects, these variants differ on are ‘where’ attention is used ( standalone, in RNN, in CNN etc) and ‘how’ attention is derived (global vs local, soft vs hard etc). This post is a brief listing of some of the variants.

**Principal Components of PCA**

Principal Component Analysis (PCA) is used in machine learning applications to reduce the dimensionality of the data. It has been especially useful for image compression among other applications. In this post we will go through Lindsay Smith’s A Tutorial on Principal Component Analysis with an implementation in python. Our objective is to develop an intuition for PCA by laying out the mathematical formulations, and go beyond fit and fit_transform methods. Before moving on it is helpful to be familiar with measures of spread and linear algebra. Feel free to skip ahead to the implementation if the explanations seem trivial.

**XLNet – a clever language modeling solution**

Unsupervised learning of probability distribution of word sequences in a language by predicting each word within its sentence context in a large corpus, has proven to be useful to create models and word representations that can then be fine tuned for downstream NLP tasks. Two factors seem to play a key role to boost performance when fine tuning models for downstream syntactic and semantic tasks:• Is the representation of a word capturing the full context of that word in a sentence? Full context is the entire sequence of words that precede and follow a word in a sentence(bidirectional context)• Is the model multilayered to output multiple representations for a word, one from each layer? Representations of a word from the different layers of a model have been found to be useful for downstream syntactic and semantic tasks

**Understanding PCA**

In data science and finance (and pretty much any quantitative discipline), we are always sifting through a lot of noise in search of signal. Now if only, there were an algorithm that could do that for us… There is! Today we will explore how PCA (Principal Components Analysis) helps us uncover the underlying drivers hidden in our data – a super useful feature as it allows us to summarize huge feature sets using just a few principal components.

**Preventing overfitting: Regularization**

The ultimate goal of any Machine Learning model is making reliable predictions on new, unknown data. Hence, while training our algorithm, we always have to keep in mind that having a good score in our train set doesn’t necessarily mean our model will adapt to new data well. Indeed, whenever we have a model which perfectly fit our training data, we are probably incurring in overfitting: our model has too many parameters which cannot be justified by data, hence it is way too complex and heavy. Why does it happen? Well, if we think about the optimization strategy of any algorithm-minimizing the loss function-we can easily understand that, to reduce the error, our algorithm will tend to increase the value/number of our parameters in order to properly fit our data. However, this might lead to poor predictions on the test set.

**Reinforcement Learning – Model Based Planning Methods**

In previous articles, we have talked about reinforcement learning methods that are all based on model-free methods, which is also one of the key advantages of RL learning, as in most cases learning a model of environment can be tricky and tough. But what if we want to learn a model of environment or what if we already have a model of environment and how can we leverage that to help the learning process? In this article, we will together explore RL methods with environment as a model. The following will be structured as:• Start with basic idea of how to model an environment• Implement an example in Python using the theory we just learnt• Further ideas to extend the theory to more general cases

**Ants and the Problems with Neural Networks**

Ants are pretty dumb. They live for a week and don’t do much besides walking around, looking for food and carrying twigs to their anthill (now that I think about it, we humans also don’t do much else). But they are also dumb apart from living uninspired ant lifes. They are dumb in a technical sense: an individual ant has just at the order of 250.000 (2,5*105) neurons. As a comparison: an average homo sapiens has on average 80 billion neurons (8*10¹°), so if we assume that intelligence scales in at least some ways with the size of the brain (ignoring the fact that some animals have larger brains than we do), then we are approximately 320.000 times as smart as ants. But despite the tininess of their brains, ants have their moments. They are constantly doing things that are so sophisticated that they could be taken straight out of a university level math exam. One of these things is called dead reckoning.

**Decision Tree Classification**

A Decision Tree is a simple representation for classifying examples. It is a Supervised Machine Learning where the data is continuously split according to a certain parameter.

**Interpretation of Kappa Values**

The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. In 1960, Jacob Cohen critiqued the use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen’s kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty.

**Neural Networks: parameters, hyperparameters and optimization strategies**

This article is not supposed to be an exhaustive list of all the characteristic elements of a NN, but it is important to understand the key differences among them and the key ideas of initialization and tuning.

**Data Visualization using matplotlib and seaborn**

In this blog we’ll try to understand what data visualization is and how it could be used for making plots using matplotlib and seaborn in Python. We will also talk about the various types of analysis along with the most common types of plots used in data visualization.

**Data Preprocessing**

At the heart of Machine Learning is to process data. Your machine learning tools are as good as the quality of your data. This blog deals with the various steps of cleaning data. Your data needs to go through a few steps before it is could be used for making predictions.

**Understanding Learning Rate**

When building a deep learning project the most common problem we all face is choosing the correct hyper-parameters (often known as optimizers). This is critical as the hyper-parameters determine the expertise of the machine learning model. In Machine Learning (ML hereafter), a hyper-parameter is a configuration variable that’s external to the model and whose value is not estimated from the data given. Hyper-parameters are an essential part of the process of estimating model parameters and are often defined by the practitioner.

**Building an ML application using MLlib in Pyspark**

This tutorial will guide you on how to create ML models in apache spark and how to interact with them.

**Introducing Tidylo**

Today I am so pleased to introduce a new package for calculating weighted log odds ratios, tidylo. Often in data analysis, we want to measure how the usage or frequency of some feature, such as words, differs across some group or set, such as documents. One statistic often used to find these kinds of differences in text data is tf-idf. Another option is to use the log odds ratio, but the log odds ratio alone does not account for sampling variability. We haven’t counted every feature the same number of times so how do we know which differences are meaningful?

**Bridging the Domain Gap for Neural Models**

Deep neural networks are a milestone technique in the advancement of modern machine perception systems. However, in spite of the exceptional learning capacity and improved generalizability, these neural models still suffer from poor transferability. This is the challenge of domain shift – a shift in the relationship between data collected across different domains (e.g., computer generated vs. captured by real cameras). Models trained on data collected in one domain generally have poor accuracy on other domains. In this article, we discuss a new domain adaptation process that takes advantage of task-specific decision boundaries and the Wasserstein metric to bridge the domain gap, allowing the effective transfer of knowledge from one domain to another. As an additional advantage, this process is completely unsupervised, i.e., there is no need for new domain data to have labels or annotations.

**How Artificial Intelligence Is Changing Science**

The latest AI algorithms are probing the evolution of galaxies, calculating quantum wave functions, discovering new chemical compounds and more. Is there anything that scientists do that can’t be automated? 21

**Generative Adversarial Networks – The Story So Far**

In case you don’t see where I’m going here, the images you just saw were utterly, undeniably, 100% … fake. Also, I don’t mean these were photoshopped, CGI-ed, or (fill in the blanks with whatever Nvidia’s calling their fancy new tech at the moment). I mean that these images are entirely generated through addition, multiplication, and splurging ludicrous amounts of cash on GPU computation. The algorithm that makes is stuff work is called a generative adversarial network (which is the long way of writing GAN, for those of you still stuck in machine learning acronym land), and over the last few years, there have been more innovations dedicated to making it work than there have been privacy scandals at Facebook.

### Like this:

Like Loading...
