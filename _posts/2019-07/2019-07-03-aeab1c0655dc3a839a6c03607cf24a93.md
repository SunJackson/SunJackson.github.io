---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/07/03/distilled-news-1120/
date:      2019-07-03
author:      Michael Laux
tags:
    - models
    - understanding
    - videos
    - networks
    - concepts
---

**Generative Query Network**

The generative query network is an unsupervised generative network, published on Science in July 2018. It’s a scene-based method, which allows the agent to infer the image from a viewpoint based on the pre-knowledge of the environment and some other viewpoints. Thanks to its unsupervised attribute, the GQN paves the way towards machines that autonomously learn to understand the world around them.

**Online Encyclopedia of Statistical Science (Free)**

This online book is intended for beginners, college students and professionals confronted with statistical analyses. It is also a refresher for professional statisticians. The book covers over 600 concepts, chosen out of more than 1,500 for their popularity. Entries are listed in alphabetical order, and broken down into 18 parts. In addition to numerous illustrations, we have added 100 topics not covered in our online series Statistical Concepts Explained in Simple English.

**Consortium of Tech Firms Sets AI Benchmarks**

A consortium of tech companies, including Facebook Inc. and Alphabet Inc.’s Google, has released a set of benchmarks for evaluating the performance of artificial-intelligence tools, aiming to help businesses navigate the fast-growing field. The benchmarks – which cover image recognition, object detection and voice translation – are meant to help companies compare various AI tools to see which work best for them as they pursue their own AI initiatives, said Peter Mattson, general chairman of the consortium, MLPerf, which counts 40 companies as members. ‘For CIOs, metrics make for better products and services they can then incorporate into their organization,’ said Mr. Mattson, a Google engineer.

**MIT’s new interactive machine learning prediction tool could give everyone AI superpowers**

Soon, you might not need anything more specialized than a readily accessible touchscreen device and any existing data sets you have access to in order to build powerful prediction tools. A new experiment from MIT and Brown University researchers have added a capability to their ‘Northstar’ interactive data system that can ‘instantly generate machine-learning models’ to use with their exiting data sets in order to generate useful predictions. One example the researchers provide is that doctors could make use of the system to make predictions about the likelihood their patients have of contracting specific diseases based on their medial history. Or, they suggest, a business owner could use their historical sales data to develop more accurate forecasts, quickly and without a ton of manual analytics work.

**Announcing the YouTube-8M Segments Dataset**

Over the last two years, the First and Second YouTube-8M Large-Scale Video Understanding Challenge and Workshop have collectively drawn 1000+ teams from 60+ countries to further advance large-scale video understanding research. While these events have enabled great progress in video classification, the YouTube dataset on which they were based only used machine-generated video-level labels, and lacked fine-grained temporally localized information, which limited the ability of machine learning models to predict video content. To accelerate the research of temporal concept localization, we are excited to announce the release of YouTube-8M Segments, a new extension of the YouTube-8M dataset that includes human-verified labels at the 5-second segment level on a subset of YouTube-8M videos. With the additional temporal annotations, YouTube-8M is now both a large-scale classification dataset as well as a temporal localization dataset. In addition, we are hosting another Kaggle video understanding challenge focused on temporal localization, as well as an affiliated 3rd Workshop on YouTube-8M Large-Scale Video Understanding at the 2019 International Conference on Computer Vision (ICCV’19).

**Nothing but NumPy: Understanding & Creating Neural Networks with Computational Graphs from Scratch**

Understanding new concepts can be hard, especially these days when there is an avalanche of resources with only cursory explanations for complex concepts. This blog is the result of a dearth of detailed walkthroughs on how to create neural networks in the form of computational graphs. In this, and some following, blog posts, I will consolidate all that I have learned as a way to give back to the community and help new entrants. I will be creating common forms of neural networks all with the help of nothing but NumPy. This blog post is divided into two parts, the first part will be understanding the basics of a neural network and the second part will comprise the code for implementing everything learned from the first part.

**How to interpret machine learning models?**

In my previous blog, I had talked about the trade-off between model performance and interpretability. More often than not you will observe business stakeholders preferring less accurate but highly interpretable linear models because of the ease of explainability. But as a data scientist you know that in real-world problems due to inherent high dimensionality and complex non-linear relationships amongst the features non-linear and complex models are always more robust and reliable. So in an ideal world, everyone wants to be in the holy grid in which you get the highest model performance with highly interpretable results. In this blog, I have briefly mentioned some of the techniques I have used in my projects to add interpretability and to combat the trade-off between high model performance and high interpretability.

**An introduction to Attention**

Let’s take a quick look at vanilla RNNs and the encoder-decoder variation used in sequence to sequence tasks, understand what drawbacks these designs have and see how attention mechanisms address them.

**Python, Performance, and GPUs**

We’re improving the state of scalable GPU computing in Python. This post lays out the current status, and describes future work. It also summarizes and links to several other more blogposts from recent months that drill down into different topics for the interested reader. Broadly we cover briefly the following categories:• Python libraries written in CUDA like CuPy and RAPIDS• Python-CUDA compilers, specifically Numba• Scaling these libraries out with Dask• Network communication with UCX• Packaging with Conda

**Lagged Variable Regressions and Truth**

Dynamic regression models offer vast representative power but also bias risk.

**Keras Hyperparameter Optimization in the Cloud**

Efficiently search a large parameter space in the cloud using Talos and Docker. When training machine learning models it is often more convenient (and necessary) to offload the computation to a remote server. While free services such as Google Colab and Azure Notebooks are great for initial manual work in a Jupyter notebook, they are not well-suited to long running experiments such as hyperparameter training because of their automated turndown after a few hours. Using AWS as an example cloud provider, the most direct translation is to run the notebook on Sagemaker. However, more cost efficiency can be gained by setting up the training within a Docker container and taking on the deployment yourself. Developing a model in this type of environment also has the advantage of portability and reproducibility (more on that later).

**CycleGAN: How Machine Learning Learns Unpaired Image-To-Image Translation**

I recently read the CycleGAN paper (link), which I found very interesting because CycleGAN models have the incredible ability to accurately change images into something they’re not (e.g. changing a picture of a horse into a picture of a zebra). Very cool. Let’s dive into how it works.

**Naive Bayes Document Classification in Python**

How well can I classify a philosophy paper based on its abstract?

**Introduction to RNNs, Sequence to Sequence Language Translation and Attention**

The goal of this post is to briefly introduce RNNs (Recurrent Neural Networks), sequence to sequence language translation (seq2seq) and Attention. I will try to make it as simple as possible. You only need to know:• Linear algebra• Neural networksIn case you feel rusty on the topics, feel free to review them before you start reading by clicking on the links above.

### Like this:

Like Loading...
