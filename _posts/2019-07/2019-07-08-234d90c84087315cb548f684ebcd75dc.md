---
layout:     post
catalog: true
title:      XGBoost and Random Forest with Bayesian Optimisation
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/OT7uvKJ3sTc/xgboost-random-forest-bayesian-optimisation.html
date:      2019-07-08
author:      Matt Mayo Editor
tags:
    - xgboost
    - decision trees
    - learning
    - randomness
    - forests
---

**By Edwin Lisowski, CTO at Addepto**

Instead of only comparing XGBoost and Random Forest in this post we will try to explain how to use those two very popular approaches with Bayesian Optimisation and that are those models main pros and cons. XGBoost (XGB) and Random Forest (RF) both are ensemble learning methods and predict (classification or regression) by combining the outputs from individual decision trees (we assume tree-based XGB or RF).

 

### **Let’s dive deeper into comparison – XGBoost vs Random Forest**

 

### **XGBoost or Gradient Boosting**

 XGBoost build decision tree one each time. Each new tree corrects errors which were made by previously trained decision tree.

**Example of XGBoost application**

At Addepto we use XGBoost models to solve anomaly detection problems e.g. in supervised learning approach. In this case XGB is very helpful because data sets are often highly imbalanced. Examples of such data sets are user/consumer transactions, energy consumption or user behaviour in mobile app.

**Pros**

Since boosted trees are derived by optimizing an objective function, basically XGB can be used to solve almost all objective function that we can write gradient out. This including things like ranking and poisson regression, which RF is harder to achieve.

**Cons**

XGB model is more sensitive to overfitting if the data is noisy. Training generally takes longer because of the fact that trees are built sequentially. GBMs are harder to tune than RF. There are typically three parameters: number of trees, depth of trees and learning rate, and the each tree built is generally shallow.

 

### **Random Forest**

 Random Forest (RF) trains each tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree. Thanks to that RF is less likely to overfit on the training data.

**Example of Random Forest application**

The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.[1] Random Forest model is very attractive for this kind of applications in the following two cases:

Our goal is to have high predictive accuracy for a high-dimensional problem with strongly correlated features.

Our data set is very noisy and contains a lot of missing values e.g., some of the attributes are categorical or semi-continuous.

**Pros**

The model tuning in Random Forest is much easier than in case of XGBoost. In RF we have two main parameters: number of features to be selected at each node and number of decision trees. RF are harder to overfit than XGB.

**Cons**

The main limitation of the Random Forest algorithm is that a large number of trees can make the algorithm slow for real-time prediction. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels.

Bayesian optimization is a technique to optimise function that is expensive to evaluate.[2] It builds posterior distribution for the objective function and calculate the uncertainty in that distribution using Gaussian process regression, and then uses an acquisition function to decide where to sample. Bayesian optimization focuses on solving the problem:

*maxx∈A f(x)*

The dimension of hyperparameters (*x∈Rd*) is often d < 20 in most successful applications.

Typically set A i a hyper-rectangle (*x∈Rd:ai ≤ xi ≤ bi*). The objective function is continuous which is required to model using Gaussian process regression. It also lacks special structure like concavity or linearity which make futile using techniques that leverage such structure to improve efficiency. Bayesian optimization consists of two main components: a Bayesian statistical model for modeling the objective function and an acquisition function for deciding where to sample next.

After evaluating the objective according to an initial space-filling experimental design they are used iteratively to allocate the remainder of a budget of N evaluations, as shown below:

Observe at initial points
1. While *n ≤ N* doUpdate the posterior probability distribution using all available dataLet *xn* be a maximizer of the acquisition functionObserve *yn = f(xn)*Increment *n*end while
