---
layout:     post
catalog: true
title:      BeautifulSoup vs. Rvest
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/hibCGpus0qY/
date:      2019-07-23
author:      Andrew Treadway
tags:
    - links
    - beautifulsoup
    - tags
    - packages
    - session
---

This post will compare Python’s BeautifulSoup package to R’s rvest package for web scraping. We’ll also talk about additional functionality in **rvest** (that doesn’t exist in **BeautifulSoup**) in comparison to a couple of other Python packages (including **pandas** and **RoboBrowser**).

## **Getting started**


BeautifulSoup and rvest both involve creating an object that we can use to parse the HTML from a webpage. However, one immediate difference is that BeautifulSoup is just a web parser, so it doesn’t connect to webpages. **rvest**, on the other hand, can connect to a webpage and scrape / parse its HTML in a single package.

In BeautifulSoup, our initial setup looks like this:

In comparison, here’s what using **rvest** is like:

## **Searching for specific HTML tags**


Next, let’s take our parser objects and find all the links on the page.

If you’re familiar with **BeautifulSoup**, that would look like:

In rvest, however, we use syntax similar to **dplyr** and other tidyverse packages by using %>%.

In **BeautifulSoup**, we use the *find_all* method to extract a list of all of a specific tag’s objects from a webpage. Thus, in the links example, we specify we want to get all of the anchor tags (or “a” tags), which create HTML links on the page. If we wanted to scrape other types of tags, such as *div* tags or *p* tags, we just need to switch out “a” with “div”, “p”, or whatever tag we want.

With **rvest**, we can get specific tags from HTML using *html_nodes*. Thus, if we wanted to scrape different tags, such as the div tags or h1 tags, we could do this:

## **Getting attributes and text from tags**


In **BeautifulSoup**, we get attributes from HTML tags using the *get* method. We can use a list comprehension to get the *href* attribute of each link (the *href* attribute of a link is its destination URL).

To get other attributes, we just need to change our input to the *get* method.

Using **rvest**, the *html_attr* function can be used to get attributes from tags. So to get the URL of each link object we scrape, we need to specify that we want to get the *href* attribute from each link, similarly to **BeautifulSoup**:

Likewise, if we want to scrape the IDs from the div tags, we can do this:

Notice how, like other tidyverse packages, we can chain together multiple operations.

If we want to scrape the text from each of the links, we can use *html_text*:

**BeautifulSoup’s** way of accomplishing this is by using the *text* method of a tag object:

## **Scraping HTML tables**

Let’s look at another example for scraping HTML tables.

We can scrape HTML tables using **rvest’s** *html_table* method. This method will extract all tables found on the input webpage. The *fill = TRUE* parameter is specifying that we want to fill any rows that have less than the maximum number of columns in a table with NAs. The tables will be stored as a list of data frames.

Scraping tables with **BeautifulSoup** into a data frame object is a bit different. One way we can scrape tables with Python is to loop through the tr (row) or td (data cell in table) tags. But the closest analogy of **rvest’s** functionality here is to use **pandas**:

Like using *html_table*, this will return a list of data frames corresponding to the tables found on the webpage.

## **Browser simulation with rvest**

An additional feature of **rvest** is that it can perform browser simulation. **BeautifulSoup** cannot do this; however, Python offers several alternatives including **requests_html** and **RoboBrowser** (each discussed here).

With **rvest**, we can start a browser session using the *html_session* function:

With our session object, we can navigate to different links on the page, just like a real web browser. There’s a couple ways of doing this. One is to input the index of the link we want to go to. For example, to navigate to the third link of the page, we would write the below code:

Here’s a couple more examples:

You can also simulate clicking on links based off text. For example, the below code will navigate to the first link containing the text “Sun”. The input is case sensitive.

You can use the session object to navigate directly to other webpages using the *jump_to* function.

If we use the **RoboBrowser** package in Python to somewhat replicate the R code above, we could write this:

The *open* method used above is analogous to **rvest’s** *jump_to* function. The *follow_link* method in **RoboBrowser** serves a similar purpose to the function of the same name in **rvest**, but behaves a little differently. This method takes a *link object* as input, rather than the index of a link, or text within a link that you’re searching for. Thus, we can use the *links* object above to specify a link that we want to “follow” or click. If we want to click on a link based off text like we did in the **rvest** example above, we could write the below code.

To learn more about browser simulation in Python, click here.

That’s it for this article! Please click here to follow my blog on Twitter.

The post BeautifulSoup vs. Rvest appeared first on Open Source Automation.


*Related*







---
