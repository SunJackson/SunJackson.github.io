---
layout:     post
catalog: true
title:      Reinforcement Learning： Life is a Maze
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/h2YYsKGPSW8/
date:      2019-07-16
author:      Learning Machines
tags:
    - bandits
    - learning
    - learned
    - functions
    - actions
---










![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/labyrinth-1013625_1280-e1558934005359-300x242.jpg?resize=300%2C242&is-pending-load=1)
![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/labyrinth-1013625_1280-e1558934005359-300x242.jpg?resize=300%2C242)
It can be argued that most important decisions in life are some variant of an *exploitation-exploration* problem. Shall I stick with my current job or look for a new one? Shall I stay with my partner or seek a new love? Shall I continue reading the book or watch the movie instead? In all of those cases the question is always whether I should “exploit” the thing I have or whether I should “explore” new things. If you want to learn how to tackle this most basic trade off read on…

At the core this can be stated as the problem a gambler has who wants to play a *one-armed bandit*: if there are several machines with different winning probabilities (a so-called *multi-armed bandit* problem) the question the gambler faces is: which machine to play? He could “exploit” one machine or “explore” different machines. So what is the best strategy given a limited amount of time… and money?

There are two extreme cases: no exploration, i.e. playing only one randomly chosen bandit, or no exploitation, i.e. playing all bandits randomly – so obviously we need some middle ground between those two extremes. We have to start with one randomly chosen bandit, try different ones after that and compare the results. So in the simplest case the first variable ![](https://i0.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-b0cde8f53406eab2a6923cd7832dc8b9_l3.png?resize=53%2C13&is-pending-load=1)
![](https://i0.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-b0cde8f53406eab2a6923cd7832dc8b9_l3.png?resize=53%2C13)
 is the probability rate with which to switch to a random bandit – or to stick with the best bandit found so far.

Let us create an example with ![](https://i2.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-e1ec51035f995b97ac2c6dd8906889fa_l3.png?resize=42%2C13&is-pending-load=1)
![](https://i2.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-e1ec51035f995b97ac2c6dd8906889fa_l3.png?resize=42%2C13)
 bandits, which return ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-d4d95642629f734574671d47307d46c3_l3.png?resize=9%2C13&is-pending-load=1)
![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-d4d95642629f734574671d47307d46c3_l3.png?resize=9%2C13)
 units on average, except the second one which returns ![](https://i2.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-48348ef601c56286abf49bafe09c7af1_l3.png?resize=8%2C13&is-pending-load=1)
![](https://i2.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-48348ef601c56286abf49bafe09c7af1_l3.png?resize=8%2C13)
 units. So the best strategy would obviously be to choose the second bandit right away and stick with it, but of course we don’t know the average returns of each bandit so this won’t work. Instead we need another vector ![](https://i2.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-dd440a7af28975f52f03607a49307d46_l3.png?resize=14%2C16&is-pending-load=1)
![](https://i2.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-dd440a7af28975f52f03607a49307d46_l3.png?resize=14%2C16)
 which tallies the results of each bandit so far. This vector has to be updated after each game, so we need an *update function* which gets as arguments the current bandit and the return of the game.

The intelligence of the strategy lies in this update function, so how should we go about? The big idea behind this strategy is called *Bellman equation* and in its simplest form it works as follows: the adjustment of the former result vector is the difference between the former result and the current result weighted by some *discount factor*, in this case the inverse of the number of games played on the respective machine. This learning strategy is called *Q-learning* and is a so called *reinforcement learning* technique.

Have a look at the following example implementation:

So, obviously the algorithm found a nearly perfect strategy all on its own!

![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/05/labyrinth_1558940933-840x420.png?w=450&is-pending-load=1)
![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/05/labyrinth_1558940933-840x420.png?w=450)


Now, this is the simplest possible application of reinforcement learning. Let us now implement a more sophisticated example: a robot navigating a maze. Whereas the difficulty in the first example was that the feedback was blurred (because the return of each one-armed bandit is only an average return) here we only get definitive feedback after several steps (when the robot reaches its goal). Because this situation is more complicated we need more memory to store the intermediate results. In our multi-armed bandit example the memory was a vector, here we will need a matrix.

The robot will try to reach the goal in the following maze and find the best strategy for each room it is placed in:
![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/maze.png?w=450&is-pending-load=1)

![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/maze.png?w=450)


Have a look at the code (it is based on the *Matlab* code from the same tutorial the picture is from, which is why the names of variables and functions are called the same way to ensure consistency):

So again, the algorithm has found the best route for each room!

Recently the combination of Neural Networks (see also Understanding the Magic of Neural Networks) and Reinforcement Learning has become quite popular. For example *AlphaGo*, the machine from Google that defeated a Go world champion for the first time in history is based on this powerful combination!


*Related*







---
