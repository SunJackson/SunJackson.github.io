---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/07/06/distilled-news-1125/
date:      2019-07-06
author:      Michael Laux
tags:
    - python
    - learned
    - models
    - modeling
    - datasets
---

**Survival analysis and germination data: an overlooked connection**

Seed germination data describe the time until an event of interest occurs. In this sense, they are very similar to survival data, apart from the fact that we deal with a different (and less sad) event: germination instead of death. But, seed germination data are also similar to failure-time data, phenological data, time-to-remission data… the first point is: germination data are time-to-event data. You may wonder: what’s the matter with time-to-event data? Do they have anything special? With few exceptions, all time-to-event data are affected by a certain form of uncertainty, which takes the name of ‘censoring’. It relates to the fact that the exact time of event may not be precisely know. I think it is good to give an example.

**Anomaly Detection for Dummies**

Unsupervised Anomaly Detection for Univariate & Multivariate Data. Anomaly detection is the process of identifying unexpected items or events in data sets, which differ from the norm. And anomaly detection is often applied on unlabeled data which is known as unsupervised anomaly detection. Anomaly detection has two basic assumptions:• Anomalies only occur very rarely in the data.• Their features differ from the normal instances significantly.

**Seven Key Dimensions to Help You Understand Artificial Intelligence Environments**

Every artificial intelligence(AI) problem is a new universe of complexities and unique challenges. Very often, the most challenging aspects of solving an AI problem is not about finding a solution but understanding the problem itself. As paradoxically as that sounds, even the most experienced AI experts have been guilty of rushing into proposing deep learning algorithms and exoteric optimization techniques without fully understanding the problem at hand. When we think about an AI problem, we tend to link our reasoning to two main aspects: datasets and models. However, that reasoning is ignoring what can be considered the most challenging aspect of an AI problem: the environment.1. Single Agent vs. Multi-Agent2. Complete vs. Incomplete3. Fully Observable vs. Partially Observable4. Competitive vs. Collaborative5. Static vs. Dynamic6. Discrete vs. Continuous7. Deterministic vs. Stochastic

**Python Tutorial For Researchers Who use R**

This tutorial is aimed at researchers who are used to using R. Data is at the center of any research project. To me, every researcher is now empowered with massive amounts of data than ever before. This puts a researcher squarely in the role of being a data scientist. If you are a researcher who’s been using R all this time, you are missing out. There are powerful models at your disposal to handle larger amounts of data available in Python. To upgrade your skills as a data scientist, give Python a try. Like me, you might find that you still love R for some tasks. Slowly, as you get used to using Python for other tasks, you may find that it’s more robust in handling larger amounts of data. I will go over python installation, data loading, simple visualization, and a linear regression example. Toward the end, just for novelty sake, I will show you how to use R in Python.

**The Complete Guide to Resampling Methods and Regularization in Python**

Understand how resampling methods and regularization can improve your models and apply these methods in a project setting.

**Examining the Transformer Architecture – Part 2: A Brief Description of How Transformers Work**

As we learned in Part 1, The GPT-2 is based on the Transformer, which is being hailed as the new NLP standard, replacing Recurrent Neural Networks. Some commentators believe that the Transformer will become the dominant NLP deep learning architecture of 2019. Let’s now take a brief look under the hood to see what makes them tick.

**Demystifying the Architecture of Long Short Term Memory (LSTM) Networks**

In my previous article, I explain RNNs’ Architecture. RNNs are not perfect and they mainly suffer from two major issues exploding gradients and vanishing gradients. Exploding gradients are easier to spot, but vanishing gradients is much harder to solve. We use the Long Short Term Memory(LSTM) and Gated Recurrent Unit(GRU) which are very effective solutions for addressing the vanishing gradient problem and they allow the neural network to capture much longer range dependencies.

**Explaining Predictions: Interpretable models (decision tree)**

This is a follow up post of using simple models to explain machine learning predictions. In the last post, we introduced logistic regression and in today’s entry we will learn about decision tree. We will continue to use the Cleveland heart dataset and use tidymodels principles where possible. The details of the Cleveland heart dataset was also described in the last post.

**AlexNet: The Architecture that Challenged CNNs**

A few years back, we still used small datasets like CIFAR and NORB consisting of tens of thousands of images. These datasets were sufficient for machine learning models to learn basic recognition tasks. However, real life is never simple and has many more variables than are captured in these small datasets. The recent availability of large datasets like ImageNet, which consist of hundreds of thousands to millions of labeled images, have pushed the need for an extremely capable deep learning model. Then came AlexNet.

**Residual Networks: Implementing ResNet in Pytorch**

This is not a technical article and I am not smart enough to explain residual connection better than the original authors. So we will limit ourself to a quick overview. Deeper neural networks are more difficult to train. Why? One big problem of a deep network is the vanishing gradient problem. Basically, the deeper the harder to train. To solve this problem, the authors proposed to use a reference to the previous layer to compute the output at a given layer. In ResNet, the output from the previous layer, called residual, is added to the output of the current layer. The following picture visualizes this operation We are going to make our implementation as scalable as possible using one thing unknown to most of the data scientists: object-oriented programming

**Text Classification by XGBoost & Others: A Case Study Using BBC News Articles**

Comparative study of different vector space models & text classification techniques like XGBoost and others. In this article, we will discuss different text classification techniques to solve the BBC new article categorization problem. We will also discuss different vector space models to represent text data. We will be using Python, Sci-kit-learn, Gensim and the Xgboost library for solving this problem.

**Here’s how you can accelerate your Data Science on GPU**

Data Scientists need computing power. Whether you’re processing a big dataset with Pandas or running some computation on a massive matrix with Numpy, you’ll need a powerful machine to get the job done in a reasonable amount of time. Over the past several years, Python libraries commonly used by Data Scientists have gotten pretty good at leveraging CPU power. Pandas, with its underlying base code written in C, does a fine job of being able to handle datasets that go over even 100GB in size. And if you don’t have enough RAM to fit such a dataset, you can always use the convenient chunking functions that can process the data one piece at a time.

**Brazilian Heavy Metal: An Exploratory Data Analysis using NLP and LDA**

An analysis across the lyrics of two of greatest bands of Brazilian Heavy Metal: Angra and Sepultura.

**How to generate new data in Machine Learning with AE (Autoencoder) applied to Mnist with Python code**

The AutoEncoders are Neural Networks used to generate new data (Unsupervised Learning). This model is used for generating new data for the dataset or also in case we want to cancel the noise from our data. The Networks is composed by multiple Neural Networks: an encoder and a decoder connected by a bottleneck that is the latent space.

**Understanding your R strategy options on the Azure AI Platform**

The R and Python programming languages are primary citizens for data science on the Azure AI Platform. These are the most common languages for performing data preparation, transformation, training and operationalization of machine learning models; the core components for one’s digital transformation leveraging AI. Yet they are fundamentally different in many aspects, directly affecting not only deployed solutions IT architectures but also but also corporate strategies for developer skills and product supportability. This series of articles is designed help you understand the options your company and customers have to support and evolve their R strategy. The articles in this series are:• Understanding your R strategy options on the Azure AI Platform (this post)• A deeper dive on R operationalization options• Big Data architectural topologies with R• Orchestrating mixed R and Python Machine Learning pipelines with AML Services

**JupyterLab – A Next Gen Python Data Science IDE**

While working on data science projects with Python, you probably asked yourself which IDE serves best your needs on data exploration, cleaning, preparing, modeling, evaluation and deployment. You’d probably also have done some research on Google, scanned through various pages titled like ‘Top Python Data Science IDE’ and started desperately realizing that none of the mentioned products combines a seamless look and feel for all necessary implementation steps. Ultimately you turned back to your well known, but yet separated set of tools. The good news is, there is a very promising project waiting just to be released as 1.0 version tackling a lot of our day to day data science needs. I am referring to JupyterLab.

### Like this:

Like Loading...
