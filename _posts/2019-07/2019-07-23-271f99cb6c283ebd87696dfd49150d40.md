---
layout:     post
catalog: true
title:      Learning Data Science： Understanding and Using k-means Clustering
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/HT8YgMZPIp4/
date:      2019-07-23
author:      Learning Machines
tags:
    - clustering
    - clusters
    - data
    - centers
    - learning
---

*k*-means (not to be confused with *k-nearest neighbours* or *KNN*: Teach R to read handwritten Digits with just 4 Lines of Code) is a simple, yet often very effective *unsupervised learning* algorithm to find similarities in large amounts of data and cluster them accordingly. The *k* stands for the number of clusters which has to be set beforehand. 

The guiding principles are:

- The *distance* between data points within clusters should be as small as possible.

- The distance of the *centroids* (= centre of the clusters) should be as big as possible.


Because there are too many possible combinations of all possible clusters comprising all possible data points *k*-means follows an *iterative* approach:

1. Initialization: assign clusters randomly to all data points

1. E-step (for expectation): assign each observation to the “nearest” (based on *Euclidean distance*) cluster

1. M-step (for maximization): determine new centroids based on the mean of assigned objects

1. Repeat steps 3 and 4 until no further changes occur


As can be seen above *k*-means is an example of a so-called *expectation-maximization algorithm*.

To implement *k*-means in R we first assign some variables and define a helper function for plotting the steps:

![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-1-1-840x600.png?w=450&is-pending-load=1)
![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-1-1-840x600.png?w=450)
Here comes the *k*-means algorithm as described above:

![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-1-840x600.png?w=450&is-pending-load=1)
![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-1-840x600.png?w=450)


![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-2-840x600.png?w=450&is-pending-load=1)
![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-2-840x600.png?w=450)


![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-3-840x600.png?w=450&is-pending-load=1)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-3-840x600.png?w=450)


![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-4-840x600.png?w=450&is-pending-load=1)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-4-840x600.png?w=450)


![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-5-840x600.png?w=450&is-pending-load=1)
![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-5-840x600.png?w=450)


![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-6-840x600.png?w=450&is-pending-load=1)
![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-6-840x600.png?w=450)


![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-7-840x600.png?w=450&is-pending-load=1)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-7-840x600.png?w=450)


![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-8-840x600.png?w=450&is-pending-load=1)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-2-8-840x600.png?w=450)


As can seen the clusters wander slowly but surely until all three are stable. We now compare the result with the `k-means` function in Base R:

![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-3-1-840x600.png?w=450&is-pending-load=1)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/07/unnamed-chunk-3-1-840x600.png?w=450)


As you can see, the result is the same!

Now, for some real-world application: clustering wholesale customer data. The data set refers to clients of a wholesale distributor. It includes the annual spending on diverse product categories and is from the renowned *UCI Machine Learning Repository* (I guess the category “Delicassen” should rather be “Delicatessen”).

Have a look at the following code:

One interpretation could be the following for the four clusters:

1. Big general shops

1. Small food shops

1. Big food shops

1. Small general shops


As you can see, the interpretation of some clusters found by the algorithm can be quite a challenge. If you have a better idea of how to interpret the result please tell me in the comments below!


*Related*







---
