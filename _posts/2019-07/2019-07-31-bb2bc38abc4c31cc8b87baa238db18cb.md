---
layout:     post
catalog: true
title:      Getting started with Tensorflow, Keras in Python and R
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/Sb_oE2MJlrU/
date:      2019-07-31
author:      Tinniam V Ganesh
tags:
    - keras
    - model
    - mnist
    - train_dataset
    - normalization
---





**The Pale Blue Dot**

*“From this distant vantage point, the Earth might not seem of any particular interest. But for us, it’s different. Consider again that dot. That’s here, that’s home, that’s us. On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every “superstar,” every “supreme leader,” every saint and sinner in the history of our species lived there—on the mote of dust suspended in a sunbeam.”*

Carl Sagan

Tensorflow and Keras are Deep Learning frameworks that really simplify a lot of things to the user. If you are familiar with Machine Learning and Deep Learning concepts then Tensorflow and Keras are really a playground to realize your ideas.  In this post I show how you can get started with Tensorflow in both Python and R

 

### Tensorflow in Python

For tensorflow in Python, I found Google’s Colab an ideal environment for running your Deep Learning code. This is an Google’s research project  where you can execute your code  on GPUs, TPUs etc

### Tensorflow in R (RStudio)

To execute tensorflow in R (RStudio) you need to install tensorflow and keras as shown belowIn this post I show how to get started with Tensorflow and Keras in R.

```




library(tensorflow)
libary(keras)
```

This post takes 3 different Machine Learning problems and uses theTensorflow/Keras framework to solve it

**Note**:You can view the Google Colab notebook at Tensorflow in PythonThe RMarkdown file has been published at RPubs and can be accessedat Getting started with Tensorflow in R

![](https://gigadom.files.wordpress.com/2017/01/Untitled.png?w=456&is-pending-load=1)
![](https://gigadom.files.wordpress.com/2017/01/Untitled.png?w=456)
Checkout my book ‘Deep Learning from first principles: Second Edition – In vectorized Python, R and Octave’. My book starts with the implementation of a simple 2-layer Neural Network and works its way to a generic L-Layer Deep Learning Network, with all the bells and whistles. The derivations have been discussed in detail. The code has been extensively commented and included in its entirety in the Appendix sections. My book is available on Amazon as paperback ($14.99) and in kindle version($9.99/Rs449).



## 

## 1. Multivariate regression with Tensorflow – Python

This code performs multivariate regression using Tensorflow and keras on the advent of Parkinson disease through sound recordings see Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set . The clinician’s motorUPDRS score has to be predicted from the set of features


In [0]:

```




library(tensorflow)
```







### 

### 1a. Multivariate Regression in Tensorflow – R

```
library(keras)

```

```
library(dplyr)``
```

```
library(dummies)
```

```
## dummies-1.5.6 provided by Decision Patterns
```

```
library(tensorflow)
library(keras)
```

## Multivariate regression

This code performs multivariate regression using Tensorflow and keras on the advent of Parkinson disease through sound recordings see Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set. The clinician’s motorUPDRS score has to be predicted from the set of features.

### Read the data

```

dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data")


names(dataset) <- c("subject","age", "sex", "test_time","motor_UPDRS","total_UPDRS","Jitter","Jitter.Abs",
 "Jitter.RAP","Jitter.PPQ5","Jitter.DDP","Shimmer", "Shimmer.dB", "Shimmer.APQ3",
 "Shimmer.APQ5","Shimmer.APQ11","Shimmer.DDA", "NHR","HNR", "RPDE", "DFA","PPE")


dataset1 <- subset(dataset, select = -c(subject))


dataset1$sex=as.factor(dataset1$sex)

dataset2 <- dummy.data.frame(dataset1, sep = ".")
```

```
## Warning in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts =
## FALSE): non-list contrasts argument ignored
```

```
dataset3 <- na.omit(dataset2)
```

### Split the data as training and test in 80/20

```

sample_size <- floor(0.8 * nrow(dataset3))


set.seed(12)
train_index <- sample(seq_len(nrow(dataset3)), size = sample_size)

train_dataset <- dataset3[train_index, ]
test_dataset <- dataset3[-train_index, ]

train_data <- train_dataset %>% select(sex.0,sex.1,age, test_time,Jitter,Jitter.Abs,Jitter.PPQ5,Jitter.DDP,
 Shimmer, Shimmer.dB,Shimmer.APQ3,Shimmer.APQ11,
 Shimmer.DDA,NHR,HNR,RPDE,DFA,PPE)

train_labels <- select(train_dataset,motor_UPDRS)
test_data <- test_dataset %>% select(sex.0,sex.1,age, test_time,Jitter,Jitter.Abs,Jitter.PPQ5,Jitter.DDP,
 Shimmer, Shimmer.dB,Shimmer.APQ3,Shimmer.APQ11,
 Shimmer.DDA,NHR,HNR,RPDE,DFA,PPE)
test_labels <- select(test_dataset,motor_UPDRS)
```

## Normalize the data

```
 
normalize<-function(x) {
 y<-(x - mean(x)) / sd(x)
 return(y)
}

normalized_train_data <-apply(train_data,2,normalize)

train_labels <- as.matrix(train_labels)
normalized_test_data <- apply(test_data,2,normalize)
test_labels <- as.matrix(test_labels)
```

### Create the Deep Learning Model

```
model <- keras_model_sequential()
model %>% 
 layer_dense(units = 6, activation = 'relu', input_shape = dim(normalized_train_data)[2]) %>% 
 layer_dense(units = 9, activation = 'relu') %>%
 layer_dense(units = 6, activation = 'relu') %>%
 layer_dense(units = 1)



model %>% compile(
 loss = 'mean_squared_error',
 optimizer = optimizer_rmsprop(),
 metrics = c('mean_absolute_error','mean_squared_error')
)



history <- model %>% fit(
 normalized_train_data, train_labels, 
 epochs = 30, batch_size = 128, 
 validation_data = list(normalized_test_data,test_labels)
)
```

### Plot mean squared error, mean absolute error and loss for training data and test data

```
plot(history)

```

Fig1![](https://gigadom.files.wordpress.com/2019/07/fig1-1-3.png?w=450&is-pending-load=1#038;h=731&fit=1024%2C731)
![](https://gigadom.files.wordpress.com/2019/07/fig1-1-3.png?w=450&h=731&fit=1024%2C731)


## 2. Binary classification in Tensorflow – Python

This is a simple binary classification problem from UCI Machine Learning repository and deals with data on Breast cancer from the Univ. of Wisconsin Breast Cancer Wisconsin (Diagnostic) Data Set **bold text**

In [31]:

```

dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data")


names(dataset) <- c("id","thickness", "cellsize", "cellshape","adhesion","epicellsize",
 "barenuclei","chromatin","normalnucleoli","mitoses","class")


dataset1 <- subset(dataset, select = -c(id, class))
dataset2 <- na.omit(dataset1)


dataset2$barenuclei <- as.numeric(dataset2$barenuclei)
```

## Normalize the data

```
train_data <-apply(dataset2,2,normalize)
train_labels <- as.matrix(select(dataset,class))


train_labels[train_labels==2,]=0
train_labels[train_labels==4,]=1
```

### Create the Deep Learning model

```
model <- keras_model_sequential()
model %>% 
 layer_dense(units = 6, activation = 'relu', input_shape = dim(train_data)[2]) %>% 
 layer_dense(units = 9, activation = 'relu') %>%
 layer_dense(units = 6, activation = 'relu') %>%
 layer_dense(units = 1)


model %>% compile(
 loss = 'binary_crossentropy',
 optimizer = optimizer_rmsprop(),
 metrics = c('accuracy') 
)
```

### Fit the model. Use 20% of data for validation

```
history <- model %>% fit(
 train_data, train_labels, 
 epochs = 30, batch_size = 128, 
 validation_split = 0.2
)
```

### Plot the accuracy and loss for training and validation data

```
plot(history)

```

![](https://gigadom.files.wordpress.com/2019/07/fig2-1-1.png?w=450&is-pending-load=1#038;h=731&fit=1024%2C731)
![](https://gigadom.files.wordpress.com/2019/07/fig2-1-1.png?w=450&h=731&fit=1024%2C731)


### 3. MNIST in Tensorflow – Python

This takes the famous MNIST handwritten digits . It ca be seen that Tensorflow and Keras make short work of this famous problem of the late 1980s

```
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

### Reshape and rescale

```

x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

x_train <- x_train / 255
x_test <- x_test / 255
```

### Convert out put to One Hot encoded format

```
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

### Fit the model

Use the softmax activation for recognizing 10 digits and categorical cross entropy for loss

```
model <- keras_model_sequential() 
model %>% 
 layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
 layer_dense(units = 128, activation = 'relu') %>%
 layer_dense(units = 10, activation = 'softmax') 

model %>% compile(
 loss = 'categorical_crossentropy',
 optimizer = optimizer_rmsprop(),
 metrics = c('accuracy')
)
```

### Fit the model

*Note*: A smaller number of epochs has been used. For better performance increase number of epochs

```
history <- model %>% fit(
 x_train, y_train, 
 epochs = 5, batch_size = 128, 
 validation_data = list(x_test,y_test)
)
```


*Related*







---
