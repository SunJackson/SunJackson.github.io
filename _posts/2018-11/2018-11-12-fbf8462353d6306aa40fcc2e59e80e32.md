---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2018/11/12/distilled-news-906/
date:      2018-11-12
author:      Michael Laux
tags:
    - data
    - distributions
    - learning
    - processes
    - processing
---

**A Deeper Look into Embeddings – A Linguistic Approach**

So you may know of Word2vec or Glove or even Elmo, and perhaps even the algorithms behind it (Skipgram, etc). However, what are the differences between them, and why is one better than another? This blog is about word embeddings came to be and the linguistic questions they solve, drawing upon a really awesome ACL Paper this year to help explain some linguistic concepts.

**A Bayesian Approach to Time Series Forecasting**

Today we are going to implement a Bayesian linear regression in R from scratch and use it to forecast US GDP growth. This post is based on a very informative manual from the Bank of England on Applied Bayesian Econometrics. I have translated the original Matlab code into R since its open source and widely used in data analysis/science. My main goal in this post is to try and give people a better understanding of Bayesian statistics, some of it’s advantages and also some scenarios where you might want to use it.

**Hacking up a reporting pipeline using Python and Excel**

When I arrived at Online Drinks, the reporting process was manual, prone to errors and was taking over a full day to consolidate. We were looking to scale up and continuing the way it was wouldn’t allow us to do that effectively. We needed a solution that would be provide a higher degree of automation.

**Plant AI – Plant Disease Detection using Convolutional Neural Network**

Here is how I built a Plant Disease Detection model using a Convolutional Neural Network (originally built for the NaijaHacks Hackathon 2018 )

**A Quick Overview of Optimization Models for Machine Learning and Statistic**

Optimization is at the heart of almost all machine learning and statistical techniques used in data science. We discuss the core optimization frameworks behind the most popular machine learning/statistical modeling methods.

**How to train Neural Network faster with optimizers?**

As I worked on the last article, I had the opportunity to create my own neural network using only Numpy. It was a very challenging task, but at the same time it significantly broadened my understanding of the processes that take place inside the NN. Among others, this experience made me truly realize how many factors influence neural net’s performance. Selected architecture,proper hyperparameter values or even correct initiation of parameters, are just some of those things… This time however, we will focus on the decision that has a huge impact on learning process speed, as well as the accuracy of obtained predictions?-?the choice of the optimization strategy. We will look inside many popular optimizers, investigate how they work and compare them with each other.

**Open Source Automated Data-Wrangling**

For those that haven’t been following along, I’ve been using this forum in the last three months to document the development of a tool for the automation of data wrangling (aka ‘munging’) of structured data sets for the direct application of machine learning in the framework of your choice. In the current iteration the tool is not yet a replacement for feature engineering, however it is suitable as a replacement for the final steps of data processing prior to application of machine learning. The culmination of this work has been a tool and a corresponding business vehicle dubbed AutoMunge, which includes the function automunge(.) for the initial processing of data intended to train a downstream model along with comparable process of corresponding test data intended to generate predictions from that same downstream model, or alternately the function postmunge(.) intended for the subsequent consistent processing of test data that wasn’t available at initial address.

**Machine Learning: Ridge Regression in Detail**

As there are already more than sufficient articles about Linear Regression here, I won’t write about it one more time. Instead I will write about one kind of normalized regression type – Ridge Regression – which solves problem of data overfitting.

**Why I don’t trust AI and you shouldn’t either**

Don’t get me wrong, I love machine learning and AI. But I don’t trust them. That’s the right instinct, because the way you build effective and reliable ML/AI solutions is to force each solution to earn your trust. Blind trust is a terrible thing. Before you start thinking that their untrustworthiness has anything to do with robots or sci-fi, stop! ML/AI systems aren’t humanlike, they’re just useful thing labelers with poetic names. Instead, their untrustworthiness comes from somewhere else entirely. I think it’s best to show not tell, so let’s see a friendly example…

**Convolution Neural Network Decryption**

This article is an attempt to explain convolution neural networks via visualizations of very simple images. Very basic objects like vertical and horizontal lines are used. Complex images from ImageNet dataset have been avoided as the visualizations are not available for easy interpretations.

**Weight Initialization Techniques in Neural Networks**

Building even a simple neural network can be confusing task and upon that tuning it to get a better result is extremely tedious. But, the first step that comes in consideration while building a neural network is initialization of parameters, if done correctly then optimization will be achieved in least time otherwise converging to a minima using gradient descent will be impossible. This article has been written under the assumption that reader is already familiar with the concept of neural network, weight, bias, activation functions, forward and backward propagation etc.

**Outlier Detection with One-Class SVMs**

Imbalanced learning problems often stump those new to dealing with them. When the ratio between classes in your data is 1:100 or larger, early attempts to model the problem are rewarded with very high accuracy but very low specificity. You can solve the specificity problem in imbalanced learning in a few different ways:• You can naively weight the classes, making your model preferential to the minority class.• You can use undersampling, oversampling or a combination of the two.• You can switch your goal from trying to balance the dataset, to trying to predict the minority class using outlier detection techniques.In this post, I’ll show you how, and more importantly, when, to use the last of these methods and compare the results to the weighting and rebalancing approaches.

**Artificial Intelligence Needs To Reset**

I had the chance to interview my colleague at ArCompany, Karen Bennet, a seasoned engineering executive in platform technology, open and closed source systems and artificial intelligence technology. A former engineering lead from Yahoo!, and part of the original team who brought Red Hat to success, Karen evolved with the technological revolution, utilizing AI in expert systems in her early IBM days, and is currently laying witness to the rapid experimentation in machine learning and deep learning. Our discussions about the current state of AI have culminated into this article.

**Building a Data Lake on Google Cloud Platform with CDAP**

It is no secret that traditional platforms for data analysis, like data warehouses, are difficult and expensive to scale, to meet the current data demands for storage and compute. And purpose-built platforms designed to process big data often require significant up-front and on-going investment if deployed on-premise. Alternatively, cloud computing is the perfect vehicle to scale and accommodate such large volumes of data in an economical way. While the economics are right, enterprises migrating their on-premises data warehouses or building a new warehouse or data lake in the cloud face many challenges along the way. These range from architecting network, securing critical data, having the right skill sets to work with the chosen cloud technologies, to figuring out the right set of tools and technologies to create operational workflows to load, transform and blend data.

**How to Create Value with Machine Learning**

Imagine the following scenario: your boss asks you to build a machine learning model to predict every month which customers of your subscription service will churn during the month with churn defined as no active membership for more than 31 days. You painstakingly make labels by finding historical examples of churn, brainstorm and engineer features by hand, then train and manually tune a machine learning model to make predictions. Pleased with the metrics on the holdout testing set, you return to your boss with the results, only to be told now you must develop a different solution: one that makes predictions every two weeks with churn defined as 14 days of inactivity. Dismayed, you realize none of your previous work can be reused because it was designed for a single prediction problem. You wrote a labeling function for a narrow definition of churn and the downstream steps in the pipeline?-?feature engineering and modeling?-?were also dependent on the initial parameters and will have to be redone. Due to hard-coding a specific set of values, you’ll have to build an entirely new pipeline to address for what is only a small change in problem definition.

**Get started with GPU Image Processing**

Besides the obvious use-case of a Graphics Processing Unit (GPU), namely rendering 3D objects, it is also possible to perform general-purpose computations using frameworks like OpenCL or CUDA. One famous use-case is bitcoin mining. We will look at an other interesting use-case: image processing. After discussing the basics of GPU programming, we implement dilation and erosion in less than 120 lines of code using Python and OpenCL.

**Outliers explained: a quick guide to the different types of outliers**

Success in business hinges on making the right decisions at the right time. You can only make smart decisions, however, if you also have the insights you need at the right time. When the right time is right now, outlier detection (aka anomaly detection) can help you chart a better course for your company as storms approach?-?or as the currents of business shift in your favor. In either case, quickly detecting and analyzing outliers can enable you to adjust your course in time to generate more revenue or avoid losses. And when it comes to analysis, the first step is knowing what types of outliers you’re up against.

**Transfer Learning : Learn and Transfer**

The second part of the title ‘Learn and transfer’ aptly describes the motivation behind penning this blog and the ones that are about to come. There is no dearth of learning resources on this topic, but only a few of them could couple the theoretical and empirical parts together and be intuitive enough. The reason ?? I guess we don’t transfer the knowledge in the exact way we store it in our minds. I believe that presenting complex topics in simple ways is an art, so lets master it. Lets begin a series of blogs where we will try to discuss why the things are the way they are and the intuitions behind them in the domain of machine learning. The first in the line is Transfer Learning (TL). We will begin with a crisp intro about TL followed by problem statement which I solved. We will see how TL can be implemented through various ways in Tensorflow and study how each of the tuning parameters in the context of TL affect the model statistics (the most interesting part !!). Finally we will see the conclusions, tips and tricks followed by some of the best references I could find.

**Facebook’s GraphQL gets its own open-source foundation**

GraphQL, the Facebook i-incubated data query language, is moving into its own open-source foundation. Like so many other similar open-source foundations, the aptly named GraphQL Foundation will be hosted by the Linux Foundation.

**Measuring Performance With Server Timing**

The Server Timing header provides a discrete and convenient way to communicate backend server performance timings to developer tools in the browser. Adding timing information to your application enables you to monitor back-end and front-end performance all in one place.

**Univariate Distribution Relationships**

The list on the left-hand side displays the names of the 76 probability distributions (19 discrete distributions given by the rectangular boxes and 57 continuous distributions given by the rectangular boxes with the rounded corners) present in the chart. Hovering your mouse over the name of a distribution highlights the distribution on the chart, along with its related distributions. Depending on the size of your browser window, you might have to adjust the display to find the distribution you are looking for. You may scroll the chart window or zoom in and out with the + and – buttons as needed. Each distribution on the chart, when clicked, links to a document showing detailed information about the distribution, including alternate functional forms of the distribution and the distribution’s mean, variance, skewness, and kurtosis.





### Like this:

Like Loading...


*Related*

