---
layout:     post
catalog: true
title:      Mastering The New Generation of Gradient Boosting
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/rAEI6eiNaCI/mastering-new-generation-gradient-boosting.html
date:      2018-11-15
author:      Matt Mayo Editor
tags:
    - â times
    - models
    - target
    - algorithms
    - algorithmic
---

**By Tal Peretz, Data Scientist**

![Catboost](https://cdn-images-1.medium.com/max/800/1*2p1GIUUcRSzyyJjSj4x7Iw.jpeg)


**Gradient Boosted Decision Trees** and Random Forest are my favorite ML models for tabular heterogeneous datasets. These models are the top performers on Kaggle competitions and in widespread use in the industry.

**Catboost**, the new kid on the block, has been around for a little more than a year now, and it is already threatening *XGBoost*, *LightGBM* and *H2O*.

 

### Why Catboost?

 **Better Results**

Catboost achieves the best results on the benchmark, and thatâ€™s great, yet I donâ€™t know if I would replace a working production model for only a fraction of a log-loss improvement alone (especially when the company who conducted the benchmark has a clear interest in the favor of Catboost ğŸ˜…).

Though, when you look at datasets where **categorical features play a large role**, such as *Amazon* and the *Internet *datasets, this improvement becomes significant and undeniable.

![](https://cdn-images-1.medium.com/max/800/1*vsg1IUlGtzCoNuGo9XqGwg.png)
GBDT Algorithms Benchmark

 **Faster Predictions**

While training time can take up longer than other GBDT implementations, prediction time is 13â€“16 times faster than the other libraries according to the Yandex benchmark.

![](https://cdn-images-1.medium.com/max/800/1*BE8PZe54DMWe6gFdHlYsxg.png)
Left: CPU, Right: GPU

 **Batteries Included**

Catboostâ€™s default parameters are a better starting point than in other GBDT algorithms. And this is good news for beginners who want a plug and play model to start experience tree ensembles or Kaggle competitions.

Yet, there are some very important parameters which we must address and weâ€™ll talk about those in a moment.

![](https://cdn-images-1.medium.com/max/800/1*znsWIb1X3Eez5LjNf4mg_g.png)
GBDT Algorithms with default parameters Benchmark

Some more noteworthy advancements by Catboost are the features interactions, object importance and the snapshot support.

In addition to classification and regression, Catboost supports **ranking** out of the box.

 **Battle Tested**

Yandex is relying heavily on Catboost for ranking, forecasting and recommendations. This model is serving more than 70 million users each month.

> CatBoost is an algorithm for **gradient boosting on decision trees**. Developed by Yandex researchers and engineers, it is the successor of the **MatrixNet algorithm**that is widely used within the company for ranking tasks, forecasting and making recommendations. It is universal and can be applied across a wide range of areas and to a variety of problems.

![](https://cdn-images-1.medium.com/max/800/1*_zP6KFIA76MQOgkslmQFzw.png)


 

### The Algorithm

 **Classic Gradient Boosting**

![](https://cdn-images-1.medium.com/max/800/1*0QYf5MRTzwtMpWuzDFTKdQ.png)
Gradient Boosting on Wikipedia

![](https://cdn-images-1.medium.com/max/800/1*6dGA56_UXJzuDAlAOd9Otw.png)


 

### Catboost Secret Sauce

 Catboost introduces two critical algorithmic advances - the implementation of **ordered boosting**, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for **processing categorical features**.

Both techniques are using random permutations of the training examples to fight the *prediction shift* caused by a special kind of *target leakage* present in all existing implementations of gradient boosting algorithms.

 

### **Cat**egorical Feature Handling

 **Ordered Target Statistic**

Most of the GBDT algorithms and Kaggle competitors are already familiar with the use of Target Statistic (or target mean encoding).

Itâ€™s a simple yet effective approach in which we encode each categorical feature with the estimate of the expected target y conditioned by the category.

Well, it turns out that applying this encoding carelessly (average value of y over the training examples with the same category) results in a target leakage.

To fight this *prediction shift *CatBoost uses a more effective strategy. It relies on the ordering principle and is inspired by online learning algorithms which get training examples sequentially in time. In this setting, the values of TS for each example rely only on the observed history.

To adapt this idea to a standard offline setting, Catboost introduces an artificial â€œtimeâ€�â€” a random permutation *Ïƒ1* of the training examples.

Then, for each example, it uses all the available â€œhistoryâ€� to compute its Target Statistic.

Note that, using only one random permutation, results in preceding examples with higher variance in Target Statistic than subsequent ones. To this end, CatBoost uses different permutations for different steps of gradient boosting.

 **One Hot Encoding**

Catboost uses a one-hot encoding for all the features with at most *one_hot_max_size* unique values. The default value is 2.

![](https://cdn-images-1.medium.com/max/800/1*nTMRk-U4KRFra3j8VMFz0A.png)
Catboostâ€™s Secret Sauce

 

### Orderd Boosting

 CatBoost has two modes for choosing the tree structure, Ordered and Plain. **Plain mode** corresponds to a combination of the standard GBDT algorithm with an ordered Target Statistic.

In **Ordered mode** boosting we perform a random permutation of the training examples - *Ïƒ2,* and maintain n different supporting models - *M1, . . . , Mn* such that the model *Mi* is trained using only the first *i* samples in the permutation.

At each step, in order to obtain the residual for *j*-th sample, we use the model *Mjâˆ’1*.

Unfortunately, this algorithm is not feasible in most practical tasks due to the need of maintaining n different models, which increase the complexity and memory requirements by n times. Catboost implements a modification of this algorithm, on the basis of the gradient boosting algorithm, using one tree structure shared by all the models to be built.

![](https://cdn-images-1.medium.com/max/800/1*SgwurdYEN5wMX-o3h6BeXw.png)
Catboost Ordered Boosting and Tree Building

In order to avoid *prediction shift*, Catboost uses permutations such that *Ïƒ1* = *Ïƒ2*. This guarantees that the target-*yi* is not used for training *Mi* neither for the Target Statistic calculation nor for the gradient estimation.

 

### Hands On

 For this section, weâ€™ll use the *Amazon Dataset*, since itâ€™s clean and has a strong emphasize on categorical features.

![](https://cdn-images-1.medium.com/max/1000/1*su6jiARoKMTWk-YmFvZzMg.png)
Dataset in a brief

 

### Tuning Catboost

 **Important Parameters**

**cat_features**â€Šâ€”â€ŠThis parameter is a must in order to leverage Catboost preprocessing of categorical features, if you encode the categorical features yourself and donâ€™t pass the columns indices as *cat_features *you are missing the essence of Catboost*.*
***one_hot_max_size****â€Šâ€”â€Š*As mentioned before,* *Catboost uses a one-hot encoding for all features with at most *one_hot_max_size* unique values. In our case, the categorical features have a lot of unique values, so we wonâ€™t use one hot encoding, but depending on the dataset it may be a good idea to adjust this parameter.
***learning_rate* & *n_estimators***â€Šâ€”â€ŠThe smaller the learning_rate, the more n_estimators needed to utilize the model. Usually the approach is to start with a relative high *learning_rate*, tune other parameters and then decrease the *learning_rate* while increasing *n_estimators*.
***max_depth****â€Šâ€”â€Š*Depth of the base trees*, *this parameter has an high impact on training time*.*
***subsampleâ€Š****â€”â€Š*Sample rate of rows, canâ€™t be used in a *Bayesian* boosting type setting.
***colsample_bylevelâ€Š****â€”â€Š*Sample rate of columns.
***l2_leaf_regâ€Š****â€”â€Š*L2 regularization coefficient.
***random_strengthâ€Š****â€”â€Š*Every split gets a score and random_strength is adding some randomness to the score, it helps to reduce overfitting.




 

### Model Exploration with Catboost

 In addition to feature importance, which is quite popular for GBDT models to share, Catboost provides **feature interactions** and **object (row) importance**

![](https://cdn-images-1.medium.com/max/800/1*6Y9gHBQLxk-PoIJLd2wr1g.png)
Catboostâ€™s Feature Importance

![](https://cdn-images-1.medium.com/max/800/1*VV1eH5Iwz3hJmKWAaV_Y6w.png)
Catboostâ€™s Feature Interactions

![](https://cdn-images-1.medium.com/max/800/1*ZoMzKdiIyLU9wDelELQMvg.png)
Catboostâ€™s Object Importance

![](https://cdn-images-1.medium.com/max/800/1*PUUt_CiVtLQ9PFOFEK91cA.png)
SHAP values can be used for other ensembles as well

 

### The Full Notebook

 Check it out for some useful Catboost code snippets

Catboost Playground Notebook

 

### Bottom Line

 ![](https://cdn-images-1.medium.com/max/1000/1*CBEhmpV8rdKWWwWnN4Db5g.png)
Catboost vs. XGBoost (default, greedy and exhaustive parameter search)

![](https://cdn-images-1.medium.com/max/800/1*LChQ07fA-MjChwv_yladpw.png)


 **Take Away**

Catboost is built with a similar approach and attributes as with the â€œolderâ€� generation of GBDT models.
Catboostâ€™s power lies in its **categorical features preprocessing**, **prediction time** and **model analysis**.
Catboostâ€™s weaknesses are its **training and optimization times**.
Donâ€™t forget to pass ***cat_features*** argument to the classifier object. You arenâ€™t really utilizing the power of Catboost without it.
Though Catboost performs well with default parameters, there are several parameters that drive a significant improvement in results when tuned.

 **Further Reading**

Huge thanks to Catboost Team Lead Anna Veronika Dorogush.

If you enjoyed this post, feel free to hit the clap button ğŸ‘�ğŸ�½ and if youâ€™re interested in posts to come, make sure to follow me on

**Medium:****https://medium.com/@talperetz24****Twitter:****https://twitter.com/talperetz24****LinkedIn:****https://www.linkedin.com/in/tal-per/**

Like every year, I want to mention DataHack- the best data driven hackathon out there. This year ×“×•×¨ ×¤×¨×¥ and I used Catboost for our project and won the 1st place ğŸ�†.

 **Bio: Tal Peretz** is a Data Scientist, Software Engineer, and a Continuous Learner. He is passionate about solving complex problems with high-value potential using data, code and algorithms. He especially likes building and improving models to separate the ones from the zeros. 0|1

Original. Reposted with permission.

**Related:**



 
