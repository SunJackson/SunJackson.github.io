---
layout:     post
catalog: true
title:      Whats new on arXiv
subtitle:      转载自：https://advanceddataanalytics.net/2018/11/04/whats-new-on-arxiv-804/
date:      2018-11-03
author:      Michael Laux
tags:
    - networks
    - networked
    - learning
    - learned
    - modeling
---

**Everything you always wanted to know about a dataset: studies in data summarisation**

Summarising data as text helps people make sense of it. It also improves data discovery, as search algorithms can match this text against keyword queries. In this paper, we explore the characteristics of text summaries of data in order to understand how meaningful summaries look like. We present two complementary studies: a data-search diary study with 69 students, which offers insight into the information needs of people searching for data; and a summarisation study, with a lab and a crowdsourcing component with overall 80 data-literate participants, which produced summaries for 25 datasets. In each study we carried out a qualitative analysis to identify key themes and commonly mentioned dataset attributes, which people consider when searching and making sense of data. The results helped us design a template to create more meaningful textual representations of data, alongside guidelines for improving data-search experience overall.

**IoT Cloud Platforms: an Application Development Perspective**

With the growing number of Internet of Things (IoT) devices, the data generated through these devices is also growing. By 2025, it has been predicted that the number of IoT devices will exceed the number of human beings on earth. Thus, the data generated through these IoT devices will be gigantic. This gives upsurge in storage. One of the most promising solutions is to store data on cloud. Market is over helming with the number of IoT cloud platforms. In-spite of availability of huge variety of IoT cloud platforms, very little attempt in classifying or comparing it for the applications to be developed is found across the literature databases. This paper categorizes IoT platforms into four categories namely: publicly traded, open source, developer friendly and end to end connectivity. Some of the popular platforms in each category are identified and compared based on the given general IoT architecture. This study can be useful for newbies and application developers in IoT to select the most appropriate platform according to requirement for building applications.

**Hypergraph based semi-supervised learning algorithms applied to speech recognition problem: a novel approach**

Most network-based speech recognition methods are based on the assumption that the labels of two adjacent speech samples in the network are likely to be the same. However, assuming the pairwise relationship between speech samples is not complete. The information a group of speech samples that show very similar patterns and tend to have similar labels is missed. The natural way overcoming the information loss of the above assumption is to represent the feature data of speech samples as the hypergraph. Thus, in this paper, the three un-normalized, random walk, and symmetric normalized hypergraph Laplacian based semi-supervised learning methods applied to hypergraph constructed from the feature data of speech samples in order to predict the labels of speech samples are introduced. Experiment results show that the sensitivity performance measures of these three hypergraph Laplacian based semi-supervised learning methods are greater than the sensitivity performance measures of the Hidden Markov Model method (the current state of the art method applied to speech recognition problem) and graph based semi-supervised learning methods (i.e. the current state of the art network-based method for classification problems) applied to network created from the feature data of speech samples.

**Explicit Feedbacks Meet with Implicit Feedbacks : A Combined Approach for Recommendation System**

Recommender systems recommend items more accurately by analyzing users’ potential interest on different brands’ items. In conjunction with users’ rating similarity, the presence of users’ implicit feedbacks like clicking items, viewing items specifications, watching videos etc. have been proved to be helpful for learning users’ embedding, that helps better rating prediction of users. Most existing recommender systems focus on modeling of ratings and implicit feedbacks ignoring users’ explicit feedbacks. Explicit feedbacks can be used to validate the reliability of the particular users and can be used to learn about the users’ characteristic. Users’ characteristic mean what type of reviewers they are. In this paper, we explore three different models for recommendation with more accuracy focusing on users’ explicit feedbacks and implicit feedbacks. First one is RHC-PMF that predicts users’ rating more accurately based on user’s three explicit feedbacks (rating, helpfulness score and centrality) and second one is RV-PMF, where user’s implicit feedback (view relationship) is considered. Last one is RHCV-PMF, where both type of feedbacks are considered. In this model users’ explicit feedbacks’ similarity indicate the similarity of their reliability and characteristic and implicit feedback’s similarity indicates their preference similarity. Extensive experiments on real world dataset, i.e. Amazon.com online review dataset shows that our models perform better compare to base-line models in term of users’ rating prediction. RHCV-PMF model also performs better rating prediction compare to baseline models for cold start users and cold start items.

**A Comparative Measurement Study of Deep Learning as a Service Framework**

Big data powered Deep Learning (DL) and its applications have blossomed in recent years, fueled by three technological trends: a large amount of digitized data openly accessible, a growing number of DL software frameworks in open source and commercial markets, and a selection of affordable parallel computing hardware devices. However, no single DL framework, to date, dominates in terms of performance and accuracy even for baseline classification tasks on standard datasets, making the selection of a DL framework an overwhelming task. This paper takes a holistic approach to conduct empirical comparison and analysis of four representative DL frameworks with three unique contributions. First, given a selection of CPU-GPU configurations, we show that for a specific DL framework, different configurations of its hyper-parameters may have significant impact on both performance and accuracy of DL applications. Second, the optimal configuration of hyper-parameters for one DL framework (e.g., TensorFlow) often does not work well for another DL framework (e.g., Caffe or Torch) under the same CPU-GPU runtime environment. Third, we also conduct a comparative measurement study on the resource consumption patterns of four DL frameworks and their performance and accuracy implications, including CPU and memory usage, and their correlations to varying settings of hyper-parameters under different configuration combinations of hardware, parallel computing libraries. We argue that this measurement study provides in-depth empirical comparison and analysis of four representative DL frameworks, and offers practical guidance for service providers to deploying and delivering DL as a Service (DLaaS) and for application developers and DLaaS consumers to select the right DL frameworks for the right DL workloads.

**Splitability Annotations: Optimizing Black-Box Function Composition in Existing Libraries**

Data movement is a major bottleneck in parallel data-intensive applications. In response to this problem, researchers have proposed new runtimes and intermediate representations (IRs) that apply optimizations such as loop fusion under existing library APIs. Even though these runtimes generally do no require changes to user code, they require intrusive changes to the library itself: often, all the library functions need to be rewritten for a new IR or virtual machine. In this paper, we propose a new abstraction called splitability annotations (SAs) that enables key data movement optimizations on black-box library functions. SAs only require that users add an annotation for existing, unmodified functions and implement a small API to split data values in the library. Together, this interface describes how to partition values that are passed among functions to enable data pipelining and automatic parallelization while respecting each library’s correctness constraints. We implement SAs in a system called Mozart. Without modifying any library function, on workloads using NumPy and Pandas in Python and Intel MKL in C, Mozart provides performance competitive with intrusive solutions that require rewriting libraries in many cases, can sometimes improve performance over past systems by up to 2x, and accelerates workloads by up to 30x.

**Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks**

While the use of bottom-up local operators in convolutional neural networks (CNNs) matches well some of the statistics of natural images, it may also prevent such models from capturing contextual long-range feature interactions. In this work, we propose a simple, lightweight approach for better context exploitation in CNNs. We do so by introducing a pair of operators: gather, which efficiently aggregates feature responses from a large spatial extent, and excite, which redistributes the pooled information to local features. The operators are cheap, both in terms of number of added parameters and computational complexity, and can be integrated directly in existing architectures to improve their performance. Experiments on several datasets show that gather-excite can bring benefits comparable to increasing the depth of a CNN at a fraction of the cost. For example, we find ResNet-50 with gather-excite operators is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters. We also propose a parametric gather-excite operator pair which yields further performance gains, relate it to the recently-introduced Squeeze-and-Excitation Networks, and analyse the effects of these changes to the CNN feature activation statistics.

**Big Data Meet Cyber-Physical Systems: A Panoramic Survey**

The world is witnessing an unprecedented growth of cyber-physical systems (CPS), which are foreseen to revolutionize our world {via} creating new services and applications in a variety of sectors such as environmental monitoring, mobile-health systems, intelligent transportation systems and so on. The {information and communication technology }(ICT) sector is experiencing a significant growth in { data} traffic, driven by the widespread usage of smartphones, tablets and video streaming, along with the significant growth of sensors deployments that are anticipated in the near future. {It} is expected to outstandingly increase the growth rate of raw sensed data. In this paper, we present the CPS taxonomy {via} providing a broad overview of data collection, storage, access, processing and analysis. Compared with other survey papers, this is the first panoramic survey on big data for CPS, where our objective is to provide a panoramic summary of different CPS aspects. Furthermore, CPS {require} cybersecurity to protect {them} against malicious attacks and unauthorized intrusion, which {become} a challenge with the enormous amount of data that is continuously being generated in the network. {Thus, we also} provide an overview of the different security solutions proposed for CPS big data storage, access and analytics. We also discuss big data meeting green challenges in the contexts of CPS.

**Using Large Ensembles of Control Variates for Variational Inference**

Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient’s variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient’s variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.

**DSKG: A Deep Sequential Model for Knowledge Graph Completion**
![](https://s0.wp.com/latex.php?latex=%28subject%2C+relation%2C+object%29&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=subject&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=relation&bg=ffffff&fg=000&s=0)


**Long Short-Term Attention**

In order to learn effective features from temporal sequences, the long short-term memory (LSTM) network is widely applied. A critical component of LSTM is the memory cell, which is able to extract, process and store temporal information. Nevertheless, in LSTM, the memory cell is not directly enforced to pay attention to a part of the sequence. Alternatively, the attention mechanism can help to pay attention to specific information of data. In this paper, we present a novel neural model, called long short-term attention (LSTA), which seamlessly merges the attention mechanism into LSTM. More than processing long short term sequences, it can distill effective and valuable information from the sequences with the attention mechanism. Experiments show that LSTA achieves promising learning performance in various deep learning tasks.

**Recurrent Attention Unit**

Recurrent Neural Network (RNN) has been successfully applied in many sequence learning problems. Such as handwriting recognition, image description, natural language processing and video motion analysis. After years of development, researchers have improved the internal structure of the RNN and introduced many variants. Among others, Gated Recurrent Unit (GRU) is one of the most widely used RNN model. However, GRU lacks the capability of adaptively paying attention to certain regions or locations, so that it may cause information redundancy or loss during leaning. In this paper, we propose a RNN model, called Recurrent Attention Unit (RAU), which seamlessly integrates the attention mechanism into the interior of GRU by adding an attention gate. The attention gate can enhance GRU’s ability to remember long-term memory and help memory cells quickly discard unimportant content. RAU is capable of extracting information from the sequential data by adaptively selecting a sequence of regions or locations and pay more attention to the selected regions during learning. Extensive experiments on image classification, sentiment classification and language modeling show that RAU consistently outperforms GRU and other baseline methods.

**Quickest Detection Of Deviations From Periodic Statistical Behavior**

A new class of stochastic processes called independent and periodically identically distributed (i.p.i.d.) processes is defined to capture periodically varying statistical behavior. Algorithms are proposed to detect changes in such i.p.i.d. processes. It is shown that the algorithms can be computed recursively and are asymptotically optimal. This problem has applications in anomaly detection in traffic data, social network data, and neural data, where periodic statistical behavior has been observed.

**A stochastic algorithm for deterministic multistage optimization problems**

Several attempt to dampen the curse of dimensionnality problem of the Dynamic Programming approach for solving multistage optimization problems have been investigated. One popular way to address this issue is the Stochastic Dual Dynamic Programming method (SDDP) introduced by Perreira and Pinto in 1991 for Markov Decision Processes.Assuming that the value function is convex (for a minimization problem), one builds a non-decreasing sequence of lower (or outer) convex approximations of the value function. Those convex approximations are constructed as a supremum of affine cuts. On continuous time deterministic optimal control problems, assuming that the value function is semiconvex, Zheng Qu, inspired by the work of McEneaney, introduced in 2013 a stochastic max-plus scheme that builds upper (or inner) non-increasing approximations of the value function. In this note, we build a common framework for both the SDDP and a discrete time version of Zheng Qu’s algorithm to solve deterministic multistage optimization problems. Our algorithm generates monotone approximations of the value functions as a pointwise supremum, or infimum, of basic (affine or quadratic for example) functions which are randomly selected. We give sufficient conditions on the way basic functions are selected in order to ensure almost sure convergence of the approximations to the value function on a set of interest.

**DropBlock: A regularization method for convolutional networks**
![](https://s0.wp.com/latex.php?latex=78.13%5C%25&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=1.6%5C%25&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=36.8%5C%25&bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=38.4%5C%25&bg=ffffff&fg=000&s=0)


**Exploration by Random Network Distillation**

We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma’s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.

• Scaling Speech Enhancement in Unseen Environments with Noise Embeddings• Volterra-assisted Optical Phase Conjugation: a Hybrid Optical-Digital Scheme For Fiber Nonlinearity Compensation• Some conjectures on the Schur expansion of Jack polynomials• Convolutional neural networks with extra-classical receptive fields• In-Silico Proportional-Integral Moment Control of Stochastic Reaction Networks with Applications to Gene Expression (with Dimerization)• Prediction of severity and treatment outcome for ASD from fMRI• Visual Re-ranking with Natural Language Understanding for Text Spotting• On Coding for Reliable VNF Chaining in DCNs• From Gene Expression to Drug Response: A Collaborative Filtering Approach• Audiovisual speaker conversion: jointly and simultaneously transforming facial expression and acoustic characteristics• Option market (in)efficiency and implied volatility dynamics after return jumps• Sesquickselect: One and a half pivots for cache-efficient selection• Beyond the rich-club: Properties of networks related to the better connected nodes• Enclosings of Decompositions of Complete Multigraphs in $2$-Edge-Connected $r$-Factorizations• Content Selection in Deep Learning Models of Summarization• Analyzing Ideological Communities in Congressional Voting Networks• Multi-label Multi-task Deep Learning for Behavioral Coding• Global Non-convex Optimization with Discretized Diffusions• Do Explanations make VQA Models more Predictable to a Human?• A Pragmatic Guide to Geoparsing Evaluation• Learning and Inference in Hilbert Space with Quantum Graphical Models• Unavoidable chromatic patterns in 2-colorings of the complete graph• Conditionals in Homomorphic Encryption and Machine Learning Applications• Reduced models of point vortex systems• Large Emission Regime in Mean Field Luminescence• Language Modeling with Sparse Product of Sememe Experts• Distinct Sampling on Streaming Data with Near-Duplicates• A Statistical Simulation Method for Joint Time Series of Non-stationary Hourly Wave Parameters• Backhaul-Aware Placement of a UAV-BS with Bandwidth Allocation for User-Centric Operation and Profit Maximization• The Impact of Bots on Opinions in Social Networks• Application of Clustering Methods to Anomaly Detection in Fibrous Media• Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks• Counting in Language with RNNs• Concentration of the Intrinsic Volumes of a Convex Body• Heteroscedastic Bandits with Reneging• Parallel Attention Mechanisms in Neural Machine Translation• Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation• The Holy Grail and the Bad Sampling – A test for the homogeneity of missing proportions for evaluating the agreement between peer review and bibliometrics in the Italian research assessment exercises• Partial Shading Detection and Smooth Maximum Power Point Tracking of PV Arrays under PSC• Resilient degree sequences with respect to Hamilton cycles and matchings in random graphs• Concealing the identity of faces in oblique images with adaptive hopping Gaussian mixtures• Object Detection based on LIDAR Temporal Pulses using Spiking Neural Networks• Prior-preconditioned conjugate gradient for accelerated Gibbs sampling in ‘large n & large p’ sparse Bayesian logistic regression models• Characterising and recognising game-perfect graphs• TallyQA: Answering Complex Counting Questions• Modelling visual-vestibular integration and behavioural adaptation in the driving simulator• Design and Implementation of Ecological Adaptive Cruise Control for Autonomous Driving with Communication to Traffic Lights• Learning Better Internal Structure of Words for Sequence Labeling• Geometric Median Shapes• Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data• String C-group representations of alternating groups• Complier stochastic direct effects: identification and robust estimation• Learning Distributed Representations of Symbolic Structure Using Binding and Unbinding Operations• Distributed Convex Optimization With Limited Communications• A Novel Approach to Quantized Matrix Completion Using Huber Loss Measure• Dance Teaching by a Robot: Combining Cognitive and Physical Human-Robot Interaction for Supporting the Skill Learning Process• Differentiable Greedy Networks• Inheritance-Based Diversity Measures for Explicit Convergence Control in Evolutionary Algorithms• Quasi-period collapse for duals to Fano polygons: an explanation arising from algebraic geometry• A Hybrid Frequency-domain/Image-domain Deep Network for Magnetic Resonance Image Reconstruction• Generating new pictures in complex datasets with a simple neural network• Polyhedral realizations of crystal bases and convex-geometric Demazure operators• Preparing for the Unexpected: Diversity Improves Planning Resilience in Evolutionary Algorithms• Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines• DARKMENTION: A Deployed System to Predict Enterprise-Targeted External Cyberattacks• On the distribution of rank statistic for strongly concave composition• Soft Activation Mapping of Lung Nodules in Low-Dose CT images• Predicting Sediment and Nutrient Concentrations in Rivers Using High Frequency Water Quality Surrogates• Unsupervised Meta-path Reduction on Heterogeneous Information Networks• A Framework for Probabilistic Generic Traffic Scene Prediction• empathi: An ontology for Emergency Managing and Planning about Hazard Crisis• Semiparametrically efficient estimation of the average linear regression function• Generalized Stability of Heisenberg Coefficients• Weak-supervision for Deep Representation Learning under Class Imbalance• DeepGRU: Deep Gesture Recognition Utility• Private Algorithms Can be Always Extended• Semiparametric response model with nonignorable nonresponse• Gated Transfer Network for Transfer Learning• Random Temporal Skipping for Multirate Video Analysis• The Many Faces of Far-from-equilibrium Thermodynamics: Deterministic Chaos, Randomness or Emergent Order?• Fully automatic structure from motion with a spline-based environment representation• Gated Hierarchical Attention for Image Captioning• Enhanced Ensemble Clustering via Fast Propagation of Cluster-wise Similarities• Explosive synchronization in phase-frustrated multiplex networks• Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks• 3D Traffic Simulation for Autonomous Vehicles in Unity and Python• Scale-Invariant Structure Saliency Selection for Fast Image Fusion• Machine Translation between Vietnamese and English: an Empirical Study• Relative Importance Sampling For Off-Policy Actor-Critic in Deep Reinforcement Learning• Shorten Spatial-spectral RNN with Parallel-GRU for Hyperspectral Image Classification• Almost-unsupervised Speech Recognition with Close-to-zero Resource Based on Phonetic Structures Learned from Very Small Unpaired Speech and Text Data• Nonlinear Prediction of Multidimensional Signals via Deep Regression with Applications to Image Coding• Optimal control of a rate-independent system constrained to parametrized balanced viscosity solutions• Rain Removal in Traffic Surveillance: Does it Matter?• Neural Nearest Neighbors Networks• Improved Network Robustness with Adversary Critic• Exploring Neural Methods for Parsing Discourse Representation Structures• Noninteracting fermions in a trap and random matrix theory• Learning-based predictive control for linear systems: a unitary approach• Inverse Quadratic Optimal Control for Discrete-Time Linear Systems• Subword Encoding in Lattice LSTM for Chinese Word Segmentation• VAPOR: a Value-Centric Blockchain that is Scale-out, Decentralized, and Flexible by Design• Waveform generation for text-to-speech synthesis using pitch-synchronous multi-scale generative adversarial networks• On the Aubin property of solution maps to parameterized variational systems with implicit constraints• Strong consistency of the AIC, BIC, $C_p$ and KOO methods in high-dimensional multivariate linear regression• About strong string stability of a vehicle chain with time-headway control• Adaptive Transfer Learning in Deep Neural Networks: Wind Power Prediction using Knowledge Transfer from Region to Region and Between Different Task Domains• Deep Learning as Feature Encoding for Emotion Recognition• On the spectral characterization of mixed extensions of $P_3$• About string stability of a vehicle chain with unidirectional controller• Towards End-to-end Automatic Code-Switching Speech Recognition• Computing the volume of the convex hull of the graph of a trilinear monomial using mixed volumes• An architecture of open-source tools to combine textual information extraction, faceted search and information visualisation• Spreading of Memes on Multiplex Networks• Large deviations for interacting particle systems: joint mean-field and small-noise limit• Triples of Orthogonal Latin and Youden Rectangles For Small Orders• Neuromorphic hardware as a self-organizing computing system• The Responsibility Quantification (ResQu) Model of Human Interaction with Automation• Prosodic entrainment in dialog acts• Deep Learning for the Gaussian Wiretap Channel• Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech• Evolutionarily Stable Preferences Against Multiple Mutations• Structure of the endpoint map near nice singular curves• Shift-enabled condition is necessary even for symmetric shift matrices• Sparse Gaussian process Audio Source Separation Using Spectrum Priors in the Time-Domain• Hybrid Knowledge Routed Modules for Large-scale Object Detection• Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior• Evaluating Text GANs as Language Models• Role of Class-specific Features in Various Classification Frameworks for Human Epithelial (HEp-2) Cell Images• Research Issues in Mining User Behavioral Rules for Context-Aware Intelligent Mobile Applications• Compositional Attention Networks for Interpretability in Natural Language Question Answering• Spectral Gap Inequality for Long-Range Random Walks• Two Questions about the Fractional Counting of Partitions• Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation• Limit theorems and fluctuations for point vortices of generalized Euler equations• On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models• Feature Trajectory Dynamic Time Warping for Clustering of Speech Segments• A note on saturation for Berge-G hypergraphs• Spoken Language Understanding on the Edge• Efficient Tree Solver for Hines Matrices on the GPU• Cluster Size Management in Multi-Stage Agglomerative Hierarchical Clustering of Acoustic Speech Segments• Gaussian Process Conditional Density Estimation• Higher-order Derivative Local Time for Fractional Ornstein-Uhlenbeck Processes• Topologies on Quotient Space of Matrices via Semi-tensor Product• Statistical robustness analysis of fractional and integer order PID controllers for the control of a nonlinear system• Reinforcement Learning and Deep Learning based Lateral Control for Autonomous Driving• Advancing PICO Element Detection in Medical Text via Deep Neural Networks• Transferable Positive/Negative Speech Emotion Recognition via Class-wise Adversarial Domain Adaptation• A rotor configuration with maximum escape rate• Antagonistic Structural Patterns in Complex Networks• Logarithmic bounds for Roth’s theorem via almost-periodicity• Average-Case Quantum Advantage with Shallow Circuits• Boost Blockchain Broadcast Propagation with Tree Routing• Piecewise Strong Convexity of Neural Networks• An Improved Algorithm for Computing Approximate Equilibria in Weighted Congestion Games• Encoded Hourglass Network for Semantic Segmentation of High Resolution Aerial Imagery• Band gap prediction for large organic crystal structures with machine learning• Nonlocal $p$-Laplacian Variational problems on graphs• Mean-square Stabilizability via Output Feedback for Non-minimum Phase Networked Feedback Systems• Informed Democracy: Voting-based Novelty Detection for Action Recognition• DeepTwist: Learning Model Compression via Occasional Weight Distortion• Sizing the length of complex networks• Cross-Modal Attentional Context Learning for RGB-D Object Detection• General audio tagging with ensembling convolutional neural network and statistical features• Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model• Constraints on Multipartite Quantum Entropies• AI for the Common Good?! Pitfalls, challenges, and Ethics Pen-Testing• Computational Intelligence in Sports: A Systematic Literature Review• Discovering state-parameter mappings in subsurface models using generative adversarial networks• Submodular Maximization Under A Matroid Constraint: Asking more from an old friend, the Greedy Algorithm• Divergence Network: Graphical calculation method of divergence functions• Optimally Weighted PCA for High-Dimensional Heteroscedastic Data• The structure of graphs with no W_4 immersion• Image Restoration using Total Variation Regularized Deep Image Prior• Exact Expectation Analysis of the Deficient-Length LMS Algorithm• Near-Optimal Coded Apertures for Imaging via Nazarov’s Theorem• The structure of graphs with no K_{3,3} immersion• Regressive and generative neural networks for scalar field theory• Data Poisoning Attack against Unsupervised Node Embedding Methods• Fractional Order Version of the HJB Equation• ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension• Simultaneous Dominating Set for Spanning Tree Factorings• Programming Substrate-Independent Kinetic Barriers with Thermodynamic Binding Networks• The Optimal Steady-State Control Problem• The 2-domination and Roman domination numbers of grid graphs• Topic-Specific Sentiment Analysis Can Help Identify Political Ideology





### Like this:

Like Loading...


*Related*

