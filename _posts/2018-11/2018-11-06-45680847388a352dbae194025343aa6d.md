---
layout:     post
catalog: true
title:      Text Preprocessing in Python： Steps, Tools, and Examples
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/Sfk--k7Q9ts/text-preprocessing-python.html
date:      2018-11-06
author:      Dan Clark
tags:
    - removing
    - removes
    - removal
    - removed
    - examples
---

**By Olga Davydova, Data Monsters**.

After a text is obtained, we start with text normalization. Text normalization includes:

- converting all letters to lower or upper case

- converting numbers into words or removing numbers

- removing punctuations, accent marks and other diacritics

- removing white spaces

- expanding abbreviations

- removing stop words, sparse terms, and particular words

- text canonicalization


We will describe text normalization steps in detail below.

![](https://www.kdnuggets.com/wp-content/uploads/videolectures-most-popular-text-mining-words.jpg)


### **Convert text to lowercase**

**Example 1. Convert text to lowercase**

**Python code:**

**Output:**

### Remove numbers

Remove numbers if they are not relevant to your analyses. Usually, regular expressions are used to remove numbers.

**Example 2. Numbers removing**

**Python code:**

**Output:**

### Remove punctuation

The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\]^_`{|}~]:

**Example 3. Punctuation removal**

**Python code:**

**Output:**

### Remove whitespaces

To remove leading and ending spaces, you can use the *strip()* function:

**Example 4. White spaces removal**

**Python code:**

**Output:**

### Tokenization

Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. In this table (“Tokenization” sheet) several tools for implementing tokenization are described.

Table 1: **Tokenization tools**

### Remove stop words

“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing.

**Example 7. Stop words removal**

**Code:**

**Output:**

A scikit-learn tool also provides a stop words list:

It’s also possible to use spaCy, a free open-source library:

### Remove sparse terms and particular words

In some cases, it’s necessary to remove sparse terms or particular words from texts. This task can be done using stop words removal techniques considering that any group of words can be chosen as the stop words.

### Stemming

Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words [14]) and Lancaster stemming algorithm (a more aggressive stemming algorithm). In the “Stemming” sheet of the table some stemmers are described.

**Stemming tools**

**Example 8. Stemming using NLTK:**

**Code:**

**Output:**

### Lemmatization

The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words.

Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core.

**Example 9. Lemmatization using NLTK:**

**Code:**

**Output:**

### Part of speech tagging (POS)

Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core.

**Example 10. Part-of-speech tagging using TextBlob:**

**Code:**

**Output:**

### Chunking (shallow parsing)

Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) [23]. Chunking tools: NLTK, TreeTagger chunker, Apache OpenNLP, General Architecture for Text Engineering (GATE), FreeLing.

**Example 11. Chunking using NLTK:**

The first step is to determine the part of speech for each word:

**Code:**

**Output:**

The second step is chunking:

**Code:**

**Output:**

It’s also possible to draw the sentence tree structure using code result.draw()

![](https://cdn-images-1.medium.com/max/1600/0*hXFkEDpJihWZ38uq)


### Named entity recognition

Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.).

Named-entity recognition tools: NLTK, spaCy, General Architecture for Text Engineering (GATE) — ANNIE, Apache OpenNLP, Stanford CoreNLP, DKPro Core, MITIE, Watson Natural Language Understanding, TextRazor, FreeLingare described in the “NER” sheet of the table.

**NER Tools**
