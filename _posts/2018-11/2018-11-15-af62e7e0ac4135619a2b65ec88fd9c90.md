---
layout:     post
catalog: true
title:      A deep dive into glmnet： standardize
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/fyV2bAjfrEo/
date:      2018-11-15
author:      kjytay
tags:
    - fitting
    - standardized
    - standardizing
    - standardization
    - scales
---

Unless otherwise stated, ![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&%23038;fg=333333&%23038;s=0)
 will denote the number of observations, ![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=333333&%23038;s=0)
 will denote the number of features, and `fit` will denote the output/result of the `glmnet` call. The data matrix is denoted by ![](https://s0.wp.com/latex.php?latex=X+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+p%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=X+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+p%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and the response is denoted by ![](https://s0.wp.com/latex.php?latex=y+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=y+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&%23038;fg=333333&%23038;s=0)
.

**standardize**

When `standardize = TRUE` (default), columns of the data matrix `x` are standardized, i.e. each column of `x` has mean 0 and standard deviation 1. More specifically, we have that for each ![](https://s0.wp.com/latex.php?latex=j+%3D+1%2C+%5Cdots%2C+p&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=j+%3D+1%2C+%5Cdots%2C+p&bg=ffffff&%23038;fg=333333&%23038;s=0)
,

![](https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+X_%7Bij%7D+%3D+0&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+X_%7Bij%7D+%3D+0&bg=ffffff&%23038;fg=333333&%23038;s=0)
, and ![](https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7BX_%7Bij%7D%5E2%7D%7Bn%7D%7D+%3D+1&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7BX_%7Bij%7D%5E2%7D%7Bn%7D%7D+%3D+1&bg=ffffff&%23038;fg=333333&%23038;s=0)
.

Why might we want to do this? Standardizing our features before model fitting is common practice in statistical learning. This is because if our features are on vastly different scales, the features with larger scales will tend to dominate the action. (One instance where we might not want to standardize our features is if they are already all measured along the same scale, e.g. meters or kilograms.)

Notice that the standardization here is slightly different from that offered by the `scale` function: `scale(x, center = TRUE, scale = TRUE)` gives the standardization

![](https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+X_%7Bij%7D+%3D+0&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+X_%7Bij%7D+%3D+0&bg=ffffff&%23038;fg=333333&%23038;s=0)
, and ![](https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7BX_%7Bij%7D%5E2%7D%7Bn-1%7D%7D+%3D+1&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7BX_%7Bij%7D%5E2%7D%7Bn-1%7D%7D+%3D+1&bg=ffffff&%23038;fg=333333&%23038;s=0)
.

We verify this with a small data example. Generate data according to the following code:

Create a version of the data matrix which has standardized columns:

Next, we run `glmnet` on `Xs` and `y` with both possible options for `standardize`:

We can check that we get the same fit in both cases (modulo numerical precision):

The documentation notes that the coefficients returned are on the original scale. Let’s confirm that with our small data set. Run `glmnet` with the original data matrix and `standardize = TRUE`:

For each column ![](https://s0.wp.com/latex.php?latex=j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=j&bg=ffffff&%23038;fg=333333&%23038;s=0)
, our standardized variables are ![](https://s0.wp.com/latex.php?latex=Z_j+%3D+%5Cdfrac%7BX_j+-+%5Cmu_j%7D%7Bs_j%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=Z_j+%3D+%5Cdfrac%7BX_j+-+%5Cmu_j%7D%7Bs_j%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
, where ![](https://s0.wp.com/latex.php?latex=%5Cmu_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmu_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=s_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=s_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
 are the mean and standard deviation of column ![](https://s0.wp.com/latex.php?latex=j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=j&bg=ffffff&%23038;fg=333333&%23038;s=0)
 respectively. If ![](https://s0.wp.com/latex.php?latex=%5Cbeta_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Cgamma_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cgamma_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
 represent the model coefficients of `fit2` and `fit3` respectively, then we should have

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cbeta_j+Z_j+%26%3D+%5Cgamma_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cgamma_j+X_j%2C+%5C%5C++%5Cbeta_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cbeta_j+%5Cfrac%7BX_j+-+%5Cmu_j%7D%7Bs_j%7D+%26%3D+%5Cgamma_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cgamma_j+X_j%2C+%5C%5C++%5Cleft%28+%5Cbeta_0+-+%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7B%5Cmu_j%7D%7Bs_j%7D+%5Cright%29+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7B%5Cbeta_j%7D%7Bs_j%7D+X_j+%26%3D+%5Cgamma_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cgamma_j+X_j%2C++%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cbeta_j+Z_j+%26%3D+%5Cgamma_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cgamma_j+X_j%2C+%5C%5C++%5Cbeta_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cbeta_j+%5Cfrac%7BX_j+-+%5Cmu_j%7D%7Bs_j%7D+%26%3D+%5Cgamma_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cgamma_j+X_j%2C+%5C%5C++%5Cleft%28+%5Cbeta_0+-+%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7B%5Cmu_j%7D%7Bs_j%7D+%5Cright%29+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7B%5Cbeta_j%7D%7Bs_j%7D+X_j+%26%3D+%5Cgamma_0+%2B+%5Csum_%7Bj%3D1%7D%5Ep+%5Cgamma_j+X_j%2C++%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)


i.e. we should have ![](https://s0.wp.com/latex.php?latex=%5Cgamma_0+%3D+%5Cbeta_0+-+%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7B%5Cmu_j%7D%7Bs_j%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cgamma_0+%3D+%5Cbeta_0+-+%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7B%5Cmu_j%7D%7Bs_j%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Cgamma_j+%3D+%5Cfrac%7B%5Cbeta_j%7D%7Bs_j%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cgamma_j+%3D+%5Cfrac%7B%5Cbeta_j%7D%7Bs_j%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 for ![](https://s0.wp.com/latex.php?latex=j+%3D+1%2C+%5Cdots%2C+p&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=j+%3D+1%2C+%5Cdots%2C+p&bg=ffffff&%23038;fg=333333&%23038;s=0)
. The code below checks that this is indeed the case (modulo numerical precision):

The discussion above has been for the standardization of `x`. What about standardization for `y`? The documentation notes that when `family = "gaussian"`, `y` is automatically standardized, and the coefficients are unstandardized at the end of the procedure.

More concretely, let the mean and standard deviation of ![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=y&bg=ffffff&%23038;fg=333333&%23038;s=0)
 be denoted by ![](https://s0.wp.com/latex.php?latex=%5Cmu_y&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmu_y&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=s_y&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=s_y&bg=ffffff&%23038;fg=333333&%23038;s=0)
 respectively. If running `glmnet` on standardized `y` gives intercept ![](https://s0.wp.com/latex.php?latex=%5Cbeta_0&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta_0&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and coefficients ![](https://s0.wp.com/latex.php?latex=%5Cbeta_1%2C+%5Cdots%2C+%5Cbeta_p&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta_1%2C+%5Cdots%2C+%5Cbeta_p&bg=ffffff&%23038;fg=333333&%23038;s=0)
, then `glmnet` on unstandardized `y` will give intercept ![](https://s0.wp.com/latex.php?latex=%5Cmu_y+%2B+s_y%5Cbeta_0&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmu_y+%2B+s_y%5Cbeta_0&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and coefficients ![](https://s0.wp.com/latex.php?latex=s_y%5Cbeta_1%2C+%5Cdots%2C+s_y%5Cbeta_p&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=s_y%5Cbeta_1%2C+%5Cdots%2C+s_y%5Cbeta_p&bg=ffffff&%23038;fg=333333&%23038;s=0)
.

Again, this can be verified empirically:


*Related*



 




---
