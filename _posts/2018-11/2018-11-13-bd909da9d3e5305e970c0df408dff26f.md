---
layout:     post
catalog: true
title:      A deep dive into glmnet： penalty.factor
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/TVDpdvRl0nw/
date:      2018-11-13
author:      kjytay
tags:
    - fit
    - objective
    - rep
    - beta
    - lambda
---

In this post, we will focus on the **penalty.factor** option.

Unless otherwise stated, ![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&%23038;fg=333333&%23038;s=0)
 will denote the number of observations, ![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&%23038;fg=333333&%23038;s=0)
 will denote the number of features, and `fit` will denote the output/result of the `glmnet` call.

**penalty.factor**

When this option is not set, for each value of ![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
 in `lambda`, `glmnet` is minimizing the following objective function:

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cunderset%7B%5Cbeta%7D%7B%5Ctext%7Bminimize%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Ctext%7BRSS%7D%7D%7Bn%7D+%2B+%5Clambda+%5Cdisplaystyle%5Csum_%7Bj%3D1%7D%5Ep+%5Cleft%28%5Cfrac%7B1+-+%5Calpha%7D%7B2%7D%5C%7C%5Cbeta_j+%5C%7C_2%5E2+%2B+%5Calpha+%5C%7C%5Cbeta_j+%5C%7C_1+%5Cright%29.+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cunderset%7B%5Cbeta%7D%7B%5Ctext%7Bminimize%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Ctext%7BRSS%7D%7D%7Bn%7D+%2B+%5Clambda+%5Cdisplaystyle%5Csum_%7Bj%3D1%7D%5Ep+%5Cleft%28%5Cfrac%7B1+-+%5Calpha%7D%7B2%7D%5C%7C%5Cbeta_j+%5C%7C_2%5E2+%2B+%5Calpha+%5C%7C%5Cbeta_j+%5C%7C_1+%5Cright%29.+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)


When the option is set to a vector `c(c_1, ..., c_p)`, `glmnet` minimizes the following objective instead:

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cunderset%7B%5Cbeta%7D%7B%5Ctext%7Bminimize%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Ctext%7BRSS%7D%7D%7Bn%7D+%2B+%5Clambda+%5Cdisplaystyle%5Csum_%7Bj%3D1%7D%5Ep+c_j+%5Cleft%28%5Cfrac%7B1+-+%5Calpha%7D%7B2%7D%5C%7C%5Cbeta_j+%5C%7C_2%5E2+%2B+%5Calpha+%5C%7C%5Cbeta_j+%5C%7C_1+%5Cright%29.+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cunderset%7B%5Cbeta%7D%7B%5Ctext%7Bminimize%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Ctext%7BRSS%7D%7D%7Bn%7D+%2B+%5Clambda+%5Cdisplaystyle%5Csum_%7Bj%3D1%7D%5Ep+c_j+%5Cleft%28%5Cfrac%7B1+-+%5Calpha%7D%7B2%7D%5C%7C%5Cbeta_j+%5C%7C_2%5E2+%2B+%5Calpha+%5C%7C%5Cbeta_j+%5C%7C_1+%5Cright%29.+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)


In the documentation, it is stated that “the penalty factors are internally rescaled to sum to nvars and the lambda sequence will reflect this change.” However, from my own experiments, it seems that the penalty factors are internally rescaled to sum to nvars **but** the lambda sequence remains the same. Let’s generate some data:

We fit two models, `fit` which uses the default options for `glmnet`, and `fit2` which has `penalty.factor = rep(2, 5)`:

What we find is that these two models have the exact same `lambda` sequence and produce the same `beta` coefficients.

The same thing happens when we supply our own `lambda` sequence:

Hence, my conclusion is that if `penalty.factor` is set to `c(c_1, ..., c_p)`, `glmnet` is really minimizing

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cunderset%7B%5Cbeta%7D%7B%5Ctext%7Bminimize%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Ctext%7BRSS%7D%7D%7Bn%7D+%2B+%5Clambda+%5Cdisplaystyle%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7Bc_j%7D%7B%5Cbar%7Bc%7D%7D+%5Cleft%28%5Cfrac%7B1+-+%5Calpha%7D%7B2%7D%5C%7C%5Cbeta_j+%5C%7C_2%5E2+%2B+%5Calpha+%5C%7C%5Cbeta_j+%5C%7C_1+%5Cright%29%2C+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cunderset%7B%5Cbeta%7D%7B%5Ctext%7Bminimize%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Ctext%7BRSS%7D%7D%7Bn%7D+%2B+%5Clambda+%5Cdisplaystyle%5Csum_%7Bj%3D1%7D%5Ep+%5Cfrac%7Bc_j%7D%7B%5Cbar%7Bc%7D%7D+%5Cleft%28%5Cfrac%7B1+-+%5Calpha%7D%7B2%7D%5C%7C%5Cbeta_j+%5C%7C_2%5E2+%2B+%5Calpha+%5C%7C%5Cbeta_j+%5C%7C_1+%5Cright%29%2C+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)


where ![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bc%7D+%3D+%5Cfrac%7B1%7D%7Bp%7D%5Csum_%7Bj%3D1%7D%5Ep+c_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbar%7Bc%7D+%3D+%5Cfrac%7B1%7D%7Bp%7D%5Csum_%7Bj%3D1%7D%5Ep+c_j&bg=ffffff&%23038;fg=333333&%23038;s=0)
.


*Related*



 




---
