---
layout:     post
title:      Convolutional Neural Network for Time Series
subtitle:   转载自：http://www.quintuitive.com/2017/04/14/convolutional-neural-network-time-series/
date:       2017-04-14
author:     quintuitive
header-img: img/background1.jpg
catalog: true
tags:
    - layers
    - cntk
    - github
    - parallelism
    - parallelizing
---



Neural networks have been around for a while, but it’s fair to say that many successful practical applications use at least one convolutional layer. Naturally, convolutions make sense for time series, so I went and added a few to the [Walk-Forward Analysis](http://www.quintuitive.com/2017/01/28/deep-learning-walk-forward-loop).To make the code easier to use, I ended up creating a self-contained [GitHub repository](https://github.com/ivannp/wfl_cntk).

[CNTK](https://www.microsoft.com/en-us/research/product/cognitive-toolkit)‘s code to create the network layers is trivial:

The above code uses the primitives from *cnkt.layers*, which provide higher-level abstractions. For a lower-level experience, similar to [Tensorflow](https://www.tensorflow.org/), one should use the functions provided by *cntk.ops*.

So I created the network, and run the training. Boy, it was slow – about 16-20 secs per point (single day). And I want to run *8,000* of these.

[CNTK](https://www.microsoft.com/en-us/research/product/cognitive-toolkit) can train in parallel, both on GPUs and CPUs, but I decided to stay with CPUs for now**(that’s how one creates desire for future posts) and to not** use CNTK’s CPU parallelism. The latter sounds weird at first (actually both do), but as we have discussed previously here (see [the old ARMA/GARCH posts](http://www.quintuitive.com/2012/11/16/parallelized-back-testing)), the bigger the chunks for each task, the better the parallel performance. In other words, running all days in parallel is better than running the days sequentially and parallelizing the computations for that day. That’s what worked for me in R, and now, in Python.

The [multiprocessing package](https://docs.python.org/3.5/library/multiprocessing.html) had everything I needed. The main bump to overcome was the sharing of a global lock between processes – we need to synchronize the database access and the log file access.

When I start with a pool of four processes (controlled by the *pool_size* parameter below):

I see the following in the process explorer:

Pretty. Mission accomplished.


