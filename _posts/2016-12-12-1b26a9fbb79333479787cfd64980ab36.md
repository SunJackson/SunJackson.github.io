---
layout:     post
title:      NIPS 2016 Reflections
subtitle:   转载自：http://www.machinedlearnings.com/2016/12/nips-2016-reflections.html
date:       2016-12-12
author:     Paul Mineiro (noreply@blogger.com)
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - simulation
    - simulated
    - simulators
    - nips
    - scientific
    - performance
    - posters
    - learning
    - generating
    - themes
    - openness
    - workshop
    - interpretable
    - interpretability
    - open basic
    - sense
    - pictures
    - discussion
    - discussed
    - undergraduate
    - rish
    - space
    - sample
    - conferences
    - basically
    - generalization
    - metric reproducible
    - public good
    - antoine
    - images
    - image training
    - bordes
    - leveraging parallel
    - dialogue
    - reflections
    - aspects
    - real
---












### 
[NIPS 2016 Reflections](http://www.machinedlearnings.com/2016/12/nips-2016-reflections.html)



It was a great conference. The organizers had to break with tradition to accommodate the rapid growth in submissions and attendance, but despite my nostalgia, I feel the changes were beneficial. In particular, leveraging parallel tracks and eliminating poster spotlights allowed for more presentations while ending the day before midnight, and the generous space allocation per poster really improved the poster session. The workshop organizers apparently thought of everything in advance: I didn't experience any hiccups (although, we only had one microphone, so I got a fair bit of exercise during discussion periods).

Here are some high-level themes I picked up on.

**Openness**. Two years ago Amazon started opening up their research, and they are now a major presence at the conference. This year at NIPS, Apple announced they would be opening up their research practices. Clearly, companies are finding it in their best interests to fund open basic research, which runs counter to folk-economic reasoning that basic research appears to be a pure public good and therefore will not be funded privately due to the free-rider problem. A real economist would presumably say that is simplistic undergraduate thinking. Still I wonder, to what extent are companies being irrational? Conversely, what real-world aspects of basic research are not well modeled as a public good? I would love for an economist to come to NIPS to give an invited talk on this issue. 

**Simulation**. A major theme I noticed at the conference was the use of simulated environments. One reason was articulated by Yann LeCun during his [opening keynote](https://nips.cc/Conferences/2016/Schedule?showEvent=6197): (paraphrasing) ``simulation is a plausible strategy for mitigating the high sample complexity of reinforcement learning.'' But another reason is scientific methodology: for counterfactual scenarios, simulated environments are the analog of datasets, in that they allow for a common metric, reproducible experimentation, and democratization of innovation. Simulators are of course not new and have had waves of enthusiasm and pessimism in the past, and there are a lot of pitfalls which basically boil down to overfitting the simulator (both in a micro sense of getting a bad model, but also in a macro sense of focusing scientific attention on irrelevant aspects of a problem). Hopefully we can learn from the past and be cognizant of the dangers. There's more than a blog post worth of content to say about this, but here are two things I heard at the [dialog workshop](http://letsdiscussnips2016.weebly.com/) along these lines: first, Jason Williams suggested that relative performance conclusions based upon simulation can be safe, but that absolute performance conclusions are suspect; and second, Antoine Bordes advocated for using an ensemble of realizable simulated problems with dashboard scoring (i.e., multiple problems for which perfect performance is possible, which exercise apparently different capabilities, and for which there is currently no single approach that is known to handle all the problems).

Without question, simulators are proliferating. I noticed the following discussed at the conference this year:and I probably missed some others.

By the way, the alternatives to simulation aren't perfect either: some of the discussion in the dialogue workshop was about how the incentives of crowdsourcing induces unnatural behaviour in participants of crowdsourced dialogue experiments.

**GANs** The frenzy of GAN research activity from other conferences (such as ICLR) colonized NIPS in a big way this year. This is related to simulation, albeit more towards the mitigating-sample-complexity theme than the scientific-methodology theme. The quirks of getting the optimization to work are being worked out, which should enable some interesting improvements in RL in the near-term (in addition to many nifty pictures). Unfortunately for NLU tasks, generating text from GANs is currently not as mature as generating sounds or images, but there were some posters addressing this.

**Interpretable Models** The idea that model should be able to “explain itself” is very popular in industry, but this is the first time I have seen interpretability receive significant attention at NIPS. Impending EU regulations have certainly increased interest in the subject. But there are other reasons as well: as Irina Rish pointed out in her [invited talk on (essentially) mindreading](https://nips.cc/Conferences/2016/Schedule?showEvent=6196), recent advances in representation learning could better facilitate scientific inquiry if the representations were more interpretable.

Would you trust a single reviewer on yelp? I wouldn't. Therefore, I think we need some way to crowdsource what people thought were good papers from the conference. I'm just one jet-lagged person with two eyeballs (btw, use bigger font people! it gets harder to see the screen every year …), plus everything comes out on arxiv first so if I read it already I don't even notice it at the conference. That makes this list weird, but here you go.

Also this paper was not at the conference, as far as I know, but I found out about it during the coffee break and it's totally awesome:- [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530). TL;DR: convnets can shatter the standard image training sets when the pixels are permuted or even randomized! Of course, generalization is poor in this case, but it indicates they are way more flexible than their “local pixel statistics composition” architecture suggests. So why do they work so well?













