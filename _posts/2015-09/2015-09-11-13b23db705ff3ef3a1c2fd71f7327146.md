---
layout:     post
title:      Simple RNN in Julia
subtitle:   转载自：http://outlace.com/rnnjulia.html
date:       2015-09-11
author:     Brandon Brown
header-img: img/background3.jpg
catalog: true
tags:
    - random number
    - block calculates
    - subset
    - updated
    - compute
---

If you've gone through my tutorial on backpropagation and gradient descent, most of this code will look very familiar so I'm not going to explain every single line. I've also commented it well. But let's go over the important/new parts of the code.

For every training iteration, we initialize s to be a random number between 1 and `(m - 1)` where `m` is the number of elements in $X$. This is sort of a mini-batch, stochastic gradient descent implementation because we're training on a random subset of our input sequence each training iteration. This will hopefully ensure our network learns how to actually compute `LEFTBIT` and not just memorize the original sequence we gave it to train on (which would be overfitting).

This is where we build up our input layer. First we get the element from the input $X$ vector that we're currently training on and add it to our input layer $a1$, then we add in our context units, and finally add in the bias value of 1. The rest is ordinary feedforward logic.

We save the current hidden layer output in the `hid_last` vector that we defined and initialized outside of the training loop.

this block calculates the network deltas and gradients (backpropagation) but not if this is the first element in the sequence because the network can't compute `LEFTBIT` on only 1 input bit. Thus is wouldn't make sense to calculate an error for the first bit when it's impossible for the RNN to get it correct.

This is where we calculate the weight updates based on the learning rate alpha, the gradients, and the previous weight update (momentum method).

To continue with the momentum method in the next iteration, we save how much we updated the weights in this iteration in our last_change vectors:

Here we reset the gradients for the next epoch (and thus training 'mini-batch'). We don't want to keep accumulating gradients forever. We reset them after accumulating gradients for each training sequence.

---


Okay, so that is how we implement and train our simple RNN to compute `LEFTBIT` on an arbitrary binary sequence.
After training it, let's run the network forward over a new binary sequence to see if it learned.
