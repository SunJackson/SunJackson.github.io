---
layout:     post
title:      Sentiment analysis for Youtube channels – with NLTK
subtitle:   转载自：https://datanice.wordpress.com/2015/09/09/sentiment-analysis-for-youtube-channels-with-nltk/
date:       2015-09-09
author:     aptissimum
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - youtube
    - videos
    - import
    - video channels
    - comments data
    - https
    - likes
    - tokens
    - tokenized
    - plots
    - nltk
    - item
    - videoid
    - def
    - tempcomments
    - prob
    - pretty
    - temp_res
    - dict
    - classifier
    - classified
    - returns
    - videolist
    - training
    - trained
    - sentiments
    - interactive
    - word
    - statistic
---

In this tutorial, we ‘ll first take a look at the Youtube API to retrieve comments data about the channel as well as basic information about the likes count and view count of the videos. Then, we will use Nltk to see most frequently used words in the comments and plot some sentiment graphs.

[![](https://datanice.files.wordpress.com/2015/09/download1-e1441826980693.jpeg?w=525)
](https://datanice.files.wordpress.com/2015/09/download1-e1441826980693.jpeg)

With the script below, we first query the video channels providing the channel ID then for every video we get a list of comments (Youtube limits this number to 20 comments per query)

```
from apiclient.discovery import build 
import pandas as pd 
import time

DEVELOPER_KEY = "" 
YOUTUBE_API_SERVICE_NAME = ""
YOUTUBE_API_VERSION = ""

youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)

def get_videos_FromChanel(youtube, channelId,order):
search_response = youtube.search().list(
 channelId=channelId,
 type="video",
 part="id,snippet",
 maxResults=50,
 order=order
).execute()

return search_response.get("items", [])

def get_comment_threads(youtube, videos):
tempComments = []
for video in videos:
time.sleep(1.0)
print video["snippet"]["title"]
 results = youtube.commentThreads().list(
 part="snippet",
 videoId=video["id"]["videoId"],
 textFormat="plainText",
 maxResults=20,
 order='relevance'
 ).execute()

 
 for item in results["items"]:
 comment = item["snippet"]["topLevelComment"]
 tempComment = dict(videoId=video["id"]["videoId"], videoName=video["snippet"]["title"],nbrReplies = item["snippet"]["totalReplyCount"],author = comment["snippet"]["authorDisplayName"],likes = comment["snippet"]["likeCount"],publishedAt=comment["snippet"]["publishedAt"],text = comment["snippet"]["textDisplay"].encode('utf-8').strip())
 tempComments.append(tempComment)
 
 return tempComments

```

then you can call the function using :

```
videos = get_videos_FromChanel(youtube, "CHANNEL_ID","viewCount")
```

Youtube has a special option to retreive the statistic data, we just have to query the video list method using the “statistic” option.

```
def getVideoInfos(videos):
    videoList = {}
    for search_result in videos:
         if search_result["id"]["kind"] == "youtube#video":
             videoList[search_result["id"]["videoId"]] = search_result["snippet"]["title"]

    s = ','.join(videoList.keys())
    videos_list_response = youtube.videos().list(id=s,part='id,statistics').execute()
    res = []
    for i in videos_list_response['items']:
         temp_res = dict(v_title = videoList[i['id']])
         temp_res.update(i['statistics'])
         res.append(temp_res)

    data = pd.DataFrame.from_dict(res)
    data['viewCount'] = data['viewCount'].map(lambda x : float(x))
    data['commentCount'] = data['commentCount'].map(lambda x : float(x))
    return data

data = getVideoInfos(videos)

infos.sort('viewCount',ascending=0).head(20).plot(kind='bar', x='v_title',y='viewCount')

```

This is what I get for the view count of the [shots of Awe](https://www.youtube.com/channel/UClYb9NpXnRemxYoWbcYANsA) videos channel:[![](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-175623.png?w=525)
](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-175623.png)

We can have the same plots for likes counts and comments count, and plot scatter plots to see if there is a correlation between these features.

We can note that the third video has only been uploaded for a few days at the time I’m writing this article, that’s what we call a buzz video.

An interesting chart to plot would be the number of views/time online.

What do people talk about in the comments ? What do they like/hate the most about the channel ?In order to answer these questions we can look at the word frequency in the comments. We can use the “nltk” package to see the distribution :

```
import nltk
from nltk.probability import *
from nltk.corpus import stopwords
import pandas as pd

all = pd.read_json("comments.csv")

stop_eng = stopwords.words('english')
customstopwords =[]

tokens = []
sentences = []
tokenizedSentences =[]
for txt in all.text:
    sentences.append(txt.lower())
    tokenized = [t.lower().encode('utf-8').strip(":,.!?") for t in txt.split()]
    tokens.extend(tokenized)
    tokenizedSentences.append(tokenized)

hashtags = [w for w in tokens if w.startswith('#')]
ghashtags = [w for w in tokens if w.startswith('+')]
mentions = [w for w in tokens if w.startswith('@')]
links = [w for w in tokens if w.startswith('http') or w.startswith('www')]
filtered_tokens = [w for w in tokens if not w in stop_eng and not w in customstopwords and w.isalpha() and not len(w)<3 and not w in hashtags and not w in ghashtags and not w in links and not w in mentions]

fd = FreqDist(filtered_tokens)

```

FreqDist returns a list of tuples containing each word and the number of its occurences. Let”s plot a bar chart to visualize it:

And the result :

[![](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-203414.png?w=726&h=420)
](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-203414.png)

The most used words are “love” (141 occurrences), “like” (89 occurrences) then “think” and “life”. Pretty deep !

In order to analyze the comments sentiments, we are going to train a Naive Bayes Classifier using a dataset provided by nltk. This could be imroved using a better training dataset for comments or tweets.

The reviews are classified as “negative” or “positive”, and our classifier will return the probability of each label. We will compute a score = prob(“positive”) – prob(“negative”) to get a score between -1 an 1.

#### Training the classifier

Once the classifier trained, we added a column with the sentiment score using the “classifier.prob_classify” function.

Let’s see the results for the first video in a scatter plot (versus likes)

[![](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-210331.png?w=799&h=471)
](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-210331.png)

For the next one,

[![](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-210427.png?w=525)
](https://datanice.files.wordpress.com/2015/09/screenshot-from-2015-09-09-210427.png)

A cool thing to do here is to see what’s the text of every comment. The best thing to do is to have an interactive plot where hovering on a point shows the comment text. This could be done with the [d3.js](http://d3js.org/) library. We can also make the axis interactive and add animations to animate the points when for example changing the sentiment axis to the publishing date of the comment…

Next Steps:

- Adding a plot for “number of views”/”time online”

- Interactive d3.js plot to see the comment text when hovering on he comment.






### Like this:

Like Loading...
