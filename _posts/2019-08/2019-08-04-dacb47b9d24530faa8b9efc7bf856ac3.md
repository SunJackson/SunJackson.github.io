---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/04/distilled-news-1152/
date:      2019-08-04
author:      Michael Laux
tags:
    - data
    - learning
    - learned
    - modeling
    - models
---

**The Limitations of Machine Learning**

Machine learning is now seen as a silver bullet for solving all problems, but sometimes it is not the answer.Limitation 1 – EthicsLimitation 2 – Deterministic ProblemsLimitation 3 – DataLimitation 4 – MisapplicationLimitation 5 – Interpretability

**Why Machine Learning won’t cut it**

Current machine learning approaches will not get us to real AI. The kind that can truly understand you, and learn new knowledge and skills by itself. Like humans do.

**Modeling with Reinforcement Learning**

Reinforcement learning involves figuring out what to do in which situation. This can be tricky. Only a tiny fraction of all possible situations might have been experienced. If that. Even in a familiar situation, a tried-and-true action might, in a particular instance, produce an unexpected result. The environment might throw a curve-ball. Actions have immediate and delayed consequences, possibly conflicting. Some delayed consequences might be unknown. Some might not repeat themselves even in the same situation. Some might depend on actions that follow the initial one. The agent seeks to sort through all of this to discover and execute action strategies that suitably balance the two. In this post, we introduce the main concepts of reinforcement learning. Our perspective is a modeling one. We consider various use cases. We simulate the process of trying to model them as reinforcement learning problems. Interesting issues arise. Insights are gained.

**Another AI Winter Could Usher in a Dark Period for Artificial Intelligence**

Self-driving cars. Faster MRI scans, interpreted by robotic radiologists. Mind reading and x-ray vision. Artificial intelligence promises to permanently alter world. (In some ways, it already has. Just ask this AI scheduling assistant.) Artificial intelligence can take many forms. But it’s roughly defined as a computer system capable of tackling human tasks like sensory perception and decision-making. Since its earliest days, AI has fallen prey to cycles of extreme hype – and subsequent collapse. While recent technological advances may finally put an end to this boom-and-bust pattern, cheekily termed an ‘AI winter,’ some scientists remain convinced winter is coming again.

**Building a Recommendation System using Word2vec: A Unique Tutorial with Case Study in Python**

Be honest – how many times have you used the ‘Recommended for you’ section on Amazon? Ever since I found out a few years back that machine learning powers this section – I have been hooked. I keep an eye on that section each time I log into Amazon. There’s a reason companies like Netflix, Google, Amazon, Flipkart, etc. spend millions perfecting their recommendation engine. It is a powerful acquisition channel and enhances the customer’s experience.

**Calibrated Quantum Mesh – Better Than Deep Learning for Natural Language Search**

Move over RNN/LSTM, there’s a new algorithm called Calibrated Quantum Mesh that promises to bring new levels of accuracy to natural language search and without labeled training data.

**Robust Neural Machine Translation**

In recent years, neural machine translation (NMT) using Transformer models has experienced tremendous success. Based on deep neural networks, NMT models are usually trained end-to-end on very large parallel corpora (input/output text pairs) in an entirely data-driven fashion and without the need to impose explicit rules of language. Despite this huge success, NMT models can be sensitive to minor perturbations of the input, which can manifest as a variety of different errors, such as under-translation, over-translation or mistranslation. For example, given a German sentence, the state-of-the-art NMT model, Transformer, will yield a correct translation.

**I didn’t mean() to ignore the median()**

This week’s post follows directly from last week’s investigation of data from the 2016 US Census Bureau’s American Community Survey (ACS) Public Use Microdata Sample (PUMS). We explored mean differences in income across several different types of employment status (self-employed, private sector, government, etc.). We found, using bayesian methods, strong evidence for differences across the categories and were able to plot them in a variety of formats using ggplot2 and ggsignif.

**NLP for Beginners: Cleaning & Preprocessing Text Data**

NLP is short for Natural Language Processing. As you probably know, computers are not as great at understanding words as they are numbers. This is all changing though as advances in NLP are happening everyday. The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP. The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language.

**The AI Who Was Born on a Farm**

In a recent post I looked at some ideas about how consciousness develops, and then proposed a sequence of stages that might allow an intelligent, learning machine to build a conscious self. Herein I let that AI tell its own story, explaining each of the 4 stages of training.

**A Guide to Building Convolutional Neural Networks from Scratch**

Convolutional neural networks are the workhorse behind a lot of the progress made in deep learning during the 2010s. These networks have revolutionized tasks such as image classification and object detection, but they also work remarkably well in other contexts such as text classification, speech recognition, or any domain where a filter can be used to detect similarities in regions of input data. In this post I will go over how to build a basic CNN in from scratch using numpy. This exercise goes into the nuts and bolts for how these networks actually work. In practice, it is common to use deep learning frameworks such as Tensorflow or Pytorch. These frameworks are great, but it is impossible to understand what a convolutional neural network is actually doing at each step when all you have to do is type a few lines of code to create a CNN.

**How to manage Machine Learning and Data Science projects**

Machine learning (ML) and Data Science (DS) projects are hard to manage. Because projects are research-like in nature, it’s difficult to predict how long it will take for them to finish. They often start off with one idea and then pivot into a new direction when the proposed technique doesn’t work or the assumptions made about the data are proven wrong. Model-building is also an inherently long process (compared to most Software Engineering and Analytics work) and it’s not uncommon for a data scientist to enter a rabbit hole and spend months on a project without a clear notion of progress. Another distinction from standard software engineering practices is that model development is usually done by a single person. This serial nature does not lend itself well to traditional collaborative SE workflows such as Kanban and Scrum.

**Introduction to Principal Component Analysis**

According to DataCamp, PCA can be viewed in the following ways:• One of the more-useful methods from applied linear algebra• Non-parametric way of extracting meaningful information from confusing data sets• Uncovers hidden, low-dimensional structures that underlie your data• These structures are more-easily visualized and are often interpretable to content experts

**Neural networks over classical models in Time Series**

This article discusses the capabilities of various kinds of neural networks in time series modeling. Classical machine learning models based on time series forecasting are much difficult to implement compared to the supervised and unsupervised learning models because of the temporal difference in the data: we work on the data plotted against the same data at a different time step. This makes the process of model-fitting and model evaluation relatively difficult. ARIMA is a very popular tool in the market, because of the reason that it can be used for almost any kind of time series data and is quite easy to understand and is effective in its implementation. There are 3 main limitations to classical models that can be overcome by deep learning methods, as discussed below.

**Implementing time series ARIMA**

We have heard a lot about the ARIMA based models in time series analysis, which is a popular tool used for forecasting a wide range of time series data. That being said, it’s not recommended to blindly go with ARIMA models without understanding the data and its underlying trends. This article discusses ARIMA and its general application. ARIMA is non-seasonal and SARIMA has seasonal applicability. SARIMA has an extended applicability of ARIMA model: it includes seasonality of the data in it’s modelling and predictions. But wait, what is an ARIMA in general?

**Why Not Airflow?**

Airflow is a historically important tool in the data engineering ecosystem. It introduced the ability to combine a strict Directed Acyclic Graph (DAG) model with Pythonic flexibility in a way that made it appropriate for a wide variety of use cases. However, Airflow’s applicability is limited by its legacy as a monolithic batch scheduler aimed at data engineers principally concerned with orchestrating third-party systems employed by others in their organizations. Today, many data engineers are working more directly with their analytical counterparts. Compute and storage are cheap, so friction is low and experimentation prevails. Processes are fast, dynamic, and unpredictable. Airflow got many things right, but its core assumptions never anticipated the rich variety of data applications that has emerged. It simply does not have the requisite vocabulary to describe many of those activities. The seed that would grow into Prefect was first planted all the way back in 2016, in a series of discussions about how Airflow would need to change to support what were rapidly becoming standard data practices. Disappointingly, those observations remain valid today. We open sourced the Prefect engine a few weeks ago as the first step toward introducing a modern data platform, and we’re extremely encouraged by the early response!

### Like this:

Like Loading...
