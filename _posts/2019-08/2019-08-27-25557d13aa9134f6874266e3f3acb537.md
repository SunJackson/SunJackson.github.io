---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/27/distilled-news-1178/
date:      2019-08-27
author:      Michael Laux
tags:
    - likes
    - learning
    - data
    - â
    - models
---

**Improving your Recommendations System using Singular Value Decomposition**

In a previous article introducing Recommendation Systems, we saw that the tool has evolved enormously in the last year. Emerging as a tool for maintaining a website or application audience engaged and using its services. Usually, Recommendation Systems use our previous activity to make specific recommendations for us (this is known as Content-based Filtering). Now, if we’re visiting an e-commerce for the first time, it won’t know anything about us, so how can it make us a recommendation? The most basic solution would be recommending the best selling products, last releases, classics for example if we are talking about movies or books, or we could even recommend the products which would give the maximum profit to the business. However, this approach has gone out of fashion since lots of eCommerce have started to use Recommendation Systems based on Collaborative Filtering. Let’s remember how it works: suppose I like the following books: ‘The blind assassin’ and ‘A Gentleman in Moscow’. And my friend Matias likes ‘The blind assassin’ and ‘A Gentleman in Moscow’ as well, but also ‘Where the crawdads sing’. It seems we both have the same interests. So you could probably affirm I’d like ‘Where the crawdads sing’ too, even though I didn’t read it.

**LineFlow: Simple NLP Dataset Handler for PyTorch or Any Framework**

For an NLP task, you might need to tokenize text or build the vocabulary in the pre-processing. And you probably have experienced that the pre-processing code is as messy as your desk. Forgive me if your desk is clean ðŸ™‚ I have such experience too. That’s why I create LineFlow to ease your pain! It will make your ‘desk’ as clean as possible. How does the real code look like? Take a look at the figure below. The pre-processing including tokenization, building the vocabulary, and indexing.

**A High-Level Guide to Autoencoders**

In the wonderful world of machine learning and artificial intelligence, there exists this structure called an autoencoder. Autoencoders are a type neural network which is part of unsupervised learning (or, to some, semi-unsupervised learning). There are many different types of autoencoders used for many purposes, some generative, some predictive, etc. This article should provide you with a toolbox and guide to the different types of autoencoders.

**New State of the Art AI Optimizer: Rectified Adam (RAdam). Improve your AI accuracy instantly versus Adam, and why it works.**

A new paper by Liu, Jian, He et al introduces RAdam, or ‘Rectified Adam’. It’s a new variation of the classic Adam optimizer that provides an automated, dynamic adjustment to the adaptive learning rate based on their detailed study into the effects of variance and momentum during training. RAdam holds the promise of immediately improving every AI architecture compared to vanilla Adam as a result: …

**A Quick Example of Time-Series Prediction Using Long Short-Term Memory (LSTM) Networks**

After seeing a lot of posts where predictions were plotted against test sets (my posts included), I wanted to do a quick demo of actually predicting beyond the time-frame of a dataset. (Although it isn’t shown, RMSE was used to tune parameters.)

**An Advanced Example of Tensorflow Estimators Part (1/3)**

Estimators were introduced in version 1.3 of the Tensorflow API, and are used to abstract and simplify training, evaluation and prediction. If you haven’t worked with Estimators before I suggest to start by reading this article and get some familiarity as I won’t be covering all of the basics when using estimators. In no means do I think this article should be seen as best practice but I hope it will demystify and clarify some aspects of using Estimators.

**Detecting outlier samples in PCA**

In this post, I present something I am currently investigating (feedback welcome!) and that I am implementing in my new package {bigutilsr}. This package can be used to detect outlier samples in Principal Component Analysis (PCA).

**Text Mining in Python: Steps and Examples**

In today’s scenario, one way of people’s success identified by how they are communicating and sharing information to others. That’s where the concepts of language come in a picture. However, there are many languages in the world. Each has many standards and alphabets, and the combination of these words arranged meaningfully resulted in the formation of a sentence. Each language has its own rules while developing these sentences and these set of rules are also known as grammar.

**When is AI trustworthy? When is AI useful?**

How can both Boltzmann’s description of entropy and conversational weather predictions inform how we design AI. When we talk about temperature, we often don’t stop to consider that we’re throwing away a lot of information. If I asked someone how hot it was outside, and they started listing positions and velocities for various air particles, I would walk away in alarm and confusion (or try to learn how they obtained such knowledge). The reality is that we, as humans, have a fairly innate grasp on the distinction between informative and useful. Telling someone it’s ‘real hot’ outside rather than saying it’s 38.94Â°C is less informative, but less cumbersome. This act of discarding and summarizing information is the very essence of prediction, and in this article I will explain how you can in fact define, measure (approximate), and take advantage of this process in order to improve predictive models and AI (and always be correct when you predict the weather).

**How to Enhance Privacy in Data Science**

In the last two decades, the ability to collect personal information on individuals has opened up a new frontier, fueling innovation and enabling companies and organizations to deliver better, more personalized services at scale. But innovation carries risks, and this new frontier is rife with them. Organizations must proactively develop controls and processes to guard personal data in order to have sustained success in a data-driven world.This eBook examines:â€¢ methods for transforming data in a manner that protects the privacy of individuals while preserving utility;â€¢ provides an overview of the challenges and opportunities of privacy-aware analytics;â€¢ and equips data analysts and scientists with a framework for implementing anonymization techniques within data projects.

**Bias, Variance, and Regularization in Linear Regression: Lasso, Ridge, and Elastic Net â€” Differences and uses**

Regression is an incredibly popular and common machine learning technique. Often the starting point in learning machine learning, linear regression is an intuitive algorithm for easy-to-understand problems. It can generally be used whenever you’re trying to predict a continuous variable (a variable that can take any value in some numeric range), linear regressions and its relatives are often strong options, and are almost always the best place to start.

**Short Text Topic Modeling**

Intuition and code to understand and implement Topic Modeling on Short Texts like social media ones (Tweets, Reddit’s posts…) Topic Modeling aims to find the topics (or clusters) inside a corpus of texts (like mails or news articles), without knowing those topics at first. Here lies the real power of Topic Modeling, you don’t need any labeled or annotated data, only raw texts, and from this chaos Topic Modeling algorithms will find the topics your texts are about!

**Freeing the data scientist mind from the curse of vectoRization**

Nowadays, most data scientists use either Python or R as their main programming language. That was also my case until I met Julia earlier this year. Julia promises performance comparable to statically typed compiled languages (like C) while keeping the rapid development features of interpreted languages (like Python, R or Matlab). This performance is achieved by just-in-time (JIT) compilation. Instead of interpreting code, Julia compiles code in runtime. While JIT compilation has been around for sometime now (e.g. Matlab introduced it in 2002), Julia was designed for performance with JIT compilation in mind. Type stability and multiple-dispatch are key design concepts in Julia that put it apart from the competition. There is a very nice notebook by the Data Science Initiative at the University of California that explains these concepts if you want to learn more.

**Top Down View at Reinforcement Learning**

Stitch together the different parts and branches of Reinforcement Learning. When you are new to Reinforcement Learning you will no doubt be bombarded with weird terms, like Model-Based, Model-Free, On Policy, Off Policy etc… Soon you will find it exhausting to keep track of this terminology that seem to appear all over the place, without obvious link between its terms. This article will try to put all these terms into perspective so that beginners don’t feel overwhelmed. Disclaimer: this article assumes that you already know what is Reinforcement Learning and some of the existing algorithms. It does not introduce or explain any particular algorithm, but it will try to put together the different branches so that you get a comprehensive big picture, and how those branches fit in.

**Order Matters: Alibaba’s Transformer-based Recommender System**

Alibaba, the largest e-commerce platform in China, is a powerhouse not only when it comes to e-commerce, but also when it comes to recommender systems research. Their latest paper, Behaviour Sequence Transformer for E-commerce Recommendation in Alibaba, is yet another publication that pushes the state of the art in recommender systems.

**Learning to Learn: A Gentle Introduction to Meta-Learning**

Meta-learning is one of the most active areas of research in the deep learning space. Some schools of thought within the artificial intelligence(AI) community subscribe to the thesis that meta-learning is one of the stepping stones towards unlocking artificial general intelligence(AGI). In recent years, we have seen an explosion in research and development of meta-learning techniques. However, some of the basic ideas behind meta-learning are still widely misunderstood by data scientists and engineers. From that perspective, I thought it might be a good idea to review some of the fundamental concepts and history of meta-learning as well as some of the popular algorithms in the space.

### Like this:

Like Loading...
