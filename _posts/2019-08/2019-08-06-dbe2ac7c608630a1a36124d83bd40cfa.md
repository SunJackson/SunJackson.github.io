---
layout:     post
catalog: true
title:      Finding out why
subtitle:      转载自：https://analytixon.com/2019/08/06/finding-out-why-27/
date:      2019-08-06
author:      Michael Laux
tags:
    - causality
    - causally
    - articles
    - modeling
    - models
---

***Paper***: ***DROGON: A Causal Reasoning Framework for Future Trajectory Forecast***

We propose DROGON (Deep RObust Goal-Oriented trajectory prediction Network) for accurate vehicle trajectory forecast by considering behavioral intention of vehicles in traffic scenes. Our main insight is that a causal relationship between intention and behavior of drivers can be reasoned from the observation of their relational interactions toward an environment. To succeed in causal reasoning, we build a conditional prediction model to forecast goal-oriented trajectories, which is trained with the following stages: (i) relational inference where we encode relational interactions of vehicles using the perceptual context; (ii) intention estimation to compute the probability distribution of intentional goals based on the inferred relations; and (iii) causal reasoning where we reason about the behavior of vehicles as future locations conditioned on the intention. To properly evaluate the performance of our approach, we present a new large-scale dataset collected at road intersections with diverse interactions of vehicles. The experiments demonstrate the efficacy of DROGON as it consistently outperforms state-of-the-art techniques.

***Paper***: ***Bivariate temporal orders for causal inference***

Causality analysis may be carried out at different levels of detail, e.g. parameter- or temporal-based (both in a global sense). There is hence a need for a local, more distinctive approach, particularly when analyzing data segments. Therefore, the bivariate temporal orders (BTO) estimation was introduced. It uses both ‘statistical’ and ‘causal’ approaches, with two different kernels in each (linear modeling and time series distance calculation, for statistical one; and entropy- and integral-approximation-based information geometric causal inference, for causal one). The algorithm was tested on cardiorespiratory data comprising tidal volume and tachogram curves, obtained from elite athletes (supine and standing, static conditions) and a control group (different rates and depths of breathing, while supine). BTO enables to find the local curves of the most optimal shifts between signals (causal vector) and to determine causally stable segments across time. In this context, the causal vectors were determined concerning body position and breathing style changes. The rate of breathing had a greater impact on the causal vector average than does the depth of breathing. The tachogram curve preceded the tidal volume more when breathing was slower. The stability was the highest for the highest breathing rate. The method is implemented in the provided R package and can be also used for other physiological studies or even different research areas.

***Article***: ***Causality in model explanations and in the real world***

At Fiddler Labs, we place great emphasis on model explanations being faithful to the model’s behavior. Ideally, feature importance explanations should surface and appropriately quantify all and only those factors that are causally responsible for the prediction. This is especially important if we want explanations to be legally compliant (e.g., GDPR, article 13 section 2f, people have a right to ‘[information about] the existence of automated decision-making, including profiling .. and .. meaningful information about the logic involved’), and actionable. Even when making post-processing explanations human-intelligible, we must preserve faithfulness to the model. How do we differentiate between features that are correlated with the outcome, and those that cause the outcome? In other words, how do we think about the causality of a feature to a model output, or to a real-world task? Let’s take those one at a time.

***Article***: ***DIM: Learning Deep Representations by Mutual Information Estimation and Maximization***

This is our second article of the series about mutual information. In the previous articles, we have seen how to maximizes the mutual information between two variables via the MINE estimator and some practical applications of maximizing mutual information. In this article, we focus on representation learning with mutual information maximization. Specifically, we will discuss an adversarial architecture for representation learning and two other objectives of mutual information maximization that has been experimentally shown to outperform MINE estimator for downstream tasks. This article is organized into four parts. First, we briefly review MINE and introduce two additional methods for mutual information maximization that have been shown to experimentally outperform MINE. Next, we address the deficiency of traditional pixel-level representation learning methods, disclosing the reason why we maximize mutual information for representation learning. We then discuss the components of Deep InfoMax(DIM) along with the corresponding objectives. We will prove the validity of the JSD-based objective in the end for completeness.

***Article***: ***Neural Networks as universal function approximators***

When you first learn about Neural Networks you are bombarded with matrix multiplications, non-linearities, and back propagation. There are many great resources, where you can learn about this (very important) stuff. This is not one of them.

***Paper***: ***Efficient computation of counterfactual explanations of LVQ models***

With the increasing use of machine learning in practice and because of legal regulations like EU’s GDPR, it becomes indispensable to be able to explain the prediction and behavior of machine learning models. An example of easy to understand explanations of AI models are counterfactual explanations. However, for many models it is still an open research problem how to efficiently compute counterfactual explanations. We investigate how to efficiently compute counterfactual explanations of learning vector quantization models. In particular, we propose different types of convex and non-convex programs depending on the used learning vector quantization model.

### Like this:

Like Loading...
