---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/20/distilled-news-1168/
date:      2019-08-20
author:      Michael Laux
tags:
    - data
    - models
    - modeling
    - learning
    - businesses
---

**The Rise of the Data Strategist – Part 1**

Have you noticed the growing array of job positions with the word data in front of them? Back in 2012, the job title ‘data scientist’ was deemed ‘the sexiest job of the 21st century’ by Harvard Business Review. Ever since, and as correctly predicted, numerous spin-offs and new job titles with the word data have been propagated, and the ambition to become a priceless ‘data science unicorn’ has been matched with the on-going talent war and feeding frenzy by companies and headhunters desperate to discover and catch one.

**Beyond Clustering: The New Methods that are Pushing the Future of Unsupervised Learning**

If you ask any group of data science students about the types of machine learning algorithms, they will answer without hesitation: supervised and unsupervised. However, if we ask that same group to list different types of unsupervised learning, we are likely to get an answer like clustering but not much more. While supervised methods lead the current wave of innovation in areas such as deep learning, there is very little doubt that the future of artificial intelligence(AI) will transition towards more unsupervised forms of learning. In recent years, we have seen a lot of progress on several new forms of unsupervised learning methods that expand way beyond traditional clustering or principal component analysis(PCA) techniques. Today, I would like to explore some of the most prominent new schools of thought in the unsupervised space and their role in the future of AI.

**3 Tips on Defining a Data Science Project Scope with Business**

As a Data Scientist, you Expect to get a job That Lets you do Cool Stuff – Big Data, Big Machine (or Cloud, Like the Grown-Ups) and Deep Neural Networks. Reality Quickly Creeps in as you Realise the Mismatch Between Your Model, Your Project Manager’s Timeline and Your Stakeholder’s Expectation. What They Needed (often) is not a 128-Layer ResNet, but a Simple Select & Group by Query That Delivers Actionable Insights. There you are two Months Into Your new job With Your Shiny Model That Just got Shelved, Grunting: ‘What is Actionable Insights Anyway. Insights for Who? Actioned With What?’ This Article Outlines the Common Traps in (not) Defining the Data Science Project Scope, and Tips and Framework on how to Diffuse or Prevent These Situations From Occuring.Trap 1: The Scope is too Broad or UndefinedTrap 2: The Scope is too Narrow or DefinedTip 1: Find out What Business is Really AskingTip 2: Don’t Just get Thrown a Problem, Help Define it

**Outlier Detection with Isolation Forest**

During a recent project, I was working on a clustering problem with data collected from users of a mobile app. The goal was to classify the users in terms of their behavior, potentially with the use of K-means clustering. However, after inspecting the data it turned out that some users represented abnormal behavior – they were outliers. A lot of machine learning algorithms suffer in terms of their performance when outliers are not taken care of. In order to avoid this kind of problem you could, for example, drop them from your sample, cap the values at some reasonable point (based on domain knowledge) or transform the data. However, in this article, I would like to focus on identifying them and leave the possible solutions for another time. As in my case, I took a lot of features into consideration, I ideally wanted to have an algorithm that would identify the outliers in a multidimensional space. That is when I came across Isolation Forest, a method which in principle is similar to the well-known and popular Random Forest. In this article, I will focus on the Isolation Forest, without describing in detail the ideas behind decision trees and ensembles, as there is already a plethora of good sources available.

**Regularization Methods**

Regularization helps to overcoming a problem of overfitting a model.Overfitting is the concept of balancing of bias and variance. If it is overfitting , model will have less accuracy.When our model tries to learn more property from data, then noise from the training data is added.Here noise means the data point that don’t really present the true property of your data.

**Selecting the Right Scoring Pattern for Machine Learning**

According to Gartner’s 2019 CIO Survey, AI adoption by businesses grew 270% over the last four years, and over 37% of businesses have implemented AI in some facet. Businesses are adopting the technology at staggering rates, and Chief Information Officers and data scientists are facing difficult decisions regarding which speed of AI fits their business needs. AI can be broken down into three scoring patterns: batch, event-driven, and real-time. Each scoring pattern provides different capabilities, depending on the goal of the model. For example, while batch computing may work ideally in a payroll setting, it would not be an effective way to track fraud in banking transactions. By exploring these three methods and their potentials, companies are able to maximize their valuable insights.

**Fundamentals of Time Series Data and Forecasting**

Time series forecasting is the use of statistical methods to predict future behavior based on historical data. This is similar to other statistical learning approaches, such as supervised or unsupervised learning. However, time series forecasting has many nuances that make it different from regular machine learning. From data processing all the way to model validation, time series forecasting is a different beast. Many companies are exploring time series forecasting as a way of making better business decisions. Take a hotel as an example. If a manager has a good idea of how many hosts to expect next summer, they can use these insights to plan for staff management, budget, or even a facility expansion. Likewise, confident insights for future events can benefit a wide range of industries and problems, from traditional agriculture to on-demand transportation and more. In this article, we explore the fundamentals of time series data. We talk about how very simple forecasting methods work. Plus, we describe the most common patterns found in time series data.

**Containerise your apps with Docker and Kubernetes**

As you move to the cloud, there’s a lot to learn about using containers to simplify the deployment and management of your applications. Advance your skillset with this e-book from Packt, along with an accompanying GitHub repository of hands-on labs to help you gain experience.Discover how to package and run both monolithic and new, microservice-based applications in containers by following real-world examples of best practices in action. Delve into the design of the Kubernetes platform to understand the steps you’ll need to take to successfully use Kubernetes with Docker to manage containers. Learn how to:Use containers to manage, scale and orchestrate applications.Improve the efficiency and security of containers.Build highly available and scalable Kubernetes clusters.Design and deploy large clusters on various cloud platforms.Update or roll back a distributed application with zero downtime.Monitor and secure your deployments.

**Z Test / T Test in One Picture**

The following picture shows the differences between the Z Test and T Test. Not sure which one to use?

**Logistic Regression For Dummies: A Detailed Explanation**

Logistic regression is a statistical method for analysing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables.

**Three Key Error Intervals Everyone Should Know**

Say the words ‘inferential statistics’ and often people’s eyes will open wide in horror, but they need not be so scary and once you know what they’re doing they can be a useful tool to have. Like it or not but data science is built around interpreting data and telling the story it gives us. To that end, for people to have confidence (p-values anyone?) in what we say we often need to quantify the values we are giving. This requires us to have a knowledge and understanding of statistics and therefore of error estimation. For example being able to tell if a large sudden change on a graph is real or not can be the difference between a company spending resources on correcting it. For that reason, it is vitally important you can understand, interpret and explain what the three main types of intervals are that you may see or use when results are presented. So, I’ll be talking about confidence intervals and its two other siblings (Predictive and Tolerance), along with confidence levels.

**Insurance Data Science: Text**

At the Summer School of the Swiss Association of Actuaries, in Lausanne, I will start talking about text based data and NLP this Thursday. Slides are available online.

**Domain-Specific Language Processing Mines Value From Unstructured Data**

Processing unstructured text data in real-time is challenging when applying NLP or NLU. Find out how Domain-Specific Language Processing can also help mine valuable information from data by following your guidance and using the language of your business.

**Missing data can be the best data**

As a Data Scientist you will often be given a set of data and given a question. Nice logical thinking can really help tease out the solutions, so let us start with a made up farcical example to get your brain cells going.

**BERT Text Classification in 3 Lines of Code Using Keras**

BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model developed by Google. It represented one of the major machine learning breakthroughs of the year, as it achieved state-of-the-art results across 11 different Natural Language Processing (NLP) tasks. Moreover, Google open-sourced the code and made pretrained models available for download similar to computer vision models pretrained on ImageNet. For these reasons, there continues to be a great deal of interest in BERT (even as other models slightly overtake it). While BERT broke records on many different tasks from Question-Answering (SQuAD v1.1) to Natural Language Inference, text classification remains one of the most practically useful and widely applicable NLP tasks. In this article, we will show how you can apply BERT to the problem of text classification in as little as 3 lines of code. To accomplish this, we will be using ktrain, a fastai-like interface to Keras.

**(Bootstrapping) Follow-Up Contrasts for Within-Subject ANOVAs (part 2)**

A while back I wrote a post demonstrating how to bootstrap follow-up contrasts for repeated-measure ANOVAs for cases where you data violates some / any assumptions. Here is a demo of how to conduct the same bootstrap analysis, more simply (no need to make your data wide!)

**Using linear models with binary dependent variables, a simulation study**

This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 8, in which I discuss advanced functional programming methods for modeling. As written just above (note: as written above in the book), map() simply applies a function to a list of inputs, and in the previous section we mapped ggplot() to generate many plots at once. This approach can also be used to map any modeling functions, for instance lm() to a list of datasets. For instance, suppose that you wish to perform a Monte Carlo simulation. Suppose that you are dealing with a binary choice problem; usually, you would use a logistic regression for this.

### Like this:

Like Loading...
