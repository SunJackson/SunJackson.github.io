---
layout:     post
catalog: true
title:      Whats new on arXiv
subtitle:      转载自：https://analytixon.com/2019/08/11/whats-new-on-arxiv-1072/
date:      2019-08-11
author:      Michael Laux
tags:
    - models
    - modelling
    - learned
    - learning methods
    - images
---

**Experiential AI**

Experiential AI is proposed as a new research agenda in which artists and scientists come together to dispel the mystery of algorithms and make their mechanisms vividly apparent. It addresses the challenge of finding novel ways of opening up the field of artificial intelligence to greater transparency and collaboration between human and machine. The hypothesis is that art can mediate between computer code and human comprehension to overcome the limitations of explanations in and for AI systems. Artists can make the boundaries of systems visible and offer novel ways to make the reasoning of AI transparent and decipherable. Beyond this, artistic practice can explore new configurations of humans and algorithms, mapping the terrain of inter-agencies between people and machines. This helps to viscerally understand the complex causal chains in environments with AI components, including questions about what data to collect or who to collect it about, how the algorithms are chosen, commissioned and configured or how humans are conditioned by their participation in algorithmic processes.

**Quantile-Frequency Analysis and Spectral Divergence Metrics for Diagnostic Checks of Time Series With Nonlinear Dynamics**
![](//s0.wp.com/latex.php?latex=p&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=000&s=0)


**Scalable Inference of System-level Models from Component Logs**

Behavioral software models play a key role in many software engineering tasks; unfortunately, these models either are not available during software development or, if available, they quickly become outdated as the implementations evolve. Model inference techniques have been proposed as a viable solution to extract finite-state models from execution logs. However, existing techniques do not scale well when processing very large logs, such as system-level logs obtained by combining component-level logs. Furthermore, in the case of component-based systems, existing techniques assume to know the definitions of communication channels between components. However, this information is usually not available in the case of systems integrating 3rd-party components with limited documentation. In this paper, we address the scalability problem of inferring the model of a component-based system from the individual component-level logs, when the only available information about the system are high-level architecture dependencies among components and a (possibly incomplete) list of log message templates denoting communication events between components. Our model inference technique, called SCALER, follows a divide and conquer approach. The idea is to first infer a model of each system component from the corresponding logs; then, the individual component models are merged together taking into account the dependencies among components, as reflected in the logs. We evaluated SCALER in terms of scalability and accuracy, using a dataset of logs from an industrial system; the results show that SCALER can process much larger logs than a state-of-the-art tool, while yielding more accurate models.

**DNNSurv: Deep Neural Networks for Survival Analysis Using Pseudo Values**

**Debiasing Linear Prediction**
![](//s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7B%5Cstar%7D&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7B%5Cstar%7D&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7B%5Cstar%7D&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7B%5Cstar%7D&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7B%5Cstar%7D&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7B%5Cstar%7D&bg=ffffff&fg=000&s=0)


**Stochastic Fusion of Interacting Particle Systems and Duality Functions**
![](//s0.wp.com/latex.php?latex=%28m%2F2%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%28m%2F2%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_%2B&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_%2B&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%28q%2Cj%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%28q%2Cj%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%28q%2Cj%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%28q%2Cj%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=q&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=q&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%28q%2Cj%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%28q%2Cj%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=q&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=q&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%28q%2Cj%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%28q%2Cj%29&bg=ffffff&fg=000&s=0)


**Limit Theorems for Data with Network Structure**

This paper develops new limit theory for data that are generated by networks or more generally display cross-sectional dependence structures that are governed by observable and unobservable characteristics. Strategic network formation models are an example. Wether two data points are highly correlated or not depends on draws from underlying characteristics distributions. The paper defines a measure of closeness that depends on primitive conditions on the distribution of observable characteristics as well as functional form of the underlying model. A summability condition over the probability distribution of observable characteristics is shown to be a critical ingredient in establishing limit results. The paper establishes weak and strong laws of large numbers as well as a stable central limit theorem for a class of statistics that include as special cases network statistics such as average node degrees or average peer characteristics. Some worked examples illustrating the theory are provided.

**Quantile Inverse Optimization: Improving Stability in Inverse Linear Programming**

Inverse linear programming (LP) has received increasing attention due to its potential to generate efficient optimization formulations that can closely replicate the behavior of a complex system. However, inversely inferred parameters and corresponding forward solutions from the existing inverse LP method can be highly sensitive to noise, errors, and uncertainty in the input data, limiting its applicability in data-driven settings. We introduce the notion of inverse and forward stability in inverse LP and propose a novel inverse LP method that determines a set of objective functions that are stable under data imperfection and generate solutions close to the relevant subset of the data. We formulate the inverse model as a mixed-integer program and elucidate its connection to bi-clique problems, which we exploit to develop efficient heuristics. We also show how this method can be used for online learning. We numerically evaluate the stability of the proposed method and demonstrate its practical use in the diet recommendation and transshipment applications.

**Cheetah: Mixed Low-Precision Hardware & Software Co-Design Framework for DNNs on the Edge**

Low-precision DNNs have been extensively explored in order to reduce the size of DNN models for edge devices. Recently, the posit numerical format has shown promise for DNN data representation and compute with ultra-low precision in [5..8]-bits. However, previous studies were limited to studying posit for DNN inference only. In this paper, we propose the Cheetah framework, which supports both DNN training and inference using posits, as well as other commonly used formats. Additionally, the framework is amenable for different quantization approaches and supports mixed-precision floating point and fixed-point numerical formats. Cheetah is evaluated on three datasets: MNIST, Fashion MNIST, and CIFAR-10. Results indicate that 16-bit posits outperform 16-bit floating point in DNN training. Furthermore, performing inference with [5..8]-bit posits improves the trade-off between performance and energy-delay-product over both [5..8]-bit float and fixed-point.

**Refining the Structure of Neural Networks Using Matrix Conditioning**

Deep learning models have proven to be exceptionally useful in performing many machine learning tasks. However, for each new dataset, choosing an effective size and structure of the model can be a time-consuming process of trial and error. While a small network with few neurons might not be able to capture the intricacies of a given task, having too many neurons can lead to overfitting and poor generalization. Here, we propose a practical method that employs matrix conditioning to automatically design the structure of layers of a feed-forward network, by first adjusting the proportion of neurons among the layers of a network and then scaling the size of network up or down. Results on sample image and non-image datasets demonstrate that our method results in small networks with high accuracies. Finally, guided by matrix conditioning, we provide a method to effectively squeeze models that are already trained. Our techniques reduce the human cost of designing deep learning models and can also reduce training time and the expense of using neural networks for applications.

**Text mining policy: Classifying forest and landscape restoration policy agenda with neural information retrieval**

Dozens of countries have committed to restoring the ecological functionality of 350 million hectares of land by 2030. In order to achieve such wide-scale implementation of restoration, the values and priorities of multi-sectoral stakeholders must be aligned and integrated with national level commitments and other development agenda. Although misalignment across scales of policy and between stakeholders are well known barriers to implementing restoration, fast-paced policy making in multi-stakeholder environments complicates the monitoring and analysis of governance and policy. In this work, we assess the potential of machine learning to identify restoration policy agenda across diverse policy documents. An unsupervised neural information retrieval architecture is introduced that leverages transfer learning and word embeddings to create high-dimensional representations of paragraphs. Policy agenda labels are recast as information retrieval queries in order to classify policies with a cosine similarity threshold between paragraphs and query embeddings. This approach achieves a 0.83 F1-score measured across 14 policy agenda in 31 policy documents in Malawi, Kenya, and Rwanda, indicating that automated text mining can provide reliable, generalizable, and efficient analyses of restoration policy.

**Model Learning: Primal Dual Networks for Fast MR imaging**

Magnetic resonance imaging (MRI) is known to be a slow imaging modality and undersampling in k-space has been used to increase the imaging speed. However, image reconstruction from undersampled k-space data is an ill-posed inverse problem. Iterative algorithms based on compressed sensing have been used to address the issue. In this work, we unroll the iterations of the primal-dual hybrid gradient algorithm to a learnable deep network architecture, and gradually relax the constraints to reconstruct MR images from highly undersampled k-space data. The proposed method combines the theoretical convergence guarantee of optimi-zation methods with the powerful learning capability of deep networks. As the constraints are gradually relaxed, the reconstruction model is finally learned from the training data by updating in k-space and image domain alternatively. Experi-ments on in vivo MR data demonstrate that the proposed method achieves supe-rior MR reconstructions from highly undersampled k-space data over other state-of-the-art image reconstruction methods.

**Strengthening the Case for a Bayesian Approach to Car-following Model Calibration and Validation using Probabilistic Programming**

Compute and memory constraints have historically prevented traffic simulation software users from fully utilizing the predictive models underlying them. When calibrating car-following models, particularly, accommodations have included 1) using sensitivity analysis to limit the number of parameters to be calibrated, and 2) identifying only one set of parameter values using data collected from multiple car-following instances across multiple drivers. Shortcuts are further motivated by insufficient data set sizes, for which a driver may have too few instances to fully account for the variation in their driving behavior. In this paper, we demonstrate that recent technological advances can enable transportation researchers and engineers to overcome these constraints and produce calibration results that 1) outperform industry standard approaches, and 2) allow for a unique set of parameters to be estimated for each driver in a data set, even given a small amount of data. We propose a novel calibration procedure for car-following models based on Bayesian machine learning and probabilistic programming, and apply it to real-world data from a naturalistic driving study. We also discuss how this combination of mathematical and software tools can offer additional benefits such as more informative model validation and the incorporation of true-to-data uncertainty into simulation traces.

**Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning**

We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.

**TinySearch — Semantics based Search Engine using Bert Embeddings**

Existing search engines use keyword matching or tf-idf based matching to map the query to the web-documents and rank them. They also consider other factors such as page rank, hubs-and-authority scores, knowledge graphs to make the results more meaningful. However, the existing search engines fail to capture the meaning of query when it becomes large and complex. BERT, introduced by Google in 2018, provides embeddings for words as well as sentences. In this paper, I have developed a semantics-oriented search engine using neural networks and BERT embeddings that can search for query and rank the documents in the order of the most meaningful to least meaningful. The results shows improvement over one existing search engine for complex queries for given set of documents.

**Task-Oriented Optimal Sequencing of Visualization Charts**

A chart sequence is used to describe a series of visualization charts generated in the exploratory analysis by data analysts. It provides information details in each chart as well as a logical relationship among charts. While existing research targets on generating chart sequences that match human’s perceptions, little attention has been paid to formulate task-oriented connections between charts in a chart design space. We present a novel chart sequencing method based on reinforcement learning to capture the connections between charts in the context of three major analysis tasks, including correlation analysis, anomaly detection, and cluster analysis. The proposed method formulates a chart sequencing procedure as an optimization problem, which seeks an optimal policy to sequencing charts for the specific analysis task. In our method, a novel reward function is introduced, which takes both the analysis task and the factor of human cognition into consideration. We conducted one case study and two user studies to evaluate the effectiveness of our method under the application scenarios of visualization demonstration, sequencing charts for reasoning analysis results, and making a chart design choice. The study results showed the power of our method.

**Near-Memory Computing: Past, Present, and Future**

The conventional approach of moving data to the CPU for computation has become a significant performance bottleneck for emerging scale-out data-intensive applications due to their limited data reuse. At the same time, the advancement in 3D integration technologies has made the decade-old concept of coupling compute units close to the memory — called near-memory computing (NMC) — more viable. Processing right at the ‘home’ of data can significantly diminish the data movement problem of data-intensive applications. In this paper, we survey the prior art on NMC across various dimensions (architecture, applications, tools, etc.) and identify the key challenges and open issues with future research directions. We also provide a glimpse of our approach to near-memory computing that includes i) NMC specific microarchitecture independent application characterization ii) a compiler framework to offload the NMC kernels on our target NMC platform and iii) an analytical model to evaluate the potential of NMC.

**Paired-Consistency: An Example-Based Model-Agnostic Approach to Fairness Regularization in Machine Learning**

As AI systems develop in complexity it is becoming increasingly hard to ensure non-discrimination on the basis of protected attributes such as gender, age, and race. Many recent methods have been developed for dealing with this issue as long as the protected attribute is explicitly available for the algorithm. We address the setting where this is not the case (with either no explicit protected attribute, or a large set of them). Instead, we assume the existence of a fair domain expert capable of generating an extension to the labeled dataset – a small set of example pairs, each having a different value on a subset of protected variables, but judged to warrant a similar model response. We define a performance metric – paired consistency. Paired consistency measures how close the output (assigned by a classifier or a regressor) is on these carefully selected pairs of examples for which fairness dictates identical decisions. In some cases consistency can be embedded within the loss function during optimization and serve as a fairness regularizer, and in others it is a tool for fair model selection. We demonstrate our method using the well studied Income Census dataset.

**Fully dynamic hierarchical diameter k-clustering and k-center**
![](//s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2C%5CDelta%5C%7D%5Ed&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2C%5CDelta%5C%7D%5Ed&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%5Clog%5E%7BO%281%29%7D+%28%5CDelta+n%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Clog%5E%7BO%281%29%7D+%28%5CDelta+n%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=n&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=%5Cell+%3E+1&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=%5Cell+%3E+1&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=O%28d+%5Cell%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=O%28d+%5Cell%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=O%28d%5E2+%5Cell+%5Clog+n+%5Clog+%5CDelta%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=O%28d%5E2+%5Cell+%5Clog+n+%5Clog+%5CDelta%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=O%28d%5E2+n%5E%7B1%2F%5Cell%7D+%5Clog%5E2+n+%5Clog+%5CDelta%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=O%28d%5E2+n%5E%7B1%2F%5Cell%7D+%5Clog%5E2+n+%5Clog+%5CDelta%29&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=1-1%2Fn&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=1-1%2Fn&bg=ffffff&fg=000&s=0)

![](//s0.wp.com/latex.php?latex=O%28d+%5Cell+%5Clog+n+%5Clog+%5CDelta%29&is-pending-load=1#038;bg=ffffff&fg=000&s=0)

![](https://s0.wp.com/latex.php?latex=O%28d+%5Cell+%5Clog+n+%5Clog+%5CDelta%29&bg=ffffff&fg=000&s=0)


**Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks**

To verify and validate networks, it is essential to gain insight into their decisions, limitations as well as possible shortcomings of training data. In this work, we propose a post-hoc, optimization based visual explanation method, which highlights the evidence in the input image for a specific prediction. Our approach is based on a novel technique to defend against adversarial evidence (i.e. faulty evidence due to artefacts) by filtering gradients during optimization. The defense does not depend on human-tuned parameters. It enables explanations which are both fine-grained and preserve the characteristics of images, such as edges and colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid model inputs. We qualitatively and quantitatively evaluate our approach on a multitude of models and datasets.

**Robust Learning with Jacobian Regularization**

Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.

**Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings**

Learning an effective similarity measure between image representations is key to the success of recent advances in visual search tasks (e.g. verification or zero-shot learning). Although the metric learning part is well addressed, this metric is usually computed over the average of the extracted deep features. This representation is then trained to be discriminative. However, these deep features tend to be scattered across the feature space. Consequently, the representations are not robust to outliers, object occlusions, background variations, etc. In this paper, we tackle this scattering problem with a distribution-aware regularization named HORDE. This regularizer enforces visually-close images to have deep features with the same distribution which are well localized in the feature space. We provide a theoretical analysis supporting this regularization effect. We also show the effectiveness of our approach by obtaining state-of-the-art results on 4 well-known datasets (Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes Retrieval).

### Like this:

Like Loading...
