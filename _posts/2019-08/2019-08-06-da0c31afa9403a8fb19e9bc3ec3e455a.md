---
layout:     post
catalog: true
title:      How to create unigrams, bigrams and n-grams of App Reviews
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/2Xn-TD0jWco/
date:      2019-08-06
author:      AbdulMajedRaja RS
tags:
    - tokenized words
    - reviews
    - data
    - app
    - tidytext
---





This is one of the frequent questions I’ve heard from the first timer NLP / Text Analytics – programmers (or as the world likes it to be called “Data Scientists”).

### Prerequisite

For simplicity, this post assumes that you already know how to install a package and so you’ve got `tidytext` installed on your R machine.

```
install.packages("tidytext")
```

### Loading the Library

Let’s start with loading the `tidytext` library.

```
library(tidytext)
```

### Tokens

Tokenization in NLP is the process of splitting a text corpus based on some splitting factor – It could be Word Tokens or Sentence Tokens or based on some advanced alogrithm to split a conversation. In this process, we’ll just simply do word tokenization.

```
reviews %>% 
 unnest_tokens(output = word, input = txt) %>% 
 head()
```

```
## word
## 1 great
## 1.1 source
## 1.2 for
## 1.3 top
## 1.4 content
## 1.5 and
```

As you can see above, `unnest_tokens()` is the function that’ll help us in this tokenization process. Since it supports `%>%` pipe operator, the first argument of the function is a `dataframe` or `tibble`, the second argument `output` is the name of the output (new) column where the tokenized words are going to be put in. The third column `input` is where the input text is fed in.

Now, this is what `unigram`s are for this Medium iOS App Reviews. As with many other data science projects, Data like this is not useful unless it’s visualized in a way to look at insights.

```
reviews %>% 
 unnest_tokens(output = word, input = txt) %>% 
 count(word, sort = TRUE) 
```

```
## # A tibble: 444 x 2
## word n
## 
## 1 the 45
## 2 i 35
## 3 and 34
## 4 of 27
## 5 to 27
## 6 a 18
## 7 it 14
## 8 medium 14
## 9 this 13
## 10 articles 12
## # … with 434 more rows
```

Roughly, looking at the most frequently appeared unigram we end up with `the`,`i`,`and` and this is one of those places where we need to *remove stopwords*

### Stopword Removal

Fortunately, `tidytext` helps us in removing stopwords by having a dataframe of stopwords from multiple lexicons. With that, we can use `anti_join` for picking the words (that are present in the left df (`reviews`) but not present in the right df (`stop_words`)).

```
reviews %>% 
 unnest_tokens(output = word, input = txt) %>% 
 anti_join(stop_words) %>% 
 count(word, sort = TRUE) 
```

```
## Joining, by = "word"
```

```
## # A tibble: 280 x 2
## word n
## 
## 1 medium 14
## 2 articles 12
## 3 app 9
## 4 reading 9
## 5 content 6
## 6 love 5
## 7 read 5
## 8 article 4
## 9 enjoy 4
## 10 i’ve 4
## # … with 270 more rows
```

With that stop word removal, now we can see better represenation of most frequently appearing unigrams in the reviews.

### unigram Visualziation

We’ve got our data in the shape that we want so, let’s go ahead and visualize it. To keep the pipeline intact, I’m not creating any temporary object to store the previous output and just simply continue using the same. Also too many bars (words) wouldn’t make any sense (except resulting in a shabby plot), We’ll filter taking the top 10 words

```
reviews %>% 
 unnest_tokens(output = word, input = txt) %>% 
 anti_join(stop_words) %>% 
 count(word, sort = TRUE) %>% 
 slice(1:10) %>% 
 ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
 theme_minimal() +
 labs(title = "Top unigrams of Medium iOS App Reviews",
 subtitle = "using Tidytext in R",
 caption = "Data Source: itunesr - iTunes App Store")
```

```
## Joining, by = "word"
```

![](https://i2.wp.com/www.programmingwithr.com/post/2019-08-06-how-to-create-unigrams-bigrams-and-n-grams-of-app-reviews_files/figure-html/unnamed-chunk-10-1.png?w=450&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/www.programmingwithr.com/post/2019-08-06-how-to-create-unigrams-bigrams-and-n-grams-of-app-reviews_files/figure-html/unnamed-chunk-10-1.png?w=450&ssl=1)


### Bigrams & N-grams

Now that we’ve got the core code for unigram visualization set up. We can slightly modify the same – just by adding a new argument `n=2` and `token="ngrams"` to the tokenization process to extract n-gram. `2` for bigram and `3` trigram – or `n` of your interest. But remember, large n-values may not useful as the smaller values.

Doing this naively also has a catch and the catch is – the stop-word removal process we used above was using `anti_join` which wouldn’t be supported in this process since we’ve a bigram (two-word combination separated by a space). So, we’ll `separate` the word by `space` and then filter out the stop words in both `word1` and `word2` and then `unite` them back – which gives us the `bigram` after stop-word removal. This is the process that you might have to carry out when you are dealing with n-grams.

```
reviews %>% 
 unnest_tokens(word, txt, token = "ngrams", n = 2) %>% 
 separate(word, c("word1", "word2"), sep = " ") %>% 
 filter(!word1 %in% stop_words$word) %>%
 filter(!word2 %in% stop_words$word) %>% 
 unite(word,word1, word2, sep = " ") %>% 
 count(word, sort = TRUE) %>% 
 slice(1:10) %>% 
 ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
 theme_minimal() +
 coord_flip() +
 labs(title = "Top Bigrams of Medium iOS App Reviews",
 subtitle = "using Tidytext in R",
 caption = "Data Source: itunesr - iTunes App Store")
```

![](https://i0.wp.com/www.programmingwithr.com/post/2019-08-06-how-to-create-unigrams-bigrams-and-n-grams-of-app-reviews_files/figure-html/unnamed-chunk-11-1.png?w=450&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/www.programmingwithr.com/post/2019-08-06-how-to-create-unigrams-bigrams-and-n-grams-of-app-reviews_files/figure-html/unnamed-chunk-11-1.png?w=450&ssl=1)


### Summary

This particular assignment that may not reveal some meaningful insights as we started with less data, but this is really useful when you have a decent amount of text corpus and this simple analysis of unigram, bigram (n-gram analysis) can reveal something business-worthy (let’s say in Customer Service, App Development or in multiple other use-cases).


*Related*






---
