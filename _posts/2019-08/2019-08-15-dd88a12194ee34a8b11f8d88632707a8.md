---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/15/distilled-news-1163/
date:      2019-08-15
author:      Michael Laux
tags:
    - detection
    - learning
    - texts
    - trading
    - trades
---

**Understand Data Normalization in Machine Learning**

If you’re new to data science/machine learning, you probably wondered a lot about the nature and effect of the buzzword ‘feature normalization’. If you’ve read any Kaggle kernels, it is very likely that you found feature normalization in the data preprocessing section. So, what is data normalization and why the heck is it so valued by data practitioners?

**Feature Selection Why & How Explained**

In the last article, I explained the problems with including irrelevant or correlated features in model building. In this article, I’ll show you several neat implementations of selection algorithms that can be easily integrated into your project pipeline. Before diving into the detailed implementation, let’s go through the dataset I created. The dataset has 20 features, among which 5 contribute to the output and 2 are correlated.1. Wrapper Feature Selection2. Filtering Feature Selection3. Embedded Feature Selection

**The tools you should know for the Machine Learning projects**

I would like to start my first Machine Learning project. But I do not have tools. What should I do? What are the tools I could use? I will give you some hints and advices based on the toolbox I use. Of course there are more great tools but you should pick the ones you like. You should also use the tools that make your work productive which means you need to pay for them (which is not always the case – I do use free tools as well). The first and most important thing is that there are lots of options! Just pick what works for you! I have divided this post into several parts like the environments, the langauges and the libraries.

**R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms**

Computer vision is an interdisciplinary field that has been gaining huge amounts of traction in the recent years(since CNN) and self-driving cars have taken centre stage. Another integral part of computer vision is object detection. Object detection aids in pose estimation, vehicle detection, surveillance etc. The difference between object detection algorithms and classification algorithms is that in detection algorithms, we try to draw a bounding box around the object of interest to locate it within the image. Also, you might not necessarily draw just one bounding box in an object detection case, there could be many bounding boxes representing different objects of interest within the image and you would not know how many beforehand.

**Visual intuition on ring-Allreduce for distributed Deep Learning**

Recently I found myself working on a very large data set, one of those that you need to parallelize learning to make it feasible. I immediately thought of Uber’s Horovod. I had previously heard about it from a tech talk at Uber but had not really played around with it. I found it very interesting and a great framework, from the high-level simplifications to the algorithm that powers this framework. In this post, I’ll try to describe my understanding of the latter.

**How to Build an Automated Trading System using R**

For all R zealots, we know that we can build any data product very efficiently using R. An automated trading system is not an exception. Whether you are doing high-frequency trading, day trading, swing trading, or even value investing, you can use R to build a trading robot that watches the market closely and trades the stocks or other financial instruments on your behalf. The benefits of a trading robot are obvious:• A trading robot follows our pre-defined trading rules strictly. Other than human beings, the robot has no emotion involved when it makes trading decisions.• A trading robot does not need rest (yet). The trading robot can watch the market price movement at every second across multiple financial instruments and execute the order immediately when the timing is correct.

**Custom model deployment on Google A.I. Platform Serving**

Recently, Google Cloud AI Platform Serving (CAIP) added a new feature which Machine Learning (ML) practitioners can now use to deploy models with customized pre-processing pipelines and prediction routines using their favorite frameworks, all under one serverless microservice. In this blog post, I explain in detail how we at Wootric make use of this feature and mention a few nitty-gritties to be careful about, as the product is still in beta.

**Anomaly Detection with PyOD!**

Are you an anomaly detection professional, or planning to advance modeling in anomaly detection? Then you should not miss this wonderful Python Outlier Detection (PyOD) Toolkit. It is a comprehensive module that has been featured by academic researches (see this summary) and the machine learning websites such as Towards Data Science, Analytics Vidhya, KDnuggets, etc.

**How to create data-driven presentations with jupyter notebooks, reveal.js, host on github, and show it to the world: Part I — make a basic slide deck**

… in which I discuss a workflow where you can start writing your contents on a jupyter notebook, create a reveal.js slide deck, and host it on github for presentations. This is for a very simple presentation that you can fully control yourself.

**One Class Learning in Manufacturing: Autoencoder and Golden Units Baselining**

Recently I’ve been working with manufacturing customers (both OEM and CM) who want to jump on the bandwagon of machine learning. One common use case is to better detect products (or Device Under Test/DUT) that are defective in their production line. Using machine learning’s terminology, this falls under the problem of binary classification as a DUT can only pass or fail.

**How to Get Started as a Developer in AI**

Artificial Intelligence. Well, it looks like this cutting-edge technology is now the most popular and at the same time the most decisive one for humanity. We are ceaselessly amazed at the AI capabilities and the effective way they can be used in almost any industry. Robots now is just like the airplane 100 years ago. So what’s next? This question raises many emotions starting from great interest, encouragement, the desire to be part of this process, and ending with the fear, complete confusion and ignorance. But what’s stopping you from sitting in one of the front seats of AI development and just don’t be a passive observer? You may assume getting started as a developer in AI is a long and hard path. Well, yes, but it doesn’t mean you can’t handle it. Let me say one word for those who doubt. Even if you don’t have any prior experience in programming, math, engineering, you can learn AI from scratch sitting at home and start applying your knowledge in practice, creating simple machine learning solutions and making first steps towards your new profession.Part I. First Off, Gain Basic Skills Required to Start Learning AIPart II. Start Learning AI – the Most Important PartPart III. Practice your skills

**How Neural Networks Are Learning to Write**

An overview of the evolution of NLP models for writing text.• Markov Chains and N-grams• Word Embeddings and Neural Language Models• Recurrent Neural Networks• Transformers

**Understanding Adam : how loss functions are minimized ?**

While using the fast.ai library built on top of PyTorch, I realized that I have never had to interact with an optimizer so far. Since fast.ai already deals with it when calling the fit_one_cycle method, I don’t have to parametrize the optimizer nor do I understand how it works. Adam is probably the most used optimizer in machine learning due to its simplicity and speed. It was developed in 2015 by Diederik Kingma and Jimmy Lei Ba and introduced in a paper called Adam : a method for stochastic optimization. As always, this blog post is a cheat sheet that I write to check my understanding of a notion. If you find something unclear or incorrect, don’t hesitate to write it in the comment section.

**Text Generation with LSTM: Economic Analysis**

Could you imagine a future where computers made economic decisions rather than governments and central bankers? With all of the economic mishaps we’ve been seeing over the past decade, one could say it isn’t a particularly bad idea! Natural language processing could allow us to make more sense of the economy than we do currently. As it stands, investors and policymakers use index benchmarks and quantitative measures such as GDP growth to gauge economic health. That said, one potential application of NLP is to analyse text data (such as through major economic policy documents), and then ‘learn’ from such texts in order to generate appropriate economic policies independently of human intervention. In this example, an LSTM model is trained using text from a sample ECB policy document, in order to generate ‘new’ text data, with a view to revealing insights from such text that could be used for policy purposes. Specifically, a temperature hyperparameter is configured to control the randomness of text predictions generated, with the relevant text vectorized into sequences of characters, and the single-layer LSTM model then used for next character sampling – with a text generation loop then used to generate a block of text for each temperature (the higher the temperature, the more randomness induced in each block of text).

**Neural Style Transfer**

Style transfer is an excited sub-field of computer vision. It aims to transfer the style of one image onto another image, known as the content image. This technique allows us to synthesize new images combining the content and style of different images. Several developments have been made in this sub-field but the most notable initial work (neural style transfer) was done by Gatys et al. in 2015. Some of the results I got by applying this technique can be seen below.

### Like this:

Like Loading...
