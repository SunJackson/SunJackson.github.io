---
layout:     post
catalog: true
title:      Propensity Score Matching in R
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/-L5RiTxG4tM/
date:      2019-08-02
author:      Kristian Larsen
tags:
    - catholic
    - matching
    - matches
    - matched
    - covariates
---






Category

Tags

Regression analysis is one of the most requested machine learning methods in 2019. One group of regression analysis for measuring effects and to evaluate the statistical effect of covariates is Propensity Score Matching (PSM). This method is well suited to investigate if the covariates are changing the effects of the estimates in the regression model. It can, therefore, be used to design the regression model to be more accurate and efficient. In this article, you learn how to do Propensity Score Matching in R.

## Tags

## Methodology

The propensity score (PS) is a probability. It is a conditional probability of being exposed given a set of covariates, Pr(E+|covariates). We can calculate a PS for each subject in an observational study regardless of her actual exposure. Once we have a PS for each subject, we then return to the real world of exposed and unexposed. We can match exposed subjects with unexposed subjects with the same (or very similar) PS. Thus, the probability of being exposed is the same as the probability of being unexposed. The exposure is “random.”

Propensity score matching (PSM) arose as a way to achieve exchangeability between exposed and unexposed groups in observational studies without relying on traditional model building. Exchangeability is critical to our causal inference. In experimental studies (e.g. randomized control trials), the probability of being exposed is 0.5. Thus, the probability of being unexposed is also 0.5. The probability of being exposed or unexposed is the same. Therefore, the subject’s actual exposure status is random. This equal probability of exposure makes us feel more comfortable asserting that the exposed and unexposed groups are alike on all factors except their exposure. Therefore, we say that we have exchangeability between groups. One of the biggest challenges with observational studies is that the probability of being in the exposed or unexposed group is not random.

There are several occasions where an experimental study is not feasible or ethical. But we still would like the exchangeability of groups achieved by randomization. PSM helps us to mimic an experimental study using data from an observational study.

## Conducting PSM

When conducting a PSM the following 5 steps are necessary:

### 1. Decide on the set of covariates you want to include

Decide on the set of covariates you want to include.This is the critical step to your PSA. We use these covariates to predict our probability of exposure. We want to include all predictors of the exposure and none of the effects of the exposure. We do not consider the outcome in deciding upon our covariates. We may include confounders and interaction variables. If we are in doubt of the covariate, we include it in our set of covariates (unless we think that it is an effect of the exposure).

### 2. Use logistic regression to obtain a PS for each subject

We use the covariates to predict the probability of being exposed (which is the PS). The more true covariates we use, the better our prediction of the probability of being exposed. We calculate a PS for all subjects, exposed and unexposed.

Using mathematical expressions:\( ln(PS/(1-PS))= β_0+β_1*X_1+…+β_p*X_p \)\( PS= (exp(β_0+β_1*X_1+…+β_p*X_p)) / \)\( (1+exp(β0 +β_1*X_1 +…+β_p*X_p)) \)

### 3. Match exposed and unexposed subjects on the PS

We want to match the exposed and unexposed subjects on their probability of being exposed (their PS). If we cannot find a suitable match, then that subject is discarded. Discarding a subject can introduce bias into our analysis.

Several methods for matching exist. Most common is the nearest neighbor within calipers. The nearest neighbor would be the unexposed subject that has a PS nearest to the PS for our exposed subject. We may not be able to find an exact match, so we say that we will accept a PS score within certain caliper bounds. We set an apriori value for the calipers. This value typically ranges from +/-0.01 to +/-0.05. Below 0.01, we can get a lot of variability within the estimate because we have difficulty finding matches and this leads us to discard those subjects (incomplete matching). If we go past 0.05, we may be less confident that our exposed and unexposed are truly exchangeable (inexact matching). Typically, 0.01 is chosen for a cutoff.

The ratio of exposed to unexposed subjects is variable. 1:1 matching may be done, but oftentimes matching with replacement is done instead to allow for better matches. Matching with replacement allows for the unexposed subject that has been matched with an exposed subject to be returned to the pool of unexposed subjects available for matching. There is a trade-off in bias and precision between matching with replacement and without (1:1). Matching with replacement allows for reduced bias because of better matching between subjects. Matching without replacement has better precision because more subjects are used.

### 4. Check the balance of covariates in the exposed and unexposed groups after matching on PS

The substantial overlap in covariates between the exposed and unexposed groups must exist for us to make causal inferences from our data. This is true in all models, but in PSA, it becomes visually very apparent. If there is no overlap in covariates (i.e. if we have no overlap of propensity scores), then all inferences would be made off-support of the data (and thus, conclusions would be model dependent).

We can use a couple of tools to assess our balance of covariates. First, we can create a histogram of the PS for exposed and unexposed groups. Second, we can assess the standardized difference. Third, we can assess the bias reduction. 

\( Standardized difference=(100*(mean(x exposed)- \)\( (mean(x unexposed)))/(sqrt((SD^2exposed+ $$SD^2unexposed)/2)) \)

More than 10% difference is considered bad. Our covariates are distributed too different between exposed and unexposed groups for us to feel comfortable assuming exchangeability between groups:\( Bias reduction= 1-(|standardized difference matched|/ \)\( |standardized difference unmatched|) \)

We would like to see a substantial reduction in bias from the unmatched to the matched analysis. What substantial means is up to you.

###  5. Calculate the effect estimate and standard errors with this matched population 

An estimate of the average treatment effect of the treated:\( (ATT)=sum(Yexposed- Yunexposed) / no. of matched pairs \)

Standard errors may be calculated using bootstrap resampling methods. The resulting matched pairs can also be analyzed using standard statistical methods, e.g. Kaplan-Meier, Cox proportional hazards models. You can include PS in final analysis model as a continuous measure or create quartiles and stratify.

##  Strengths and Limitations of PSM 

### Strengths

It is possible to include interaction terms in calculating PSM. PSM uses one score instead of multiple covariates in estimating the effect. This allows an investigator to use dozens of covariates, which is not usually possible in traditional multivariable models because of limited degrees of freedom and zero count cells arising from stratifications of multiple covariates.PSM can be used for dichotomous and continuous variables (continuous variables has lots of ongoing research).Students included in this study may be a more representative sample of “real world” patients than a Randomised Controlled Trial (RCT) would provide. Since we don’t use any information on the outcome when calculating the PS, no analysis based on the PS will bias effect estimation. We avoid off-support inference. We rely less on p-values and other model-specific assumptions. We don’t need to know the causes of the outcome to create exchangeability.

### Limitations of PSM

The most serious limitation is that PSM only controls for measured covariates. Group overlap must be substantial (to enable appropriate matching). Matching on observed covariates may open backdoor paths in unobserved covariates and exacerbate hidden bias. PSM works best in large samples to obtain a good balance of covariates. If we have missing data, we get a missing PS. Does not take into account clustering (problematic for neighborhood-level research).

## First analysis: Differences in means in R

We start by loading the packages and dataset in R:

First, it is time to look at the difference in means of the outcome variable:

Let us test if table above with difference in means is statistically significant at conventional levels of confidence:

Now let us look at the difference in means in the pre-treatment covariates. We’ll work with the following covariates for now: race_white: Is the student white (1) or not (0)?, p5hmage: Mother’s age, w3income: Family income, p5numpla: Number of places the student has lived for at least 4 months, w3momed_hsb: Is the mother’s education level high-school or below (1) or some college or more (0)?.

Let us calculate the mean for each covariate by the treatment status:

We can also carry out t-tests to evaluate whether these means are statistically distinguishable:

## Propensity Scores in R

We estimate the propensity score by running a logit model (probit also works) where the outcome variable is a binary variable indicating treatment status. For the matching to give a causal estimate in the end, we need to include any covariate that is related to both the treatment assignment and potential outcomes. Let us choose just a few covariates below—they are unlikely to capture all covariates that should be included.

Using this model, we can now calculate the propensity score for each student. It is the student’s predicted probability of being Treated, given the estimates from the logit model. Below, we calculate this propensity score using predict() and create a dataframe that has the propensity score as well as the student’s actual treatment status:

### Plotting Propensity Scores in R

After estimating the propensity score, it is useful to plot histograms of the estimated propensity scores by treatment status:

The above coding gives us the following histogram:![](https://i0.wp.com/datascienceplus.com/wp-content/uploads/2019/07/H1-490x350.png?w=450&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/datascienceplus.com/wp-content/uploads/2019/07/H1-490x350.png?w=450&ssl=1)


## Creating a matching algorithm

A method for estimating the treatment effect of Catholic schooling is to restrict the sample to observations within the region of common support, and then to divide the sample within the region of common support into 5 quintiles, based on the estimated propensity score. Within each of these 5 quintiles, we can then estimate the mean difference in student achievement by treatment status.The method we use below is to find pairs of observations that have very similar propensity scores, but that differ in their treatment status. We use the package MatchIt for this. This package estimates the propensity score in the background and then matches observations based on the method of choice (“nearest” in this case).

We can get some information about how successful the matching was using summary(mod_match) and plot(mod_match). To create a dataframe containing only the matched observations, we use the match.data() function:

Note that the final dataset is smaller than the original: it contains 2704 observations, meaning that 1352 pairs of treated and control observations were matched. Also note that the final dataset contains a variable called distance, which is the propensity score.

## Examining covariate balance in the matched sample

We’ll do 2 things to assess covariate balance in the matched sample:

### 1: Visual inspection

It is useful to plot the mean of each covariate against the estimated propensity score, separately by treatment status. If matching is done well, the treatment and control groups will have (near) identical means of each covariate at each value of the propensity score.

Below is an example using the four covariates in our model. Here we use a ‘loess smoother’ to estimate the mean of each covariate, by treatment status, at each value of the propensity score.

The above coding gives us the following plot:![](https://i0.wp.com/datascienceplus.com/wp-content/uploads/2019/07/H2-392x490.png?resize=392%2C490&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/datascienceplus.com/wp-content/uploads/2019/07/H2-392x490.png?resize=392%2C490&ssl=1)


### 2: Difference-in-means

The means below indicate that we have attained a high degree of balance on the five covariates included in the model.

We can test this more formally using t-tests. Ideally, we should not be able to reject the null hypothesis of no mean difference for each covariate:

## Estimating treatment effects

Estimating the treatment effect is simple once we have a matched sample that we are happy with. We can use a t-test:

Or we can use OLS with or without covariates:

### OLS without covariates

### OLS with covariates

## References

1. Gou and Fraser (2010) – Propensity Score Analysis: Statistical Methods and Applications 

****
Related Post
