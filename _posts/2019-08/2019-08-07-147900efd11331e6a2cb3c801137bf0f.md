---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/07/distilled-news-1155/
date:      2019-08-07
author:      Michael Laux
tags:
    - models
    - modelling
    - data
    - learning lifecycle
    - explainable
---

**Skewed Data: A problem to your statistical model**

This article will help you understand what skewed data is, and how it can affect your statistical insights that you want to achieve using your statistical model.

**A Data Science Playbook for explainable ML/xAI**

This technical webinar on Aug 14 discusses traditional and modern approaches for interpreting black box models. Additionally, we will review cutting edge research coming out of UCSF, CMU, and industry.

**The machine learning lifecycle**

Based on our experience working together with various clients, we believe that this inability to operationalize is partly due to machine learning projects being seen as an inherently mathematical problem. By looking at the project from ideation to operationalization from a business, data science and engineering collaboration, the probability of success significantly increases with more realistic expectations from the start. Enter: the machine learning lifecycle.

**Probabilistic tools for Privacy in Data Analysis**

If you went through the last article of this series, you already know that any randomized algorithm M can be ( e, d) differentially private algorithm, if it satisfies the properties of being one such. One of the core properties of that approach was to inject noise to the data itself, before doing any kind of analysis, or even putting it into a database. Introducing randomness to the data like this is quite common in privacy-preserving data analysis, in fact, it has a special name called Local Differential Privacy.

**Understand Self-Attention in BERT Intuitively**

Let rephrase it on my words. When the model processing one sentence, self-attention allows each word in the sentence to look at other words to better know which word contribute for the current word. More intuitively, we can think ‘self-attention’ means the sentence will look at itself to determine how to represent each token.

**The Complete Guide to Unsupervised Learning**

Unsupervised learning is a set of statistical tools for scenarios in which there is only a set of features and no targets. Therefore, we cannot make predictions, since there are no associated responses to each observation. Instead, we are interested in finding an interesting way to visualize data or in discovering subgroups of similar observations. Unsupervised learning tends to be more challenging, because there is no clear objective for the analysis, and it is often subjective. Additionally, it is hard to assess if the obtained results are good, since there is no accepted mechanism for performing cross-validation or validating results on an independent dataset, because we do not know the true answer. Two techniques will be the focus of this guide: principal component analysis and clustering. We will explore how each work mathematically, and we will implement each of them in two mini projects. Let’s get to it!

**The How of Explainable AI: Pre-modelling Explainability**

AI explainability is a broad and multi-disciplinary domain, being studied in several fields including machine learning, knowledge representation and reasoning, human-computer interaction, and the social sciences. Accordingly, XAI literature includes a large and growing number of methodologies. There are many factors that could contribute to how an AI model operates and makes its predictions, and thus many ways to explain them. This is also partially due to a lack of an agreed-upon definition for XAI. In general, explainability can be applied throughout the entire AI development pipeline. Specifically, it can be applied before (pre-modelling explainability), during (explainable modelling), and after (post-modelling explainability) the modelling stage. What follows is a non-exhaustive overview of some of the most important XAI methodologies and approaches, split up into these three stages: pre-modelling explainability, explainable modelling, and post-modelling explainability.

**The How of Explainable AI: Explainable Modelling**

In the first part of our overview of the How of Explainable AI, we looked a pre-modelling explainability. However, the true scope of explainability is much broader. Explainability can be considered at all stages of AI development, namely, pre-modelling, model development, and post-modelling. The majority of AI explainability literature aims at explaining a black-box model that is already developed, namely, post-modelling explainability. We will review post-modelling explainability methodologies in the next part. However, ideally we can avoid the black-box problem from the beginning by developing a model that is explainable by design. This explainable modelling approach is the focus of this entry in the series.

**The How of Explainable AI: Post-modelling Explainability**

In the first two parts of our overview of the How of XAI, we looked into pre-modelling explainability and explainable modelling methodologies, which focus on explainability at the dataset stage and during model development. Yet these are relatively minor areas of interest compared with explainability after the fact, and post-modelling explainability is where the majority of XAI scientists have focused their attention and research.

**Can we trust AutoML to go on full autopilot?**

We put an AutoML tool to a test on a real-world problem – the results surprised us! The good news is you still need expert data scientists even with AutoML.

**Report Time Execution Prediction with Keras and TensorFlow**

The aim of this post is to explain Machine Learning to software developers in hands-on terms. Model is based on a common use case in enterprise systems – predicting wait time until the business report is generated.

**Understanding Decision Trees for Classification (Python)**

Decision trees are a popular supervised learning method for a variety of reasons. Benefits of decision trees include that they can be used for both regression and classification, they are easy to interpret and they don’t require feature scaling. They have several flaws including being prone to overfitting. This tutorial covers decision trees for classification also known as classification trees.

**Interpreting recurrent neural networks on multivariate time series**

A guide on how to get theoretically sound explanations from complex deep learning models trained on multivariate time series.

**DIM: Learning Deep Representations by Mutual Information Estimation and Maximization**

This is our second article of the series about mutual information. In the previous articles, we have seen how to maximizes the mutual information between two variables via the MINE estimator and some practical applications of maximizing mutual information. In this article, we focus on representation learning with mutual information maximization. Specifically, we will discuss an adversarial architecture for representation learning and two other objectives of mutual information maximization that has been experimentally shown to outperform MINE estimator for downstream tasks. This article is organized into four parts. First, we briefly review MINE and introduce two additional methods for mutual information maximization that have been shown to experimentally outperform MINE. Next, we address the deficiency of traditional pixel-level representation learning methods, disclosing the reason why we maximize mutual information for representation learning. We then discuss the components of Deep InfoMax(DIM) along with the corresponding objectives. We will prove the validity of the JSD-based objective in the end for completeness.

**BigQuery and Data Studio for Model Monitoring**

In this post, we are going to discuss one stage inside Machine Learning (ML) Model’s lifecycle: the model’s performance monitoring. This is one of the kinds of things that you just face when you are dealing with real online systems that need to be continuously maintained. In many places, you can see that the ML model lifecycle ends once you have evaluated offline the model. However, in most of the real use cases, this is not true.

**Sharpening The AI Problem**

Artificial general intelligence will be humanity’s greatest achievement. But researchers must first agree on the problem they’re solving.

### Like this:

Like Loading...
