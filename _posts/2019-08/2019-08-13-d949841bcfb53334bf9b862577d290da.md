---
layout:     post
catalog: true
title:      Learn how to use PySpark in under 5 minutes (Installation + Tutorial)
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/Pt8wLoQaPRk/learn-pyspark-installation-tutorial.html
date:      2019-08-13
author:      Asel M
tags:
    - processing
    - processes
    - machines
    - python
    - applications
---

**By Georgios Drakos, Data Scientist at TUI**

I’ve found that is a little difficult to get started with Apache Spark (this will focus on PySpark) and install it on local machines for most people. With this simple tutorial you’ll get there really fast!

**Apache Spark****is a must for Big data’s lovers**as it is a fast, easy-to-use general engine for big data processing with built-in modules for streaming, SQL, machine learning and graph processing. This technology is an in-demand skill for data engineers, but also data scientists can benefit from learning Spark when doing Exploratory Data Analysis (EDA), feature extraction and, of course, ML. But please remember that Spark is only truly realized when it is run on a cluster with a large number of nodes.

 

### Table of Contents

 

Introduction
Spark definition
Spark Application
Install PySpark on Mac
Open Jupyter Notebook with PySpark
Launching a SparkSession
Conclussion
References
![](https://i.ibb.co/khdsHrP/11.png)


Introduction
 

 Apache Spark is one of the hottest and largest open source project in data processing framework with rich high-level APIs for the programming languages like Scala, Python, Java and R. It realizes the potential of bringing together both Big Data and machine learning. This is because:

Spark is fast (up to 100x faster than traditional Hadoop MapReduce) due to in-memory operation.
It offers robust, distributed, fault-tolerant data objects (called RDDs)
It integrates beautifully with the world of machine learning and graph analytics through supplementary packages like MLlib and GraphX.

Spark is implemented on Hadoop/HDFS and written mostly in Scala, a functional programming language.However, for most beginners, Scala is not a great first language to learn when venturing into the world of data science.

Fortunately, Spark provides a wonderful Python API called PySpark. This allows Python programmers to interface with the Spark framework — letting you manipulate data at scale and work with objects over a distributed file system. So, Spark is not a new programming language that you have to learn but a framework working on top of HDFS.

This presents new concepts like nodes, lazy evaluation, and the transformation-action (or ‘map and reduce’) paradigm of programming.In fact, Spark is versatile enough to work with other file systems than Hadoop — like Amazon S3 or Databricks (DBFS).

Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes.

 

### Spark Definition

 Typically when you think of a computer you think about one machine sitting on your desk at home or at work. This machine works perfectly well for applying machine learning on small dataset . However, when you have huge dataset(in tera bytes or giga bytes), there are some things that your computer is not powerful enough to perform. One particularly challenging area is data processing. Single machines do not have enough power and resources to perform computations on huge amounts of information (or you may have to wait for the computation to finish).
![](https://i.ibb.co/jr1zzRY/1-nr-JHTt-QND3a-Zf-TEyd7-Prqg.png)


A cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative resources as if they were one. Now a group of machines alone is not powerful, you need a framework to coordinate work across them. **Spark is a tool for just that, managing and coordinating the execution of tasks on data across a cluster of computers.**

 

### Spark Application

 A Spark Application consists of:

Driver
Executors (set of distributed worker processes)

 

### Driver

 The Driver runs the main() method of our application having the following duties:

Runs on a node in our cluster, or on a client, and schedules the job execution with a cluster manager
Responds to user’s program or input
Analyzes, schedules, and distributes work across the executors

 

### Executors

 An executor is a distributed process responsible for the execution of tasks. Each Spark Application has its own set of executors, which stay alive for the life cycle of a single Spark application.

Executors perform all data processing of a Spark job
Stores results in memory, only persisting to disk when specifically instructed by the driver program
Returns results to the driver once they have been completed
Each node can have anywhere from 1 executor per node to 1 executor per core

** Node is single entity machine or server .
![](https://i.ibb.co/Qr97zrS/1-GZG2aog-NS8-Jg14j-OM2rjm-Q.png)


 

### Spark’s Application Workflow

 When you submit a job to Spark for processing, there is a lot that goes on behind the scenes.

Our Standalone Application is kicked off, and initializes its SparkContext. Only after having a SparkContext can an app be referred to as a Driver
Our Driver program asks the Cluster Manager for resources to launch its executors
The Cluster Manager launches the executors
Our Driver runs our actual Spark code
Executors run tasks and send their results back to the driver
SparkContext is stopped and all executors are shut down, returning resources back to the cluster

 

### Install Spark on Mac (locally)

 ***First Step: Install Brew***

You will need to install brew if you have it already skip this step:

1. open terminal on your mac. You can go to spotlight and type terminal to find it easily (alternative you can find it on /Applications/Utilities/).2. Enter the command bellow.



3. Hit Return and the script will run. It will output to your terminal a log of what is going to install. Hit Return to continue or any other key to abort.

4. It might ask for **sudo**privileges. If this happens you will have to type your admin password and hit Return again.

**Notes:** Command line tools (Apple's XCode) will be installed after this guide.

The installation will look like as the image below.
![](https://i.ibb.co/P4k7Z3d/Homebrew-1.png)

