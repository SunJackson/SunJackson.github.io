---
layout:     post
catalog: true
title:      Maximum likelihood estimation from scratch
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/ChN67hkLuoQ/
date:      2019-08-26
author:      R on Alejandro Morales&#039; Blog
tags:
    - values
    - sampling
    - samples
    - x_i sampled
    - functions
---

Maximum likelihood estimation (MLE) is a method to estimate the parameters of a random population given a sample. I described what this population means and its relationship to the sample in a previous post.

Before we can look into MLE, we first need to understand the difference between probability and probability density for continuous variables. Probability density can be seen as a measure of relative probability, that is, values located in areas with higher probability will get have higher probability density. More precisely, probability is the integral of probability density over a range. For example, the classic “bell-shaped” curve associated to the Normal distribution is a measure of probability density, whereas probability corresponds to the area under the curve for a given range of values:

![](https://i2.wp.com/alemorales.info/post/04_mle_nonlinear_files/figure-html/unnamed-chunk-2-1.png?w=450&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/alemorales.info/post/04_mle_nonlinear_files/figure-html/unnamed-chunk-2-1.png?w=450&ssl=1)


If we assign an statistical model to the random population, any particular value (let’s call it \(x_i\)) sampled from the population will have a probability density according to the model (let’s call it \(f(x_i)\)). If we then assume that all the values in our sample are statistically independent (i.e. the probability of sampling a particular value does not depend on the rest of values already sampled), then the *likelihood* of observing the whole sample (let’s call it \(L(x)\)) is defined as the product of the probability densities of the individual values (i.e. \(L(x) = \prod_{i=1}^{i=n}f(x_i)\) where \(n\) is the size of the sample).

For example, if we assume that the data were sampled from a Normal distribution, the likelihood is defined as:

{% raw %}
\[L(x) = \prod_{i=1}^{i=n}\frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{\left(x_i – \mu \right)^2}{2\sigma^2}}\]
{% endraw %}

Note that \(L(x)\) does not depend on \(x\) only, but also on \(\mu\) and \(\sigma\), that is, the parameters in the statistical model describing the random population. The idea behind MLE is to find the values of the parameters in the statistical model that maximize \(L(x)\). In other words, it calculates the random population that is most likely to generate the observed data, while being constrained to a particular type of distribution.

One complication of the MLE method is that, as probability densities are often smaller than 1, the value of \(L(x)\) can become very small as the sample size grows. For example the likelihood of 100 values sampled from a standard Normal distribution is very small:

```
set.seed(2019)
sample = rnorm(100)
prod(dnorm(sample))
```

```
## [1] 2.23626e-58
```

When the variance of the distribution is small it is also possible to have probability densities higher than one. In this case, the likelihood function will grow to very large values. For example, for a Normal distribution with standard deviation of 0.1 we get:

```
sample_large = rnorm(100, sd = 0.1)
prod(dnorm(sample_large, sd = 0.1))
```

```
## [1] 2.741535e+38
```

The reason why this is a problem is that computers have a limited capacity to store the digits of a number, so they cannot store very large or very small numbers. If you repeat the code above but using sample sizes of say 1000, you will get `0` or `Inf` instead of the actual values, because your computer will just give up. Although it is possible to increase the amount of digits to be stored per number, this does not really solve the problem, as it will eventually come back with larger samples. Furthermore, in most cases we will need to use numerical optimization algorithms (see below) which will make the problem even worse. Therefore, we cannot work directly with the likelihood function.

One trick is to use the natural logarithm of the likelihood function instead (\(log(L(x))\)). A nice property is that the logarithm of a product of values is the sum of the logarithms of those values, that is:

\[\text{log}(L(x)) = \sum_{i=1}^{i=n}\text{log}(f(x_i))\]

Also, the values of log-likelihood will always be closer to 1 and the maximum occurs for the same parameter values as for the likelihood. For example, the likelihood of the first sample generated above, as a function of \(\mu\) (fixing \(\sigma\)) is:

![](https://i1.wp.com/alemorales.info/post/04_mle_nonlinear_files/figure-html/unnamed-chunk-5-1.png?w=450&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/alemorales.info/post/04_mle_nonlinear_files/figure-html/unnamed-chunk-5-1.png?w=450&ssl=1)


whereas for the log-likelihood it becomes:

![](https://i0.wp.com/alemorales.info/post/04_mle_nonlinear_files/figure-html/unnamed-chunk-6-1.png?w=450&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/alemorales.info/post/04_mle_nonlinear_files/figure-html/unnamed-chunk-6-1.png?w=450&ssl=1)


Although the shapes of the curves are different, the maximum occurs for the same value of \(\mu\). Note that there is nothing special about the natural logarithm: we could have taken the logarithm with base 10 or any other base. But it is customary to use the natural logarithm as some important probability density functions are exponential functions (e.g. the Normal distribution, see above), so taking the natural logarithm makes mathematical analyses easier.

You may have noticed that the optimal value of \(\mu\) was not exactly 0, even though the data was generated from a Normal distribution with \(\mu\) = 0. This is the reason why it is called a maximum likelihood *estimator*. The source of such deviation is that the sample is not a perfect representation of the population, precisely because of the randomness in the sampling procedure. A nice property of MLE is that, generally, the estimator will converge asymptotically to the true value in the population (i.e. as sample size grows, the difference between the estimate and the true value decreases).

The final technical detail you need to know is that, except for trivial models, the MLE method cannot be applied analytically. One option is to try a sequence of values and look for the one that yields maximum log-likelihood (this is known as *grid approach* as it is what I tried above). However, if there are many parameters to be estimated, this approach will be too inefficient. For example, if we only try 20 values per parameter and we have 5 parameters we will need to test 3.2 million combinations.

Instead, the MLE method is generally applied using algorithms known as non-linear optimizers. You can feed these algorithms any function that takes numbers as inputs and returns a number as ouput and they will calculate the input values that minimize or maximize the output. It really does not matter how complex or simple the function is, as they will treat it as a *black box*. By convention, non-linear optimizers will *minimize* the function and, in some cases, we do not have the option to tell them to maximize it. Therefore, the convention is to minimize the *negative log-likelihood* (NLL).

Enough with the theory. Let’s estimate the values of \(\mu\) and \(\sigma\) from the first sample we generated above. First, we need to create a function to calculate NLL. It is good practice to follow some template for generating these functions. An NLL function should take two inputs: (i) a vector of parameter values that the optimization algorithm wants to test (`pars`) and (ii) the `data` for which the NLL is calculated. For the problem of estimating \(\mu\) and \(\sigma\), the function looks like this:

```
NLL = function(pars, data) {
 # Extract parameters from the vector
 mu = pars[1]
 sigma = pars[2]
 # Calculate Negative Log-LIkelihood
 -sum(dnorm(x = data, mean = mu, sd = sigma, log = TRUE))
}
```

The function `dnorm` returns the probability density of the data assuming a Normal distribution with given mean and standard deviation (`mean` and `sd`). The argument `log = TRUE` tells R to calculate the logarithm of the probability density. Then we just need to add up all these values (that yields the log-likelihood as shown before) and switch the sign to get the NLL.

We can now minimize the NLL using the function `optim`. This function needs the initial values for each parameter (`par`), the function calculating NLL (`fn`) and arguments that will be passed to the objective function (in our example, that will be `data`). We can also tune some settings with the `control` argument. I recommend to set the setting `parscale` to the absolute initial values (assuming none of the initial values are 0). This setting determines the scale of the values you expect for each parameter and it helps the algorithm find the right solution. The `optim` function will return an object that holds all the relevant information and, to extract the optimal values for the parameters, you need to access the field `par`:

```
mle = optim(par = c(mu = 0.2, sigma = 1.5), fn = NLL, data = sample,
 control = list(parscale = c(mu = 0.2, sigma = 1.5)))
mle$par
```

```
## mu sigma 
## -0.07332745 0.90086176
```

It turns out that this problem has an analytical solution, such that the MLE values for \(\mu\) and \(\sigma\) from the Normal distribution can also be calculated directly as:

```
c(mu = mean(sample), sigma = sd(sample))
```

```
## mu sigma 
## -0.0733340 0.9054535
```

There is always a bit of numerical error when using `optim`, but it did find values that were very close to the analytical ones. Take into account that many MLE problems (like the one in the section below) cannot be solved analytically, so in general you will need to use numerical optimization.
