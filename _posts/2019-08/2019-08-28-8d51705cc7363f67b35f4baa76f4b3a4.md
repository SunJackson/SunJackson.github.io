---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/28/distilled-news-1179/
date:      2019-08-28
author:      Michael Laux
tags:
    - learning
    - data
    - modeling machine
    - sequence models
    - networks
---

**How to Train MAML(Model-Agnostic Meta-Learning)**

An elaborate explanation for MAML and more. Model-Agnostic Meta-Learning(MAML) has been growing more and more popular in the field of meta-learning since it’s first introduced by Finn et al. in 2017. It is a simple, general, and effective optimization algorithm that does not place any constraints on the model architecture, nor loss functions. As a result, it can be combined with arbitrary networks and different types of loss functions, which makes it applicable to a variety of different learning processes. This article consists of two parts: we first explain MAML, presenting a detailed discussion and visualizing the learning process; Then we describe some of the potential problems of the original MAML and address them following the work of Antoniou et al..

**Outlier Detection for a 2D Feature Space in Python**

How to detect outliers using plotting and clustering techniques to analyze the dependency of two features.

**Anomaly Detection with Time Series Forecasting**

**Introduction to Transformers Architecture**

We know that we used logo from Transformers in the featured image, so if you are a toy/movies/cartoon fan, sorry to disappoint you. We won’t cover any of those topics in this blog post. However, if you are data science and deep learning fan, you are in the right place. In this article, we explore the interesting architecture of Transformers. They are a special type of sequence-to-sequence models used for language modeling, machine translation, image captioning and text generation. Generally speaking, sequence-to-sequence models are a type of models that receives a sequence of input data and provides another sequence of data as an output. This is completely different from Standard Feed Forward Neural Networks and Convolutional Neural Networks, which are accepting a fixed-size vector as input and produce a fixed-sized vector as an output. For example if you want to translate sentence ‘You are awesome.’ from English to Serbian, sequence-to-sequence model will receive word by word as an input and generate output ‘Ti si super’.

**Mixing policy gradient and Q-learning**

Policy gradient algorithms is a big family of reinforcement learning algorithms, including reinforce, A2/3C, PPO and others. Q-learning is another family, with many significant improvements over the past few years : target network, double DQN, experience replay/sampling … I’ve always wondered if it was possible to take the best of the two and create a better learning algorithm. And this dream has come true, when I discovered Mean actor critic .

**Five Hypotheses as to why Artificial Intelligence and Machine Learning projects fail**

There are numerous articles and published papers around why AI/Machine Learning/Natural Language Processing projects never make it to ‘production’ or fail to deliver on the value proposed (Gartner predicts that through 2022, 85 percent of AI projects will deliver erroneous outcomes due to bias in data, algorithms or the teams responsible for managing them). The following are a few hypotheses as to why Artificial Intelligence projects fail and some observations on company reactions (technical and organizational) to AI projects lack of delivered value. Please note these opinions do not reflect on any current or prior employers, but are a synthesis of conversations with data scientists, engineers, product managers and architects across industries.Hypothesis #1: Data Science initial models don’t scale or are too experimental to be used by internal or external customers.Hypothesis #2: Data science/ML models while brilliant and innovative don’t meet the business requirements or are too fragile to respond to change in the supporting data.Hypothesis #3: AI initiatives are driven by the company’s internal IT organizations and inherit ‘waterfall’ challenges.Hypothesis #4: Companies don’t have the patience for the time it takes to deliver on AI/ML projects.Hypothesis #5: A lack of a ‘Product’ approach to AI/ML projects is core to project failure and increased risk.

**Neural Cryptography**

Encryption has been the way to establish a secure connection for a couple of years. It is secure, computationally efficient and almost supported by every platform. But a downside of the encryption is that an encrypted message is meaningless. It is a bunch of non-sense bits or characters that are not interpretable by any human. In this article, we want to design and train a neural network. It encrypts and decrypts messages while the ciphers are understandable by human. More specifically, each cipher is an image and the message is embedded in it. Given the new image, the other part of the network can extract the message (sentence) from the image.

**Chatbot and Related Research Paper Notes with Images**

Papers related to chatbot models in chronological order spanning about 5 years from 2014. Some papers are not about chatbots, but I included them because they are interesting, and they may provide insights into creating new and different conversation models. For each paper I provided a link, the names of the authors, and GitHub implementations of the paper (noting the deep learning framework) if I happened to find any. Since I tried to make these notes as concise as possible they are in no way summarizing the papers but are merely a starting point to get a hang of what the paper is about, and to mention main concepts with the help of pictures.

**How Transformers Work**

The Neural Network used by Open AI and DeepMind. Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar – their program to defeat a top professional Starcraft player. Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..

**The Importance of Human Interpretable Machine Learning**

A brief introduction into human interpretable machine learning and model interpretation. This article is the first in my series of articles aimed at ‘Explainable Artificial Intelligence (XAI)’. The field of Artificial Intelligence powered by Machine Learning and Deep Learning has gone through some phenomenal changes over the last decade. Starting off as just a pure academic and research-oriented domain, we have seen widespread industry adoption across diverse domains including retail, technology, healthcare, science and many more. Rather than just running lab experiments to publish a research paper, the key objective of data science and machine learning in the 21st century has changed to tackling and solving real-world problems, automating complex tasks and making our life easier and better. More than often, the standard toolbox of machine learning, statistical or deep learning models remain the same. New models do come into existence like Capsule Networks, but industry adoption of the same usually takes several years. Hence, in the industry, the main focus of data science or machine learning is more ‘applied’ rather than theoretical and effective application of these models on the right data to solve complex real-world problems is of paramount importance.

**Quantum Machine Learning: Inference on Bayesian Networks**

Over the last three months I’ve been doing a lot of work on probabilistic graphical models – a way of expressing probabilistic relationships in the form of a graph. These models are super useful when applied to machine learning tasks, with Bayesian networks, hidden Markov models, and many other important classes of learning models directly implementing different kinds of probabilistic graphs.

**Pitfalls of Algorithmic Decisions and How to Handle Them**

Machine learning algorithms with access to large data sets are making myriads of critical decisions, such as medical diagnoses, welfare eligibility, and job recruitment, traditionally made by humans. However, high-profile incidents of biases and discrimination perpetuated by such algorithms are eroding people’s confidence in these decisions. In response, policy and law makers are proposing stringent regulations on complex algorithms to protect those affected by the decisions.

**Visualizing Eigenvalues and Eigenvectors**

Eigenvalues and Eigenvectors are a very important concept in Linear Algebra and Machine Learning in general. In my previous article, I’ve been introducing those concepts in terms of Principal Components Analysis, providing practical examples. In this article, I’m going to dwell more on the maths behind those concepts, providing a geometric interpretation of what I’m about to explain.

**Isolation Forest with Statistical Rules**

This is a follow up article about anomaly detection with isolation forest . In the previous article we saw about anomaly detection with time series forecasting and classification. With isolation forest we had to deal with the contamination parameter which sets the percentage of points in our data to be anomalous. While that could be a good start to see initial results but that puts you in a problem where at any point x% of point will be returned anomalous. Lets look at one possible way to avoid it.

**Reinforcement Learning – TD(lambda) Introduction(1)**

In this article, we will be talking about TD(lambda), which is a generic reinforcement learning method that unities both Monte Carlo simulation and 1-step TD method. We have been talking about TD method exhaustively, and if you remember, in TD(n) method, I have said it is also a unification of MC simulation and 1-step TD, but in TD(n), one has to keep track of all the traces along the way and update current estimation based on value n-step ahead, in TD(lambda), we are going to see a more elegant unification.

### Like this:

Like Loading...
