---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/08/distilled-news-1156/
date:      2019-08-08
author:      Michael Laux
tags:
    - learning
    - programming
    - based
    - python
    - function
---

**XLNET explained in simple terms !!**

In this article, I am excited to take you through the most recently published Natural Language Understanding algorithm by Google Brain and CMU – XLNET. This algorithm is a breakthrough in NLP as it outperforms the state-of-the art BERT algorithm in 20 different tasks. Come on, let us explore what this new algorithm got to offer to the world !!

**Baby Steps to TensorFlow**

Training your first Tensorflow based Neural Network model for Celsius to Fahrenheit conversion.

**Easily Deploy Deep Learning Models in Production**

Getting trained neural networks to be deployed in applications and services can pose challenges for infrastructure managers. Challenges like multiple frameworks, underutilized infrastructure and lack of standard implementations can even cause AI projects to fail. This blog explores how to navigate these challenges.

**Probabilistic Machine Learning Series Post 1: Using Neural Networks as part of a Bayesian model**

This series will be about different experiments and examples in probabilistic machine learning. The advantages of probabilistic machine learning is that we will be able to provide probabilistic predictions and that the we can separate the contributions from different parts of the model. In this first post, we will experiment using a neural network as part of a Bayesian model. This allows us to use the feature learning aspect of deep learning with the uncertainty estimates provided by the Bayesian framework. For those not familiar with the Bayesian framework, the first chapter of Probabilistic Programming and Bayesian Methods for Hackers is suggested. In a nutshell, in the Bayesian framework, probabilities are seen as a degree of belief based on prior knowledge. The net result being that we will see the data as fixed and the parameters as random variables. Because of this, the parameters of our model will be represented by distributions. In comparison, in the frequentist framework, the parameters are fixed, but the data is random. The confidence interval representing the expected result of different samples. We will evaluate the posterior P(?|y) using numerical methods. The posterior is proportional to the likelihood P(y|?) times the prior P(?).

**7 Innovative Machine Learning GitHub Projects you Should Try Out in Python**

• PyTorch-Transformers (NLP)• NeuralClassifier (NLP)• TDEngine (Big Data)• Video Object Removal (Computer Vision)• Python Autocomplete (Programming)• tfpyth – TensorFlow to PyTorch to TensorFlow (Programming)• MedicalNet

**Feature selection techniques for classification and Python tips for their application**

Selecting which features to use is a crucial step in any machine learning project and a recurrent task in the day-to-day of a Data Scientist. In this article, I review the most common types of feature selection techniques used in practice for classification problems, dividing them into 6 major categories. I provide tips on how to use them in a machine learning project and give examples in Python code whenever possible. Are you ready?

**Machine Learning Model for Stochastic Processes**

Using the loan_timing.csv dataset provided, we built a simple model using the Monte Carlo simulation for predicting the fraction of loans that will default after the 3-year duration of the loan. Our model revealed a 95% confidence interval of 14.8% +/- 0.2% for Monte-Carlo simulation of N = 1000 replicated copies of the dataset. Based on these analyses, if 50,000 loans were given out with a loan term of 3 years, approximately 15% of these loans will default during the loan term.

**Introduction to Decision Intelligence**

Curious to know what the psychology of avoiding lions on the savannah has in common with responsible AI leadership and the challenges of designing data warehouses? Welcome to decision intelligence!

**Time Series Analysis with Wind Resource Assessment in R**

One of the sectors with a huge demand for data science/analysis is the energy sector. A branch of this sector where demand is high is the green wind energy turbine sector. In this analysis, you will learn to do Time Series Analysis with Wind Ressource Assesment in R.

**Reinforcement Learning – Generalisation of On-policy Function Approximation**

In previous posts, we have extended the idea of reinforcement learning from discrete state space to continuous state space, and a 1000 state random walk example was implemented, in which case a policy is given, as at all states the action of going left or right always has equal probability, and the only problem is to measure the value function of each state based on the given policy(we call these sort of problems a predict problem).In this article, let’s extend the idea to more general cases called control problem, where a policy is not given and our goal is to use Q function to learn the best action at each state. In this post, I will:• Introduce the on-policy algorithm for continuous state space setting• Implement and apply the algorithm to the classic mountain car problem

**Program Evaluation: Regression Discontinuity Design in R**

Regression analysis is one of the most requested machine learning methods in 2019. One group of regression analysis for measuring effects and to evaluate a policy program is Regression Discontinuity Design. This method is well suited for benchmarking and finding improvements for optimization in organizations. It can, therefore, be used to design organizations so they generate more value for employees and customers. In this article, you learn how to do Regression Discontinuity Design in R.

**LSTM-based African Language Classification**

Tired of German-French dataset? Look at Yemba, and stand out. Mechanics of LSTM, GRU explained and applied, with powerful visuals and code in Keras.

**Linear Discriminant Analysis In Python**

Linear Discriminant Analysis (LDA) is a dimensionality reduction technique. As the name implies dimensionality reduction techniques reduce the number of dimensions (i.e. variables) in a dataset while retaining as much information as possible. For instance, suppose that we plotted the relationship between two variables where each color represent a different class.

**Neural Networks as universal function approximators**

When you first learn about Neural Networks you are bombarded with matrix multiplications, non-linearities, and back propagation. There are many great resources, where you can learn about this (very important) stuff. This is not one of them.

### Like this:

Like Loading...
