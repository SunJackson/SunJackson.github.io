---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/12/distilled-news-1160/
date:      2019-08-12
author:      Michael Laux
tags:
    - data
    - ml models
    - learned
    - accelerate
    - acceleration
---

**The hidden cost of artificial intelligence**

If you ask the average machine learning professional what her algorithms actually do, you’ll notice that you have to do quite a bit of digging before you understand their precise workings. The first layer of explanation will include the word learn as a given, an attempt to employ an experience all humans understand in the abstract. Data (or more specifically, training data) will also feature. Probe again and you’ll get to the idea of a loss function – some metric of correctness that can be optimised via college-level mathematics. At this point the well runs dry. Her algorithms, like most, detect patterns simply by finding a representation of the incoming data that is least wrong. This is how machines learn.

**Math behind Perceptrons**

Inspired by the human brain, perceptron is a very basic implementation of the artificial neural networks. In this post, we are going to consider a classification example to try and understand the working of the perceptrons.

**Boosting Algorithms Explained**

Unlike many ML models which focus on high quality prediction done by a single model, boosting algorithms seek to improve the prediction power by training a sequence of weak models, each compensating the weaknesses of its predecessors.

**The Myth of Agile AI/Machine Learning in the Enterprise**

Today, ‘Agile’ AI/Machine Learning (AI/ML) in the enterprise is largely a myth – and it has little to do with building a model. Rather, enterprise AI/ML agility is constrained by bureaucratic data acquisition processes, complex security needs, immature cloud security practices, and cumbersome AI/ML governance processes. Unfortunately, in many enterprises these issues have resulted in failed projects, deflated expectations, and perhaps most importantly, missed opportunities to deliver real value. I have spent several years helping large banks accelerate the adoption of AI/Machine learning and related technologies. In this article I will discuss the core issues and obstacles to agile AI/ML in the enterprise that I have experienced and then offer a few lessons learned and some practical steps that provide a starting point for turning the Agile AI/ML myth into reality.

**Clustering & Visualizing Travelers’ Stories with Doc2Vec and WebGL**

This project is an Interactive Map Visualization intended to explore thousands of travelers’ stories and their connections. Its goal is to make the submissions of aspiring writers fun to discover. It is intended for a wide audience of users; whether it be aspiring travel writers, daydreaming office workers thinking about exploring a new destination, or social scientists interested in understanding why and how people travel. In a time where it feels like differences between nations and their people are amplified, I hope this project serves as reminder that many of us are connected through the way we experience our planet and each other.

**Setting up your own Data Science workspace with Visual Studio Code and Anaconda (Python)**

If you’re just starting out in the field of data science, creating a personal workspace will help you keeping your projects organized. There are lots of different tools to use and if you’re just like me, starting out in the field, you might find it daunting to dive right in. In this post I’ll show you the basics of how you can set up your own workspace on macOS with some of the most commonly used tools in the trade. While I use macOS in this guide, the steps are almost identically for Windows platforms and I expect you should have no trouble using this guide on Windows. At the end of this guide you’ll be able to:• Set up a Python environment with Anaconda.• Create a Visual Studio Code workspace and run Python scripts.• Install packages and managing different Anaconda environments.Let’s get you started!

**How to attack Machine Learning ( Evasion, Poisoning, Inference, Trojans, Backdoors)**

In my previous article i mentioned three categories of AI threats (espionage, sabotage, and fraud). If looking at a technical level, attacks can occur on 2 different stages: during training or production. Attacks during training take place more often than you can imagine. Most of the production ML models retrain their system periodically with new data. For example, social network continuously analyze user’s behavior, which means that each user may retrain this system by modifying the behavior.

**Diabetes Prediction with Ensemble Techniques**

Yes, the above quote is so true. We humans have ability to rate things and find some or the other matrices to measure things and evaluate them or their performances. Similarly, in Data Science you can measure your model’s accuracy and performance! The very first model that you build, you can set its matrices from the results and these matrices become your benchmark. Moving ahead you strive to or try to make other models on the same data which can break those benchmark. Which can predict better, which has better accuracy and so on… matrices can be anything. This vital technique is known as ‘Ensemble’ where you build one model, set its performance as measuring matrices and try to build other models which are better performers then previous matrices. This is the topic of the week! We will learn how to Ensemble models on a very interesting ‘Diabetes’ data.

**Feature Engineer Optimization in HyperparameterHunter 3.0**

The long wait is over. HyperparameterHunter 3.0 (Artemis) has arrived, adding support for Feature Engineering, and it comes bearing gifts!Gift #1) Clear and customizable Feature Engineer syntax: a list of your own functionsGift #2) Consistent scaffolding for building Feature Engineering workflows, which are automatically recordedGift #3) Optimization for Feature Engineering steps, and (obviously) detection of past Experiments to jump-start optimizationGift #4) Your sanity and time: stop keeping track of janky lists of Feature Engineering steps and how they work with all your other hyperparameters

**TensorFlow vs PyTorch vs Keras for NLP**

Comparison between TensorFlow vs PyTorch vs Keras

**Feature selection by random search in Python**

Feature selection has always been a great task in machine learning. According to my experience, I can surely say that feature selection is much more important than model selection itself.

**EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML**

For several decades, computer processors have doubled their performance every couple of years by reducing the size of the transistors inside each chip, as described by Moore’s Law. As reducing transistor size becomes more and more difficult, there is a renewed focus in the industry on developing domain-specific architectures – such as hardware accelerators – to continue advancing computational power. This is especially true for machine learning, where efforts are aimed at building specialized architectures for neural network (NN) acceleration. Ironically, while there has been a steady proliferation of these architectures in data centers and on edge computing platforms, the NNs that run on them are rarely customized to take advantage of the underlying hardware. Today, we are happy to announce the release of EfficientNet-EdgeTPU, a family of image classification models derived from EfficientNets, but customized to run optimally on Google’s Edge TPU, a power-efficient hardware accelerator available to developers through the Coral Dev Board and a USB Accelerator. Through such model customizations, the Edge TPU is able to provide real-time image classification performance while simultaneously achieving accuracies typically seen only when running much larger, compute-heavy models in data centers.

### Like this:

Like Loading...
