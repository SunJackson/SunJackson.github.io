---
layout:     post
catalog: true
title:      Detecting outlier samples in PCA
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/ZPPNDxjdfMk/
date:      2019-08-22
author:      Florian Privé
tags:
    - tukey_up
    - hist_out
    - tukey rule
    - red
    - dist
---

Threshold of `6` for the first criterion presented here may appear arbitrary. If the data you have is normally distributed, each sample (for each PC) has a probability of `2 * pnorm(-6)` (2e-9) of being considered as an outlier by this criterion.

Accounting for multiple testing, *for 10K samples and 10 PCs*, there is a chance of `1 - (1 - 2 * pnorm(-6))^100e3` (2e-4) of detecting at least one outlier. If choosing `5` as threshold, there is 5.6% chance of detecting at least one outlier when PCs are normally distributed. If choosing `3` instead, this probability is 1.

### Tukey’s rule

Tukey’s rule (Tukey 1977) is a standard rule for detecting outliers. Here, we will apply it on the previously computed statistics. Note that we could use it directly on PCs, which is not much different from the robust version of the first criterion we introduced.

```
x <- rnorm(10000)
(tukey_up <- quantile(x, 0.75) + 1.5 * IQR(x))
```

```
## 75% 
## 2.70692
```

```
(tukey_low <- quantile(x, 0.25) - 1.5 * IQR(x))
```

```
## 25% 
## -2.725665
```

```
hist(x); abline(v = c(tukey_low, tukey_up), col = "red")
```

![](https://i0.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-17-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-17-1.png?w=80%25&ssl=1)


```
mean(x < tukey_low | x > tukey_up)
```

```
## [1] 0.0057
```

where `IQR(x)` is equal to `quantile(x, 0.75) - quantile(x, 0.25)` (the InterQuartile Range).

However, there are two pitfalls when using Tukey’s rule:


Tukey’s rule assumes a normally distributed sample. When the data is skewed, it does not work that well.

x <- rchisq(10000, df = 5)
(tukey_up <- quantile(x, 0.75) + 1.5 * IQR(x))

## 75% 
## 12.42084

`(tukey_low <- quantile(x, 0.25) - 1.5 * IQR(x))`

## 25% 
## -3.232256

`hist(x, "FD"); abline(v = c(tukey_low, tukey_up), col = "red")`

![](https://i2.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-18-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-18-1.png?w=80%25&ssl=1)


`mean(x < tukey_low | x > tukey_up)`

`## [1] 0.0294`
To solve the problem of skewness, the medcouple (mc) has been introduced (Hubert and Vandervieren 2008) and is implemented in `robustbase::adjboxStats()`.


Tukey’s rule uses a fixed coefficient (`1.5`) that does not account for multiple testing, which means that for large samples, you will almost always get some outliers if using `1.5`.


To solve these two issues, we implemented `tukey_mc_up()` that accounts both for skewness and multiple testing by default.

```
x <- rchisq(10000, df = 5)
(tukey_up <- quantile(x, 0.75) + 1.5 * IQR(x))
```

```
## 75% 
## 12.48751
```

```
hist(x, "FD"); abline(v = tukey_up, col = "red")
abline(v = print(tukey_mc_up(x, coef = 1.5)), col = "blue")
```

```
## [1] 16.74215
```

```
abline(v = print(tukey_mc_up(x)), col = "green") # accounts for multiple testing
```

![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-19-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-19-1.png?w=80%25&ssl=1)


```
## [1] 25.93299
```

Applying this corrected Tukey’s rule to our statistics:

```
## [1] 6.406337
```

```
qplot(dist2, llof) +
 geom_vline(xintercept = tukey_mc_up(dist2), color = "red") +
 geom_hline(yintercept = tukey_mc_up(llof), color = "red")
```

![](https://i0.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-20-1.png?w=70%25&is-pending-load=1#038;ssl=1)
![](https://i0.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-20-1.png?w=70%25&ssl=1)


### Histogram’s gap

This rule I come up with assumes that the “normal” data is somewhat grouped and the outliers have some gap (in the histogram, there is a bin with no value in it) with the rest of the data.

For example, for `dist`, there is a gap just before 6, and we can derive an algorithm to detect this:

```
hist(dist, breaks = nclass.scottRob)
str(hist_out(dist))
```

```
## List of 2
## $ x : num [1:515] 2.08 2.06 1.74 1.86 2.04 ...
## $ lim: num [1:2] -Inf 5.75
```

```
abline(v = hist_out(dist)$lim[2], col = "red")
```

![](https://i2.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-21-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i2.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-21-1.png?w=80%25&ssl=1)


```
hist(dist2, breaks = nclass.scottRob)
abline(v = hist_out(dist2)$lim[2], col = "red")
```

![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-22-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-22-1.png?w=80%25&ssl=1)


```
hist(llof, breaks = nclass.scottRob)
abline(v = hist_out(llof)$lim[2], col = "red")
```

![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-23-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-23-1.png?w=80%25&ssl=1)


This criterion is convenient because it does not assume any distribution of the data, just that it is compact and that the outliers are not in the pack.

It could be used in other contexts, e.g. choosing the number of outlier principal components:

```
eigval <- pca$sdev^2
hist(eigval, breaks = "FD") # "FD" gives a bit more bins than scottRob
abline(v = hist_out(eigval, breaks = "FD")$lim[2], col = "red")
```

![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-24-1.png?w=80%25&is-pending-load=1#038;ssl=1)
![](https://i1.wp.com/privefl.github.io/blog/knitr_files/post-outlier-pca_files/figure-html/unnamed-chunk-24-1.png?w=80%25&ssl=1)


```
sum(eigval > hist_out(eigval, breaks = "FD")$lim[2])
```

```
## [1] 3
```

```
pca_nspike(eigval) # directly implemented in {bigutilsr}
```

```
## [1] 3
```

Note the possible use of bootstrap to make `hist_out()` and `pca_nspike()` more robust.
