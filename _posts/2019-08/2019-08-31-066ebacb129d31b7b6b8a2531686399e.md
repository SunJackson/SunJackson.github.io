---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/31/distilled-news-1181/
date:      2019-08-31
author:      Michael Laux
tags:
    - models
    - analysis
    - learning
    - cloud
    - algorithms
---

**The AI Edge Engineer: Extending the power of CI/CD to Edge devices using containers**

At the Artificial Intelligence – Cloud and Edge implementations course – I have been exploring the idea of extending CI/CD to Edge devices using containers. In this post, I present these ideas under the framework of the ‘AI Edge Engineer’. Note that the views presented here are personal. I seek your comments if you are exploring similar ideas – especially if you are in academia / research. We are happy to share insights/code as we develop it.

**Maximum likelihood estimation from scratch**

Maximum likelihood estimation (MLE) is a method to estimate the parameters of a random population given a sample.

**Distilling BERT models with spaCy**

Transfer learning is one of the most impactful recent breakthroughs in Natural Language Processing. Less than a year after its release, Google’s BERT and its offspring (RoBERTa, XLNet, etc.) dominate most of the NLP leaderboards. While it can be a headache to put these enormous models into production, various solutions exist to reduce their size considerably. At NLP Town we successfully applied model distillation to train spaCy’s text classifier to perform almost as well as BERT on sentiment analysis of product reviews.

**How to count Big Data: Probabilistic data structures and algorithms**

Learn how probabilistic data structures and algorithms can be used for cardinality estimation in Big Data streams.

**MDFS: MultiDimensional Feature Selection in R**

Abstract Identification of informative variables in an information system is often performed using simple one-dimensional filtering procedures that discard information about interactions between variables. Such an approach may result in removing some relevant variables from consideration. Here we present an R package MDFS (MultiDimensional Feature Selection) that performs identification of informative variables taking into account synergistic interactions between multiple descriptors and the decision variable. MDFS is an implementation of an algorithm based on information theory (Mnich and Rudnicki, 2017). The computational kernel of the package is implemented in C++. A high-performance version implemented in CUDA C is also available. The application of MDFS is demonstrated using the well-known Madelon dataset, in which a decision variable is generated from synergistic interactions between descriptor variables. It is shown that the application of multidimen sional analysis results in better sensitivity and ranking of importance.

**The Landscape of R Packages for Automated Exploratory Data Analysis**

The increasing availability of large but noisy data sets with a large number of heterogeneous variables leads to the increasing interest in the automation of common tasks for data analysis. The most time-consuming part of this process is the Exploratory Data Analysis, crucial for better domain understanding, data cleaning, data validation, and feature engineering. There is a growing number of libraries that attempt to automate some of the typical Exploratory Data Analysis tasks to make the search for new insights easier and faster. In this paper, we present a systematic review of existing tools for Automated Exploratory Data Analysis (autoEDA). We explore the features of fifteen popular R packages to identify the parts of analysis that can be effectively automated with the current tools and to point out new directions for further autoEDA development.

**auditor: an R Package for Model-Agnostic Visual Validation and Diagnostics**

Machine learning models have spread to almost every area of life. They are successfully applied in biology, medicine, finance, physics, and other fields. With modern software it is easy to train even a complex model that fits the training data and results in high accuracy on test set. The problem arises when models fail confronted with the real-world data. This paper describes methodology and tools for model-agnostic audit. Introduced techniques facilitate assessing and comparing the goodness of fit and performance of models. In addition, they may be used for analysis of the similarity of residuals and for identification of outliers and influential observations. The examination is carried out by diagnostic scores and visual verification. Presented methods were implemented in the auditor package for R. Due to flexible and consistent grammar, it is simple to validate models of any classes.

**Fixed Point Acceleration in R**

A fixed point problem is one where we seek a vector, X, for a function, f, such that f(X) = X. The solution of many such problems can be accelerated by using a fixed point acceleration algorithm. With the release of the FixedPoint package there is now a number of algorithms available in R that can be used for accelerating the finding of a fixed point of a function. These algorithms include Newton acceleration, Aitken acceleration and Anderson acceleration as well as epsilon extrapolation methods and minimal polynomial methods. This paper demonstrates the use of fixed point accelerators in solving numerical mathematics problems using the algorithms of the FixedPoint package as well as the squarem method of the SQUAREM package.

**unival: An FA-based R Package For Assessing Essential Unidimensionality Using External Validity Information**

The unival package is designed to help researchers decide between unidimensional and correlated factors solutions in the factor analysis of psychometric measures. The novelty of the approach is its use of external information, in which multiple factor scores and general factor scores are related to relevant external variables or criteria. The unival package’s implementation comes from a series of procedures put forward by Ferrando and Lorenzo-Seva (2019) and new methodological developments proposed in this article. We assess models fitted using unival by means of a simulation study extending the results obtained in the original proposal. Its usefulness is also assessed through a real-world data example. Based on these results, we conclude unival is a valuable tool for use in applications in which the dimensionality of an item set is to be assessed.

**Probabilistic Machine Learning Series Post 2: Model Comparison**

Many steps must be followed to transform raw data into a machine learning model. Those steps may be hard for non-experts and the amount of data keeps growing. A proposed solution to the artificial intelligence skill crisis is to do Automated Machine Learning (AutoML). Some notable projects are the Google Cloud AutoML and the Microsoft AutoML. The problem of automated machine learning consists of different parts: neural architecture search, model selection, features engineering, model selection, hyperparameter tuning and model compression. In this post, we will be interested in model selection.

**Bi-Tempered Logistic Loss for Training Neural Nets with Noisy Data**

The quality of models produced by machine learning (ML) algorithms directly depends on the quality of the training data, but real world datasets typically contain some amount of noise that introduces challenges for ML models. Noise in the dataset can take several forms from corrupted examples (e.g., lens flare in an image of a cat) to mislabelled examples from when the data was collected (e.g., an image of cat mislabelled as a flerken).

**data_algebra**

The data_algebra is part of a powerful cross-language and mutli-implementaiton family data manipulation tools. These tools can greatly reduce the development and maintenance cost of data science projects, while improving the documentation of project intent. Multi-language data science is an important trend, so a cross-language query system that supports at least R and Python is going to be a useful tool or capability going forward. Obviously SQL itself is fairly cross-language, but data_algebra adds a few features we hope are real advantages.

**Rethinking Enterprise Data Security and Privacy for Cloud-Native AI / Machine Learning**

Valid cloud security and privacy concerns are conflicting with AI/ML’s voracious appetite for data. And this is stalling enterprise adoption of AI/ML on the cloud. But older security approaches may not work as well as needed. What does a next generation cloud data security and privacy approach look like?

**How a Computerized Chess Opponent ‘Thinks’ – The Minimax Algorithm**

But the great minds behind the chess computer problem had started publishing in the subject nearly 6 decades earlier. Known as the father of modern computer science, Alan Turing is credited with provoking the investigation dating all the way back to the 1940’s. The expansive timeframe over which the chess computer problem has been pondered lends credence to the complexity of the solution. Textbooks have been written on the computer chess problem alone. However, the general decision-making strategy used by computers remains largely the same across a variety of strategic games, including chess, checkers, mancala, tic-tac-toe, you name it.

### Like this:

Like Loading...
