---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/16/distilled-news-1164/
date:      2019-08-15
author:      Michael Laux
tags:
    - learning
    - learns
    - data
    - tasks
    - models
---

**Metrics to Evaluate your Semantic Segmentation Model**

Semantic segmentation. My absolute favorite task. (More than NLP you ask? Yes.) I would make a deep learning model, have it all nice and trained… but wait. How do I know my model is performing well? In other words, what are the most common metrics for semantic segmentation? Here’s a clear cut guide to the essential metrics that you need to know to ensure your model is ?? ??. I have also included Keras implementations below.

**Using parallelization, multiple git repositories and setting permissions when automating R applications with Jenkins**

In this post, we look at various tips that can be useful when automating R application testing and continuous integration, with regards to orchestrating parallelization, combining sources from multiple git repositories and ensuring proper access right to the Jenkins agent.

**Python Data Transformation Tools for ETL**

The other day, I went on Reddit to ask if I should use Python for ETL related transformations, and the overwhelming response was yes. However, while my fellow Redditors enthusiastically supported using Python, they advised looking into libraries outside of Pandas – citing concerns about Pandas performance with large datasets. After doing some research, I found a ton of Python libraries built for data transformation: some improve Pandas performance, while others offer their own solutions. I couldn’t find a comprehensive list of these tools, so I thought I’d compile one using the research I did – if I missed something or got something wrong, please let me know!

**Anomaly detection in Martian Surface**

I got this wonderful opportunity to work on the project ‘Anomaly detection in Martian Surface’ though Omdena community. The objective of this project is to detect the Anomalies on the martian (MARS) surface caused by non-terrestrial artifacts like derbies of MARS lander missions, rovers, etc. Recently the search for so-called ‘Techno-Signatures’ – measurable properties or effect that provide scientific evidence of past or present extraterrestrial technology, has gained new interests. NASA hosted a ‘Techno-Signature’ Workshop at the Lunar and Planetary Institute in Houston, Texas, on September 2018 to learn more about the current field and state of the art of searches for ‘Techno-Signatures’, and what role NASA might play in these searches in the future. One area in this field of research is the search for non-terrestrial artifacts in the Solar System. This AI challenge is aimed at developing ‘AI Toolbox’ for the Planetary Scientists to help in identifying non-terrestrial artifacts.

**Data Augmentation for Deep Learning**

Having a large dataset is crucial for the performance of the deep learning model. However, we can improve the performance of the model by augmenting the data we already have. Deep learning frameworks usually have built-in data augmentation utilities, but those can be inefficient or lacking some required functionality. In this article, I would like to make an overview of most popular image augmentation packages, designed specifically for machine learning, and demonstrate how to use these packages with PyTorch framework.

**How to run RStudio on AWS in under 3 minutes for free**

When it comes to data analytics there are my reasons to move from your local computer to the cloud. Most prominently, you can run an indefinite number of machines without needing to own or maintain them. Furthermore, you can scale up and down as you wish in a matter of minutes. And if you choose to run t2.micro servers you can run for 750 hours a month for free within the first 12 months! After that it’s a couple of bucks per month and server. Alright, let’s get to it then! Understandably you won’t have time to read a ten minute article about RStudio Sever and Amazon Web Services after clicking a title that promised you a solution in 3 minutes. So I skip the formal introduction and cut to the chase.

**Uber’s Ludwig v0.2: New features and Improvements**

Uber released a new version of Ludwig with new features as well as some improvements to old once. If you don’t already know Uber’s Ludwig is a machine learning toolbox aimed at opening the world of machine learning to none-coders by providing a simple interface to create deep neural networks for lots of different applications. I already covered the basics of Uber’s Ludwig in two other articles. I released the first one right after the release of Uber’s Ludwig in February 2019. It covers the core principles and basics of Uber’s Ludwig. In the second article, I covered how to use Uber’s Ludwig for tabular, image and text data.

**Version Control ML Model**

Machine Learning operations (let’s call it mlOps under the current buzzword pattern xxOps) are quite different from traditional software development operations (devOps). One of the reasons is that ML experiments demand large dataset and model artifact besides code (small plain file). This post presents a solution to version control machine learning models with git and dvc (Data Version Control).

**Analogies from Word Vectors?**

Entry level articles on word vectors often contain examples of calculated analogies, such as king-man+woman=queen. Striking examples like this clearly have their place. They guide our interest towards similarity as one of the hidden treasures to delve for. In real data, however, analogies are often not so clear and easy to use.

**Market Profile: a statistical view on financial markets**

In the article, I have briefly presented Market Profile. I have covered why I think Market Profile is still relevant today and some reasoning on why I think in that way. I have also enumerated the three main classic books which cover the theory and a small excerpt of code on how to plot market profiles. A routine to get market profile is not presented because it is highly specific on how do you store your data, but in this example, a prototype in Python was build in just 50 lines. That is just one page of code.

**To dance or not to dance? – The Machine Learning approach.**

I love dancing! There, I said it. Even though I may not want to dance all the time, I do find myself often scrolling through my playlists in search of my most danceable songs. And here’s the thing, it has nothing to do with genres – at least not for me. But it has everything to do with the music.

**A different take on Bayes Rule**

Most people reading this article have seen demonstrations of each probability distribution in Bayes Rule. Most people reading this have been formally introduced to the terms ‘posterior’, ‘prior’, and ‘likelihood’. If not, even better! I think that viewing Bayes Rule as an incremental learning rule would be a novel perspective for many. Further, I believe this perspective would give much better intuition for why we use the terms ‘posterior’ and ‘prior’. Finally, I think this perspective helps explain why I don’t think Bayesian statistics lead to any more inductive bias than Frequentist statistics.

**Modernize your IT Infrastructure Monitoring by Combining Time Series Databases with Machine Learning**

Let’s explore the complexity and vulnerability of IT infrastructure and how to build a modern IT infrastructure monitoring solution, using a combination of time series databases with machine learning.

**t-SNE Python Example**

t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique used to represent high-dimensional dataset in a low-dimensional space of two or three dimensions so that we can visualize it. In contrast to other dimensionality reduction algorithms like PCA which simply maximizes the variance, t-SNE creates a reduced feature space where similar samples are modeled by nearby points and dissimilar samples are modeled by distant points with high probability. At a high level, t-SNE constructs a probability distribution for the high-dimensional samples in such a way that similar samples have a high likelihood of being picked while dissimilar points have an extremely small likelihood of being picked. Then, t-SNE defines a similar distribution for the points in the low-dimensional embedding. Finally, t-SNE minimizes the Kullback-Leibler divergence between the two distributions with respect to the locations of the points in the embedding.

**ERNIE 2.0: A Continual Pre-training Framework for Language Understanding**

Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese.

**Measurable Counterfactual Local Explanations for Any Classifier**

We propose a novel method for explaining the predictions of any classifier. In our approach, local explanations are expected to explain both the outcome of a prediction and how that prediction would change if ‘things had been different’. Furthermore, we argue that satisfactory explanations cannot be dissociated from a notion and measure of fidelity, as advocated in the early days of neural networks’ knowledge extraction. We introduce a definition of fidelity to the underlying classifier for local explanation models which is based on distances to a target decision boundary. A system called CLEAR: Counterfactual Local Explanations via Regression, is introduced and evaluated. CLEAR generates w-counterfactual explanations that state minimum changes necessary to flip a prediction’s classification. CLEAR then builds local regression models, using the w-counterfactuals to measure and improve the fidelity of its regressions. By contrast, the popular LIME method [15], which also uses regression to generate local explanations, neither measures its own fidelity nor generates counterfactuals. CLEAR’s regressions are found to have significantly higher fidelity than LIME’s, averaging over 45% higher in this paper’s four case studies.

### Like this:

Like Loading...
