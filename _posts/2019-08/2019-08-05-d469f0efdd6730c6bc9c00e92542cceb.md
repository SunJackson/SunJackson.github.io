---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/05/distilled-news-1153/
date:      2019-08-05
author:      Michael Laux
tags:
    - learning
    - clustering
    - clusters
    - humans
    - programs
---

**One simple graphic: Researchers love PyTorch and TensorFlow**

In a recent survey – AI Adoption in the Enterprise, which drew more than 1,300 respondents – we found significant usage of several machine learning (ML) libraries and frameworks. About half indicated they used TensorFlow or scikit-learn, and a third reported they were using PyTorch or Keras.

**AutoPandas**

AutoPandas is a program synthesis engine for the Pandas python library. It adopts the programming-by-example approach in that users specify intent using input-output examples which are then used to synthesize a program that correctly produces the desired output given the input. This website serves as an interactive interface to the core engine.

**Machine Learning and Medical Diagnosis**

Healthcare industry is one of the most affected fields by machine learning implementations: just think about all the wrong diagnoses that can change people’s life just because of (more than legit and possible) human errors. What if machine learning algorithms could lower that error and prevent wrong diagnoses, or even prevent diseases before they get more difficult to treat? The purpose of this experiment is showing you how to train an ML algorithm on a clinical dataset so that you can appreciate its power.

**Decision Tree in Layman’s Terms**

A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions. Tree models where the target variable can take a finite set of values are called classification trees and target variable can take continuous values (numbers) are called regression trees.

**What’s New In Pandas Version 0.25.0?**

Pandas is one of the most popular Python libraries. It is widely used in a large number of data science projects. This article documents the list of features and enhancements which have been introduced in the Pandas version 0.25.0.

**Program Synthesis**

Program synthesis is the task of automatically finding a program in the underlying programming language that satisfies the user intent expressed in the form of some specification. Since the inception of AI in the 1950s, this problem has been considered the holy grail of Computer Science. Despite inherent challenges in the problem such as ambiguity of user intent and a typically enormous search space of programs, the field of program synthesis has developed many different techniques that enable program synthesis in different real-life application domains. It is now used successfully in software engineering, biological discovery, computeraided education, end-user programming, and data cleaning. In the last decade, several applications of synthesis in the field of programming by examples have been deployed in mass-market industrial products. This survey is a general overview of the state-of-the-art approaches to program synthesis, its applications, and subfields. We discuss the general principles common to all modern synthesis approaches such as syntactic bias, oracle-guided inductive search, and optimization techniques. We then present a literature review covering the four most common state-of-the-art techniques in program synthesis: enumerative search, constraint solving, stochastic search, and deduction-based programming by examples. We conclude with a brief list of future horizons for the field.

**Why Genuine Human Intelligence Is Key for the Development of AI**

The developments have been fast and furious in recent months. Microsoft announced that it will invest $1 billion in a partnership with research lab OpenAI to create artificial general intelligence (AGI), the holy grail of artificial intelligence. OpenAI’s CEO Sam Altman has boasted that ‘the creation of AGI will be the most important technological development in human history’ Computers can do many very specific tasks much better than humans, but they do not have anything remotely resembling the wisdom, common sense, and critical thinking that humans use to deal with ill-defined situations, vague rules, and ambiguous, even contradictory, goals. The development of computers that can do everything the human brain does would be astonishing, but Microsoft’s record is not encouraging.

**An Example of Hyperparameter Optimization on XGBoost, LightGBM and CatBoost using Hyperopt**

This serves an introduction to the major boosting libraries and hyperopt. There are more topics in parallel running for boosting and speeding up the computation with GPU and MongoDB for hyperopt in the documentations that you may find inspiring as well.

**Hyperopt-sklearn**

Finding the right classifier to use for your data can be hard. Once you have chosen a classifier, tuning all of the parameters to get the best results is tedious and time consuming. Even after all of your hard work, you may have chosen the wrong classifier to begin with. Hyperopt-sklearn provides a solution to this problem.

**Dealing with Missing Data**

I recently completed a project using machine learning to predict cervical cancer – the Jupyter notebook and all related source materials can be found in this GitHub repo. One of the challenges with this project was how to deal with missing values in many of the predictive variables. This article describes how I addressed this missing data. Many machine learning models throw an error when they encounter missing data and there are many methods (and opinions!) for dealing with missing values. Among the simplest approaches is to fill the missing values with the mean of a particular variable, and I’ve seen many projects where that is the approach taken. It’s fast, easy, and can sometimes have good results. At the more difficult end of the spectrum are complex methods to impute missing values based on comparing the record having a missing value to similar records that are not missing a value for that factor. There are pros and cons to every approach and in my opinion, the best choice will always be case-specific.

**Interview with the AI**

Wouldn’t it be better if AI bots got real with us, instead of being only sophisticated parrots? A conscious AI would seem to be present, to have an inner life, and would act as if we did, too.

**Practical Data Augmentation Techniques for Predictive Models**

In summary, predictive models benefit from aggregating additional information about the user that genuinely captures who the customers are and why they want to buy. Such information tends to improve the accuracy of the model and helps the retailer personalize their services to their end customer. Remember a model is as good as what was fed as the input to learn from. Better the information that captures the buyers intent, the better would be the model.

**Machine Learning Cheat Sheet – Unsupervised Learning**

• K-Means Clustering• Hierarchical Clustering• DBSCAN• GMM• Cluster Validation• PCA• Random Projection• ICA

**What is clustering and why is it hard?**

Clustering is difficult because it is an unsupervised learning problem: we are given a dataset and are asked to infer structure within it (in this case, the latent clusters/categories in the data). The problem is that there isn’t necessarily a ‘correct’ or ground truth solution that we can refer to if we want to check our answers. This is in contrast to classification problems, where we do know the ground truth. Deep artificial neural networks are very good at classification (NYT article; Deng et al. 2009), but clustering is still a very open problem.

**Handling Big Datasets for Machine Learning**

More than 2.5 quintillion bytes of data are created each day. 90% of the data in the world was generated in the past two years. The prevalence of data will only increase, so we need to learn how to deal with such large data.

### Like this:

Like Loading...
