---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/25/distilled-news-1174/
date:      2019-08-25
author:      Michael Laux
tags:
    - data
    - python
    - packages
    - stationary
    - models trained
---

**The Confusion Matrix for Classification**

Understanding Accuracy, Recall, Precision, ROC, AUC and F1-score. The Confusion-matrix yields the most ideal suite of metrics for evaluating the performance of a classification algorithm such as Logistic-regression or Decision-trees. It’s typically used for binary classification problems but can be used for multi-label classification problems by simply binarizing the output. Without rhetorics, The Confusion-matrix can certainly tell us the Accuracy, Recall, Precision, ROC, AUC, as well as the F1-score, of a classification model. We shall look at these metrics closely in a few minutes…

**Basics of Bayesian Network**

There is innumerable text available in the net on Bayesian Network, but most of them are have heavy mathematical formulas and concepts thus quite difficult to understand. Here, I have tried to explain the topic as simple as possible with minimum equations and a real-world example. Bayesian Network is a very important tool in understanding the dependency among events and assigning probabilities to them thus ascertaining how probable or what is the change of occurrence of one event given the other. For example, suppose you are getting scolded at school by your teacher for being late and there could be many reasons for being late like waking up late, traffic jams, etc. So here, scolding is dependent on the events like waking up late or traffic jam i.e., these reasons have a direct influence on you being scolded. This can be efficiently represented using Bayesian Network which we will see soon.

**Data Revolution Inside Organizations**

Worldwide access to vast amounts of data has changed the business landscape. Competitive marketing depends on knowing how to manage, process, and analyze that data. This article describes the path organizations need to take from collecting data to maximizing its use. Today’s organizations are undergoing a challenging transformation process around their technical systems. The static software platforms that might have stored and processed a business’ data are no longer sustainable in the current web environment. Enterprises need cutting-edge technology to collect big data in real-time, analyze that data, and then get the information they need to stay competitive in today’s marketplace.

**A new way to sentiment-tag financial news**

Over the past few years, financial-news sentiment analysis has taken off as a commercial natural language processing (NLP) application. Like any other type of sentiment analysis, there are two main approaches: one, more traditional, is by using sentiment-labelled word lists (which we will also refer to as dictionaries). The other, is using sentiment classifiers based on language models trained on huge corpora (such as Amazon product reviews or IMDB film reviews). For domain-specific sentiment analysis, these latter language models tend to perform poorly. Hardly a surprise: a medical article reads nothing like a film review. In this respect, transfer learning is an interesting growing field. Currently, though, a dictionary still lies at the core of many domain-specific sentiment-analysis applications. Within finance, those trying to build on open-source resources will likely end up with Notre Dame’s McDonald- Loughran (M-L) word lists, which were created by analysing over fifty thousand earnings reports over the 1994-2008 period. This dictionary has been used by, among others, Google, Fidelity, Citadel, Dow Jones, and S&P Global.

**Why Has Cloud-Native AI/Machine Learning Stalled in the Enterprise?**

Mostly, because of valid data security and privacy concerns. And AI/ML’s voracious appetite for data is creating conflicts with enterprises’ traditionally cautious security policies. Simply put, data security and privacy concerns are stalling the enterprise adoption of cloud-native AI/ML. These concerns are material, real, and valid. But they conflict with AI/ML’s voracious appetite for data. And until the conflicts are addressed, cloud-native AI/ML adoption in the enterprise will stall.

**A guide for getting started NLP and spaCy**

A major challenge of text data is extracting meaningful patterns and using those patterns to find actionable insights.NLP can be thought of as a two part problem:• Processing. Converting the text data from its original form into a form the computer can understand. This includes data cleaning and feature extraction.• Analysis. Using the processed data to extract insights and make predictions.Here we will focus on the processing step.

**The Ultimate Guide to using the Python regex module**

One of the main tasks while working with text data is to create a lot of text-based features. One could like to find out certain patterns in the text, emails if present in a text as well as phone numbers in a large text. While it may sound fairly trivial to achieve such functionalities it is much simpler if we use the power of Python’s regex module. For example, let’s say you are tasked with finding the number of punctuations in a particular piece of text. Using Shakespeare here. How do you normally go about it?

**Intelligent Loan Selection for Peer-to-Peer Lending**

In this article I describe how to train a neural network to evaluate loans that are offered on the crowd lending platform Lending Club. I also cover how to test the model, how to adjust the risk in loan selection, and how to use the model to make automatic investments using Lending Club’s API.

**Deploying BERT in production**

In this guide, I’ll use BERT to train a sentiment analysis classifier and Cortex to deploy it as a web API on AWS. The API will autoscale to handle production workloads, support rolling updates so that models can be updated without any downtime, stream logs to make debugging easy, and support inference on CPUs and GPUs.

**Data Science Roles: A Classification Problem**

This post will take a look at the ways some teams have attempted to classify various Data Science roles. It will focus on how job titles are organized at two large companies with mature Data Science programs, and will also introduce some essential non-technical skills from the perspective of one Data Scientist at a smaller organization. It will consider the trajectory of Data Science careers as perceived by industry practitioners and thought leaders. Finally, it will offer recommendations for some Data Science ‘adjacent’ skills to obtain in order to add value and remain relevant in this rapidly evolving industry.

**10 Powerful Python Tricks for Data Science you Need to Try Today**

1. zip: Combine Multiple Lists in Python2. gmplot: Plot the GPS Coordinates in your Dataset on Google Maps3. category_encoders: Encode your Categorical Variables using 15 Different Encoding Schemes4. progress_apply: Monitor the Time you Spend on Data Science Tasks5. pandas_profiling: Generate a Detailed Report of your Dataset6. grouper: Grouping Time Series Data7. unstack: Transform the Index into Columns of your Dataframe8. %matplotlib Notebook: Interactive Plots in your Jupyter Notebook9. %%time: Check the Running Time of a Particular Block of Python Code10: rpy2: R and Python in the Same Jupyter Notebook!

**An Overview of Python’s Datatable package**

Python library for efficient multi-threaded data processing, with the support for out-of-memory datasets. If you are an R user, chances are that you have already been using the data.table package. Data.table is an extension of the data.frame package in R. It’s also the go-to package for R users when it comes to the fast aggregation of large data (including 100GB in RAM). The R’s data.table package is a very versatile and a high-performance package due to its ease of use, convenience and programming speed. It is a fairly famous package in the R community with over 400k downloads per month and almost 650 CRAN and Bioconductor packages using it(source). So, what is in it for the Python users? Well, the good news is that there also exists a Python counterpart to thedata.table package called datatable which has a clear focus on big data support, high performance, both in-memory and out-of-memory datasets, and multi-threaded algorithms. In a way, it can be called as data.table’s younger sibling.

**Data Engineering, Preparation, and Labeling for AI 2019 – Getting Data Ready for Use in AI and Machine Learning Projects**

The big challenge for organizations looking to make use of advanced machine learning is getting access to large volumes of clean, accurate, complete, and well-labeled data to train ML models. More advanced forms of ML like deep learning neural networks require especially large volumes of data to create accurate models. In this 2019 report, AI analyst firm Cognilytica shares valuable insight to help you get to market quickly with data and vendors that you can trust.• How to optimize and accelerate the data preparation that takes up almost 80% of AI project time• Important factors to consider when choosing third-party data labeling vendors and data providers• What solutions and hiring needs to expect from the machine learning and AI data preparation market in the next few years

**LSTM Networks**

LSTM is a very special kind of Recurrent Neural Network (RNN) which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them.

**Using The Super-Resolution Convolutional Neural Network for Image Restoration**

**Stochasticity test for time series**

Using non-stationary time series data in forecasting models produces unreliable and spurious results that leads to poor understanding and forecasting. The solution to the problem is to transform the time series data so that it becomes stationary. ADF and KPSS are quick stationary statistical tests to understand the data you are dealing with. Most statistical forecasting methods are based on the assumption that the time series are approximately stationary. A stationary series is relatively easy to predict: you simply forecast that its statistical properties will be the same in the future as they have been in the past. Analysis of time series patterns is the first step of converting non-stationary data in to stationary data (for example by trend removal), so that the statistical forecasting methods could be applied. There are three fundamental steps of building a quality forecasting time series model: making the data stationary, selecting the right model, and evaluating model accuracy. This article will focus on the first step making the data stationarity.

**How Much Can We Afford to Forget, If We Train Machines to Remember?**

Civilizations evolve through strategic forgetting of once-vital life skills. But can machines do all our remembering? When I was a student, in the distant past when most computers were still huge mainframes, I had a friend whose PhD advisor insisted that he carry out a long and difficult atomic theory calculation by hand. This led to page after page of pencil scratches, full of mistakes, so my friend finally gave in to his frustration. He snuck into the computer lab one night and wrote a short code to perform the calculation. Then he laboriously copied the output by hand, and gave it to his professor. Perfect, his advisor said – this shows you are a real physicist. The professor was never any the wiser about what had happened. While I’ve lost touch with my friend, I know many others who’ve gone on to forge successful careers in science without mastering the pencil-and-paper heroics of past generations.

**Deep Learning for Image Classification with Less Data**

In this blog I will be demonstrating how deep learning can be applied even if we don’t have enough data. I have created my own custom car vs bus classifier with 100 images of each category. The training set has 70 images while validation set makes up for the 30 images.

### Like this:

Like Loading...
