---
layout:     post
catalog: true
title:      A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'： Robust Feature Leakage
subtitle:      转载自：https://distill.pub/2019/advex-bugs-discussion/response-2
date:      2019-08-06
author:      未知
tags:
    - features
    - robust feature
    - robustness
    - settings
    - models
---


 Ilyas et al. report a surprising result: a model trained on
 adversarial examples is effective on clean data. They suggest this transfer is driven by adverserial
 examples containing geuinely useful non-robust cues. But an alternate mechanism for the transfer could be a
 kind of “robust feature leakage” where the model picks up on faint robust cues in the attacks.
 


Ilyas et al. show that a model trained on adversarial examples is effective on clean data, a highly surprising result. The primary driver behind this, suggests, is the non-robust cues overlaid on the dataset by the adversery. If adverserial examples contain robust cues too, however, (light as they may be) this presents an alternative mechanism for the transfer.



 We show that at least 23.5% (out of 88%) of the accuracy can be explained by robust features in
 DrandD_\text{rand}Drand​. This is a weak lower bound, established by a linear model, and does not perclude the
 possibility of further leakage. On the other hand, we find no evidence of leakage in DdetD_\text{det}Ddet​.
 

### Lower Bounding Leakage


 Our technique for quantifying leakage consisting of two steps:
 

First, we construct features fi(x)=wiTxf_i(x) = w_i^Txfi​(x)=wiT​x that are provably robust, in a sense we will soon
 specify.
Next, we train a linear classifier as per ,
{% raw %}
 Equation 3 on the datasets D^det\hat{\mathcal{D}}_{\text{det}}D^det​ and
{% endraw %}
{% raw %}
 D^rand\hat{\mathcal{D}}_{\text{rand}}D^rand​ (Defined , Table 1) on
{% endraw %}
 these robust features *only*.
 

 Since Ilyas et al. only specify robustness in the two class
 case, we propose two possible specifications for what constitutes a *robust feature* in the multiclass
 setting:

 



 We find features that satisfy *both* specifications by using the 10 linear features of a robust linear
 model trained on CIFAR-10. Because the features are linear, the above two conditions can be certified
 analytically. We leave the reader to inspect the weights corresponding to the features manually:
 


{% raw %}
 Training a linear model on the above robust features on D^rand\hat{\mathcal{D}}_{\text{rand}}D^rand​ and testing on the
{% endraw %}
 CIFAR test set incurs an accuracy of **23.5%** (out of 88%). Doing the same on
{% raw %}
 D^det\hat{\mathcal{D}}_{\text{det}}D^det​ incurs an accuracy of **6.81%** (out of 44%).
{% endraw %}
 


 The contrasting results suggest that the the two experiements should be interpreted differently. The
{% raw %}
 transfer results of D^rand\hat{\mathcal{D}}_{\text{rand}}D^rand​ in Table 1 of 
{% endraw %}
 should approached with caution: A non-trivial portion of the accuracy can be attributed to robust
 features. Note that this bound is weak: this bound could be possibly be improved if we used nonlinear
 features, e.g. from a robust deep neural network.
 


{% raw %}
 The results of D^det\hat{\mathcal{D}}_{\text{det}}D^det​ in Table 1 of 
{% endraw %}
 however, are on stronger footing. We find no evidence of feature leakage (in fact, we find negative leakage — an influx!). We thus conclude that it is plausible the majority of the accuracy is driven by
 non-robust features, exactly the thesis of .
 


 The contrasting results, thus, point to methodological advise  when disentangling two cues in an image, we believe safer is better to drive correlations negative than it is to 0.
 


**Response**: This comment raises a valid concern which was in fact one of
{% raw %}
 the primary reasons for designing the D^det\widehat{\mathcal{D}}_{det}D
{% endraw %}
det​ dataset.
{% raw %}
 In particular, recall the construction of the D^rand\widehat{\mathcal{D}}_{rand}D
{% endraw %}
rand​
 dataset: assign each input a random target label and do PGD towards that label.
{% raw %}
 Note that unlike the D^det\widehat{\mathcal{D}}_{det}D
{% endraw %}
det​ dataset (in which the
{% raw %}
 target class is deterministically chosen), the D^rand\widehat{\mathcal{D}}_{rand}D
{% endraw %}
rand​
 dataset allows for robust features to actually have a (small) positive
 correlation with the label. 

To see how this can happen, consider the following simple setting: we have a
 single feature f(x)f(x)f(x) that is 111 for cats and −1-1−1 for dogs. If ϵ=0.1\epsilon = 0.1ϵ=0.1
 then f(x)f(x)f(x) is certainly a robust feature. However, randomly assigning labels
{% raw %}
 (as in the dataset D^rand\widehat{\mathcal{D}}_{rand}D
{% endraw %}
rand​) would make this feature
 uncorrelated with the assigned label, i.e., we would have that E[f(x)⋅y]=0E[f(x)\cdot y] = 0E[f(x)⋅y]=0. Performing a
 targeted attack might in this case induce some correlation with the
 assigned label, as we could have E[f(x+η⋅∇f(x))⋅y]>E[f(x)⋅y]=0\mathbb{E}[f(x+\eta\cdot\nabla
 f(x))\cdot y] > \mathbb{E}[f(x)\cdot y] = 0E[f(x+η⋅∇f(x))⋅y]>E[f(x)⋅y]=0, allowing a model to learn
 to correctly classify new inputs. 

In other words, starting from a dataset with no features, one can encode
 robust features within small perturbations. In contrast, in the
{% raw %}
 D^det\widehat{\mathcal{D}}_{det}D
{% endraw %}
det​ dataset, the robust features are correlated
 with the original label (since the labels are permuted) and since they are
 robust, they cannot be flipped to correlate with the newly assigned (wrong)
{% raw %}
 label. Still, the D^rand\widehat{\mathcal{D}}_{rand}D
{% endraw %}
rand​ dataset enables us to show
 that (a) PGD-based adversarial examples actually alter features in the data and
 (b) models can learn from human-meaningless/mislabeled training data. The
{% raw %}
 D^det\widehat{\mathcal{D}}_{det}D
{% endraw %}
det​ dataset, on the other hand, illustrates that the
 non-robust features are actually sufficient for generalization and can be
 preferred over robust ones in natural settings.

The experiment put forth in the comment is a clever way of showing that such
 leakage is indeed possible. However, we want to stress (as the comment itself
 does) that robust feature leakage does *not* have an impact on our main
{% raw %}
 thesis — the D^det\widehat{\mathcal{D}}_{det}D
{% endraw %}
det​ dataset explicitly controls
 for robust
 feature leakage (and in fact, allows us to quantify the models’ preference for
 robust features vs non-robust features — see Appendix D.6 in the
 paper).
