---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/02/distilled-news-1151/
date:      2019-08-02
author:      Michael Laux
tags:
    - data
    - learning
    - models
    - modeling
    - machines
---

**Entity embedding using t-SNE**

The well known dimensionality reduction tool can be useful for categorical feature embedding. We have been discussing some of the possible ways to embed categorical features: Kernel PCA and Spectral Encoding. The goal of such embedding is to map categorical features to vectors in a low dimensional space. The advantage is this mapping dramatically reduce overfitting, compared to 1-hot encoding. However we can lose information and make the learning more difficult if the embedding is chosen incorrectly. In order to increase the quality of embedding, we use the category similarity information (that we can set a priori or by computing the similarity of conditional probability distribution). Kernel PCA method is also using the probability distribution of the categorical variable, whereas the Spectral Encoding does not. For completeness, we will also use t-SNE method and we can discuss its advantages and disadvantages.

**How to Run Scrum in Data Science Teams**

Hands-on experience tips on team structure, skills, cross functionality, product backlog items, sprint lengths, difficulties and benefits when using scrum framework on a data project. Follow-up – don’t miss my coming article with product backlog items of a case study which shows how to break PBIs into smaller pieces and create vertical slices. I am working in an agile tribe composed of 7 teams and 58 people established to deliver AI functionality to existing products of the bank. One of the teams is architectural team and the others are delivery teams.

**Instance Selection: The myth behind Data Sampling**

One of the most common and most challenging issues in any Big Data system is to select stratified samples in a way that it’s representative of characteristics of the overall data population. From data annotation to the selection of evaluation dataset, Data sampling is key to success behind every Data Science solution. Efficient sampling is a critical requirement also because it is assumed that the machine learning models trained on this sampled set and the insights generated hold true for the broader set.

**Decision Tree vs Random Forest vs Gradient Boosting Machines: Explained Simply**

Decision Trees, Random Forests and Boosting are among the top 16 data science and machine learning tools used by data scientists. The three methods are similar, with a significant amount of overlap. In a nutshell:• A decision tree is a simple, decision making-diagram.• Random forests are a large number of trees, combined (using averages or ‘majority rules’) at the end of the process.• Gradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end.

**Program Evaluation: Interrupted Time Series in R**

Regression analysis is one of the most demanding machine learning methods in 2019. One group of regression analysis for measuring effects and to evaluate a policy program is Interrupted Time Series. This method is well suited for benchmarking and finding improvements for optimization in organizations. It can, therefore, be used to design organizations so they generate more value for employees and customers. In this article, you learn how to do Interrupted Time Series in R.

**SHAP: A reliable way to analyze model interpretability**

I had started this series of blogs on Explainable AI with 1st understanding what’s the balance between accuracy vs interpretability, then moving on to explaining what are some of the rudimentary model agnostic techniques to understand reasons behind model predictions, and finally had explained the know hows of LIME. Below are the links to these blogs for your quick reference:1. The balance: Accuracy vs. Interpretability2. How to interpret machine learning models?3. LIME: Explaining predictions of machine learning modelsIn this blog, I will be talking about one of the most popular model agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations.

**Uplift Modeling**

This series of articles was designed to explain how to use Python in a simplistic way to fuel your company’s growth by applying the predictive approach to all your actions. It will be a combination of programming, data analysis, and machine learning.

**Getting Started with Experimental Design in R**

This quick blog is designed to help you get off to the races quickly in world of data science; and here specifically, Experimental design. Enjoy!

**The Inception of Machine learning**

In the match of buzzwords, the term ‘Machine Learning’ has become a great contender. But have you ever wondered how this term came about? Let’s dive deeper into its history and learn how it evolved to what it is today.

**Taking Azure AutoML for a test drive**

Microsoft recently introduced Azure AutoML in preview. I love the concept of automating machine learning as it is probably the best thing to have happened to people who aren’t data scientists. Its taking data science to masses one AutoML experiment at a time.

**The Economic and Business Impacts of Artificial Intelligence: Reality, not Hype**

The debate on Artificial Intelligence (AI) is characterized by hyperbole and hysteria. The hyperbole is due to two effects: first, the promotion of AI by self-interested investors. It can be termed the ‘Google-effect’, after its CEO Sundar Pichai, who declared AI to be ‘probably the most important thing humanity has ever worked on’. He would say that. Second, the promotion of AI by tech-evangelists as a solution to humanity’s fundamental problems, even death. It can be termed the ‘Singularity-effect’, after Ray Kurzweil, who believes AI will cause a ‘Singularity’ by 2045.

**Free Data Sets for Machine Learning**

• UCI – The UCI Machine Learning Repository• Kaggle• Scikit-learn• Drivendata.org• FiveThirtyEight

**Uber’s Ludwig Applications**

Believe it or not, but a lack of coding skills is one of the most common self-excuses for not starting to work on data science and machine learning. This is why in the last few months and years a lot of companies created and open-sourced libraries and tools that allow no-coders to create their own machine learning projects. These tools include Microsoft Azure ML Studio, Weka, Uber’s Ludwig and many others. This article is a follow-up on my post ‘Introduction to Uber’s Ludwig’, where I explained what Uber’s Ludwig is and what it can be used for. In this article, I will try to explain how to use Uber’s Ludwig for different data including tabular data, image data, and text data. The main focus of the article will be to help you get a good understanding of how to approach different problems so that after this article you will be able to solve your problems using Uber’s Ludwig.

**Tidying up with PCA: An Introduction to Principal Components Analysis**

Principal component analysis (PCA) is a technique for dimensionality reduction, which is the process of reducing the number of predictor variables in a dataset. More specifically, PCA is an unsupervised type of feature extraction, where original variables are combined and reduced to their most important and descriptive components. The goal of PCA is to identify patterns in a data set, and then distill the variables down to their most important features so that the data is simplified without losing important traits. PCA asks if all the dimensions of a data set spark joy and then gives the user the option to eliminate ones that do not.

**36 Ways Pytorch Lightning Can Supercharge Your AI Research**

AGI is not going to solve itself ( deep down you know we’re the AGI of another AI ??). But let’s say it did…

**The Complete Guide to Support Vector Machine (SVM)**

We have seen how to approach a classification problem with logistic regression, LDA, and decision trees. Now, yet another tool is introduced for classification: support vector machine. The support vector machine is a generalization of a classifier called maximal margin classifier. The maximal margin classifier is simple, but it cannot be applied to the majority of datasets, since the classes must be separated by a linear boundary. That is why the support vector classifier was introduced as an extension of the maximal margin classifier, which can be applied in a broader range of cases.

### Like this:

Like Loading...
