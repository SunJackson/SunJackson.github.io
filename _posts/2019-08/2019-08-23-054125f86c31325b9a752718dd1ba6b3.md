---
layout:     post
catalog: true
title:      Nothing but NumPy： Understanding & Creating Neural Networks with Computational Graphs from Scratch
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/F9bQBDJGlTU/numpy-neural-networks-computational-graphs.html
date:      2019-08-23
author:      mtdearing
tags:
    - learning
    - steps
    - â
    - gradients
    - bias
---

### Learning Rate

Recall, how we calculated the new bias, above, by moving in the direction opposite of the gradient(i.e. ***gradient descent***).


![](https://miro.medium.com/max/179/1*uVv6QFIJcqGDEiErz5Hujg.png)


Notice that when we updated the bias we moved ***1 step in the opposite direction of the gradient.***


![](https://miro.medium.com/max/198/1*xX63wGFR0ghaH2EPrw-5gA.png)


We could have moved 0.5, 0.9, 2, 3 or whatever fraction of steps we desired in the opposite direction of the gradient. This â€˜*number of stepsâ€™ is what we define as the ****learning rate***, often denoted with*** Î±***(alpha)***.***


![](https://miro.medium.com/max/566/1*XeOLDBlfQoQqN3seB8AVXA.png)


Learning rate defines how quickly we reach the minimum loss. Letâ€™s visualize below what the learning rate is doing:


![](https://miro.medium.com/max/1363/1*BkzqzoHeDgcK_00jBLzjUg.png)


As you can see with a lower learning rate(Î±=0.5) our descent along the curve is slower and we take many steps to reach the minimum point. On the other hand, with a higher learning rate(Î±=5) we take much bigger steps and reach the minimum point much faster.


*The keen-eyed may have noticed that gradient descent steps(green arrows) keep getting smaller as we get closer and closer to the minimum, why is that? Recall, that the learning rate is being multiplied by the gradient at that point along the curve; as we descend away from sloping regions to flatter regions of the u-shaped curve, near the minimum point, the gradient keeps getting smaller and smaller, thus the steps also get smaller. Therefore, changing the learning rate during training is not necessary(some variations of gradient descent start with a high learning rate to descend quickly down the slope and then reduce it gradually, this is called â€œannealing the learning rateâ€�)*


So whatâ€™s the takeaway? Just set the learning rate as high possible and reach the optimum loss quickly. NO. Learning rate can be a double-edged sword. Too high a learning rate and the parameters(weights/biases) donâ€™t reach the optimum instead start to diverge away from the optimum. To small a learning rate and the parameters take too long to converge to the optimum.



![](https://miro.medium.com/max/1136/1*nYFrrV1r4KnAGRMeLA4UNQ.png)


Small learning rate(Î±=5*10â�»Â¹â�°) resulting is numerous steps to reach the minimum point is self-explanatory; multiply gradient with a small number(Î±) results in a proportionally small step.


Large learning rate(Î±=50) causing gradient descent to diverge may be confounding, but the answer is quite simple; note that at each step gradient descent approximates its path downward by moving in straight lines(green arrows in the figures), in short, it estimates its path downwards. When the learning rate is too high we force gradient descent to take larger steps. Larger steps tend to overestimate the path downwards and shoot past the minimum point, then to correct the bad estimate gradient descent tries to move towards the minimum point but again overshoots past the minimum due to the large learning rate. This cycle of continuous overestimates eventually cause the results to diverge(Loss after each training cycle increase, instead of decrease).


Learning rate is whatâ€™s called a ***hyper-parameter***. Hyper-parameters are parameters that the neural network canâ€™t essentially learn through backpropagation of gradients, they have to be hand-tuned according to the problem and its dataset, by the creator of the neural network model. *(The choice of the Loss function, above, is also hyper-parameter)*


In short, the goal is not the find the â€œperfect learning rate â€� but instead a learning rate large enough so that the neural network trains successfully and efficiently without diverging.

