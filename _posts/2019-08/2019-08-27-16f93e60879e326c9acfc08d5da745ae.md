---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://analytixon.com/2019/08/27/if-you-did-not-already-know-833/
date:      2019-08-27
author:      Michael Laux
tags:
    - modeling
    - models
    - paths
    - regularization
    - regularized
---

**Discrete-Time Method of Successive Approximations (MSA)** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529&is-pending-load=1)
![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin’s maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices. … 

**Basis-Path Norm** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529&is-pending-load=1)
![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
Recently, path norm was proposed as a new capacity measure for neural networks with Rectified Linear Unit (ReLU) activation function, which takes the rescaling-invariant property of ReLU into account. It has been shown that the generalization error bound in terms of the path norm explains the empirical generalization behaviors of the ReLU neural networks better than that of other capacity measures. Moreover, optimization algorithms which take path norm as the regularization term to the loss function, like Path-SGD, have been shown to achieve better generalization performance. However, the path norm counts the values of all paths, and hence the capacity measure based on path norm could be improperly influenced by the dependency among different paths. It is also known that each path of a ReLU network can be represented by a small group of linearly independent basis paths with multiplication and division operation, which indicates that the generalization behavior of the network only depends on only a few basis paths. Motivated by this, we propose a new norm \emph{Basis-path Norm} based on a group of linearly independent paths to measure the capacity of neural networks more accurately. We establish a generalization error bound based on this basis path norm, and show it explains the generalization behaviors of ReLU networks more accurately than previous capacity measures via extensive experiments. In addition, we develop optimization algorithms which minimize the empirical risk regularized by the basis-path norm. Our experiments on benchmark datasets demonstrate that the proposed regularization method achieves clearly better performance on the test set than the previous regularization approaches. … 

**PyText** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529&is-pending-load=1)
![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
We introduce PyText – a deep learning based NLP modeling framework built on PyTorch. PyText addresses the often-conflicting requirements of enabling rapid experimentation and of serving models at scale. It achieves this by providing simple and extensible interfaces for model components, and by using PyTorch’s capabilities of exporting models for inference via the optimized Caffe2 execution engine. We report our own experience of migrating experimentation and production workflows to PyText, which enabled us to iterate faster on novel modeling ideas and then seamlessly ship them at industrial scale. … 

**Regularized Empirical Risk Minimization (R-ERM)** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529&is-pending-load=1)
![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning algorithms.![](http://aboutdataanalytics.files.wordpress.com/2015/04/link.png?w=529&is-pending-load=1)
![](http://aboutdataanalytics.files.wordpress.com/2015/04/link.png?w=529)
 Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction … 

### Like this:

Like Loading...
