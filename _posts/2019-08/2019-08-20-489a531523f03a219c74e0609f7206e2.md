---
layout:     post
catalog: true
title:      How to get an AUC confidence interval
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/q8k5PttF3TY/
date:      2019-08-20
author:      Andrew Treadway
tags:
    - auc
    - test_need
    - movies
    - gross
    - budget
---

## **Background**

AUC is an important metric in machine learning for classification. It is often used as a measure of a model’s performance. In effect, AUC is a measure between 0 and 1 of a model’s performance that rank-orders predictions from a model. For a detailed explanation of AUC, see this link.

Since AUC is widely used, being able to get a confidence interval around this metric is valuable to both better demonstrate a model’s performance, as well as to better compare two or more models. For example, if model A has an AUC higher than model B, but the 95% confidence interval around each AUC value overlaps, then the models may not be statistically different in performance. We can get a confidence interval around AUC using R’s **pROC** package, which uses bootstrapping to calculate the interval.

## **Building a simple model to test**

To demonstrate how to get an AUC confidence interval, let’s build a model using a movies dataset from Kaggle ([you can get the data here](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset/downloads/imdb-5000-movie-dataset.zip/1)).

#### **Reading in the data**

#### **Split into train / test**

Next, let’s randomly select 70% of the records to be in the training set and leave the rest for testing.

#### **Create the label**

Lastly, we need to create our label i.e. what we’re trying to predict. Here, we’re going to predict if a movie’s gross beats its budget (1 if so, 0 if not).

#### **Train a random forest**

Now, let’s train a simple random forest model with just 50 trees.

## **Getting an AUC confidence interval**

Next, let’s use our model to get predictions on the test set.

And now, we’re reading to get our confidence interval! We can do that in just one line of code using the *ci.auc* function from **pROC**. By default, this function uses 2000 bootstraps to calculate a 95% confidence interval. This means our 95% confidence interval for the AUC on the test set is between 0.6198 and 0.6822, as can be seen below.

We can adjust the confidence interval using the *conf.level* parameter:

That’s it for this post! Please click here to follow this blog on Twitter!

See here to learn more about the **pROC** package.

The post How to get an AUC confidence interval appeared first on Open Source Automation.


*Related*






---
