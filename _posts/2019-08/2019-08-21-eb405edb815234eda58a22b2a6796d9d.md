---
layout:     post
catalog: true
title:      Comparing Decision Tree Algorithms： Random Forest vs. XGBoost
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/LrUjIKENQWA/activestate-decision-tree-random-forest-xgboost.html
date:      2019-08-21
author:      Matt Mayo Editor
tags:
    - learning
    - scientists
    - tree
    - nlp
    - tutorial
---

This tutorial walks you through a comparison of XGBoost and Random Forest, two popular decision tree algorithms, and helps you identify the best use cases for ensemble techniques like bagging and boosting.

By following the tutorial, you’ll learn:

How to create a decision tree using Python and Pandas
How to do tree bagging with sklearn’s RandomForestClassifier
How to do tree boosting with XGBoost

Understanding the benefits of bagging and boosting—and knowing when to use which technique—will lead to less variance, lower bias, and more stability in your machine learning models. Try it for yourself!


![](http://feedproxy.google.com/images/read-now-red-180.jpg)








|**Most Popular**- **How to Become More Marketable as a Data Scientist**|

![](http://feedproxy.google.com/wp-content/uploads/skills-ds-need-today.jpg)


**12 NLP Researchers, Practitioners & Innovators You Should Be Following**
**How to Become More Marketable as a Data Scientist**
**6 Key Concepts in Andrew Ng’s “Machine Learning Yearning”**
**Understanding Cancer using Machine Learning**
**The Easy Way to Do Advanced Data Visualisation for Data Scientists**
**Learn how to use PySpark in under 5 minutes (Installation + Tutorial)**
**Command Line Basics Every Data Scientist Should Know**


