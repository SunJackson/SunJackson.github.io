---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/23/distilled-news-1172/
date:      2019-08-23
author:      Michael Laux
tags:
    - modelling
    - models
    - modeling
    - scientists
    - building
---

**Econometric Approach to Time Series Analysis – Seasonal ARIMA in Python**

At this post, we will talk about the analysis of time series data with Trend and Seasonal components. An econometric approach will be followed to model the statistical properties of the data. The business objective here is forecasting. We attempted to explain various concepts involved in time series modelling, such as time series components, serial correlation, model fitting, metrics, etc. We will use SARIMAX model provided by statsmodels library to model both, seasonality and trend in the data. SARIMA (Seasonal ARIMA) is capable of modelling seasonality and trend together, unlike ARIMA which can only model trend.

**Working with Google Cloud AutoML in Python**

Its easy to have a CSV file and implement it in various ML models. But, the difficulty lies in implementing the e2e process of getting a video, extracting images, uploading them on Google cloud storage and later performing AutoML on them, entirely using Python. Nowadays, most of the companies have their own in-built models; if not, then they use Google ML models or others.

**ktrain: A Lightweight Wrapper for Keras to Help Train Neural Networks**

ktrain is a library to help build, train, debug, and deploy neural networks in the deep learning software framework, Keras. Inspired by the fastai library, with only a few lines of code, ktrain allows you to easily:• estimate an optimal learning rate for your model given your data using a learning rate finder• employ learning rate schedules such as the triangular learning rate policy, 1cycle policy, and SGDR to more effectively train your model• employ fast and easy-to-use pre-canned models for both text classification (e.g., NBSVM, fastText, GRU with pretrained word embeddings) and image classification (e.g., ResNet, Wide Residual Networks, Inception)• load and preprocess text and image data from a variety of formats• inspect data points that were misclassified to help improve your model• leverage a simple prediction API for saving and deploying both models and data-preprocessing steps to make predictions on new raw data

**Feature Transformation for Machine Learning, a Beginners Guide**

**A Visual Guide to Time Series Decomposition Analysis**

A step-by-step guide to performing additive and multiplicative decomposition. Last time, we talked about the main patterns found in time series data. We saw that, trend, season, and cycle are the most common variations in data recorded through time. However, each of these patterns might affect the time series in different ways. In fact, when choosing a forecasting model, after identifying patterns like trend and season, we need to understand how each one behaves in the series. With this goal in mind, let’s explore two different pre-processing techniques – additive and multiplicative decomposition.

**AI discovers the heart beat in your face**

It can sometimes be hard to see how today’s clunky stethoscopes will turn into tomorrow’s Star Trek tricorders. This post will help you better envision that path by explaining one concrete development in healthcare: a technology that determines your heart rate just from video.

**Discriminating Systems – Gender, Race, and Power in AI**

Research Findings:• There is a diversity crisis in the AI sector across gender and race.• The AI sector needs a profound shift in how it addresses the current diversity crisis.• The overwhelming focus on ‘women in tech’ is too narrow and likely to privilege white women over others.• Fixing the ‘pipeline’ won’t fix AI’s diversity problems.• The use of AI systems for the classification, detection, and prediction of race and gender is in urgent need of re-evaluation.

**Cheap Fakes beat Deep Fakes**

I’ve always been sceptical of deepfakes. What are they good for? I’ve never understood the excitement over the perceived utility of deep fakes for disinformation in information warfare. Information warfare does not need deepfakes, cheapfakes are more than enough. Finally, someone has found a use for deepfakes as offensive cyber tools, so let’s deep dive deep fakes!

**Evaluate Topic Models: Latent Dirichlet Allocation (LDA)**

In the previous article, I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using sklearn implementation. Pursuing on that understanding, in this article, we’ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.

**Hypothesis tests and p-value: a gentle introduction**

Whenever statisticians are asked to make inference on some population parameters, which cannot be observed, they need to start from a representative sample of that population. However, once obtained the estimate of that parameter (called statistic), how can they state whether it corresponds to the real parameter, since the latter is unknown? Because of the impossibility of comparing the two results (again, one is not observable), it will be necessary to make some assumptions and run the so-called hypothesis tests. Those tests are meant to evaluate the likelihood of the estimate of being equal to the real parameter. The idea is that it always exists a situation which can be considered as the default: it is the conservative scenario, the one you’d be better to keep if you are not enough sure of your assumption. This situation will be our Null Hypothesis, or H0. On the other hand, there is the alternative scenario, that, if accepted, will change the status quo ante. It is the Alternative Hypothesis, or H1.

**Several Ways for Machine Learning Model Serving (Model as a Service)**

No matter how well you build a model, no one knows it if you cannot ship model. However, lots of data scientists want to focus on model building and skipping the rest of the stuff such as data ingestion and model serving. DevOps for data scientists is very important. There are multi ways to deliver your model serves as an API for the downstream product team.

**Data Science in Production**

One of my biggest regrets as a data scientist is that I avoided learning Python for too long. I always figured that other languages provided parity in terms of accomplishing data science tasks, but now that I’ve made the leap to Python there is no looking back. I’ve embraced a language that can help data scientists quickly take ideas from conception to prototype to production. And that last term, production, is perhaps the most important aspect in the ever evolving discipline of data science. Knowing how to build machine learning models is useful, but new tools such as AutoML are beginning to commoditize the work of data scientists. Instead of having a data scientist build a bespoke model for a single product, you can now build robust models that scale to a portfolio for products. As new roles emerge, such as applied scientist, with a hybrid of ML engineering and data science competencies, there’s new opportunities for data science.

**Sentiment Analysis of Economic Reports Using Logistic Regression**

Sentiment analysis is a hot topic in NLP, but this technology is increasingly relevant in the financial markets – which is in large part driven by investor sentiment. With so many reports and economic bulletins being generated on a daily basis, one of the big challenges for policymakers is to extract meaningful information in a short period of time to inform policy decisions. In this example, two reports from the European Central Bank website (available from the relevant GitHub repository) are converted into text format, and then a logistic regression is used to rank keywords by positive and negative sentiment. The bulletins in question are sourced from the European Central Bank website.

**Scholarly Network Analysis (SNA)**

Rapid advancements in science and research has lead to an enormous amount of digital scholarly data being produced and collected every day¹. This scholarly data can be in the form of scientific publications, books, teaching materials, and many other scholarly sources of information. Over the course of time, these information sources start to build complex relationships among them through citation, co-authorship, and lead to the formation of Big Scholarly Networks which become increasingly challenging to decode.

**MachineX: Data Cleaning in Python**

In this Blog, we are going to learn about how to do Data Cleaning in Python. Most data scientists spend only 20 percent of their time on actual data analysis and 80 percent of their time finding, cleaning, and reorganizing huge amounts of data, which is an inefficient data strategy. The reason data scientists are hired in the first place is to develop algorithms and build machine learning models – and these are typically the parts of the job that they enjoy most. Yet in most companies today, 80 percent of a data scientist’s valuable time is spent simply finding, cleaning and re-organizing huge amounts of data. If you are just stepping into this field or planning to make your career in this field, it is important to be able to deal with messy data, whether that means missing values, inconsistent formatting, malformed records, or nonsensical outliers. In this tutorial, we are going to use python’s NumPy and Pandas libraries to clean data and see in how many ways we can use them.

### Like this:

Like Loading...
