---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/22/distilled-news-1171/
date:      2019-08-22
author:      Michael Laux
tags:
    - learning
    - articles
    - different data
    - researchers
    - algorithms
---

**Open GDA Score Project**

The GDA Score compares the privacy and utility of different data de-identification methods (including anonymization and pseudonymization) based on the EU Article 29 criteria for anonymity. The goal of the Open GDA Score Project is to build tools that generate GDA Scores for a variety of anonymization methods, to refine and improve the GDA Score methodology, and to serve as a repository for the resulting scores. The Open GDA Score Project is an initiative of the Max Planck Institute for Software Systems (MPI-SWS). We openly solicit participation from researchers and practitioners alike.

**Modern R with the tidyverse**

This book is still being written. Chapters 1 to 8 are almost ready, but more content is being added (especially to chapter 8). 9 and 10 are empty for now. Some exercises might be at the wrong place too and more are coming.

**Using Deep Learning to Classify Relationship State with DeepConnection**

If there is a root domain to the recent explosion in deep learning, it’s certainly computer vision, the analysis of image and video data. So it doesn’t come as a huge surprise that you try your luck with some computer vision techniques while studying deep learning. Long story short, my partner (Maximiliane Uhlich) and I decided to apply this form of deep learning to images of romantic couples because Maximiliane is a relationship researcher and couple therapist. Specifically, we wanted to find out whether we can accurately tell if any given couple, depicted on an image or video, is happy in their relationship or not? Turns out, we can! With a classification accuracy of nearly 97 percent, our final model (which we dubbed DeepConnection) was able to clearly differentiate between unhappy and happy couples. You can read the full story in our preprint, what follows here is a rough sketch of what we did.

**Machine Learning Algorithms for Every Occasion**

A machine learning algorithm is a method that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Algorithms like linear regression, deep learning, convolutional neural networks and recommendation systems are widely being used and explained. It is easy to get lost in this ocean of information. Understanding which algorithm would suit any given situation becomes a winning quality. Generally this role is played by a product manager. Regardless of who calls the shots, it is important to understand that different situations demand different algorithms.

**Overcoming the Barriers to Production-Ready Machine Learning Workflows**

<Slides>

**Breaking Down Review Articles**

Review or survey articles are a great way to become oriented with a new field. They include historical information about the original problems in the field, along with past research efforts to try and solve them. Current methods are presented in a way to compare them with other methods in use. Having well-identified problem areas can help you figure out what you should focus your efforts on based on your situation or interests. Moreover, the best part of review articles is that they act as a well-curated list of references that you can use to learn more about the specifics of the method. Now I want you to walk you through how I break down review articles for both my Ph.D. research and my interests.

**Machine learning on categorical variables**

At first blush, categorical variables aren’t that different from numerical ones. But once you start digging deeper and implement your machine learning (and preprocessing) ideas in code, you will stop every minute asking questions such as ‘Do I do feature engineering on both train and test sets?’ or ‘I heard something about cardinality – what is that and should I Google more about it?’ Let’s see if we can clear some of this up with an action plan for how you deal with data sets that have a lot of categorical variables and train a couple of models.

**Easy Steps To Plot Geographic Data on a Map – Python**

Assume that you are working in a startup and you need to conduct spatial data analysis and prediction to users’ geographical data. Or your company runs a lot of delivery operations and your job again to analyze, visualize and maybe predict the drivers or users’ geographical data. So, visualizing your data (predicted ones maybe) on a map will be very necessary. In this article, I will go through easy steps of how to plot geographic data on any map using Python. The thing that I found it very useful and helpful in my previous projects using the same language: Python- check my article: Spatial Data Analysis for Traffic Management.

**An introduction to frequent pattern mining research**

Frequent patterns are collections of items which appear in a data set at an important frequency (usually greater than a predefined threshold)and can thus reveal association rules and relations between variables. Frequent pattern mining is a research area in data science applied to many domains such as recommender systems (what are the set of items usually ordered together), bioinformatics (what are the genes co-expressed in a given condition), decision making, clustering, website navigation.

**Profiling Matrix Factorization and Recommendation Engines with Python**

In my previous blog, I narrated about different matrix factorization techniques, citing pros and cons of each of the libraries. In this blog, I will give some code samples of some of the previously discussed libraries. We will also see the profiling statistics of some of the python libraries (for SVD/Recommendation Engine) so as to understand the time taken to execute the training engine, predict the model and finally providing the recommendation. In the end, we will also see new research and emerging applications in the field of Matrix Factorization.

**Local Model Interpretation: An Introduction**

This article is a continuation of my series of articles on Model Interpretability and Explainable Artificial Intelligence. If you haven’t read the first two articles I would highly recommend you to do so. The first article of the series, ‘Introduction to Machine Learning Model Interpretation’, covers the basics of Model Interpretation. The second article, ‘Hands-on Global Model Interpretation’, goes over the details of global model interpretation and how to apply it to a real-world problem using Python. In this article, we will pick up where we left off by diving into local model interpretation. First we will take a look at what local model interpretation is and what it can be used for. Then we will dive into the theory of two of it’s most popular methods, Lime (Local Interpretable Model-Agnostic Explanations) and Shapley Value, and apply them to get information about individual predictions on the heart disease data-set.

**Measures of Proximity in Data Mining & Machine Learning**

In one of my previous posts, I talked about Assessing the Quality of Data for Data Mining & Machine Learning Algorithms. This will continue on that, if you haven’t read it, read it here in order to have a proper grasp of the topics and concepts I am going to talk about in the article. The term proximity between two objects is a function of the proximity between the corresponding attributes of the two objects. Proximity measures refer to the Measures of Similarity and Dissimilarity. Similarity and Dissimilarity are important because they are used by a number of data mining techniques, such as clustering, nearest neighbour classification, and anomaly detection. We will start the discussion with high-level definitions and explore how they are related. Then, we move forward to talk about Proximity in two data objects with one simple attribute and moving to objects with multiple attributes.

**Converting a Deep Learning Model with Multiple Outputs from PyTorch to TensorFlow**

Some time back, I wrote an article describing how you could convert a simple deep learning model from PyTorch to TensorFlow using ONNX. Although this is applicable to many use cases, there are situations where you would need to convert a model with multiple outputs (e.g. object detection models). Here I would like to show how you could convert a model with multiple outputs from PyTorch to TensorFlow. Many of the parts of the whole conversion process is a repetition of what I have mentioned in my previous article, so I will only mention the parts which are added or modified for the conversion of a model with multiple outputs.

**Probability of an Approaching AI Winter**

This article addresses the question of whether the field of Artificial Intelligence (AI) is approaching another AI winter or not. Both industries and governments alike have invested significantly in the AI field, with many AI-related startups established in the last 5 years. If another AI winter were to come about many people could lose their jobs, and many startups might have to shut down, as has happened before. Moreover, the economic difference between an approaching winter period or ongoing success is estimated to be at least tens of billions of dollars by 2025, according to McKinsey & Company. This paper does not aim to discuss whether progress in AI is to be desired or not. Instead, the purpose of the discussions and results presented herein is to to inform the reader of how likely progress in AI research is.

**The Branch and Bound Algorithm**

Most of you have probably heard of mixed integer programming, or discrete optimization that is a bit more general. In this article we are going to talk about the algorithm that is the driving force behind mixed integer programmming, the branch and bound algorithm.

**Better Simulations will revolutionize Machine Learning**

How simulations will solve the biggest problem in ML. There is no doubt that Machine Learning often feels magical. As a Machine Learning Engineer myself, I am still fascinated when my model does solve some very hard high-dimensional problem that would have been unsolvable otherwise. I am convinced that data-driven solutions will solve the most challenging problems in the future like self-driving cars, and that Software 2.0 will play a very important role. The performance of those algorithms highly depends on your data though. If you have ever trained a Neural Network yourself, you will quickly find out that you are limited by both the quality and the quantity of your data.

**Reinforcement Learning — Generalisation of Off-Policy Learning**

Till now, we have extended our reinforcement learning topic from discrete state to continuous state and have elaborated a bit on applying tile coding to on-policy learning, that is the learning process follows the trajectory the agent takes. Now let’s have a talk of off-policy learning in continuous settings. While in discrete settings, on-policy learning can easily be generalised to off-policy learning(say, from Sarsa to Q-learning), in continuous settings, the generalisation can be a little troublesome, and in some scenarios can cause divergence issues. In this article, I will:• Introduce the problems caused by the extension to off-policy learning• Illustrate the problem by giving the Baird counter example• Introduce a potential method to solve the problem

**AI for Industrial Process Control**

Determining optimal control settings for an industrial process can be tough. For instance, controls can interact, where adjusting one setting requires readjustment of other settings. Also, the relationship between a control and its effect can be very complex. Such complications can be challenging for optimizing a process. This article explores a reinforcement learning solution for controlling an industrial conveyor oven.

### Like this:

Like Loading...
