---
layout:     post
catalog: true
title:      Finding out why
subtitle:      转载自：https://analytixon.com/2019/08/28/finding-out-why-36/
date:      2019-08-28
author:      Michael Laux
tags:
    - methods
    - paper
    - populations
    - data
    - causally
---

***Paper***: ***Bayesian Hierarchical Factor Regression Models to Infer Cause of Death From Verbal Autopsy Data***

In low-resource settings where vital registration of death is not routine it is often of critical interest to determine and study the cause of death (COD) for individuals and the cause-specific mortality fraction (CSMF) for populations. Post-mortem autopsies, considered the gold standard for COD assignment, are often difficult or impossible to implement due to deaths occurring outside the hospital, expense, and/or cultural norms. For this reason, Verbal Autopsies (VAs) are commonly conducted, consisting of a questionnaire administered to next of kin recording demographic information, known medical conditions, symptoms, and other factors for the decedent. This article proposes a novel class of hierarchical factor regression models that avoid restrictive assumptions of standard methods, allow both the mean and covariance to vary with COD category, and can include covariate information on the decedent, region, or events surrounding death. Taking a Bayesian approach to inference, this work develops an MCMC algorithm and validates the FActor Regression for Verbal Autopsy (FARVA) model in simulation experiments. An application of FARVA to real VA data shows improved goodness-of-fit and better predictive performance in inferring COD and CSMF over competing methods. Code and a user manual are made available at https://github.com/kelrenmor/farva.

***Paper***: ***Deterministic epidemic models for ebola infection with time-dependent controls***

In this paper, we have studied epidemiological models for Ebola infection using nonlinear ordinary differential equations and optimal control theory. We considered optimal control analysis of SIR and SEIR models for the deadly Ebola virus infection using vaccination, treatment and educational campaign as time-dependent controls functions. We have applied indirect methods to study existing deterministic optimal control epidemic models for Ebola virus disease. These methods in optimal control are based on Hamiltonian function and the Pontryagin maximum principle to construct adjoint equations and optimality systems. The forward-backward sweep numerical scheme with fourth-order Runge-Kutta method is used to solve the optimality system for the various optimal control strategies. From our simulation results, we observed that, SIR model with optimal control strategies shows a significant decrease in the proportions of infected and susceptible individuals and a rapid increase in the recovered individuals compared to SIR model without optimal control. A similar effect was observed in the SEIR model with control strategies. Following the numerical solutions, we can conclude that, effective educational campaigns and vaccination of susceptible individuals as were as effective treatments of infected individuals can help reduce the disease transmission.

***Article***: ***Cause Effect Graphs***

In software testing, a cause-effect graph is a directed graph that maps a set of causes to a set of effects. The causes may be thought of as the input to the program, and the effects may be thought of as the output. Usually the graph shows the nodes representing the causes on the left side and the nodes representing the effects on the right side. There may be intermediate nodes in between that combine inputs using logical operators such as AND and OR. Constraints may be added to the causes and effects. These are represented as edges labeled with the constraint symbol using a dashed line. For causes, valid constraint symbols are E (exclusive), O (one and only one), I (at least one), and R (Requires). The exclusive constraint states that at most one of the causes 1 and 2 can be true, i.e. both cannot be true simultaneously. The Inclusive (at least one) constraint states that at least one of the causes 1, 2 or 3 must be true, i.e. all cannot be false simultaneously. The one and only one (OaOO or simply O) constraint states that only one of the causes 1, 2 or 3 can be true. The Requires constraint states that if cause 1 is true, then cause 2 must be true, and it is impossible for 1 to be true and 2 to be false. For effects, valid constraint symbol is M (Mask). The mask constraint states that if effect 1 is true then effect 2 is false. Note that the mask constraint relates to the effects and not the causes like the other constraints.

***Paper***: ***Regression Analysis of Unmeasured Confounding***

When studying the causal effect of $x$ on $y$, researchers may conduct regression and report a confidence interval for the slope coefficient $\beta_{x}$. This common confidence interval provides an assessment of uncertainty from sampling error, but it does not assess uncertainty from confounding. An intervention on $x$ may produce a response in $y$ that is unexpected, and our misinterpretation of the slope happens when there are confounding factors $w$. When $w$ are measured we may conduct multiple regression, but when $w$ are unmeasured it is common practice to include a precautionary statement when reporting the confidence interval, warning against unwarranted causal interpretation. If the goal is robust causal interpretation then we can do something more informative. Uncertainty in the specification of three confounding parameters can be propagated through an equation to produce a confounding interval. Here we develop supporting mathematical theory and describe an example application. Our proposed methodology applies well to studies of a continuous response or rare outcome. It is a general method for quantifying error from model uncertainty. Whereas confidence intervals are used to assess uncertainty from unmeasured individuals, confounding intervals can be used to assess uncertainty from unmeasured attributes.

***Paper***: ***Online Inference for Advertising Auctions***

Advertisers that engage in real-time bidding (RTB) to display their ads commonly have two goals: learning their optimal bidding policy and estimating the expected effect of exposing users to their ads. Typical strategies to accomplish one of these goals tend to ignore the other, creating an apparent tension between the two. This paper exploits the economic structure of the bid optimization problem faced by advertisers to show that these two objectives can actually be perfectly aligned. By framing the advertiser’s problem as a multi-armed bandit (MAB) problem, we propose a modified Thompson Sampling (TS) algorithm that concurrently learns the optimal bidding policy and estimates the expected effect of displaying the ad while minimizing economic losses from potential sub-optimal bidding. Simulations show that not only the proposed method successfully accomplishes the advertiser’s goals, but also does so at a much lower cost than more conventional experimentation policies aimed at performing causal inference.

***Paper***: ***Nonparametric estimation of causal heterogeneity under high-dimensional confounding***

This paper considers the practically important case of nonparametrically estimating heterogeneous average treatment effects that vary with a limited number of discrete and continuous covariates in a selection-on-observables framework where the number of possible confounders is very large. We propose a two-step estimator for which the first step is estimated by machine learning. We show that this estimator has desirable statistical properties like consistency, asymptotic normality and rate double robustness. In particular, we derive the coupled convergence conditions between the nonparametric and the machine learning steps. We also show that estimating population average treatment effects by averaging the estimated heterogeneous effects is semi-parametrically efficient. The new estimator is an empirical example of the effects of mothers’ smoking during pregnancy on the resulting birth weight.

***Paper***: ***Population-aware Hierarchical Bayesian Domain Adaptation via Multiple-component Invariant Learning***

Observational transport relates to transferring a statistical relation $\mathcal{R}(P)$ from environment ($\pi$) characterized by probability distribution $P$ and causal diagram $G$ to another environment ($\pi^*$) characterized by $P^*$, $G^*$. In doing so it is expected that the causal mechanism is known a priori and the relation to be transferred is learned from both the source environment consisting of variables $V$ and the target environment comprised of variables $V^*$. The causal diagram helps to identify what part of the statistical relation $\mathcal{R}(P)$ (invariant information) is transportable from the source environment ($\pi$) while also identifying the target environment ($\pi^*$) specific relation $\mathcal{R}(P^*)$ which is learned empirically from the available data. While domain adaptation is a common technique to learn the invariant information across the entire population (dataset), in health care/consumer transactions there is invariant information based on population subgroups. A further issue is that all subgroups are not equally represented across the different environments resulting into selection bias. We posit that we can combine the environment and population invariant information in a novel multi-component population-aware hierarchical domain adaptation Bayesian framework in the presence of the selection bias. We also study the conditions under which invariant learning fails; leading to reliance on the environment-specific attributes. Experimental results on real-world data for influenza prediction show the model can improve prediction in the case of largely unlabelled target data by harnessing both domain and population invariant information, with implications for human-generated data, fair algorithms and human well-being.

***Paper***: ***Efficient and robust methods for causally interpretable meta-analysis: transporting inferences from multiple randomized trials to a target population***

We present methods for causally interpretable meta-analyses that combine information from multiple randomized trials to estimate potential (counterfactual) outcome means and average treatment effects in a target population. We consider identifiability conditions and obtain identification results for transporting causal inferences from a collection of independent randomized trials to a new target population in which experimental data are not available. We propose estimators for potential outcome means and average treatment effects in the target population that use covariate, treatment, and outcome data from the collection of trials, but only covariate data from the target population sample. We show that the estimators are doubly robust, in the sense that they remain consistent and asymptotically normal under misspecification of some of the working models on which they rely. We study the finite sample properties of the estimators in simulation studies and demonstrate their implementation using data from a multi-center clinical trial.

### Like this:

Like Loading...
