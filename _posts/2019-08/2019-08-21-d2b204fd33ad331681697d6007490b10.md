---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/21/distilled-news-1170/
date:      2019-08-21
author:      Michael Laux
tags:
    - models
    - predictive
    - predicting
    - data
    - predictions generated
---

**The Bayesian Brain Hypothesis**

Life is ridden with uncertainty, and no one can tell the future. According to Blaise Pascal, we sail within a vast sphere, ever drifting in uncertainty, driven from end to end. No one knows when death might come, when life will throw hardships at us, when life will reward us. While we all have to learn this dreary lesson at some point in our lives, we nevertheless manage admiringly well to prevail in a universe shaped by uncertainty. We build houses, we put money into our bank account, we save up for our retirement fund and for our grandchildren. We shape stable relationships and build monuments to last lifetimes. We have a sense of control about what’s going on and we deserve to have it. This is pretty remarkable for something such as us, who came to exist out of the random and chaotic fancies of evolution. How did we develop the ability to carve a sense of certainty out of a future defined by uncertainty?

**Mish: A Self Regularized Non-Monotonic Neural Activation Function**

A novel, robust and efficient activation function. After 4 months of experimentation and research, I have finally released my novel activation function – Mish. Inspired from Google Brain’s Paper – Swish: A Self-Gated Activation Function, Mish is an easy to implement and intuitively simple activation function. Before moving on to the details and the math under the hood, let’s do a quick detour of Activation Functions and look at where and why they are used.

**Why Machine Learning is more Practical than Econometrics in the Real World**

In this article, I am going to show you an experiment I ran that compares machine learning models and Econometrics models for time series forecasting on an entire company’s set of stores and departments. Before I kick this off, I have to mention that I’ve come across several articles that describe how one can utilize ML for forecasting (typically with deep learning models) but I haven’t seen any that truly gives ML the best chance at outperforming traditional Econometric models. On top of that, I also haven’t seen too many legitimate attempts to showcase the best that Econometric models can do either. That’s where this article and evaluation differ. The suite of functions I tested are near-fully optimized versions of both ML models and Econometric models (list of models and tuning details are below). The functions come from the R open source package RemixAutoML, which is a suite of functions for automated machine learning (AutoML), automated forecasting, automated anomaly detection, automated recommender systems, automated feature engineering, and more. I provided the R script at the bottom of this article so you can replicate this experiment. You can also utilize the functions in Python via the r2py package and Julia via the RCall package.

**The Natural Roots of Artificial Intelligence**

In this article, I share my explorations of the literature on the foundations of artificial intelligence (AI) and its interrelationships with intelligence. I will outline how AI differs from, but is rooted in, conceptions of ‘natural’ intelligence. I have done my best to present technical concepts in a simplified and representative manner.

**How Concerned Should You be About Predictor Collinearity? It Depends…**

This past Northern Hemisphere summer I gave several talks (some in the Southern Hemisphere) in which one of the Q&A topics was the problem of collinearity between predictor variables (also known as multicollinearity). My stock response to a question on this topic was (and is) to reply with the clarifying question ‘How many rows do you have to develop the model?’ If the follow-up response was in the tens of thousands, my counter response was ‘Don’t worry about collinearity.’ In contrast, if the audience member’s response was a few hundred rows or less, my response was ‘Very!’ While these two different responses may seem contradictory, they actually are not. I’ve found that many people who build predictive models know from the instruction they have received that predictor collinearity is ‘bad’, but they often don’t have the intuition behind why it is bad, and when it will be relatively more or less bad. The goal of this post is to provide you with some intuition behind the problems associated with predictor collinearity, and provide some rules of thumb about when, and when not, to be concerned.

**The Transformer: Attention Is All You Need**

The Transformer paper, ‘Attention is All You Need’ is the #1 all-time paper on Arxiv Sanity Preserver as of this writing (Aug 14, 2019). This paper showed that using attention mechanisms alone, it’s possible to achieve state-of-the-art results on language translation. Subsequent models built on the Transformer (e.g. BERT) have achieved excellent performance on a wide range of natural language processing tasks.

**Insurance Data Science: Networks**

At the Summer School of the Swiss Association of Actuaries, in Lausanne, I will start talking about networks and insurance this Friday. Slides are available online

**Predicting the Effect of More Training Data, by Using Less**

A step-by-step guide to estimating the value of increased training data, using a case study from geospatial deep learning. Here’s the scenario: You’ve gone to great difficulty and expense to collect some training data, and you’ve used that data to train a deep neural net. Still, you find yourself wondering, ‘Do I have enough?’ Would more training data cause a meaningful performance boost, or just waste time and money for an inconsequential gain? At first, it seems almost paradoxical: You can’t make an informed decision about gathering more data without an idea of how much it will help, but you can’t measure how much it will help unless you’ve made the decision and acquired the additional data already. There is a solution, however. By repeatedly retraining a deep neural net with different-sized subsets of the training data that’s already in hand, it may be possible to extrapolate performance out to quantities of training data beyond what’s presently available. We’ll walk through the procedure using an important test case from the field of computer vision: tracing out the outlines of buildings in satellite photos. The ability to automatically map buildings, without relying on extensive manual labor every time things change, has applications ranging from disaster relief to urban planning.

**Cost Functions: The Underpinnings of Machine Learning**

This pivotal concept determines your ability to fit models to data. Fitting data sets to models is the basis of machine learning and data science. In this way we are able to create predictive tools that help people make important decisions. This is typically done by minimizing the difference between the predictions generated by the model and the actual data in the model training, validating, and testing phases. To do this, we must be able to identify the difference between the model prediction and the actual values of the data. To fit the model to the data and create a valuable prediction tool, we must be able to rapidly calculate that difference using a minimizable function. This is the goal of the cost function.

**Spectral Graph Convolution Explained and Implemented Step By Step**

As part of the ‘Tutorial on Graph Neural Networks for Computer Vision and Beyond’. First, let’s recall what is a graph. A graph G is a set of nodes (vertices) connected by directed/undirected edges. In this post, I will assume an undirected graph G with N nodes. Each node in this graph has a C-dimensional feature vector, and features of all nodes are represented as an N×C dimensional matrix X???. Edges of a graph are represented as an N×N matrix A, where the entry A?? indicates if node i is connected (adjacent) to node j. This matrix is called an adjacency matrix.

**Simulating stock prices in Python using Geometric Brownian Motion**

It would be great if we can precisely predict how stock prices will change in near or far future. We would be rich, but it is almost impossible to create exact predictions. There are so many factors involved in the movement of stock prices that are hard to model. Human psychology is one of them. Investors, for sure, make their decisions based on empirical evidence and stock market indicators. However, they are still humans. Different people can interpret data differently, both because of their risk appetite and their mood at the moment. While a piece of breaking news in the country causes an investor to buy a stock, it causes another one to sell that same stock. Therefore, predicting stock prices is a difficult job, but we still have valuable tools which can help us to understand the stock price movement up to some point. In this article, we discuss how to construct a Geometric Brownian Motion(GBM) simulation using Python. While building the script, we also explore the intuition behind the GBM model. I will not be getting into the theoretical background of its derivation. It’s beyond the scope of this article. I care more about giving a high-level understanding of what GBM needs as parameters, what its components are and how it creates predictions. I will try to have a bottom-up approach and build up the logic of GBM starting from its components. The simulation model we develop here is a discrete-time model. Therefore, all mathematics discussed here is the discrete-time analogy of Geometric Brownian Motion for continuous stochastic processes. At the end of this article, we learn how to create simulations using GBM and you will have a full code.

**10 Bits: the Data News Hotlist**

1. Making School Bus Routes More Efficient2. Predicting Food Product Recalls3. Using AI to Help Fish Cross Dams4. Detecting Diseased Bananas5. Analyzing Calls to Detect Fraud6. Training an AI Model in Record Time7. Estimating the Solar Power Potential of Roofs8. Creating a More Rigorous Benchmark for AI Models9. Tracking Drug Abuse in Real-Time10. Proving Biology’s Oldest Mathematical Model

**Pytorch Lightning vs PyTorch Ignite vs Fast.ai**

PyTorch-lightning is a recently released library which is a Kera-like ML library for PyTorch. It leaves core training and validation logic to you and automates the rest. (BTW, by Keras I mean no boilerplate, not overly-simplified). As the core author of lightning, I’ve been asked a few times about the core differences between lightning and fast.ai, PyTorch ignite. Here, I will attempt an objective comparison between all three frameworks. This comparison comes from laying out similarities and differences objectively found in tutorials and documentation of all three frameworks.

**Understanding Cancer using Machine Learning**

Use of Machine Learning (ML) in Medicine is becoming more and more important. One application example can be Cancer Detection and Analysis.

**Can your People Analytics do this?**

**4 Product-Driven Steps to an AI Roadmap**

Artificial Intelligence (AI) is regularly breaking new ground, from DeepMind’s AlphaGo Zero teaching itself to play Go and beating human champions to text-generating algorithms so powerful that their creators at OpenAI decided not to release them publicly for fear of malicious use. While these accomplishments are rightfully generating enormous buzz and inspiring entrepreneurs and investors, most of them lack a specific business application. If AI is in fact a transformative technology, it raises a big question: How can your company use AI to create actual business value, today? Most AI business applications focus on automation and personalization. But automate what? And personalize how? Technology has automated many previously labor-intensive processes. What makes AI special? To understand this question, we must first answer a more fundamental one: what kind of products can you build with AI that you can’t build without it? The answer is simple. With AI, you can build products that make decisions.

### Like this:

Like Loading...
