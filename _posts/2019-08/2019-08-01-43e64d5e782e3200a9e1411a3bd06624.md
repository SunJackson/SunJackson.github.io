---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/01/distilled-news-1149/
date:      2019-08-01
author:      Michael Laux
tags:
    - projects
    - models
    - data science
    - learning
    - neural network
---

**It’s time to think more about the pipeline**

It is always exciting to see that data science is consistently concerned with improving algorithms and techniques to analyze the booming data to extract patterns. These patterns can create new insights or become new decision-making methods, and these patterns are otherwise hard to learn without the help of the data science process. The process, however, often requires the skills and experiences from the practitioners. They should be able to gather the sufficient and appropriate dataset, clean and preprocess it, pick and compare multiple models, tune the parameters of the models, and articulate the results. This chain of processing elements constitutes the pipeline of data analysis. While many data scientists are eagerly improving each element (more often the model, which is similar to the production element in a pipeline and thus is the fun part), it is also important to promote the pipeline design. Therefore, an automatic pipeline can not only be assistant to the data scientists but also be more accessible to the non-experts.

**Convolution vs. Cross-Correlation**

This post will overview the difference between convolution and cross-correlation. This post is the only resource online that contains a step-by-step worked example of both convolution and cross-correlation together (as far as I know – and trust me, I did a lot of searching). This post also deals precisely with indices, which it turns out are critical to get right if you want to demonstrate by example how convolution and cross-correlation are related. I spent a large part of preparing this post tearing my hair out over indices, but now they are all beautiful and organized for you to enjoy. First, a little motivation on this topic…

**How to Communicate Clearly About Machine Learning.**

It’s no secret that the field of machine learning is considered heavy on theory: Math, statistics and computer science, mainly. While practitioners tend to enjoy the depth, colleagues and clients ‘outside the bubble’ find it difficult to participate in the conversation. That results in bad communication.

**Differentiable Programming – Inverse Graphics AutoEncoder**

DeepLearning classifier, LSTM, YOLO detector, Variational AutoEncoder, GAN – are these guys truly architectures in sense meta-programs or just wise implementations of ideas on how to solve particular optimization problems? Are machine learning engineers actually developers of decision systems or just operators of GPU-enabled computers with a predefined parameterized optimization program? Differentiable Programming can answer these and some other questions like to explain the model and to stop heat our planet with brute-force end-to-end DNNs. Data Scientists already do a lot of custom programming to clean, filter and normalize data before passing it into a standard neural network block or to stack models for a better result. Now it is time to do custom programming after the standard NN block and this programming has to be differentiable.

**Fantastic Four of Data Science Project Preparation**

This article takes a closer look at the four fantastic things we should keep in mind when approaching every new data science project.

**Breaking BERT Down**

BERT is short for Bidirectional Encoder Representations from Transformers. It is a new type of language model developed and released by Google in late 2018. Pre-trained language models like BERT play an important role in many natural language processing tasks, such as Question Answering, Named Entity Recognition, Natural Language Inference, Text Classification etc. BERT is a multi-layer bidirectional Transformer encoder based on fine-tuning. At this point it is important to introduce the Transformer architecture.

**Creating Reproducible Data Science Projects**

Imagine you completed a one-off analysis a few months ago, creating a fairly complex data pipeline, machine learning model and visualisations. Fast forward to today and you have Emily, a senior executive at your company, asking you to reuse that work to help solve a similar, time-critical business problem. She looks stressed. Now if only you could remember which copy of your model was the correct one; if you could make sense of the spaghetti code scattered throughout your Jupyter Notebooks, each with helpful names such as Untitled_1 and Untitled_2; what did the data_process function do and why are there six slightly different versions of it? If only there was some documentation! ‘No problem!’ you assure her, and after a few sleepless nights, during which you had to reverse engineer the entire codebase, the analysis is ready. Emily looks impressed, that promotion you’ve been waiting for might finally happen.

**In the New Era of Knowledge, Connection Beats Collection Every Time**

Look at the data – the numbers don’t lie.’ It’s an often given piece of advice, but a less often understood one. Because what the person giving the advice really means is ‘Look at the data, and think about what it means for the situation we’re facing. Once you consider it in the broader context, you’ll see – and intuitively know – what to do next.’ That’s a very different challenge to meet, but an infinitely more valuable one when you’re trying to make sense of a complex situation. And it’s possible only when you realise that connecting your data is even more important than collecting it.

**Lightweight Visualization of Keras Models**

I love how simple and clear Keras makes it to build neural networks. It’s not hard to connect Keras to Tensorboard but that has always felt to me like a heavyweight solution is overly complicated for many of Keras’s users who often want to take a quick look at the underlying model. With wandb, you can visualize your network’s performance and architecture with a single extra line of python code. To show how this works, I modified a few scripts in the Keras examples directory. To install wandb, just run ‘pip install wandb’ and all of my Keras examples should work for you.

**High-Quality AI And Machine Learning Data Labeling At Scale: A Brief Research Report**

Analyst firm Cognilytica estimates that as much as 80% of machine learning project time is spent on aggregating, cleaning, labeling, and augmenting machine learning model data. So, how do innovative machine learning teams prepare data in such a way that they can trust its quality, cost of preparation, and the speed with which it’s delivered?

**Sentiment Analysis: a benchmark**

This article gently introduces recurrent units, how their memory works and how they are used to handle sequence data such as text. With hands-on practical Python code, we demonstrate limitations of simple recurrent neural networks and show how embeddings improve fully connected neural networks and convolutional neural networks for the IMDb movie reviews classification task.

**A Gentle Introduction to Noise Contrastive Estimation**

Find out how to use randomness to learn your data by using Noise Contrastive Estimation with this guide that works through the particulars of its implementation.

**Machine Learning for Content Moderation – Challenges**

Now that we have gone over an overview of machine learning systems used for automatic content moderation, we can address the main challenges faced by these systems. These potential problems can lead to difficulties in evaluating the model, determining approaching classifier thresholds, and using it fairly and without unintentional bias. Since content moderation systems act on complex social phenomena, they face problems that are not necessarily encountered in other machine learning contexts.

**Time Series Clustering and Dimensionality Reduction**

Time Series must be handled with care by data scientists. This kind of data contains intrinsic information about temporal dependency. it’s our work to extract these golden resources, where it is possible and useful, in order to help our model to perform the best. With Time Series I see confusion when we face a problem of dimensionality reduction or clustering. We are used to think about these tasks in more classical domains, while they remain a tabù when we deal with Time Series. In this post, I try to clarify these topics developing an interesting solution where I work with multidimensional Series coming from different individuals. Our purpose is to cluster them in an unsupervised way making use of deep lerning, being wary of correlations, and pointing an useful tecnique that every data scientists must know!

**GPU Accelerated Data Analytics & Machine Learning**

The future is here! Speed up your Machine Learning workflow using Python RAPIDS libraries support.

**Classification of unbalanced datasets**

Let’s imagine you have a dataset with a dozen features and need to classify each observation. It can be either a two-class problem (your output is either 1 or 0; true or false) or a multi-class problem (more than two alternatives are possible). In this case, however, there is a twist. The data is unbalanced. Think about patients who may or may not have cancer (the majority probably won’t) or a decision to extend a credit line (the majority of bank clients get an extension). Your machine learning algorithm will be ‘overexposed’ to one class, and ‘underexposed’ to another. There are numerous articles online about this problem with different approaches. Here I will combine a few of them to get a robust solution. We’ll start with more straight-forward linear models, then add a couple of tweaks, move to boosted trees, and finally to neural networks.

**Hands On Memory-Augmented Neural Networks Build(Part One)**

Memory-Augmented Neural Networks(MANN), which can be used for one-shot learning, in this post, I’ll show you how to build a Memory-augmented Neural Network. Before we start building our MANN, I wanna illustrate its predecessor, Neural Turing Machine(NTM). In this part, I gonna show you how NTMs make use of external memory for storing and retrieving information. So, let’s jump into NTM.

**How Do You Organise Your R Project? This Is What We Do.**

The Biometrics group at Telethon Kids Institute uses a standardised template project directory to manage our biostatistical consultation projects. This approach allows us to streamline our workflow, initiate projects, and produce professional looking reports directly from the statistical analysis platform minimising the time spent on the non-analytical aspects of our projects. This project structure is identical and successful for both simple and large-scale projects. Although this workflow has been developed in R, the broader principles discussed here are applicable to any scripting language and non-coding projects alike. This blog is based on recent presentations that I made at the UserR!2019, Toulouse, France (lightning talk) and the 40th Annual Conference of the International Society for Clinical Biostatistics (ISCB40), Leuven, Belgium (winner of best poster award) conferences in July 2019.

**Manifolds and Neural Activity: An Introduction**

Manifolds are important objects in mathematics and physics because they allow more complicated structures to be expressed and understood in terms of simpler spaces. This is a key motivation to connect this theory with neuroscience to understand and interpret complex neural activity.

### Like this:

Like Loading...
