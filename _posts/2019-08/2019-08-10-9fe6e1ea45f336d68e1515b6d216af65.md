---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/10/distilled-news-1158/
date:      2019-08-10
author:      Michael Laux
tags:
    - data
    - modeling
    - models
    - learning
    - python
---

**The Influence Of Data Scaling On Machine Learning Algorithms**

Preprocessing data involves transforming and scaling the data, up or down, before it becomes used for further steps. Every so often attributes are not expressed by the same standards, scales or measures, to such an extent that their statistics yield distorted data modeling results. For instance, K-Means Clustering algorithm is not scale invariant; it computes the space between two points by the Euclidean distance. To refresh one’s memory on the concept of Euclidean distance – is the non-negative value difference between two points in one-dimensional space. So, if one of the attributes has a broad range of values, the computed distance will be skewed by this attribute (i.e. smaller valued attribute will contribute very little). For example, if one of the attributes is measured in centimeters, and one then decided to convert the measure to millimeters (i.e. multiplying the values by 10), the resulting Euclidean distance can be significantly affected. To have the attribute adds approximately proportionately to the final computed distance, the range of that attribute should be normalized.

**11 Important Model Evaluation Metrics for Machine Learning Everyone should know**

• Confusion Matrix• F1 Score• Gain and Lift Charts• Kolmogorov Smirnov Chart• AUC – ROC• Log Loss• Gini Coefficient• Concordant – Discordant Ratio• Root Mean Squared Error• Cross Validation (Not a metric though!)

**Automated Machine Learning for Professionals – Updated**

As the Automated Machine Learning (AML) movement got underway a few years back there was an early branch between proprietary platforms and open source platforms. In this article we’ll update you on leading open source AML tools. Since they continue to require fluency in Python or R we label them ‘professional’.

**Data literacy matters. Do we have to spell it out?!**

In the words of Thomas Edison, the value of an idea lies in the using of it. Yeah, I’ve used that quote a lot to talk about the value of data. Data isn’t inherently valuable. But when you use it to better understand customers or to identify bottlenecks in a process or to detect fraud or revenue leakage, then it’s valuable. More simply put, Data + Use = Value. Another quote I repeat over and over. Yet according to our new Business Technographics Data and Analytics Survey 2019, on average, only 48.3% of decisions are made based on quantitative information and analysis as compared to other more qualitative decision factors such as experience, ‘gut feeling,’ or opinions. That number hasn’t really moved in years. Back in 2016, survey respondents even reported that 48.9% of decisions were made using quantitative analysis. We’ve really made little progress on that front over the years.

**Python code for ArtificialIntelligence: Foundations of Computational Agents**

We use Python because Python programs can be close to pseudo-code. It isdesigned for humans to read.Python is reasonably ef?cient. Ef?ciency is usually not a problem for smallexamples. If your Python code is not ef?cient enough, a general procedureto improve it is to ?nd out what is taking most the time, and implement justthat part more ef?ciently in some lower-level language. Most of these lower-level languages interoperate with Python nicely. This will result in much lessprogramming and more ef?cient code (because you will have more time tooptimize) than writing everything in a low-level language. You will not haveto do that for the code here if you are using it for course projects.

**Lagrange multipliers with visualizations and code**

We can see that constrained optimization can solve many practical, real world problems arising in domains from logistics to finance. In the rest of the blog, we will start with vanilla optimization without constraints. Then, we will add equality constraints. Then, we will describe the solution to completely general constrained optimization problem with both equality and inequality constraints (the conditions are called KKT – Karush, Kuhn, Tucker conditions). Finally, we demonstrate the power of these conditions on some toy problems. Many people consider the book by Nocedal and Wright the bible of numerical optimization and we will roughly follow chapter 13, side-stepping rigorous proofs (which can always be read from the text) and focusing more on visual intuition.

**Using Machine Learning Algorithms**

At this point in time, you must have a basic idea of the most commonly used machine learning algorithms such as linear regression, logistic regression and neural networks. However, there are a couple of pointers that you might want to read upon for effective application of these algorithms.

**Probability Learning I : Bayes’ Theorem**

This post assumes you have some basic knowledge of probability and statistics. If you don’t, do not be afraid, I have gathered a list of the best resources I could find to introduce you to these subjects, so that you can read this post, understand it, and enjoy it to its fullest. In it, we will talk about one of the most famous and utilised theorems of probability theory: Bayes’ Theorem. Never heard about it? Then you are in for a treat! Know what it is already? Then read on to consolidate your knowledge with an easy, everyday example, so that you too can explain it in simple terms to others. In the following posts we will learn about some simplifications of Baye’s theorem that are more practical, and about other probabilistic approaches to machine learning like Hidden Markov Models.

**Smarter K-Nearest Neighbours**

The K-nearest neighbours (KNN) algorithm is one of the most simple and intuitive machine learning algorithms to understand. If you are trying to classify something into two categories: ‘Yes’ or ‘No’, for example, you take the ‘k’ most similar data points, and see if they are ‘Yes’ or ‘No’. If k=5, you look at the five most similar data points, and if, for example, you see 4 ‘Yes’ and 1 ‘No’, you would guess that the point is a ‘Yes’.

**Federated Learning: A New AI Business Model**

Federated learning is not only a promising technology but also a possible brand new AI business model. Indeed, as a consultant, I have been recently tasked with making recommendations about how a healthcare company could create a ‘data alliance’ with some competitors by creating a Federated Learning framework. The goal of this article is to explain to you how FL might give birth to a new data ecosystem and create data alliances.

**GANs and Missing Data Imputation**

Looking at the impressive performance of GANs, it is not surprising that they have become so popular. They are widely used for editing or generating images, security purposes, and in many other areas, outperforming most of the Neural Network architectures. They also have some less serious applications such as transforming horses to zebras or rendering cityscapes into GTA style.

**Hypothesis Testing: Were Housing Prices of University Towns less affected by the recession?**

Recession is a business cycle contraction when there is a general decline in economic activity. A recession occurs when there is a widespread drop in spending which may be due to various events like financial crisis, an external trade shock or bursting of an economic bubble. During a recession, we find a steady decline in GDP, income, employment, sales, and industrial production. The recession hit the United States in December 2007 and lasted for about 18 months (June 2009) In this blog post, let’s find out if mean housing prices in University Towns (e.g. Syracuse is a University town as it has more student population then traditional non-student population) were less affected by the recession than other Towns.

**Send Data Alert with Python SMTP**

For data scientists, it is very important to communicate reports to the non technical users especially if there are some abnormality within the data. From my working experience, most analysts would send the users business reports manually – they would process the data, run some validations, and generate reports to send via Outlook/gmail. All of these take much time and leaves rooms for human errors.

**Learning Attention Mechanism from scratch!**

In this post, I will show you how attention is implemented. The main focus would be on implementing attention in isolation from a larger model. That’s because when we implement attention in a real-world model, a lot of the focus goes into piping the data and juggling numerous vectors rather than the concepts of attention themselves.

**How to better manage your data science team’s workflow**

This workshop, Aug 14 @ 12 PM ET, will give you the proper tools and tactics to manage the entire lifecycle of your machine learning projects, from research to exploration to development and production.

**Introduction to Quantum Programming**

Quantum computers exist! And so does quantum programming! In this article, I’ll walk you through everything you need to know to get started with quantum programming. I’ll start off with some context about how quantum computers differ from computers like your laptop, then explain the fundamentals of quantum programming, and finish with how you can run programs on a real quantum computer for free today. Before we begin, please note that this article is intended for people who want to learn the full technical details of quantum programming. This article builds up from the mathematical foundation of qubits, quantum gates, and quantum circuit diagrams. This article will not explain quantum algorithms or their advantages, as those topics deserve their own articles.

**Robustly optimized BERT Pretraining Approaches**

Natural language processing (NLP) has been in the rise recently. All due to the language model (lm) pretraining method. A careful comparison between lm pretraining methods is near impossible because of various reasons (training data, computational costs, and hyperparameter choices). In this post, I will summarize some of the changes made in the BERT’s pretraining method which impacted the final results so that it achieves state-of-the-art results in 4 out of 9 GLUE tasks. For detailed understanding please read the following paper.

**Basic Feature Engineering to Reach More Efficient Machine Learning – Part 1**

If I ask you a question like does machine learning difficult, you might obviously say yes. But machine learning is not a difficult task. Because every algorithm you might need is pre implemented and available as packages (mostly in python libraries). You can import whatever library you want and then start building the model. But almost all of these algorithms are operating on numerical data or binary data. So you can’t just directly input a feature set and expect the algorithm to work. Especially when there are non-numerical data in your dataset, machine learning models won’t work as expected and gives very low accuracy rates. We have to clean these non-numerical data and extract useful numerical embeddings. Cleaning is not only that. We have to deal with missing values, collinear features, zero important features and so on. But that is not enough. We have to derive more meaningful features to a machine learning model out of the cleaned features. You can find these topics in more detail later on this article. All of these things are referred to as Feature Engineering. Feature engineering is the most important and well, I might say somewhat difficult task in machine learning.

### Like this:

Like Loading...
