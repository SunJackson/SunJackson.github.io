---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/08/26/distilled-news-1177/
date:      2019-08-26
author:      Michael Laux
tags:
    - data
    - models
    - learned
    - images
    - image classifier
---

**Blockchain Supercharged with AI: The Next Revolution?**

At first glance, blockchain and AI seem poles apart and it is! But recent developments in Big data has created a conducive environment for the amalgamation of these technologies.

**Why Chatbots Can’t Read Your Mind**

Chatbots are cropping up everywhere, from customer service to internal help desks, but what makes them tick? When we interact with chatbots, we’re often coming in from the end user side and confronting a chatbot window. When we ask a question, the chatbot may or may not be able to figure out what we’re saying. Why is this?

**Learn faster with smarter data labeling**

The vast amount of deep learning tools enables us to quickly build new apps with incredible performance, from computer vision classifying complex objects on photo to natural language understanding by extracting semantics from texts. However, the main bottleneck for all these methods is the massive amount of data needed to train all these models – typically hundred thousands of training examples. If you are starting to build image classifier from scratch, say, for detecting stale products on a lane, you’ll get stuck for weeks or months to collect and manually annotate all these photos. Fortunately, there are a bunch of deep neural networks, that are already pre-trained on large image datasets with many classes, and ready to use for mitigating that cold start problem. The core idea of transfer learning is to leverage the outputs of these models, capturing high-level image semantics, as inputs to new classifiers that solve the target task. It significantly reduces the amount of data needed to be annotated by human labelers, from hundred thousand to thousands of images.

**AI Powered Search for Extra-terrestrial Intelligence – Deep Learning Signal Classifiers**

Welcome (or welcome back!) to the AI for social good series! In the second part, of this two-part series of articles, we will look at how Artificial intelligence (AI) coupled with the power of open-source tools and techniques like deep learning can help us further the quest for finding extra-terrestrial intelligence! In the first part of this two-part series, we formulated our key objective and motivation behind doing this project. Briefly, we were looking at different radio-telescope signals simulated from SETI (Search for Extra-terrestrial Intelligence) Institute data. We leveraged techniques to process, analyze and visualize radio signals as spectrograms, which are basically visual representations of the raw signal.

**AI Safety – How Do you Prevent Adversarial Attacks?**

I made a commitment to write about AI Safety for 50 days, however recently I have focused more on AI Safety and ethics. I would argue the goals of how we are applying solutions within the field of artificial intelligence is an equally important consideration within AI Safety as defence or attack. On the other hand the technical side of AI Safety is important to consider, therefore I will focus more on this aspect the next few days. Today I had a chat with Pin-yu and Sijia. Pin-Yu Chen and Sijia Liu are Research Staff Members of IBM Research AI (T. J. Watson Research Center), MIT-IBM Watson AI Lab.

**Rolling in the Deep: RBMs**

All you need to know about Restricted Boltzmann MachinesAnother type of networks used in deep learning are Restricted Boltzmann Machines (RBM). RBMs are shallow networks used for data reconstruction and feature extraction.Common applications of RBMs:• feature extraction• dimensionality reduction• pattern recognition• recommendation systems• missing values handling• topic modeling

**Bias and Variance – Cut Through the Noise**

Bias and Variance are amongst the more misunderstood concepts in ML, as they’re usually described using superficial explanations of under-fitting and over-fitting. In this post, we lay down the statistical groundwork to understand where they come from. The maths is thoroughly explained, so you won’t need to be an expert in Statistics to understand it.

**How can you Convert a Business Problem into a Data Problem? A Successful Data Science Leader’s Guide**

How effectively can you convert a business problem into a data problem? This question holds the key to unlocking the potential of your data science project. There is no one-size-fits-all approach here. This is a nontrivial effort with positive long-term results and hence deserves a great deal of focused collaboration across the product team, the data science team, and the engineering team. Every leader knows that being able to measure progress is an invaluable aspect of any project. This understanding goes to an entirely different level when it comes to data science projects.

**What is NLP & What Do NLP Scientists Do?**

I recently started working as an NLP developer at a company. I am obviously happy and relieved to be gainfully employed again. But one thing I’ve noticed since I started working is that a good amount of people including my dad have asked me, ‘What’s NLP and what is it exactly that you do?’ Normally, I would refer them to my blog but I realized I have never written on this before. I’ve written a few articles about specific data science and machine learning concepts but I never personally defined what the profession and industry mean to me. So let’s rectify that right now.

**Maximum Likelihood Estimation Explained – Normal Distribution**

Wikipedia defines Maximum Likelihood Estimation (MLE) as follows: ‘A method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.’ To get a handle on this definition, let’s look at a simple example. Let’s say we have some data and we assume that it is normally distributed. By assuming normality, we simply assume the shape of our data distribution to conform to the ever-so-popular Gaussian bell curve. What we don’t know is how ‘fat’ or ‘skinny’ the curve is, or where along the x-axis the peak occurs.

**How to do Topic Extraction from Customer Reviews in R**

Topic Extraction is an integral part of IE (Information Extraction) from Corpus of Text to understand what are all the key things the corpus is talking about. While this can be achieved naively using unigrams and bigrams, a more intelligent way of doing it with an algorithm called RAKE is what we’re going to see in this post.

**The case against the jupyter notebook**

To most data scientists, the jupyter notebook is a staple tool: it’s where they learned the ropes, it’s where they go to prototype models or explore their data – basically, it’s the default arena for their all their data science work. But Joel Grus isn’t like most data scientists: he’s a former hedge fund manager and former Googler, and author of Data Science From Scratch. He currently works as a research engineer at the Allen Institute for Artificial Intelligence, and maintains a very active Twitter account. Oh, and he thinks you should stop using Jupyter noteoboks. Now.

**Independent Component Analysis (ICA) In Python**

Suppose that you’re at a house party and you’re talking to some cute girl. As you listen, your ears are being bombarded by the sound coming from the conversations going on between different groups of people through out the house and from the music that’s playing rather loudly in the background. Yet, none of this prevents you from focusing in on what the girl is saying since human beings possess the innate ability to differentiate between sounds. If, however, this were taking place as part of scene in a movie, the microphone which we’d use to record the conversation would lack the necessary capacity to differentiate between all the sounds going on in the room. This is where Independent Component Analysis, or ICA for short, comes in to play. ICA is a computational method for separating a multivariate signal into its underlying components. Using ICA, we can extract the desired component (i.e. conversation between you and the girl) from the amalgamation of multiple signals.

**Who Owns the Future? Data Trusts, Data Commons, and the Future of Data Ownership**

Who owns your data? And why do they? In this article, I consider various stakeholder claims to data ownership and the value generated by data, through a political economy lens. Following a data value framework established by the Open Data Initiative (2019), I first consider how data generates value from the point of its creation, how data as a resource imbues various stewardship obligations onto data controllers, and finally how – given competing interests – decision-making authority is apportioned across stakeholders. This analysis is then applied to three emerging models of data ownership: Laissez Faire, Data Trusts and Data Commons. The structural qualities of each model are revealed by an in-depth critique, before a visualisation of the data flows between stakeholders is offered. Finally, I compare these models across categoric issues that emerge from this analysis, considering how each model tackles issues such as incentives, competition, innovation and feasibility.

### Like this:

Like Loading...
