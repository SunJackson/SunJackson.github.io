---
layout:     post
title:      The Model Complexity Myth
subtitle:   转载自：http://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/
date:       2015-07-06
author:     Jake VanderPlas
header-img: img/background1.jpg
catalog: true
tags:
    - regularization
    - regularized
    - frequentist
    - theta
    - maximum likelihood estimate
---

In a frequentist approach, this type of conditioning is known as *regularization*.
Regularization is motivated by a desire to penalize large values of model parameters.
For example, in the underdetermined fit above (with $(x, y) = (0.37, 0.95)$), you could fit the data perfectly with a slope of one billion and an intercept near negative 370 million, but in most real-world applications this would be a silly fit.
To prevent this sort of canceling parameter divergence, in a frequentist setting you can "regularize" the model by adding a penalty term to the $\chi^2$; one popular choice is a penalty term proportional to the sum of squares of the model parameters themselves:

$$
\chi^2_{reg} = \chi^2 + \lambda~\theta^T\theta
$$

Here $\lambda$ is the degree of regularization, which must be chosen by the person implementing the model.

Using the expression for the regularized $\chi^2$, we can minimize with respect to $\theta$ by again taking the derivative and setting it equal to zero:

$$
\frac{d\chi^2}{d\theta} = -2[X^T(y - X\theta) - \lambda\theta] = 0
$$

This leads to the following regularized maximum likelihood estimate for $\theta$:

$$
\hat{\theta}_{MLE} = [X^TX + \lambda I]^{-1} X^T y
$$

Comparing this to our conditioning above, we see that the regularization degree $\lambda$ is identical to the conditioning term $\sigma$ that we considered above.
That is, regulariation of this form is nothing more than a simple conditioning of $X^T X$, with $\lambda = \sigma$.
The result of this conditioning is to push the absolute values of the parameters toward zero, and in the process make an ill-defined problem solvable.

I'll add that the above form of regularization is known variably as *L2-regularization* or *Ridge Regularization*, and is only one of the possible regularization approaches.
Another useful form of regularization is *L1-regularization,* also known as *Lasso Regularization*, which has the interesting property that it favors sparsity in the model.
