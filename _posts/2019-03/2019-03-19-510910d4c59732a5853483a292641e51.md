---
layout:     post
catalog: true
title:      Learning Data Science： Predicting Income Brackets
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/B_YJYBe7eKA/
date:      2019-03-19
author:      Learning Machines
tags:
    - modelling
    - models
    - incomes
    - attributes
    - predict income
---










![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/white-male-2064875_1280-300x300.jpg?resize=300%2C300)
![](https://i2.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/white-male-2064875_1280-300x300.jpg?resize=300%2C300)
As promised in the post Learning Data Science: Modelling Basics we will now go a step further and try to predict income brackets with real world data and different modelling approaches. We will learn a thing or two along the way, e.g. about the so-called *Accuracy-Interpretability Trade-Off*, so read on…

The data we will use is from here: Marketing data set. The description reads:

> 
This dataset contains questions from questionnaires that were filled out by shopping mall customers in the San Francisco Bay area. The goal is to predict the Annual Income of Household from the other 13 demographics attributes.


The following extra information (or *metadata*) is provided with the data:

Our task is to predict the variable “Income”.

So, let us first load the data, have a look at some of its characteristics and perform a little bit of formatting. After that we divide it into a *training* (80%) and a *test set* (20%) to account for potential *overfitting* (also see Learning Data Science: Modelling Basics):

We start with a simple but comprehensible model, `OneR` (on CRAN), as a benchmark:

![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/OneR-income-brackets-1024x731.png?w=450)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/OneR-income-brackets-1024x731.png?w=450)


What can we conclude from this? First the most important feature is “Age” while “Marital Status”, “Occupation” and “Household Status” perform comparably well. The overall *accuracy* (i.e. the percentage of correctly predicted instances) is with about 30% not that great, on the other hand not that extraordinarily uncommon when dealing with real-world data. Looking at the model itself, in the form of *rules* and the *diagnostic plot*, we can see that we have *non-linear relationship* between Age and Income: the older one gets the higher the income bracket, except for people who are old enough to retire. This seems plausible.

OneR is basically a one step *decision tree*, so the natural choice for our next analysis would be to have a full decision tree built (all packages are on CRAN):

![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/rpart-income-brackets-1024x731.png?w=450)
![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/rpart-income-brackets-1024x731.png?w=450)


The new model has an accuracy that is a little bit better but the interpretability is a little bit worse. You have to go through the different branches to see in which income bracket it ends. So for example when the age bracket is below ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-8c267d62c3d7048247917e13baec69a5_l3.png?resize=8%2C12)
![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-8c267d62c3d7048247917e13baec69a5_l3.png?resize=8%2C12)
 (which means that it is ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-69a7c7fb1023d315f416440bca10d849_l3.png?resize=7%2C13)
) it predicts income bracket ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-69a7c7fb1023d315f416440bca10d849_l3.png?resize=7%2C13)
![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-69a7c7fb1023d315f416440bca10d849_l3.png?resize=7%2C13)
, when it is bigger than ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-8c267d62c3d7048247917e13baec69a5_l3.png?resize=8%2C12)
![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-8c267d62c3d7048247917e13baec69a5_l3.png?resize=8%2C12)
 and the Household Status bracket is ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-69a7c7fb1023d315f416440bca10d849_l3.png?resize=7%2C13)
![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-69a7c7fb1023d315f416440bca10d849_l3.png?resize=7%2C13)
 it predicts income income bracket ![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-e4888e98f77eb93ff65bfecac28d3c5e_l3.png?resize=9%2C12)
![](https://i1.wp.com/blog.ephorie.de/wp-content/ql-cache/quicklatex.com-e4888e98f77eb93ff65bfecac28d3c5e_l3.png?resize=9%2C12)
 and so on. The most important variable, as you can see is again Age (which is reassuring that OneR was on the right track) but we also see Household Status and Occupation again.

What is better than one tree? Many trees! So the next natural step is to have many trees built, while varying the features and the examples that should be included in each tree. At the end it may be that different trees give different income brackets as their respective prediction, which we solve via voting as in a good democracy. This whole method is fittingly called *Random Forests* and fortunately there are many good packages available in R. We use the `randomForest` package (also on CRAN) here:

![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/randomForrest-income-brackets-1024x731.png?w=450)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2019/02/randomForrest-income-brackets-1024x731.png?w=450)


As an aside you can see that the basic modelling workflow stayed the same – no matter what approach (OneR, decision tree or random forest) you chose. This standard is kept for most (modern) packages and one of the great strengths of R! With thousands of packages on CRAN alone there are of course sometimes differences but those are normally well documented (so the help function or the vignette are your friends!)

Now, back to the result of the last analysis: again, the overall accuracy is better but because we have hundreds of trees now the interpretability suffered a lot. This is also known under the name Accuracy-Interpretability Trade-Off. The best we can do in the case of random forests is to find out which features are the most important ones. Again Age, Occupation and Household Status show up, which is consistent with our analyses so far. Additionally, because of the many trees that had to be built, this analysis ran a lot longer than the other two.

Random forests are one of the best methods out of the box, so the accuracy of about 34% tells us that our first model (OneR) wasn’t doing too bad in the first place! Why are able to reach comparatively good results with just one feature. This is true for many real-world data sets. Sometimes simple models are not much worse than very complicated ones – you should keep that in mind!

If you play around with this dataset I would be interested in your results! Please post them in the comments – Thank you and stay tuned!


*Related*








---
