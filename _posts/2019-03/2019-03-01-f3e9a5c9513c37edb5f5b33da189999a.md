---
layout:     post
catalog: true
title:      Bayesian state space modelling of the Australian 2019 election by @ellis2013nz
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/BOOkDPxTCTA/
date:      2019-03-01
author:      free range statistics - R
tags:
    - data
    - modelling
    - modelled
    - iâ
    - elections
---





So Iâ€™ve been back in Australia for five months now. While things have been very busy in my new role at Nous Group, itâ€™s not so busy that Iâ€™ve failed to notice thereâ€™s a Federal election due some time by November this year. Iâ€™m keen to apply some of the techniques I used in New Zealand in the richer data landscape (more polls, for one) and complex environment of Australian voting systems.

## Polling data

The Australian Electoral Commission has wonderful, highly detailed data on actual results, which Iâ€™ll doubtless be coming to at some point. However, I thought for today Iâ€™d start with the currency and perpetual conversation-making power (at least in the media) of polling data.

Thereâ€™s no convenient analysis-ready collection of Australian polling data that Iâ€™m aware of. I used similar methods to whatâ€™s behind my `nzelect` package to grab these survey results from Wikipedia where it is compiled by some anonymous benefactor, from the time of the 2007 election campaign until today.

Thanks are owed to Emily Kothe who did a bunch of this scraping herself for 2010 and onwards and put the results on GitHub (and on the way motivated me to develop a date parser for the horror that is Wikipediaâ€™s dates), but in the end I started from scratch so I had all my own code convenient for doing updates, as Iâ€™m sure Iâ€™ll be wanting.

All the code behind this post is in its own GitHub repository. It covers grabbing the data, fitting the model Iâ€™ll be talking about soon, and the graphics for this post. That repo is likely to grow as I do more things with Australian voting behaviopur data.

Hereâ€™s how that polling data looks when you put it together:

![](http://freerangestats.info/img/0145-first-pref.svg)
![](http://freerangestats.info/img/0145-first-pref.svg)


Notes on the abbreviations of Australian political parties in that chart:

- **ONP** ~ â€œPauline Hansonâ€™s One Nationâ€� â€“ nationalist, socially conservative, right-wing populism

- **Oth** ~ Other parties

- **Grn** ~ â€œAustralian Greensâ€� ~ left wing, environment and social justice focus

- **ALP** ~ â€œAustralian Labor Partyâ€� ~ historically the party of the working person, now the general party of the centre left

- **Lib/Nat** ~ â€œLiberal Partyâ€� or â€œNational Partyâ€� ~ centre and further right wing, long history of governing in coalition (and often conflated in opinion polling, hence the aggregation into one in this chart)


Iâ€™m a huge believer in looking at polling data in the longer term, not just focusing on the current term of government and certainly not just todayâ€™s survey release. The chart above certainly tells some of the story of the last decade or so; even a casual observer of Australian politics will recognise some of the key events, and particularly the comings and goings of Prime Ministers, in this chart.

Prior to 2007 thereâ€™s polling data available in Simon Jackmanâ€™s `pscl` package which has functionality and data relating to political science, but it only covers the first preference of voters so I havenâ€™t incorporated it into my cleaned up data. I need both the first preference and the estimated two-party-preference of voters.

*(Note to non-Australian readers â€“ Australia has a Westminster-based political system, with government recommended to the Governor General by whomever has the confidence of the lower house, the House of Representatives; which is electorate based with a single-transferrable-vote aka â€œAustralian voteâ€� system. And if the USA could just adopt something as sensible as some kind of preferential voting system, half my Twitter feed would probably go quiet).*

## Two-party-preferred vote

For my introduction today to analysis with this polling data, I decided to focus on the minimal simple variable for which a forecast could be credibly seen as a forecast of the outcome on election day, whenever it is. I chose the two-party-preferred voting intention for the Australian Labor Party or ALP. We can see that this is pretty closely related to how many seats they win in Parliament:

![](http://freerangestats.info/img/0145-2pp-seats.svg)
![](http://freerangestats.info/img/0145-2pp-seats.svg)


The vertical and horizontal blue lines mark 50% of the vote and of the seats respectively.

US-style gerrymanders generally donâ€™t occur in Australia any more, because of the existence of an independent electoral commission that draws the boundaries. So winning on the two-party-preferred national vote generally means gaining a majority in the House of Representatives.

Of course there are no guarantees; and with a electoral preference that is generally balanced between the two main parties even a few accidents of voter concentration in the key electorates can make a difference. This possibility is enhanced in recent years with a few more seats captured by smaller parties and independents:

![](http://freerangestats.info/img/0145-other-parties.svg)
![](http://freerangestats.info/img/0145-other-parties.svg)


All very interesting context.

## State space modelling

My preferred model of the two I used for the last New Zealand election was a Bayesian state space model. These are a standard tool in political science now, and Iâ€™ve written about them in both the Australian and New Zealand context.

To my knowledge, the seminal paper on state space modelling of voting intention based on an observed variety of polling data is Jackmanâ€™s â€œPooling the Polls Over an Election Campaignâ€�. I may be wrong; happy to be corrected. Iâ€™ve made a couple of blog posts out of replicating some of Jackmanâ€™s work with first preference intention for the ALP in the 2007 election. In fact, this was one of my self-imposed homework tasks in learning to use Stan, the wonderfully expressive statistcal modelling and high-performance statistical computation tool and probability programming language.

My state space model of the New Zealand electorate was considerably more complex than I need today, because in New Zealand I needed to model (under proportional representation) the voting intention for multiple parties at once. Whereas today I can focus on just two-party-preferred vote for either of the main blocs. Obviously a better model is possible, but not today!

The essence of this modelling approach is that we theorise the existence of an unobserved latent voting intention, which is measured imperfectly and irregularly by opinion poll surveys. These surveys have sampling error and other sources of â€œtotal survey errorâ€�, including â€œhouse effectsâ€� or statistical tendencies to over- or under-estimate vote in particular ways. Every few years, the true voting intention manifests itself in an actual election.

Using modern computational estimation methods we can estimate the daily latent voting intention of the public based on our imperfect observations, and also model the process of change in that voting intention over time and get a sense of the plausibility of different outcomes in the future. Hereâ€™s what it looks like for the 2019 election:

![](http://freerangestats.info/img/0145-model.svg)
![](http://freerangestats.info/img/0145-model.svg)


This all seems plausible and Iâ€™m pretty happy with the way the model works. The model specification written in Stan and the data management in R are both available on GitHub.

An important use for a statistical model in my opinion is to reinforce how uncertain we should be about the world. I like the representation above because it makes clear, in the final few months of modelled voting intention out to October or November 2019, how much change is plausible and consistent with past behaviour. So anyone who feels certain of the election outcome should have a look at the widening cone of uncertainty on this chart and have another think.

A particularly useful side effect of this type of model is statistical estimates of the over- or under-estimation of different survey types or sources. Because Iâ€™ve confronted the data with four successive elections we can get a real sense of what is going on here. This is nicely shown in this chart:

![](http://freerangestats.info/img/0145-density-d.svg)
![](http://freerangestats.info/img/0145-density-d.svg)


We see the tendency of Roy Morgan polls to overestimate the ALP vote by one or two percentage points, and of YouGov to underestimate it. These are interesting and important findings (not new to this blog post though). Simple aggregations of polls canâ€™t incorporate feedback from election results in this way (although of course experienced people routinely make more ad hoc adjustments).

A more sophisticated model would factor in change over time in polling firms methods and results, but again that would take me well beyond the scope of this blog post.

Looking forward to some more analysis of election issues, including of other data sources and of other aspects, over the next few months.

Hereâ€™s a list of the contributors to R that made todayâ€™s analysis possible:

```
thankr::shoulders() %>% knitr::kable() %>% clipr::write_clip()
```


*Related*








---
