---
layout:     post
catalog: true
title:      On the poor performance of classifiers in insurance models
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/MWt_eY4ZFMI/
date:      2019-03-14
author:      arthur charpentier
tags:
    - aucs
    - models
    - modeling
    - v_auc
    - vm
---





Each time we have a case study in my actuarial courses (with real data), students are surprised to have hard time getting a “good” model, and they are always surprised to have a low AUC, when trying to model the probability to claim a loss, to die, to fraud, etc. And each time, I keep saying, “yes, I know, and that’s what we expect because there a lot of ‘randomness’ in insurance”. To be more specific, I decided to run some simulations, and to compute AUCs to see what’s going on. And because I don’t want to waste time fitting models, we will assume that we have each time a perfect model. So I want to show that the upper bound of the AUC is actually quite low ! So it’s not a modeling issue, it is a fondamental issue in insurance !

By ‘perfect model’ I mean the following : ![](//s0.wp.com/latex.php?latex=%5COmega&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5COmega&bg=ffffff&fg=000&s=0)
 denotes the heterogeneity factor, because people are different. We would love to get ![](//s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY%3D1%7C%5COmega%5D&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY%3D1%7C%5COmega%5D&bg=ffffff&fg=000&s=0)
. Unfortunately, ![](//s0.wp.com/latex.php?latex=%5COmega&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5COmega&bg=ffffff&fg=000&s=0)
  is unobservable ! So we use covariates (like the age of the driver of the car in motor insurance, or of the policyholder in life insurance, etc). Thus, we have data ![](//s0.wp.com/latex.php?latex=%28y_i%2C%5Cboldsymbol%7Bx%7D_i%29&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%28y_i%2C%5Cboldsymbol%7Bx%7D_i%29&bg=ffffff&fg=000&s=0)
‘s and we use them to train a model, in order to approximate ![](//s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY%3D1%7C%5Cboldsymbol%7BX%7D%5D&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY%3D1%7C%5Cboldsymbol%7BX%7D%5D&bg=ffffff&fg=000&s=0)
. And then, we check if our model is good (or not) using the ROC curve, obtained from confusion matrices, comparing ![](//s0.wp.com/latex.php?latex=y_i&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=y_i&bg=ffffff&fg=000&s=0)
‘s and ![](//s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D_i&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D_i&bg=ffffff&fg=000&s=0)
‘s where ![](//s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D_i%3D1&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D_i%3D1&bg=ffffff&fg=000&s=0)
 when ![](//s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY_i%3D1%7C%5Cboldsymbol%7Bx%7D_i%5D&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY_i%3D1%7C%5Cboldsymbol%7Bx%7D_i%5D&bg=ffffff&fg=000&s=0)
 exceeds a given threshold. Here, I will not try to construct models. I will predict ![](//s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D_i%3D1&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D_i%3D1&bg=ffffff&fg=000&s=0)
 each time the true underlying probability ![](//s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY_i%3D1%7C%5Comega_i%5D&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%5BY_i%3D1%7C%5Comega_i%5D&bg=ffffff&fg=000&s=0)
 exceeds a threshold ! The point is that it’s possible to claim a loss (![](//s0.wp.com/latex.php?latex=y%3D1&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=y%3D1&bg=ffffff&fg=000&s=0)
) even if the probability is 3% (and most of the time ![](//s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D%3D0&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D%3D0&bg=ffffff&fg=000&s=0)
), and to not claim one (![](//s0.wp.com/latex.php?latex=y%3D0&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=y%3D0&bg=ffffff&fg=000&s=0)
) even if the probability is 97% (and most of the time ![](//s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D%3D1&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=%5Cwidehat%7By%7D%3D1&bg=ffffff&fg=000&s=0)
). That’s the idea with randomness, right ?

So, here ![](//s0.wp.com/latex.php?latex=p%28%5Comega_1%29%2C%5Ccdots%2Cp%28%5Comega_n%29&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=p%28%5Comega_1%29%2C%5Ccdots%2Cp%28%5Comega_n%29&bg=ffffff&fg=000&s=0)
 denote the probabilities to claim a loss, to die, to fraud, etc. There is heterogeneity here, and this heterogenity can be small, or large. Consider the graph below, to illustrate,

![](https://i1.wp.com/f.hypotheses.org/wp-content/blogs.dir/253/files/2019/03/Capture-d%E2%80%99e%CC%81cran-2019-03-13-a%CC%80-22.39.48.png?w=456&ssl=1)
![](https://i1.wp.com/f.hypotheses.org/wp-content/blogs.dir/253/files/2019/03/Capture-d%E2%80%99e%CC%81cran-2019-03-13-a%CC%80-22.39.48.png?w=456&ssl=1)


In both cases, there is, on average, 25% chance to claim a loss. But on the left, there is more heterogeneity, more dispersion. To illustrate, I used the arrow, which is a classical 90% interval : 90% of the individuals have a probability to claim a loss in that interval. (here 10%-40%), 5% are below 10% (low risk), and 5% are above 40% (high risk). Later on, we will say that we have 25% on average, with a dispersion of 30% (40% minus 10%). On the right, it’s more 25% on average, with a dispersion of of 15%. What I call dispersion is the difference between the 95% and the 5% quantiles.

Consider now some dataset, with Bernoulli variables ![](//s0.wp.com/latex.php?latex=y&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=000&s=0)
, drawn with those probabilities ![](//s0.wp.com/latex.php?latex=p%28%5Comega%29&bg=ffffff&fg=000&s=0)
![](http://s0.wp.com/latex.php?latex=p%28%5Comega%29&bg=ffffff&fg=000&s=0)
. Then, let us assume that we are able to get a perfect model : I do not estimate a model based on some covariates, here, I assume that I know perfectly the probability (which is true, because I did generate those data). More specifically, to generate a vector of probabilities, here I use a Beta distribution with a given mean, and a given variance (to capture the heterogeneity I mentioned above)
|123|a=m*(m*(1-m)/v-1)b=(1-m)*(m*(1-m)/v-1)p=rbeta(n,a,b)|

from those probabilities, I generate occurences of claims, or deaths,
|1|Y=rbinom(n,size = 1,prob = p)|

Then, I compute the AUC of my “perfect” model,
|1|auc.tmp=performance(prediction(p,Y),"auc")|

And then, I will generate many samples, to compute the average value of the AUC. And actually, we can do that for many values of the mean and the variance of the Beta distribution. Here is the code
{% raw %}
|123456789101112131415161718192021222324252627282930313233343536373839|library(ROCR)n=1000ns=200ab_beta = function(m,inter){ a=uniroot(function(a) qbeta(.95,a,a/m-a)-qbeta(.05,a,a/m-a)-inter, interval=c(.0000001,1000000))$root b=a/m-a return(c(a,b))}Sim_AUC_mean_inter=function(m=.5,i=.05){ V_auc=rep(NA,ns) b=-1 essai = try(ab<-ab_beta(m,i),TRUE) if(inherits(essai,what="try-error")) a=-1 if(!inherits(essai,what="try-error")){ a=ab[1] b=ab[2] } if((a>=0)&(b>=0)){ for(s in 1:ns){ p=rbeta(n,a,b) Y=rbinom(n,size = 1,prob = p) auc.tmp=performance(prediction(p,Y),"auc") V_auc[s]=as.numeric(auc.tmp@y.values)} L=list(moy_beta=m, var_beat=v, q05=qbeta(.05,a,b), q95=qbeta(.95,a,b), moy_AUC=mean(V_auc), sd_AUC=sd(V_auc), q05_AUC=quantile(V_auc,.05), q95_AUC=quantile(V_auc,.95)) return(L)} if((a<0)|(b<0)){return(list(moy_AUC=NA))}}Vm=seq(.025,.975,by=.025)Vi=seq(.01,.5,by=.01)V=outer(X = Vm,Y = Vi, Vectorize(function(x,y) Sim_AUC_mean_inter(x,y)$moy_AUC))library("RColorBrewer")image(Vm,Vi,V, xlab="Probability (Average)", ylab="Dispersion (Q95-Q5)", col= colorRampPalette(brewer.pal(n = 9, name = "YlGn"))(101))contour(Vm,Vi,V,add=TRUE,lwd=2)|
{% endraw %}

![](https://i0.wp.com/f.hypotheses.org/wp-content/blogs.dir/253/files/2019/03/Capture-d%E2%80%99e%C4%9Bcran-2019-02-15-a%C4%9A-22.31.31.png?w=456&ssl=1)
![](https://i0.wp.com/f.hypotheses.org/wp-content/blogs.dir/253/files/2019/03/Capture-d%E2%80%99e%C4%9Bcran-2019-02-15-a%C4%9A-22.31.31.png?w=456&ssl=1)


On the *x*-axis, we have the average probability to claim a loss. Of course, there is a symmetry here. And on the *y*-axis, we have the dispersion : the lower, the less heterogeneity in the portfolio. For instance, with a 30% chance to claim a loss on average, and 20% dispersion (meaning that in the portfolio, 90% of the insured have between 20% and 40% chance to claim a loss, or 15% and 35% chance), we have on average a 60% AUC. With a perfect model ! So with only a few covariates, having 55% should be great !

My point here is that with a low dispersion, we cannot expect to have a great AUC (again, even with a perfect model). In motor insurance, from my experience, 90% of the insured are between 3% chance and 20% chance to claim a loss ! That’s less than 20% dispersion ! and in that case, even if the (average) probability is rather small, it is very difficult to expect an AUC above 60% or 65% !


*Related*








---
