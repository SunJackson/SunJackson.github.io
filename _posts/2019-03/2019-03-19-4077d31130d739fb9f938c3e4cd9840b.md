---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/03/19/distilled-news-1003/
date:      2019-03-19
author:      Michael Laux
tags:
    - data
    - modeling
    - algorithms
    - distributions
    - distributed
---

**A brilliant, simple exercise to teach privacy fundamentals**

Kate Klonick, an assistant professor at St John’s Law School, teaches an Information Privacy course for second- and third-year law students; she devised a wonderful and simply exercise to teach her students about ‘anonymous speech, reasonable expectation of privacy, third party doctrine, and privacy by obscurity’ over the spring break. Klonick’s students were assigned to sit in a public place and eavesdrop on nearby conversations, then, using only Google searches, ‘see if you can de-anonymize someone based on things they say loudly enough for lots of others to hear and/or things that are displayed on their clothing or bags.’

**All AI and Data Science News in one Place**

–

**Top 10 Big Data Tools in 2019**

1. Apache Spark2. Apache Kafka3. Flink4. Hadoop5. Cassandra6. Apache Storm7. RapidMiner8. Graph Databases (Neo4J and GraphX)9. Elastic Search10. Tableau

**Classification Algorithm Using Probability Patterns**

This article focuses on filtering algorithms’ structure used in image processing. The novel algorithm is based on binary matrices of input data to create sub-clusters which are used as filters later in classification. These sub-clusters are called Probability Kernels. The developed algorithm was tested using different data-sets and shows various accuracy rate which depends on diversity of data-set and the matching value which is used for comparison of matrices. Diabetes data-set, Breast Cancer data-set and Ionosphere data-set are used for comparison of performance of the developed algorithm and already known algorithms such as Naïve Bayes, AdaBoost, Random Forest and, Multilayer Perceptron. Test results regarding accuracy rate confirm advantages of proposed algorithm.

**Markov Chain Monte Carlo**

What is MCMC exactly? To answer that question we first need a refresher on Bayesian statistics. Bayesian statistics are built on the idea that the probability of a thing happening is influenced by the prior assumption of the probability and the likelihood that something happened as indicated by the data. With Bayesian statistics, probability is represented by a distribution. If the prior and likelihood probability distributions are normally distributed, we are able to describe the posterior distribution with a function. This is called a closed-form solution. This type of Bayes is shown below. As you can see the posterior distribution is shaped by both the prior and likelihood distributions and ends up somewhere in the middle.

**Feature Engineering Time**

How do you deal with events in the cluster that cross over to the next day?Some users might be night owls or from another timezone; If their activity spans midnight the math becomes harder. If a user sends messages at 23:57, 23:58 and 23:59, calculating the average is straightforward. What if the user is a minute late? Obviously we don’t want to add 00:00 to the average. Imagine you are trying to predict some outcome that is dependant on the time of day, a simple model (regression) will suffer the discontinuity at midnight.

**Data Science With No Data**

In this article, we will demonstrate how to generate a dataset to build a machine learning model. According to this, Medicare fraud and abuse cost taxpayers $60 billion per year. AI/ML could significantly help identify and prevent fraud and abuse, but since privacy is of utmost importance in medical patient data, it is extremely difficult to access this data. This prevents data scientists from generating models which would potentially have a positive impact on this field. Is there a way to design and develop models without access to underlying data? Yes, you can generate a prototype using realistic randomly generated data. Concretely, we will build an auto-generated medical insurance dataset and use it to identify potentially fraudulent claims.

**Exploring Neural Networks with Activation Atlases**

By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned which can reveal how the network typically represents some concepts.

**Introducing Activation Atlases**

We’ve created activation atlases (in collaboration with Google researchers), a new technique for visualizing what interactions between neurons can represent. As AI systems are deployed in increasingly sensitive contexts, having a better understanding of their internal decision-making processes will let us identify weaknesses and investigate failures.

**Exploring Neural Networks with Activation Atlases**

Neural networks have become the de facto standard for image-related tasks in computing, currently being deployed in a multitude of scenarios, ranging from automatically tagging photos in your image library to autonomous driving systems. These machine-learned systems have become ubiquitous because they perform more accurately than any system humans were able to directly design without machine learning. But because essential details of these systems are learned during the automated training process, understanding how a network goes about its given task can sometimes remain a bit of a mystery. Today, in collaboration with colleagues at OpenAI, we’re publishing ‘Exploring Neural Networks with Activation Atlases’, which describes a new technique aimed at helping to answer the question of what image classification neural networks ‘see’ when provided an image. Activation atlases provide a new way to peer into convolutional vision networks, giving a global, hierarchical, and human-interpretable overview of concepts within the hidden layers of a network. We think of activation atlases as revealing a machine-learned alphabet for images – an array of simple, atomic concepts that are combined and recombined to form much more complex visual ideas. We are also releasing some jupyter notebooks to help you get you started in making your own activation atlases.

**The ‘Less is More’ of Machine Learning**

Review of Dropout: A Simple Way to Prevent Neural Networks from Overfitting: In their paper, Srivastava et al. claim that repeatedly eliminating randomly selected nodes and their corresponding connections of a neural network during training will reduce overfitting and result in a signficantly improved neural network. Dropout was the first method to address the issue of overfitting in parametric models. Even today, Dropout remains efficient and works well. This review will go over the basics of understanding this paper such as ‘What is a neural network’ and ‘What is overfitting.’

**What does it mean to “productionize” data science?**

It’s easy to find examples of businesses getting value from data. It’s also easy to find examples of all of the technical tools you can use to leverage data. Those technical tools are often what people have in mind when they talk about data science, but I’ve found the tools matter less than how they are deployed. Those technologies are part of a larger system of business processes that, taken all together, produces business outcomes. Integration of data science into the business can mean you simply output reports and then someone decides how to act on the findings, or it can mean a whole lot more. This post is about that whole lot more, which is often called ‘productionization.’ For something to be ‘in production’ means it is part of the pipeline from the business to its customers. In manufacturing, if something is in production if it exists somewhere in the process that will result in actual goods being put in stores where consumers can buy them and take them home. In data science, is something is in production it’s on the path to putting information in a place where it is consumed.

**Announcement: TensorFlow 2.0 Has Arrived!**

**Experiments with Seq2seq: Data Dependency**

How much data do you need to train a seq2seq model? Let’s say that you want to translate sentences from one language to another. You probably need a bigger dataset to translate longer sentences than if you wanted to translate shorter ones. How does the need for data grow as the sentence length increases?

**Assessing Predictive Performance: From Precipitation Forecasts over the Tropics to Receiver Operating Characteristic Curves and Back**

Educated decision making involves two major ingredients: probabilistic forecasts for future events or quantities and an assessment of predictive performance. This thesis focuses on the latter topic and illustrates its importance and implications from both theoretical and applied perspectives. Receiver operating characteristic (ROC) curves are key tools for the assessment of predictions for binary events. Despite their popularity and ubiquitous use, the mathematical understanding of ROC curves is still incomplete. We establish the equivalence between ROC curves and cumulative distribution functions (CDFs) on the unit interval and elucidate the crucial role of concavity in interpreting and modeling ROC curves. Under this essential requirement, the classical binormal ROC model is strongly inhibited in its flexibility and we propose the novel beta ROC model as an alternative. For a class of models that includes the binormal and the beta model, we derive the large sample distribution of the minimum distance estimator. This allows for uncertainty quantification and statistical tests of goodness-of-fit or equal predictive ability. Turning … mehr





### Like this:

Like Loading...


*Related*

