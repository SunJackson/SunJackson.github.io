---
layout:     post
catalog: true
title:      Jesse or Celine? Text Classification on Before Sunrise Dialog： Part II
subtitle:      转载自：http://datameetsmedia.com/jesse-or-celine-text-classification-on-before-sunrise-dialog-part-ii/
date:      2019-03-31
author:      Pio Calderon
tags:
    - models
    - embedding model
    - word
    - embeddings
    - text
---





In the last article, we converted movie dialog into numbers using a bag-of-words approach. Here we go in another direction and use an embedding model to represent text in a dense manner. So instead of capturing stylistic tendencies of Jesse and Celine (via word choice in a bag-of-words model), we will try to capture the semantic differences between their words (via an embedding model).

## Word Embeddings

![](https://i0.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/Word-Vectors.png?resize=800%2C280)


A word embedding model converts words into vectors. By training a neural network model to predict context from a target word (or vice versa) over a large corpus, the model implicitly returns embedding vectors that retain the semantics of the tokens in the corpus. One interesting thing about the model is that it is able to learn analogies. The most famous equation “man” + “queen” – “woman” = “king” can be recovered using vector algebra on the embedding. For some background on word embeddings, check out my other post on Geospatial Associations with the Tagalog Embedding.

For this study, we will be using the FastText English model. As the model is extremely large, we clip the model to only include the top 100K words, resulting in a smaller memory footprint.

## Deep Averaging Network

For our classification task, we will be using a simple yet powerful model called the Deep Averaging Network (DAN). The model was introduced in 2015 by Iyyer and others in the paper “Deep Unordered Composition Rivals Syntactic Methods for Text Classification“.

Usual state-of-the-art models in sequence/text classification use complex and computationally taxing models like recurrent neural networks (RNN). Iyyer was able to show that a simple neural network that takes the averaged embedding vector of the input yields competitive performance. Note that the DAN does not take into consideration the ordering of words in the input text (due to the averaging procedure) while RNN-based ones take word order into account.

![](https://i2.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/2018-05-10-13.29.52.png?resize=366%2C261)






For the dataset, we will be using the same one as in the last study: Before Sunrise and Before Sunset dialog. There are two columns in the input file: a text column containing the dialog and a label column containing either ‘Jesse’ or ‘Celine’. We will be performing a different filtering procedure on the text as compared to the last study. After tokenizing the text with the NLTK tokenizer and removing stop words, we will retain only dialog that has 3 or more tokens. We do this since short text are often just retorts or remarks. We are mostly interested in “sense” since we’re considering a semantic model and shorter text (like “yup” or “great”) oftentimes have little to no semantic content. The dialog representation is then obtained as the average vector of the embeddings of the tokens, following the Deep Averaging Network model. In total, 791 lines of dialog constitute our dataset.

![](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-26-at-2.16.51-PM.png?resize=800%2C536)


For this analysis, we use Keras to build our neural network on top of the FastText embedding and try out different model architectures for the neural network. We perform nested cross validation to test out models with 1 to 5 inner layers. We split the dataset 80-20, with 80% being train and validation and 20% being test. From the 80% we perform stratified 4-fold cross-validation to test out different architectures.

Our evaluation metric is accuracy. Note that the two classes (Jesse’s dialog, Celine’s dialog) are almost evenly split, with a 51:49 class imbalance.

## Model Results

The accuracy of the model for different numbers of (inner) hidden layers are shown below. As we can see, accuracy is highest (~60%) for two hidden layers. Following standard model selection protocol, we would pick this as our final model. Evaluating the model on the test set, we see about 55% accuracy for two layers. Three hidden layers seem to perform the best on the test set.

Considering the class imbalance 51:49, we can confidently say that our model at least does better than random guessing.

![](https://i2.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/fasttext_accuracy.png?resize=538%2C371)


As hypothesized in the last article, the model doesn’t really perform that well compared to the bag of words approach. Style over semantics seems to be the predominant difference between Jesse’s words and Celine’s. However, one must note that the comparison between this study and the last is not one that is completely fair. For one, the training and evaluation datasets aren’t the same since we implemented a different filtering procedure to come up with the training and test sets.





## Conclusion

From first impressions, it seems that a stylistic model (based on actual words used, ala bag of words) trumps a semantic model (based on meaning, ala embeddings). It must be noted that these two models (bag of words, embeddings) contain different aspects of the dialog and might not be correlated with one another. It would be interesting to see how a combined model would improve on either model and see how big a jump in accuracy it would produce. That would be the subject of the next article in the “Jesse or Celine?” series.

