---
layout:     post
catalog: true
title:      Probing Provincial Geospatial Associations in Tagalog Word Embeddings
subtitle:      转载自：http://datameetsmedia.com/probing-provincial-geospatial-associations-in-tagalog-word-embeddings/
date:      2019-03-25
author:      Pio Calderon
tags:
    - word
    - associations
    - models
    - probably associate
    - embeddings
---




![](https://i0.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/Word-Vectors.png?resize=800%2C280)


## Word Embedding Models

NLP has progressed tremendously over the past few years. One of the most exciting advancements in the field is the concept of word embeddings. When one deals with text in a machine learning setting, he has to find a way to turn words into numbers, since machine learning models are only able to take numbers as inputs. Before word embeddings came along, the standard way to vectorize text was to one-hot encode them, which meant adding one dimension for each word in the vocabulary of the input text and then setting 1 to the dimension if the input text contains the word and 0 otherwise. Given a very large set of text, this implies a very high dimensional numeric representation of text data. This in turn leads to a plethora of problems: overfitting, curse of dimensionality, but most importantly (from a semantic perspective) that the semantic interrelatedness of words is not carried over. In other words, the one-hot representation doesn’t know that cat and kitten are semantically related, nor does it know that man and woman are related.

Enter word embeddings. The one-hot representation of text is a type of “sparse” representation, since the resulting representation has a lot of zeros. Word embeddings on the other hand are a type of “dense” representation, which means that the representation has dimension values that are continuous.

Remember in a one-hot representation that each dimension pertains to a word in the input text vocabulary? In a word embedding, each dimension is an implicit concept learned from training. One dimension could be “masculinity” or it could be “cuteness”. Usually the interpretation of the dimensions isn’t really that big a deal. What is important is that semantically related words are close to one another in the vector space, and we can use linear algebra to query the model.

How do we train word embeddings? I don’t want to go over the details of this since there are lots of awesome resources on the net, but the gist is that we feed a massive corpus of text (like Wikipedia) into a neural network where we scan over the whole corpus and we predict the context of each word from each target word, or vice versa, using a neural network. By doing so, we learn the statistics of each word in the corpus and pick up on the meaning of each word from its neighbors. The resulting output of training is a vector space in which words are embedded. Standard vector algebra is valid in this space, and so we can recover the famous analogy king + woman – man = queen.

## Associations

![](https://i0.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/monkshobbit_catholic_church_hypocrite_duterte_201605223.png?resize=800%2C400)


Now, the focus of this article is not an introduction on the theory of word embeddings but on what we can extract from them: associations. Word embeddings are really cool since we can implicitly study associations by checking the similarity of two words in the model. By association, there remains one important question: whose association? This is where psychology intersects with natural language processing. Remember that word embeddings are trained on a large input corpus? An embedding model captures associations and implicit biases from the text, which implies that it correlates with two audiences: the writers and the readers. Naturally, the writers will impart his own associations on the things he writes — so a pro-Trump writer would probably associate Trump with some positive aspects. Looking at it from the other side, the reader is the consumer of the text and consequently will be influenced by what he reads.

For associations, we will use the standard measure: cosine similarity. Cosine similarity measures the angle between two vectors. In an embedding model, meaning is represented in terms of direction, and so the cosine similarity is a measure of the relative direction (i.e. the angle) between two vectors. The score varies from -1 to 1, wherein -1 implies an opposite sense (since the two vectors are antiparallel) and 1 implies complete overlap in meaning (since the two vectors are perfectly aligned).

FastText is one of the new embedding models in the wild. It builds on the Word2vec model but instead of looking at the words in the input text, it looks at n-grams, the building blocks of words. FastText is owned by Facebook and they were cool enough to build models for a ton of languages, one of which is Tagalog. FastText models are built on CommonCrawl and Wikipedia. In this article we will probe the implicit associations captured by the Tagalog model.

What are the geographic associations implicit in the Tagalog model? We look at associations on the provincial level in this article and then zoom in to city level for Metro Manila on the next  Will the model recover the fact that the North is Marcos territory? That in Mindanao there are a lot of Muslims? That Palawan is vacation territory?

## Technicalities

![](https://i2.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/Meaning-meanings-5900e3935f9b581d59f5f498.jpg?resize=768%2C512)






There are 82 provinces in the Philippines, and not all of them exist as tokens in the Tagalog model. This puts into light some of the technical problems at the forefront of NLP research: polysemy and multi-word embeddings.

Polysemy is the coexistence of many possible meanings of a word. For example, Quezon as a token could refer to the province, the city in Metro Manila, or the president Manuel Quezon. However, since an embedding model is a function from words to vectors, it can only return one vector — all meanings are squished together into one vector which muddles the overall sense. The easiest way to disambiguate the different senses of Quezon is to preprocess mentions of Quezon in the input corpus into Quezon_province, Quezon_city, and Quezon_president before feeding into the training algorithm. But since we are using a pre-trained unigram model, we really can’t do anything about polysemy.

The Tagalog FastText model is a unigram embedding, so only words are mapped into vectors. This means that multi-word concepts and places like New York and Agusan del Norte aren’t represented in the model. For provinces like Agusan del Norte and Lanao del Sur, we take the easy way out and represent them as the base words Agusan and Lanao, respectively.

For this study we will be combining two awesome packages in Python: the NLP package Gensim and the geospatial package Geopandas. We will be loading and querying the Tagalog Fasttext model with the first and cutting and plotting the Philippine shapefile with the second. What we get out of this study are choropleth maps showing the degree of association each province has with an attribute of interest.

Let’s look at something basic first.

## Bisaya (Visayan)

 

![](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_bisaya-2.png?resize=423%2C682)


Querying the model for Bisaya, we can see that the map activates mostly in Central and Southern Philippines (Visayas and Mindanao). We also see some activation up north, but majority is in the south, where Bisaya is the predominant group and language.

 

## Marcos

![](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_marcos.png?resize=423%2C682)


If we check for provincial associations for Marcos, we see that the North (Ilocos area) activates. This is where Marcos is from and where most of his ardent supporters can be located.

## MILF

![](https://i2.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_milf.png?resize=423%2C682)


For MILF (Moro Islamic Liberation Front), Maguindanao and some other Southern provinces activate.

## Igorot

![](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_igorot.png?resize=423%2C682)


For Igorot, an indigenous tribe up North, Ifugao and the neighbors activate.

## Turista (Tourist)

![](https://i0.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_turista.png?resize=427%2C682)


If we query for tourist, we recover the fact that Palawan is a tourist spot. But we get nontrivial matches for Cagayan and Surigao.

## Kaibigan (Friend)

![](https://i2.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_kaibigan.png?resize=423%2C682)


For something abstract like friend, we see that Cebu takes the top spot. Is Cebuano hospitality and friendship embedded in the model? Basilan and Rizal also have high associations.

## Kaaway (Enemy)

![](https://i0.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_bisaya-1.png?resize=423%2C682)


 

Looking at enemy, we see that Bataan and Cavite are top matches. Thinking about it, Bataan and Cavite were major landmarks during the wartime period. That might have led to high association with enemy.

## Ganda (Beauty)

![](https://i2.wp.com/datameetsmedia.com/wp-content/uploads/2019/03/provinces_ganda.png?resize=423%2C682)
For beauty we get matches in Palawan (beautiful beaches, probably), Kalinga and Sultan Kudarat. These are quite nontrivial matches.

## Conclusion

As we can see, the Tagalog FastText model contains some trivial associations that we might expect (like Marcos up north and MILF down south), but we also recover some implicit and abstract associations that aren’t really trivial to explain like Cebu with friend.

As I mentioned in the first section, the model captures associations of the producers and consumers of the input text corpus. As the FastText model was mostly built on Wikipedia data, it only captures associations on the general level. By building a model on social media or forum text, for example, we can capture what’s in the thought process of the modern Filipino on the web. By building a model on Filipino news sites, we can capture  and measure the implicit biases of the media.





In the next post, I drill down on Metro Manila and probe city-level associations in the FastText model.

