---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/03/01/distilled-news-989/
date:      2019-03-01
author:      Michael Laux
tags:
    - learns
    - learning
    - word
    - machines
    - customer
---

**A journey into Convolutional Neural Network visualization**

There is one famous urban legend about computer vision. Around the 80s, the US military wanted to use neural networks to automatically detect camouflaged enemy tanks. They took a number of pictures of trees without tanks and then pictures with the same trees with tanks behind them. The results were impressive. So impressive that the army wanted to be sure the net had correctly generalized. They took new pictures of woods with and without tanks and they showed them again to the network. This time, the model performed terribly, it was not able to discriminate between pictures with tanks behind woods and just trees.It turned out that all the pictures without tanks were taken on a cloudy day while the ones with tanks on a sunny day! In reality, the network learns to recognize the weather, not the enemy tanks.

**Reinforcement Learning from Scratch: Simple Application and Evaluating Parameters in Detail**

This project was created as a means to learn Reinforcement Learning (RL) independently in a Python notebook. Applying RL without the need of a complex, virtual environment to interact with. By fully defining the probabilistic environment, we are able to simplify the learning process and clearly demonstrate the effect changing parameters has on the results. This is a valuable in any Machine Learning task but particularly in Reinforcement Learning where it can be hard to understand the impact varying parameters without a clear and well-defined example.

**Understanding Hypothesis Testing**

Hypothesis testing is the bedrock of the scientific method and by implication, scientific progress. It allows you to investigate a thing you’re interested in and tells you how surprised you should be about the results. It’s the detective that tells you whether you should continue investigating your theory or divert efforts elsewhere. Does that diet pill you’re taking actually work? How much sleep do you really need? Does that HR-mandated team-building exercise really help strengthen your relationship with your coworkers?

**Modeling Telecom Customer Churn with Variational Autoencoder**

How to apply deep convolutional neural networks and auto-encoders for building a churn prediction model. An autoencoder is deep learning’s answer to dimensionality reduction. The idea is pretty simple: transform the input through a series of hidden layers but ensure that the final output layer is the same dimension as the input layer. However, the intervening hidden layers have progressively smaller number of nodes (and hence, reduce the dimension of the input matrix). If the output matches or encodes the input closely, then the nodes of the smallest hidden layer can be taken as a valid dimension reduced data set. A variational autoencoder (VAE) resembles a classical autoencoder and is a neural network consisting of an encoder, a decoder and a loss function. They let us design complex generative models of data, and fit them to large data sets. After reading an article on using convolutional networks and autoencoders to provide insights into user churn. I decided to implement VAE to a telecom churn data set that can be downloaded from IBM Sample Data Sets. It is a bit of overkill to apply VAE to a relative small data set like this, but for the sake of learning VAE, I am going to do it anyway.

**Descriptive/Summary Statistics with descriptr**

We are pleased to announce the descriptr package, a set of tools for generating descriptive/summary statistics.

**How to Transfer SQL Knowledge to R**

Why would you ever use SQL syntax in the context of R? Maybe you took a few bioinformatics courses but then steered towards Python and are shaky coming back into the syntax of R. Maybe you want to expand your data science toolkit and are in desperate need of coding band-aids to keep you off Stackoverflow. Maybe you have already written the SQL query but would like to visualize the results using ggplot2. In an environment where data structures can often be confusing, SQL syntax can simplify dataframe manipulations in R so that the more time can be spent making great visualizations than on wrangling data. The code here can be found in its entirety on my GitHub.

**Machine Learning in Snowflake**

Snowflake is currently making waves globally as exciting new data warehouse built from scratch for the cloud. It already has an impressive number of aggregation and analytic functions of the traditional statistical variety, but the upcoming release of Javascript Stored Procedures lets you put its efficient, elastic, distributed compute engine to use on more general tasks outside the SQL dialect. I thought an ML workload would be a great way to road test the feature, and it turns out to be really good at it!

**How to track Machine Learning Readiness and why we should all care**

Whether a mathematician, an expert in data analytics, an experienced software engineer or a viewer at home receiving recommendations to watch the new series of Luther, Peaky Blinders or Two Doors Down, we all play a role in how artificial intelligence and machine learning evolve. We should all pay attention to how these systems evolve to handle our personal information and drive decisions that impact our lives and those of our loved ones. Recently, I’ve found that conversations around ‘machine learning’ and ‘machine insight’ lead to more practical, honest, progressive conversation about the evolution of technology than those themed Artificial Intelligence (AI). The latter tends to inspire more creative, romantic images of a future where humanity and machinery become intertwined and almost indistinguishable, such that we need to develop new thinking around rights, justice, morality and what it means to be alive. I am also up for those conversations but, in this article, I intentionally refer to machine learning, such that there less allusion to machines possessing intelligence and other human characteristics. Here I treat machines and machine learning as tools and methods by which we collect, process and present insight about our past, present and predicted future. They crunch large sets of data to generate reusable, statistical insight, features, trends and anomalies. Even when the mathematics and computer science can appear overwhelming, we in society still need to pay attention to how machine learning progresses, educate ourselves and our children, and not distance ourselves from the ongoing dialogue. Digital media and information are woven into our everyday lives and influence our views on the state of the world, people, politics, the economy and our identities. Machine Learning is very relevant for how media is commissioned, developed, delivered and consumed now and in the future.

**Data Science Project Process: Scoping and Research**

Importance of quality data is being discussed consistently nowadays and you may already have become familiar with the reasons for the value placed on it. From allowing business leaders to acquire better customer intelligence and improving customer service to optimizing internal operations and becoming a more efficient organization, data has the power to truly transform a business. We’ve already explored how to begin leveraging data to drive business growth, and now you may be curious – how do data projects actually start?

**6 Books About Open Data Every Data Scientist Should Read**

1. ‘Open Data Structures: An Introduction’ – Pat Morin2. ‘The Global Impact of Open Data’ – Stefaan Verhulst and Andrew Young3. ‘Data for the People: How to Make Our Post-Privacy Economy Work for You’ – Andreas Weigend4. ‘The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences’ – Rob Kitchin5. ‘Open Data Now: The Secret to Hot Startups, Smart Investing, Savvy Marketing and Fast Innovation’ – Joel Gurin6. ‘Data Science for Transport: A Self-Study Guide With Computer Exercises’ – Charles Fox

**Engineering + Data Science: The Ultimate (Yet Somehow Missing) Duo**

Marketing + Data Science, Business + Data Science: natural duos that are naturally integrated. What about Engineering and Data Science? You can almost guarantee that every engineer will consistently come into contact with data, no matter the engineer’s focus. Data will (should) always drive their decisions. And oftentimes, the data available to engineers is expansive; yet many engineers are unequipped to handle the data. Why then are data scientists not integrated into engineering teams at all levels of manufacturing, design, etc.? How can engineers avoid wasting the massive data sets piling up in manufacturing plants, processing plants, and other data heavy areas where data scientists are less commonly found?

**State of the art in AI and Machine Learning – highlights of papers with code**

We introduce papers with code, the free and open resource of state-of-the-art Machine Learning papers, code and evaluation tables.

**Word Embeddings in NLP and its Applications**

Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Word embeddings are distributed representations of text in an n-dimensional space. These are essential for solving most NLP problems. Domain adaptation is a technique that allows Machine learning and Transfer Learning models to map niche datasets that are all written in the same language but are still linguistically different. For example, legal documents, customer survey responses, and news articles are all unique datasets that need to be analyzed differently. One of the tasks of the common spam filtering problem involves adopting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). The importance of word embeddings in the field of deep learning becomes evident by looking at the number of researches in the field. One such research in the field of word embeddings conducted by Google led to the development of a group of related algorithms commonly referred to as Word2Vec.

**Solving Simpson’s Paradox**

Simpson’s paradox is a great example. At first, it challenges our intuition, but then, if we are able to dissect it properly, gives a lot of ideas about how to handle analysis of observational data (data that hadn’t been obtained through a well-designed experiment). It appears in many data analysis. We will walk through it using the well-known case of kidney stones. The techniques explained here can be found in detail in Pearl’s et al ‘Causal Inference in Statistics: A Primer’.

**Sentiment Preserving Word Embeddings**

Word embedding Is a technique which maps words into a space using continuous value vectors such that the words which have similar contexts appear closer to each other. Usually, this is done by taking a large corpus of data and then extracting word embeddings from it using Word2Vec or GloVE or some other algorithm. These algorithms help in capturing the semantic and syntactic contexts of different words but suffer a lot when it comes to sentiment. Let’s look at an example. Here we look at the 10 nearest neighbors of the word ‘good’ in the word embedding space. We use the Word2Vec model trained on google news with 300 dimensions. Following is the code to look at the top 10 neighbors.





### Like this:

Like Loading...


*Related*

