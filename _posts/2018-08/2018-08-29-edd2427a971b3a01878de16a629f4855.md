---
layout:     post
catalog: true
title:      Document worth reading： “Idealised Bayesian Neural Networks Cannot Have Adversarial Examples： Theoretical and Empirical Study”
subtitle:      转载自：https://advanceddataanalytics.net/2018/08/29/document-worth-reading-idealised-bayesian-neural-networks-cannot-have-adversarial-examples-theoretical-and-empirical-study/
date:      2018-08-29
author:      Michael Laux
tags:
    - images
    - uncertainty
    - perfect
    - examples
    - data
---

We prove that idealised discriminative Bayesian neural networks, capturing perfect epistemic uncertainty, cannot have adversarial examples: Techniques for crafting adversarial examples will necessarily fail to generate perturbed images which fool the classifier. This suggests why MC dropout-based techniques have been observed to be fairly robust to adversarial examples. We support our claims mathematically and empirically. We experiment with HMC on synthetic data derived from MNIST for which we know the ground truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold. Using our new-found insights we suggest a new attack for MC dropout-based models by looking for imperfections in uncertainty estimation, and also suggest a mitigation. Lastly, we demonstrate our mitigation on a cats-vs-dogs image classification task with a VGG13 variant. Idealised Bayesian Neural Networks Cannot Have Adversarial Examples: Theoretical and Empirical Study





### Like this:

Like Loading...


*Related*

