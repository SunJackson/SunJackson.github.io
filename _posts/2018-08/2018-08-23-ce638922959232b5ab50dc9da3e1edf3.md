---
layout:     post
catalog: true
title:      Document worth reading： “An Information-Theoretic Analysis of Deep Latent-Variable Models”
subtitle:      转载自：https://advanceddataanalytics.net/2018/08/24/document-worth-reading-an-information-theoretic-analysis-of-deep-latent-variable-models/
date:      2018-08-23
author:      Michael Laux
tags:
    - rates
    - generative
    - framework
    - trade
    - distortion
---

We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family. An Information-Theoretic Analysis of Deep Latent-Variable Models





### Like this:

Like Loading...


*Related*

