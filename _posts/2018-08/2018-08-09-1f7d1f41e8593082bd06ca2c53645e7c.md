---
layout:     post
catalog: true
title:      Whats new on arXiv
subtitle:      转载自：https://advanceddataanalytics.net/2018/08/09/whats-new-on-arxiv-730/
date:      2018-08-09
author:      Michael Laux
tags:
    - models
    - modeling
    - modelled
    - modelling
    - networks
---

**MCRM: Mother Compact Recurrent Memory A Biologically Inspired Recurrent Neural Network Architecture**

LSTMs and GRUs are the most common recurrent neural network architectures used to solve temporal sequence problems. The two architectures have differing data flows dealing with a common component called the cell state also referred to as the memory. We attempt to enhance the memory by presenting a biologically inspired modification that we call the Mother Compact Recurrent Memory MCRM. MCRMs are a type of a nested LSTM-GRU architecture where the cell state is the GRU’s hidden state. The relationship between the womb and the fetus is analogous to the relationship between the LSTM and GRU inside MCRM in that the fetus is connected to its womb through the umbilical cord. The umbilical cord consists of two arteries and one vein. The two arteries are considered as an input to the fetus which is analogous to the concatenation of the forget gate and input gate from the LSTM. The vein is the output from the fetus which plays the role of the hidden state of the GRU. Because MCRMs has this type of nesting, MCRMs have a compact memory pattern consisting of neurons that act explicitly in both long-term and short-term fashions. For some specific tasks, empirical results show that MCRMs outperform previously used architectures.

**Principles for Developing a Knowledge Graph of InterlinkedEvents from News Headlines on Twitter**

The ever-growing datasets published on Linked Open Data mainly contain encyclopedic information. However, there is a lack of quality structured and semantically annotated datasets extracted from unstructured real-time sources. In this paper, we present principles for developing a knowledge graph of interlinked events using the case study of news headlines published on Twitter which is a real-time and eventful source of fresh information. We represent the essential pipeline containing the required tasks ranging from choosing background data model, event annotation (i.e., event recognition and classification), entity annotation and eventually interlinking events. The state-of-the-art is limited to domain-specific scenarios for recognizing and classifying events, whereas this paper plays the role of a domain-agnostic road-map for developing a knowledge graph of interlinked events.

**Outlier detection on network flow analysis**

It is important to be able to detect and classify malicious network traffic flows such as DDoS attacks from benign flows. Normally the task is performed by using supervised classification algorithms. In this paper we analyze the usage of outlier detection algorithms for the network traffic classification problem.

**Directed Random Geometric Graphs**

Many real-world networks are intrinsically directed. Such networks include activation of genes, hyperlinks on the internet, and the network of followers on Twitter among many others. The challenge, however, is to create a network model that has many of the properties of real-world networks such as powerlaw degree distributions and the small-world property. To meet these challenges, we introduce the \textit{Directed} Random Geometric Graph (DRGG) model, which is an extension of the random geometric graph model. We prove that it is scale-free with respect to the indegree distribution, has binomial outdegree distribution, has a high clustering coefficient, has few edges and is likely small-world. These are some of the main features of aforementioned real world networks. We empirically observe that word association networks have many of the theoretical properties of the DRGG model.

**Semblance: A Rank-Based Kernel on Probability Spaces for Niche Detection**

Kernel methods provide a principled approach for detecting nonlinear relations using well understood linear algorithms. In exploratory data analyses when the underlying structure of the data’s probability space is unclear, the choice of kernel is often arbitrary. Here, we present a novel kernel, Semblance, on a probability feature space. The advantage of Semblance lies in its distribution free formulation and its ability to detect niche features by placing greater emphasis on similarity between observation pairs that fall at the tail ends of a distribution, as opposed to those that fall towards the mean. We prove that Semblance is a valid Mercer kernel and illustrate its applicability through simulations and real world examples.

**The Internals of the Data Calculator**

Data structures are critical in any data-driven scenario, but they are notoriously hard to design due to a massive design space and the dependence of performance on workload and hardware which evolve continuously. We present a design engine, the Data Calculator, which enables interactive and semi-automated design of data structures. It brings two innovations. First, it offers a set of fine-grained design primitives that capture the first principles of data layout design: how data structure nodes lay data out, and how they are positioned relative to each other. This allows for a structured description of the universe of possible data structure designs that can be synthesized as combinations of those primitives. The second innovation is computation of performance using learned cost models. These models are trained on diverse hardware and data profiles and capture the cost properties of fundamental data access primitives (e.g., random access). With these models, we synthesize the performance cost of complex operations on arbitrary data structure designs without having to: 1) implement the data structure, 2) run the workload, or even 3) access the target hardware. We demonstrate that the Data Calculator can assist data structure designers and researchers by accurately answering rich what-if design questions on the order of a few seconds or minutes, i.e., computing how the performance (response time) of a given data structure design is impacted by variations in the: 1) design, 2) hardware, 3) data, and 4) query workloads. This makes it effortless to test numerous designs and ideas before embarking on lengthy implementation, deployment, and hardware acquisition steps. We also demonstrate that the Data Calculator can synthesize entirely new designs, auto-complete partial designs, and detect suboptimal design choices.

**Efficient Principal Subspace Projection of Streaming Data Through Fast Similarity Matching**

Big data problems frequently require processing datasets in a streaming fashion, either because all data are available at once but collectively are larger than available memory or because the data intrinsically arrive one data point at a time and must be processed online. Here, we introduce a computationally efficient version of similarity matching, a framework for online dimensionality reduction that incrementally estimates the top K-dimensional principal subspace of streamed data while keeping in memory only the last sample and the current iterate. To assess the performance of our approach, we construct and make public a test suite containing both a synthetic data generator and the infrastructure to test online dimensionality reduction algorithms on real datasets, as well as performant implementations of our algorithm and competing algorithms with similar aims. Among the algorithms considered we find our approach to be competitive, performing among the best on both synthetic and real data.

**Machine-learning error models for approximate solutions to parameterized systems of nonlinear equations**

This work proposes a machine-learning framework for constructing statistical models of errors incurred by approximate solutions to parameterized systems of nonlinear equations. These approximate solutions may arise from early termination of an iterative method, a lower-fidelity model, or a projection-based reduced-order model, for example. The proposed statistical model comprises the sum of a deterministic regression-function model and a stochastic noise model. The method constructs the regression-function model by applying regression techniques from machine learning (e.g., support vector regression, artificial neural networks) to map features (i.e., error indicators such as sampled elements of the residual) to a prediction of the approximate-solution error. The method constructs the noise model as a mean-zero Gaussian random variable whose variance is computed as the sample variance of the approximate-solution error on a test set; this variance can be interpreted as the epistemic uncertainty introduced by the approximate solution. This work considers a wide range of feature-engineering methods, data-set-construction techniques, and regression techniques that aim to ensure that (1) the features are cheaply computable, (2) the noise model exhibits low variance (i.e., low epistemic uncertainty introduced), and (3) the regression model generalizes to independent test data. Numerical experiments performed on several computational-mechanics problems and types of approximate solutions demonstrate the ability of the method to generate statistical models of the error that satisfy these criteria and significantly outperform more commonly adopted approaches for error modeling.

**Paying Attention to Attention: Highlighting Influential Samples in Sequential Analysis**

In (Yang et al. 2016), a hierarchical attention network (HAN) is created for document classification. The attention layer can be used to visualize text influential in classifying the document, thereby explaining the model’s prediction. We successfully applied HAN to a sequential analysis task in the form of real-time monitoring of turn taking in conversations. However, we discovered instances where the attention weights were uniform at the stopping point (indicating all turns were equivalently influential to the classifier), preventing meaningful visualization for real-time human review or classifier improvement. We observed that attention weights for turns fluctuated as the conversations progressed, indicating turns had varying influence based on conversation state. Leveraging this observation, we develop a method to create more informative real-time visuals (as confirmed by human reviewers) in cases of uniform attention weights using the changes in turn importance as a conversation progresses over time.

**Active Learning based on Data Uncertainty and Model Sensitivity**

Robots can rapidly acquire new skills from demonstrations. However, during generalisation of skills or transitioning across fundamentally different skills, it is unclear whether the robot has the necessary knowledge to perform the task. Failing to detect missing information often leads to abrupt movements or to collisions with the environment. Active learning can quantify the uncertainty of performing the task and, in general, locate regions of missing information. We introduce a novel algorithm for active learning and demonstrate its utility for generating smooth trajectories. Our approach is based on deep generative models and metric learning in latent spaces. It relies on the Jacobian of the likelihood to detect non-smooth transitions in the latent space, i.e., transitions that lead to abrupt changes in the movement of the robot. When non-smooth transitions are detected, our algorithm asks for an additional demonstration from that specific region. The newly acquired knowledge modifies the data manifold and allows for learning a latent representation for generating smooth movements. We demonstrate the efficacy of our approach on generalising elementary skills, transitioning across different skills, and implicitly avoiding collisions with the environment. For our experiments, we use a simulated pendulum where we observe its motion from images and a 7-DoF anthropomorphic arm.

**Probabilistic Causal Analysis of Social Influence**

Mastering the dynamics of social influence requires separating, in a database of information propagation traces, the genuine causal processes from temporal correlation, homophily and other spurious causes. However, most of the studies to characterize social influence and, in general, most data-science analyses focus on correlations, statistical independence, conditional independence etc.; only recently, there has been a resurgence of interest in ‘causal data science’, e.g., grounded on causality theories. In this paper we adopt a principled causal approach to the analysis of social influence from information-propagation data, rooted in probabilistic causal theory. Our approach develops around two phases. In the first step, in order to avoid the pitfalls of misinterpreting causation when the data spans a mixture of several subtypes (‘Simpson’s paradox’), we partition the set of propagation traces in groups, in such a way that each group is as less contradictory as possible in terms of the hierarchical structure of information propagation. For this goal we borrow from the literature the notion of ‘agony’ and define the Agony-bounded Partitioning problem, which we prove being hard, and for which we develop two efficient algorithms with approximation guarantees. In the second step, for each group from the first phase, we apply a constrained MLE approach to ultimately learn a minimal causal topology. Experiments on synthetic data show that our method is able to retrieve the genuine causal arcs w.r.t. a known ground-truth generative model. Experiments on real data show that, by focusing only on the extracted causal structures instead of the whole social network, we can improve the effectiveness of predicting influence spread.

**Privacy in Social Media: Identification, Mitigation and Applications**

The increasing popularity of social media has attracted a huge number of people to participate in numerous activities on a daily basis. This results in tremendous amounts of rich user-generated data. This data provides opportunities for researchers and service providers to study and better understand users’ behaviors and further improve the quality of the personalized services. Publishing user-generated data risks exposing individuals’ privacy. Users privacy in social media is an emerging task and has attracted increasing attention in recent years. These works study privacy issues in social media from the two different points of views: identification of vulnerabilities, and mitigation of privacy risks. Recent research has shown the vulnerability of user-generated data against the two general types of attacks, identity disclosure and attribute disclosure. These privacy issues mandate social media data publishers to protect users’ privacy by sanitizing user-generated data before publishing it. Consequently, various protection techniques have been proposed to anonymize user-generated social media data. There is a vast literature on privacy of users in social media from many perspectives. In this survey, we review the key achievements of user privacy in social media. In particular, we review and compare the state-of-the-art algorithms in terms of the privacy leakage attacks and anonymization algorithms. We overview the privacy risks from different aspects of social media and categorize the relevant works into five groups 1) graph data anonymization and de-anonymization, 2) author identification, 3) profile attribute disclosure, 4) user location and privacy, and 5) recommender systems and privacy issues. We also discuss open problems and future research directions for user privacy issues in social media.

**Generalized Integrative Principal Component Analysis for Multi-Type Data with Block-Wise Missing Structure**

High-dimensional multi-source data are encountered in many fields. Despite recent developments on the integrative dimension reduction of such data, most existing methods cannot easily accommodate data of multiple types (e.g., binary or count-valued). Moreover, multi-source data often have block-wise missing structure, i.e., data in one or more sources may be completely unobserved for a sample. The heterogeneous data types and presence of block-wise missing data pose significant challenges to the integration of multi-source data and further statistical analyses. In this paper, we develop a low-rank method, called Generalized Integrative Principal Component Analysis (GIPCA), for the simultaneous dimension reduction and imputation of multi-source block-wise missing data, where different sources may have different data types. We also devise an adapted BIC criterion for rank estimation. Comprehensive simulation studies demonstrate the efficacy of the proposed method in terms of rank estimation, signal recovery, and missing data imputation. We apply GIPCA to a mortality study. We achieve accurate block-wise missing data imputation and identify intriguing latent mortality rate patterns with sociological relevance.

**Importance of the Mathematical Foundations of Machine Learning Methods for Scientific and Engineering Applications**

There has been a lot of recent interest in adopting machine learning methods for scientific and engineering applications. This has in large part been inspired by recent successes and advances in the domains of Natural Language Processing (NLP) and Image Classification (IC). However, scientific and engineering problems have their own unique characteristics and requirements raising new challenges for effective design and deployment of machine learning approaches. There is a strong need for further mathematical developments on the foundations of machine learning methods to increase the level of rigor of employed methods and to ensure more reliable and interpretable results. Also as reported in the recent literature on state-of-the-art results and indicated by the No Free Lunch Theorems of statistical learning theory incorporating some form of inductive bias and domain knowledge is essential to success. Consequently, even for existing and widely used methods there is a strong need for further mathematical work to facilitate ways to incorporate prior scientific knowledge and related inductive biases into learning frameworks and algorithms. We briefly discuss these topics and discuss some ideas proceeding in this direction.

**STTM: A Tool for Short Text Topic Modeling**

**Anonymity and Confidentiality in Secure Distributed Simulation**

Research on data confidentiality, integrity and availability is gaining momentum in the ICT community, due to the intrinsically insecure nature of the Internet. While many distributed systems and services are now based on secure communication protocols to avoid eavesdropping and protect confidentiality, the techniques usually employed in distributed simulations do not consider these issues at all. This is probably due to the fact that many real-world simulators rely on monolithic, offline approaches and therefore the issues above do not apply. However, the complexity of the systems to be simulated, and the rise of distributed and cloud based simulation, now impose the adoption of secure simulation architectures. This paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. A performance evaluation based on an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The obtained results show that this is a viable solution.

**Deep Stacked Stochastic Configuration Networks for Non-Stationary Data Streams**

The concept of stochastic configuration networks (SCNs) others a solid framework for fast implementation of feedforward neural networks through randomized learning. Unlike conventional randomized approaches, SCNs provide an avenue to select appropriate scope of random parameters to ensure the universal approximation property. In this paper, a deep version of stochastic configuration networks, namely deep stacked stochastic configuration network (DSSCN), is proposed for modeling non-stationary data streams. As an extension of evolving stochastic connfiguration networks (eSCNs), this work contributes a way to grow and shrink the structure of deep stochastic configuration networks autonomously from data streams. The performance of DSSCN is evaluated by six benchmark datasets. Simulation results, compared with prominent data stream algorithms, show that the proposed method is capable of achieving comparable accuracy and evolving compact and parsimonious deep stacked network architecture.

**The Window Validity Problem in Rule-Based Stream Reasoning**

Rule-based temporal query languages provide the expressive power and flexibility required to capture in a natural way complex analysis tasks over streaming data. Stream processing applications, however, typically require near real-time response using limited resources. In particular, it becomes essential that the underpinning query language has favourable computational properties and that stream processing algorithms are able to keep only a small number of previously received facts in memory at any point in time without sacrificing correctness. In this paper, we propose a recursive fragment of temporal Datalog with tractable data complexity and study the properties of a generic stream reasoning algorithm for this fragment. We focus on the window validity problem as a way to minimise the number of time points for which the stream reasoning algorithm needs to keep data in memory at any point in time.

**MaRe: Container-Based Parallel Computing with Data Locality**

**Optimal stopping via deeply boosted backward regression**

In this note we propose a new approach towards solving numerically optimal stopping problems via boosted regression based Monte Carlo algorithms. The main idea of the method is to boost standard linear regression algorithms in each backward induction step by adding new basis functions based on previously estimated continuation values. The proposed methodology is illustrated by several numerical examples from finance.

**Granger Causality Analysis Based on Quantized Minimum Error Entropy Criterion**

Linear regression model (LRM) based on mean square error (MSE) criterion is widely used in Granger causality analysis (GCA), which is the most commonly used method to detect the causality between a pair of time series. However, when signals are seriously contaminated by non-Gaussian noises, the LRM coefficients will be inaccurately identified. This may cause the GCA to detect a wrong causal relationship. Minimum error entropy (MEE) criterion can be used to replace the MSE criterion to deal with the non-Gaussian noises. But its calculation requires a double summation operation, which brings computational bottlenecks to GCA especially when sizes of the signals are large. To address the aforementioned problems, in this study we propose a new method called GCA based on the quantized MEE (QMEE) criterion (GCA-QMEE), in which the QMEE criterion is applied to identify the LRM coefficients and the quantized error entropy is used to calculate the causality indexes. Compared with the traditional GCA, the proposed GCA-QMEE not only makes the results more discriminative, but also more robust. Its computational complexity is also not high because of the quantization operation. Illustrative examples on synthetic and EEG datasets are provided to verify the desirable performance and the availability of the GCA-QMEE.

**Robust Implicit Backpropagation**

Arguably the biggest challenge in applying neural networks is tuning the hyperparameters, in particular the learning rate. The sensitivity to the learning rate is due to the reliance on backpropagation to train the network. In this paper we present the first application of Implicit Stochastic Gradient Descent (ISGD) to train neural networks, a method known in convex optimization to be unconditionally stable and robust to the learning rate. Our key contribution is a novel layer-wise approximation of ISGD which makes its updates tractable for neural networks. Experiments show that our method is more robust to high learning rates and generally outperforms standard backpropagation on a variety of tasks.

**Mixed Integer Linear Programming for Feature Selection in Support Vector Machine**

This work focuses on support vector machine (SVM) with feature selection. A MILP formulation is proposed for the problem. The choice of suitable features to construct the separating hyperplanes has been modelled in this formulation by including a budget constraint that sets in advance a limit on the number of features to be used in the classification process. We propose both an exact and a heuristic procedure to solve this formulation in an efficient way. Finally, the validation of the model is done by checking it with some well-known data sets and comparing it with classical classification methods.

**Data augmentation using synthetic data for time series classification with deep residual networks**

Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network’s generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN’s accuracy on some datasets and significantly improve the deep model’s accuracy when the method is used in an ensemble approach.

• Objective and Subjective Solomonoff Probabilities in Quantum Mechanics• Acoustic Scene Classification: A Competition Review• Did you take the pill? – Detecting Personal Intake of Medicine from Twitter• A novel topology design approach using an integrated deep learning network architecture• On local matching property in groups and vector space• Lower bounds for trace reconstruction• Troy: Give Attention to Saliency and for Saliency• Withholding aggressive treatments may not accelerate time to death among dying ICU patients• A Note on the Equitable Choosability of Complete Bipartite Graphs• Classical versus quantum probability: Comments on the paper ‘On universality of classical probability with contextually labeled random variables’ by E. Dzhafarov and M. Kon• Optimal voltage control using singular value decomposition of fast decoupled load flow jacobian• Structure and substructure connectivity of balanced hypercubes• Coalescence Model of Rock-Paper-Scissors Particles• Multi-Message Private Information Retrieval using Product-Matrix MSR and MBR Codes• Cores of Cubelike Graphs• Multi-Estimator Full Left Ventricle Quantification through Ensemble Learning• On the number of real classes in the finite projective linear and unitary groups• A Simple Practical Algorithm for a Computationally Hard Problem• Approval Gap of Weighted k-Majority Tournaments• Unbiased Implicit Variational Inference• Low-latency Networking: Where Latency Lurks and How to Tame It• Deep Generative Modeling for Scene Synthesis via Hybrid Representations• Nonlinear Model Order Reduction via Lifting Transformations and Proper Orthogonal Decomposition• Learning to Share and Hide Intentions using Information Regularization• Non-crossing Annular Pairings and The Infinitesimal Distribution of the GOE• Inner Space Preserving Generative Pose Machine• Flow Smoothing and Denoising: Graph Signal Processing in the Edge-Space• Heavy-Traffic Insensitive Bounds for Weighted Proportionally Fair Bandwidth Sharing Policies• Composite Convex Optimization with Global and Local Inexact Oracles• Non-Learning based Deep Parallel MRI Reconstruction (NLDpMRI)• Structure Learning for Relational Logistic Regression: An Ensemble Approach• Attentive Semantic Alignment with Offset-Aware Correlation Kernels• CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps• Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart Surveillance as an Edge Service• The bipartite $K_{2,2}$-free process and bipartite Ramsey number $b(2, t)$• Routing in Wireless Networks with Interferences• The image of a tropical linear space• High Order M-QAM Massive MIMO Detector with Low Computational Complexity for 5G Systems• Weakly Supervised Local Attention Network for Fine-Grained Visual Classification• EOE: Expected Overlap Estimation over Unstructured Point Cloud Data• Duality between front and rear mutations in cluster algebras• The Parameterized Complexity of Finding Point Sets with Hereditary Properties• A Note on Transportation Cost Inequalities for Diffusions with Reflections• Efficient Fusion of Sparse and Complementary Convolutions for Object Recognition and Detection• Fast Variance Reduction Method with Stochastic Batch Size• Dialog-context aware end-to-end speech recognition• Adapted $θ$-Scheme and Its Error Estimates for Backward Stochastic Differential Equations• Test without Trust: Optimal Locally Private Distribution Testing• Random conductance models with stable-like jumps: heat kernel estimates and Harnack inequalities• Instance-Dependent PU Learning by Bayesian Optimal Relabeling• Optimal bail-out dividends problem with transaction cost and capital injection constraint• Round-Table Group Optimization for Sequencing Problems• Robust path-following control for articulated heavy-duty vehicles• A Bayesian Downscaler Model to Estimate Daily PM2.5 levels in the Continental US• Quantized Densely Connected U-Nets for Efficient Landmark Localization• Fisher information matrix for moving single molecules with stochastic trajectories• The Persistent Homology of Random Geometric Complexes on Fractals• Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image• On second-order sufficient optimality conditions for $C^1$ vector optimization problems• Generative Adversarial Estimation of Channel Covariance in Vehicular Millimeter Wave Systems• Completely Positive Binary Tensors• Contemplating Visual Emotions: Understanding and Overcoming Dataset Bias• Allocations of Standby Redundancies to Coherent Systems with Dependent Components• Power Constrained Parallel Queuing with Contention• Simulation of the electrical conductivity of two-dimensional films with aligned rod-like conductive fillers: effect of the filler length dispersity• Hierarchical Clustering better than Average-Linkage• Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection• Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep Learning• Robust Pricing with Refunds• Inferring Molecular Pathology and micro-RNA Transcriptome from mRNA Profiles of Cancer Biopsies through Deep Multi-Task Learning• Speeding Up Distributed Gradient Descent by Utilizing Non-persistent Stragglers• Optimal Subpattern Assignment Metric for Multiple Tracks (OSPAMT Metric)• Modularity of Erdös-Rényi random graphs• A Generic Multi-Projection-Center Model and Calibration Method for Light Field Cameras• SAM-RCNN: Scale-Aware Multi-Resolution Multi-Channel Pedestrian Detection• The Second Neighborhood Conjecture for some Special Classes of Graphs• Automorphisms on the ring of symmetric functions and stable and dual stable Grothendieck polynomials• GEEC: Scalable, Efficient, and Consistent Consensus for Blockchains• Performance-Aware Management of Cloud Resources: A Taxonomy and Future Directions• Observability transitions in clustered networks• Multi-Output Convolution Spectral Mixture for Gaussian Processes• MatchBench: An Evaluation of Feature Matchers• Privacy-aware Minimum Error Probability Estimation: An Entropy Constrained Approach• ODSQA: Open-domain Spoken Question Answering Dataset• Predicting Visual Context for Unsupervised Event Segmentation in Continuous Photo-streams• How did the discussion go: Discourse act classification in social media conversations• Motorcycle detection and classification in urban Scenarios using a model based on Faster R-CNN• Universal Perceptual Grouping• Deep Factorised Inverse-Sketching• Modelling hidden structure of signals in group data analysis with modified (Lr, 1) and block-term decompositions• Codegree threshold for tiling $k$-graphs with two edges sharing exactly $\ell$ vertices• Deep Learning for Domain Adaption: Engagement Recognition• Small time asymptotics for Brownian motion with singular drift• Viscosity solutions to Hamilton-Jacobi-Bellman equations associated with sublinear Lévy(-type) processes• Finitary codings for the random-cluster model and other infinite-range monotone models• A Very Brief Introduction to Machine Learning With Applications to Communication Systems• On the integrality gap of the maximum-cut semidefinite programming relaxation in fixed dimension• Approximations of Schatten Norms via Taylor Expansions• YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud• On the behavior of random RNA secondary structures near the glass transition• VC dimension and a union theorem for set systems• Capturing global spatial context for accurate cell classification in skin cancer histology• On the Computational Complexity of Length- and Neighborhood-Constrained Path Problems• Adaptive optimal kernel density estimation for directional data• Adhesion-induced Discontinuous Transitions and Classifying Social Networks• Emitter Identification Using CNN IQ Imbalance Estimators• Word-Level Loss Extensions for Neural Temporal Relation Classification• The functional Breuer-Major theorem• A distributed regression analysis application based on SAS software. Part I: Linear and logistic regression• A distributed regression analysis application based on SAS software Part II: Cox proportional hazards regression• Control of Multi-Agent Systems with Finite Time Control Barrier Certificates and Temporal Logic• Application of End-to-End Deep Learning in Wireless Communications Systems• Building Encoder and Decoder with Deep Neural Networks: On the Way to Reality• Log-Contrast Regression with Functional Compositional Predictors: Linking Preterm Infant’s Gut Microbiome Trajectories in Early Postnatal Period to Neurobehavioral Outcome• Stein’s method for asymmetric $α$-stable distributions, with application to the stable CLT• Spinal Cord Gray Matter-White Matter Segmentation on Magnetic Resonance AMIRA Images with MD-GRU• Fast and Accurate Camera Covariance Computation for Large 3D Reconstruction• The largest real eigenvalue in the real Ginibre ensemble and its relation to the nonlinear Schrödinger equation• Quantum Lower Bound for Approximate Counting Via Laurent Polynomials• Low-temperature anomalies in disordered solids: A cold case of contested relics?• Fluctuation bounds for continuous time branching processes and nonparametric change point detection in growing networks• Overhead Detection: Beyond 8-bits and RGB• Evolution of Preferences in Multiple Populations• The Sample Complexity of Up-to-$\varepsilon$ Multi-Dimensional Revenue Maximization• Communicating Using Spatial Mode Multiplexing: Potentials, Challenges and Perspectives• The Dyson Game• Physics-based modeling and data representation of pedestrian pairwise interactions• Theory of supports for linear codes endowed with the sum-rank metric• SketchyScene: Richly-Annotated Scene Sketches• Multi-Label Zero-Shot Learning with Transfer-Aware Label Embedding Projection• The Caching Broadcast Channel with a Wire and Cache Tapping Adversary of Type II





### Like this:

Like Loading...


*Related*

