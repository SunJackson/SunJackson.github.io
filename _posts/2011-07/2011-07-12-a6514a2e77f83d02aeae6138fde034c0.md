---
layout:     post
title:      Ridge regression path
subtitle:   转载自：http://fa.bianp.net/blog/2011/ridge-regression-path/
date:       2011-07-12
author:     Fabian Pedregosa
header-img: img/background0.jpg
catalog: true
tags:
    - ridge
    - cross
    - post
---

Ridge coefficients for multiple values of the regularization parameter
can be elegantly computed by updating the *thin* SVD decomposition of
the design matrix:

This can be used to efficiently compute what it's regularization
path, that is, to plot the coefficients as a function of the
regularization parameter. Since the bottleneck of the algorithm is the
singular value decomposition, computing the coefficients for other
values of the regularization parameter basically comes for free.
![](http://fseoane.net/blog/static/uploads/2011/07/ridge_nocv.png)


A variant of this algorithm can then be used to compute the
optimal regularization parameter in the sense of leave-one-out
cross-validation and is implemented in scikit-learn's [RidgeCV](http://scikit-learn.sourceforge.net/dev/modules/linear_model.html#generalized-cross-validation) (for
which Mathieu Blondel has an [excelent post](http://www.mblondel.org/journal/2011/02/09/regularized-least-squares) by ). This optimal
parameter is denoted with a vertical dotted line in the following
picture, full code can be found [here](https://gist.github.com/1076844).
![](http://fseoane.net/blog/static/uploads/2011/07/ridge.png)

