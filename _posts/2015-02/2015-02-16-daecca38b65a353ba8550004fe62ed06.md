---
layout:     post
title:      Hadoop Command Line Cheatsheet
subtitle:   转载自：http://danielnee.com/2015/02/hadoop-command-line-cheatsheet/
date:       2015-02-16
author:     dn
header-img: img/background0.jpg
catalog: true
tags:
    - files
    - file_name
    - hadoop
    - hdfs dfs
    - jobs
---

Useful commands when using Hadoop on the command line

Full reference can be found in [Hadoop Documentation](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html).

#### ls







||

List the contents of provided directory.

#### put







||hdfs dfs -put <local file> <dst file>|

Put the local file to provided HDFS location

#### get







||hdfs dfs -get <hdfs loc> <local loc>|

Copy the file to the local file system

#### cat/text







||hdfs dfs -cat <file>hdfs dfs -text <file>|

hdfs dfs -text <file>

Outputs the contents of HDFS file to standard output. Text command will also read compressed files and output uncompressed data.

Common usecase is that you want to check contents of the file, but not output the whole file. Pipe the contents to head.



||hdfs dfs -text <file> | head|


cp/mv




||hdfs dfs -cp <source> <dst>hdfs dfs -mv <source> <dst>|

hdfs dfs -mv <source> <dst>

cp is short for copy, copy file from source to destination.

mv is short for move, move file from source to destination.

#### chmod







||hdfs dfs -chmod [-R] <mode> <file/dir>|

Change the permissions of the file/directory. Uses standard Unix file permissions.

#### getmerge







||hdfs dfs -getmegre <source dir> <local file>|

Takes a source directory and concatenates all the content and outputs to a local file. Very useful as commonly Hadoop jobs will output multiple output files depending on the number of mappers/reducers you have.

#### rm







||hdfs dfs -rm [-r] [-skipTrash] <file/dir>|

Deletes a file from HDFS. The -r means perform recursively. You will need to do this for directories.

By default the files will be moved to trash that will eventually be cleaned up. This means the space will not be immediately freed up. If you need the space immediately you can use -skipTrash, note this will mean you can reverse the delete.

#### du







||hdfs dfs -du [-s] [-h] <file/dir>|

Displays the sizes of directories/files. Does this recursively, so extremely useful for find out how much space you have. The -h option makes the sizes human readable.  The -s option summarises all the files, instead of giving you individual file sizes.

One thing to note is that the size reported is un-replicated.  If your replication factor is 3, the actual disk usage will be 3 times this size.

#### count







||hfds dfs -count [-q] <dir>|

I commonly use this command to find out how much quota I have available on a specific directory (you need to add the -q options for this).



||$ hadoop fs -count -q /path/to/directory  QUOTA  REMAINING_QUOTA     SPACE_QUOTA  REMAINING_SPACE_QUOTA    DIR_COUNT  FILE_COUNT      CONTENT_SIZE FILE_NAME   none              inf  54975581388800          5277747062870        3922       418464    16565944775310 hdfs://master:54310/path/to/directory|

  QUOTA  REMAINING_QUOTA     SPACE_QUOTA  REMAINING_SPACE_QUOTA    DIR_COUNT  FILE_COUNT      CONTENT_SIZE FILE_NAME

To work out how much quota you have used, SPACE_QUOTA – REMAINING_SPACE_QUOTA, e.g. 54975581388800 – 5277747062870 or 54.97TB – 5.27TB = 49.69TB left.

Note this figures are the replicated numbers.

#### Admin Report







||

Useful command for finding out total usage on the cluster. Even without superuser access you can see current total capacity and usage.

#### Launching Hadoop Jobs

Commonly you will have a fat jar file containing all the code for your map reduce job. Launch via:



||hadoop jar <jar> [mainClass] args...|

A Scalding job is launched using:



||hadoop jar <jar> com.twitter.scalding.Tool [mainClass] args|

If you need to kill a map-reduce job, use:



||

You can find the job id in the resource manager or in the log of the job launch. This can be used to kill any map-reduce job (Standard Hadoop, Scalding, Hive, etc.) but not Impala or Spark jobs for instance.
