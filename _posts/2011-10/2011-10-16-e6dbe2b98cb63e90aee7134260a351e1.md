---
layout:     post
title:      Speeding up pandas's file parsers with Cython
subtitle:   转载自：http://wesmckinney.com/blog/speeding-up-pandass-file-parsers-with-cython/
date:       2011-10-16
author:     Wes McKinney
header-img: img/background3.jpg
catalog: true
tags:
    - indexed
    - indexes
    - values
    - data
    - na value
---





** Sun 16 October 2011

 

I've tried to make the file parsing functions in pandas, `read_csv` and `read_table`, as robust (they do the right thing) and fast as possible. What do we really care about?

- Good performance: can read a CSV file as fast as other statistical computing / data analysis languages, like R

- Proper type handling: if a column of data has integers, the resulting column will be integer dtype. But you can't be too aggressive, values like '1.0' and '2.0' should not get converted to integers.

- Support for converting strings into datetime objects

- Support for returning indexed DataFrame (with a simple or hierarchical index)

- NA value handling. This is tricky as you want to recognize lots of common NA representations ("NA", "NULL", etc.) and also allow the user to specify custom NA value strings.


While hacking yesterdays with folks at the [Data Without Borders](http://datawithoutborders.cc/) event, I realized that boolean values
("True" and "False") weren't resulting in an boolean array in the result. Also,
I was not satisfied with the performance of the pure Python code. Since I've
had great results using [Cython](http://cython.org/.) to create C extensions, it was the natural
choice. The results were great: parsing the data set we were looking at at DWB
went from about 8 seconds before to 800ms, a full 10x improvement. I also fixed
a number of bugs / corner cases with type handling.

### TL;DR pandas.read_csv is a lot faster now

Here was the speed of `read_csv` before these changes on a fairly big file (46738×54):

This obviously will not do. And here post-Cythonization:

As a point of comparison, R is pretty speedy but about 2x slower.

In fairness I am 100% sure that `read.csv` is "doing" a lot more, but it shows that I'm at least on the right track.

I won't rehash all the code, but there were a number of interesting things along the way.

### The basics: working with NumPy arrays in Cython

One of the truly beautiful things about programming in Cython is that you can get the speed of working with a C array representing a multi-dimensional array (e.g. `double *`) without the headache of having to handle the striding information of the ndarray yourself. Also, you can work with non-contiguous arrays and arrays with dtype=object (which are just arrays of `PyObject*` underneath) with no code changes (!). Cython calls this the [buffer interface](http://docs.cython.org/src/tutorial/numpy.html):

For multi-dimensional arrays, you specify the number of dimensions in the buffer. and pass multiple indexes (`Py_ssize_t` is the proper C "index type" to use). I'll demonstrate this in:

### Converting rows to columns faster than zip(*rows)

A cool Python trick to convert rows to column is:

While `zip` is very fast (a built-in Python function), the larger problem here is that our target data structure is NumPy arrays to begin with. So it would make sense to write out the rows directly to a 2-dimensional object array:

And lo and behold, this function is significantly faster than the zip trick:

It's even more of a big deal if you zip **and** convert to ndarray:

### Numeric conversion: floats, ints, and NA's, oh my

When converting the Python strings to numeric data, you must:

- Check that the value is not among the set of NA values

- Handle data like `['1', '2', '3.5', '4']` where you may have "integer-like" data until you observe a floating point value


Unfortunately, code for this sort of thing ends up looking like a state machine 99% of the time, but at least it's fairly tidy in Cython and runs super fast:

Adopting the Python philosophy that it's "easier to ask forgiveness than permission" if float conversion ever fails, the exception will get raised and the code will just leave the column as dtype=object. And this function would obviously have problems with European decimal format— but I'm not willing to compromise performance in 99% cases for the sake of the 1% cases. It will make sense to write a slower function that also handles a broader variety of formatting issues.
