---
layout:     post
catalog: true
title:      Intuition for principal component analysis (PCA)
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/J-aQntbRcww/
date:      2018-12-06
author:      Learning Machines
tags:
    - dat
    - pca_
    - teapot
    - mathematically
    - eigenval
---










Principal component analysis (PCA) is a dimension-reduction method that can be used to reduce a large set of (often correlated) variables into a smaller set of (uncorrelated) variables, called principal components, which still contain most of the information.

PCA is a concept that is traditionally hard to grasp so instead of giving you the n’th mathematical derivation I will provide you with some intuition.

Basically PCA is nothing else but a projection of some higher dimensional object into a lower dimension. What sounds complicated is really something we encounter every day: when we watch TV we see a 2D-projection of 3D-objects!

Have a look at this instructive youtube video.

So what we want is not only a projection but a projection which retains as much information as possible. A good strategy is to find the longest axis of the object and after that turn the object around this axis so that we again find the second longest axis.

Mathematically this can be done by calculating the so called eigenvectors of the covariance matrix, after that we use just use the n eigenvectors with the biggest eigenvalues to project the object into n-dimensional space (in our example n = 2) – but as I said I won’t go into the mathematical details here (many good explanations can be found here on stackexchange).

To make matters concrete we will now recreate the example from the video above! The teapot data is from https://www.khanacademy.org/computer-programming/3d-teapot/971436783 – there you can also turn it around interactively.

Have a look at the following code:

![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2018/12/teapot-in-3D-1024x731.png?w=450)
![](https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2018/12/teapot-in-3D-1024x731.png?w=450)


And now we project that teapot from 3D-space onto a 2D-plane while retaining as much information as possible by taking the two eigenvectors with biggest eigenvalues:

![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2018/12/teapot-in-2D-1024x731.png?w=450)
![](https://i0.wp.com/blog.ephorie.de/wp-content/uploads/2018/12/teapot-in-2D-1024x731.png?w=450)


That was fun, wasn’t it!

In data science you will often use PCA to condense large datasets into smaller datasets while retaining most of the information. PCA is like clustering an unsupervised learning method.


*Related*








---
