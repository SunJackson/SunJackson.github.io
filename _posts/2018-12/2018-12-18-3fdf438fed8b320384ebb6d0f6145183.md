---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://analytixon.com/2018/12/18/if-you-did-not-already-know-582/
date:      2018-12-18
author:      Michael Laux
tags:
    - emph metric
    - experiments
    - shufflenet
    - comprehensive ablation
    - direct
---

**Synthetic Gradient (SG)** ![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
Artifical Neural Network are a particular class of learning system modeled after biological neural functions with an interesting penchant for Hebbian learning, that is ‘neurons that wire together, fire together’. However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred. Such a constraint though may have sufficed for a while, is now no longer feasible in the era of very-large-scale machine learning, coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. To solve this problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are introduced as a viable alternative to the backpropagation algorithm. This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as over to a standard neural interface using multilayer perceptron MLP. SG-DNI shows good promise, in that it not only captures the learning problem, it is also over 3-fold faster due to it asynchronous learning capabilities. … 

**Semantic Analysis** ![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
Semantic analysis may refer to:· Semantic analysis (compilers)· Semantic analysis (machine learning)· Semantic analysis (knowledge representation)· Semantic analysis (linguistics) … 

**ShuffleNet V2** ![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. … 





### Like this:

Like Loading...


*Related*

