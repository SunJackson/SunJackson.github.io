---
layout:     post
title:      Receiver Operating Characteristic Curves Demystified (in Python)
subtitle:   转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/ljlQvKSOuoA/receiver-operating-characteristic-curves-demystified-python.html
date:       2018-07-20
author:     Matt Mayo Editor
header-img: img/background2.jpg
catalog: true
tags:
    - https
    - models
    - roc curves
    - classes
    - distributions
    - model performance
    - positive
    - total
    - threshold
    - plotting
    - plots
    - random
    - function
    - fraud
    - auc
    - true
    - receiver
    - false
    - bad
    - rate
    - additional
    - derived
    - deriving
---

**By [Syed Sadat Nazrul](https://www.linkedin.com/in/snazrul1), Analytic Scientist**

![](https://cdn-images-1.medium.com/max/800/1*k65OKy7TOhBWRIfx0u6JqA.png)


In Data Science, evaluating model performance is very important and the most commonly used performance metric is the classification score. However, when dealing with fraud datasets with heavy class imbalance, a classification score does not make much sense. Instead, Receiver Operating Characteristic or ROC curves offer a better alternative. ROC is a plot of signal (True Positive Rate) against noise (False Positive Rate). The model performance is determined by looking at the area under the ROC curve (or AUC). The best possible AUC is 1 while the worst is 0.5 (the 45 degrees random line). Any value less than 0.5 means we can simply do the exact opposite of what the model recommends to get the value back above 0.5.

While ROC curves are common, there aren’t that many pedagogical resources out there explaining how it is calculated or derived. In this blog, I will reveal, step by step, how to plot an ROC curve using Python. After that, I will explain the characteristics of a basic ROC curve.

 

### Probability Distribution of Classes

 First off, let us assume that our hypothetical model produced some probabilities for predicting the class of each record. As with most binary fraud models, let’s assume our classes are ‘good’ and ‘bad’ and the model produced probabilities of P(X=’bad’). To create this, probability distribution, we plot a Gaussian distribution with different mean values for each class. For more information on Gaussian distribution, read [this blog](https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2).



Now that we have the distribution, let’s create a function to plot the distributions.



Now let’s use this **plot_pdf** function to generate the plot:



![](https://cdn-images-1.medium.com/max/800/1*qLDlIbU26FY_PNRTdqRboQ.png)


Now we have the probability distribution of the binary classes, we can now use this distribution to derive the ROC curve.

 

### Deriving ROC Curve

 To derive the ROC curve from the probability distribution, we need to calculate the True Positive Rate (TPR) and False Positive Rate (FPR). For a simple example, let’s assume the threshold is at P(X=’bad’)=0.6 .

![](https://cdn-images-1.medium.com/max/800/1*hf2fRUKfD-hCSw1ifUOCpg.png)


True positive is the area designated as “bad” on the right side of the threshold. False positive denotes the area designated as “good” on the right of the threshold. Total positive is the total area under the “bad” curve while total negative is the total area under the “good” curve. We divide the value as shown in the diagram to derive TPR and FPR. We derive the TPR and FPR different threshold values to get the ROC curve. Using this knowledge, we create the ROC plot function:



Now let’s use this **plot_roc** function to generate the plot:



![](https://cdn-images-1.medium.com/max/800/1*d0U_sTGJ40CKHYxEyG3A8w.png)


Now plotting the probability distribution and the ROC next to eachother for visual comparison:



![](https://cdn-images-1.medium.com/max/800/1*k65OKy7TOhBWRIfx0u6JqA.png)


 

### Effect of Class Separation

 Now that we can derive both plots, let’s see how the ROC curve changes as the class separation (i.e. the model performance) improves. We do this by altering the mean value of the Gaussian in the probability distributions.



![](https://cdn-images-1.medium.com/max/800/1*OBZ6Z1_i_iF9qj4R6I4eUA.png)


As you can see, the AUC increases as we increase the separation between the classes.

 

### Looking Beyond The AUC

 Beyond AUC, the ROC curve can also help debug a model. By looking at the shape of the ROC curve, we can evaluate what the model is misclassifying. For example, if the bottom left corner of the curve is closer to the random line, it implies that the model is misclassifying at X=0. Whereas, if it is random on the top right, it implies the errors are occurring at X=1. Also, if there are spikes on the curve (as opposed to being smooth), it implies the model is not stable.

 **Additional Information**

 **Bio: [Syed Sadat Nazrul](https://www.linkedin.com/in/snazrul1)** is using Machine Learning to catch cyber and financial criminals by day... and writing cool blogs by night.

[Original](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0). Reposted with permission.

**Related:**



 
