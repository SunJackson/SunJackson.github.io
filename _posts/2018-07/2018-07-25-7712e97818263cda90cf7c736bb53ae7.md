---
layout:     post
catalog: true
title:      Differentiable Image Parameterizations
subtitle:      转载自：https://distill.pub/2018/differentiable-parameterizations
date:      2018-07-25
img:      3
author:      未知
tags:
    - images
    - optimization
    - optimized
    - optimizing
    - optimizes
---
神经网络可视化和艺术的一个强大的，探索不足的工具。


受过训练以对图像进行分类的神经网络具有非凡的意义和惊人的! - 生成图像的能力。
 诸如DeepDream，样式传输和特征可视化等技术利用这种能力作为探索神经网络内部工作的强大工具，并为基于神经艺术的小型艺术运动提供动力。


所有这些技术的工作方式大致相同。
 计算机视觉中使用的神经网络具有他们所看到的图像的丰富内部表示。
 我们可以使用此表示来描述我们希望图像具有的属性(例如样式)，然后优化输入图像以具有这些属性。
 这种优化是可能的，因为网络在输入方面是可微分的：我们可以稍微调整图像以更好地适应所需的属性，然后在梯度下降中迭代地应用这种调整。


通常，我们将输入图像参数化为每个像素的RGB值，但这不是唯一的方法。
 只要从参数到图像的映射是可微的，我们仍然可以使用梯度下降来优化替代参数化。


{% raw %}
[1]({{U＃R·L}})：
{% endraw %}
 只要一个
图像参数化
是可区分的，我们可以反向传播
()
通过它。


可区分的图像参数化邀请我们询问“我们可以反向传播哪种图像生成过程？”
 答案非常多，一些更奇特的可能性可以产生各种有趣的效果，包括3D神经艺术，透明图像和对齐插值。
 之前使用特定异常图像参数化的工作已经显示出令人兴奋的结果 - 我们认为缩小并整体观察这个区域表明存在更多潜力。


替代参数化可以实现许多有趣的效果，包括3D神经艺术，具有透明度的图像和对齐插值。
 我们相信
 虽然最近的工作已经开始探索其中的一些可能性[]，但我们认为还有很多尚未开发的潜力

### 为什么参数化很重要？


尽管实际优化的目标函数保持不变，但改变优化问题的参数化可能会显着改变结果，这似乎令人惊讶。
 我们看到参数化选择可能产生重大影响的四个原因：


**(1) - 改进优化**  - 
转换输入以使优化问题更容易 - 一种称为“预处理”的技术 - 是优化的主要内容。
 
 预处理通常表现为梯度的变换
 (通常将其乘以正定的“预处理器”矩阵)。
 但是，这相当于优化输入的备用参数化。
 
我们发现参数化的简单变化使得神经艺术和图像优化的图像优化变得更加容易。


**(2) - 景点**  - 
当我们优化对神经网络的输入时，通常存在许多不同的解决方案，对应于不同的局部最小值。
 
 训练以复杂优化景观为特征的深度神经网络，其对于给定目标可具有许多同样良好的局部最小值。
 (请注意，找到全局最小值并不总是可取的，因为它可能会导致模型过度拟合。)
 因此，优化神经网络的输入也会产生许多局部最小值也就不足为奇了。
 
我们的优化过程落入任何特定局部最小值的概率由其吸引力盆地(即，在最小值的影响下的优化景观的区域)控制。
已知改变优化问题的参数化会改变不同吸引盆的大小，从而影响可能的结果。


**(3) - 附加限制**  - 
一些参数化仅涵盖可能输入的子集，而不是整个空间。
在这样的参数化中工作的优化器仍将找到最小化或最大化目标函数的解决方案，但它们将受到参数化的约束。
通过选择正确的约束集，可以施加各种约束，范围从简单约束(例如，图像的边界必须是黑色)到丰富的细微约束。


**(4) - 隐式优化其他物体**  - 
 参数化可以在内部使用与其输出的对象不同的对象，并且我们进行优化。
 例如，虽然视觉网络的自然输入是RGB图像，但我们可以将该图像参数化为3D对象的渲染，并且通过在渲染过程中反向传播来优化该图像。
 因为3D对象具有比图像更多的自由度，所以我们通常使用*随机*参数化来生成从不同视角渲染的图像。

在本文的其余部分，我们给出了具体的例子，这些方法是有益的，并导致令人惊讶和有趣的视觉效果。

---

{% raw %}
[1]({{U＃R·L}})
{% endraw %}

## [对齐特征可视化插值](https://distill.pub/2018/differentiable-parameterizations#section-aligned-interpolation)


作为介绍性示例，我们探索如何创建[对齐插值神经元可视化](https://distill.pub/2017/feature-visualization#interpolation)。
在整篇文章中，我们将经常优化随机初始化的参数化，以生成由神经网络中的神经元，通道或层检测到的模式。
我们将此过程的结果称为特征可视化目标函数的优化，因为它揭示了网络在不同层中检测到的特征。


特征可视化最常用于可视化单个神经元，
 但它也可以用于[可视化神经元的组合](https://distill.pub/2017/feature-visualization#interaction)，以研究它们如何相互作用。
 不是优化图像以使单个神经元激发，而是优化它以使多个神经元激发。


当我们想要真正理解两个神经元之间的相互作用时，
 我们可以更进一步，创建多个可视化，
 逐渐将目标从优化一个神经元转移到对其他神经元发射更加重视。
 这在某些方面类似于像GAN这样的生成模型的潜在空间中的插值。


尽管如此，还是存在一个小挑战：特征可视化是随机的。
 即使您针对完全相同的目标进行优化，每次都会以不同的方式布置可视化。
 通常，这不是问题，但它确实减损了插值可视化。
 如果我们天真地制作它们，结果可视化将是*未对齐*：
 诸如眼睛的视觉地标出现在每个图像中的不同位置。
 这种缺乏对齐可能会因为目标略有不同而更难看出差异，
 因为他们被布局中更大的差异所淹没。

如果我们将插值帧看作动画，我们可以看到独立优化的问题：


{% raw %}
![]({{U＃R·L}})
{% endraw %}
  - >
{% raw %}
[2]({{U＃R·L}})
{% endraw %}


 我们如何实现这种对齐插值，其中视觉地标不在帧之间移动？
 可以尝试许多可能的方法
 例如，可以明确地惩罚相邻帧之间的差异。我们的最终结果和我们的colab笔记本将此技术与共享参数化结合使用。
 ，其中一个是使用*共享参数化*：每个帧被参数化为其自己的唯一参数化和单个共享参数化的组合。
 

{% raw %}
[3]({{U＃R·L}})
{% endraw %}


 通过在帧之间部分共享参数化，我们鼓励生成的可视化自然对齐。
 直观地，共享参数化为视觉标志的位移提供了公共参考，而独特的参数化基于其插值权重给予每个帧其自身的视觉吸引力。
 
{% raw %}
 具体而言，我们结合了通常较低分辨率的共享参数化Pshared P _ {\ text {shared}} Pshared和全分辨率独立参数化PuniqueiP _ {\ text {unique}} ^ iPuniquei，它们对于可视化的每个帧iii都是唯一的。
{% endraw %}
{% raw %}
 然后将每个单独的帧iii参数化为两者的组合PiP ^ iPi，Pi =σ(Puniquei + Pshared)P ^ i = \ sigma(P _ {\ text {unique}} ^ i + P _ {\ text {shared} })Pi =σ(Puniquei + Pshared)，其中σ\sigmaσ是逻辑sigmoid函数。
{% endraw %}
 此参数化不会改变目标，但它确实扩大了可视化对齐的**(2)吸引池**。
 
 我们可以明确地想象共享参数化如何影响玩具示例中的吸引力盆地。
 让我们考虑优化两个变量xxx和yyy以最小化L(z)=(z2-1)2L(z)=(z ^ 2-1)^ 2L(z)=(z2-1)2。
 由于L(z)L(z)L(z)有两个吸引盆z = 1z = 1z = 1或z = -1z = -1z = -1，因此这对优化问题有四个解：
 (x，y)=(1,1)(x，y)=(1,1)(x，y)=(1,1)，(x，y)=( -  1,1)(x，y )=(-1,1)(x，y)=( -  1,1)，(x，y)=(1，-1)(x，y)=(1，-1)(x，y) =(1，-1)，或(x，y)=( -  1，-1)(x，y)=( -  1，-1)(x，y)=( -  1，-1)。
 让我们考虑随机初始化xxx和yyy，然后优化它们。
 通常，优化问题是独立的，因此xxx和yyy同样可能会将未对齐的解决方案(它们具有不同的符号)作为对齐的解决方案。
 但是如果我们添加共享参数化，问题就会变得耦合，并且它们对齐的吸引力范围变得更大。


我们如何实现这种对齐插值，其中视觉地标不在帧之间移动？
 可以尝试许多可能的方法
 例如，可以明确地惩罚相邻帧之间的差异。我们的最终结果和我们的colab笔记本将此技术与共享参数化结合使用。
 
 ，其中一个是使用*共享参数化*：每个帧被参数化为其自己的唯一参数化和单个共享参数化的组合。


通过在帧之间部分共享参数化，我们鼓励生成的可视化自然对齐。
 直观地，共享参数化为视觉标志的位移提供了公共参考，而独特的参数化基于其插值权重给予每个帧其自身的视觉吸引力。
 
{% raw %}
 具体而言，我们结合了通常较低分辨率的共享参数化Pshared P _ {\ text {shared}} Pshared和全分辨率独立参数化PuniqueiP _ {\ text {unique}} ^ iPuniquei，它们对于可视化的每个帧iii都是唯一的。
{% endraw %}
{% raw %}
 然后将每个单独的帧iii参数化为两者的组合PiP ^ iPi，Pi =σ(Puniquei + Pshared)P ^ i = \ sigma(P _ {\ text {unique}} ^ i + P _ {\ text {shared} })Pi =σ(Puniquei + Pshared)，其中σ\sigmaσ是逻辑sigmoid函数。
{% endraw %}
 此参数化不会改变目标，但它确实扩大了可视化对齐的**(2)吸引池**。
 
 我们可以明确地想象共享参数化如何影响玩具示例中的吸引力盆地。
 让我们考虑优化两个变量xxx和yyy以最小化L(z)=(z2-1)2L(z)=(z ^ 2-1)^ 2L(z)=(z2-1)2。
 由于L(z)L(z)L(z)有两个吸引盆z = 1z = 1z = 1或z = -1z = -1z = -1，因此这对优化问题有四个解：
 (x，y)=(1,1)(x，y)=(1,1)(x，y)=(1,1)，(x，y)=( -  1,1)(x，y )=(-1,1)(x，y)=( -  1,1)，(x，y)=(1，-1)(x，y)=(1，-1)(x，y) =(1，-1)，或(x，y)=( -  1，-1)(x，y)=( -  1，-1)(x，y)=( -  1，-1)。
 让我们考虑随机初始化xxx和yyy，然后优化它们。
 通常，优化问题是独立的，因此xxx和yyy同样可能会将未对齐的解决方案(它们具有不同的符号)作为对齐的解决方案。
 但是如果我们添加一个共享参数化，那么问题会变得耦合，并且它们对齐的吸引力范围会变得更大。![](https://distill.pub/2018/images/diagrams/basin-alignment.png)

这是一个初始的例子，说明可微分参数化通常如何成为可视化神经网络的有用的附加工具。

---

{% raw %}
[2]({{U＃R·L}})
{% endraw %}

## [使用非VGG架构的样式传输](https://distill.pub/2018/differentiable-parameterizations#section-styletransfer)


神经风格转移有一个谜：
 尽管取得了巨大的成功，但几乎所有的风格转移都是通过** VGG架构**的变体完成的。
 这并不是因为没有人对在其他体系结构上进行样式传输感兴趣，而是因为在其他体系结构上进行样式操作的尝试始终效果不佳。
 
{% raw %}
 使用不同体系结构执行的实验示例可以在[Medium](https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8)，[Reddit](https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style)和[Twitter]({{u＃r} ＃1}})。
{% endraw %}


已经提出了几个假设来解释为什么VGG比其他模型工作得更好。
 一个建议的解释是，VGG的大尺寸使其能够捕获其他模型丢弃的信息。
 假设的这些额外信息对分类没有帮助，但它确实使模型更好地用于样式转移。
 另一种假设是，其他模型的下采样比VGG更为积极，丢失了空间信息。
 我们怀疑可能还有另一个因素：大多数现代视觉模型的梯度都有棋盘格，这可能使得风格化图像的优化变得更加困难。


在以前的工作中，我们发现[去相关参数化可以显着改善优化](https://distill.pub/2017/feature-visualization#preconditioning)。
 我们发现相同的方法也改进了样式转换，允许我们使用一个模型，否则不会产生视觉上吸引人的样式转移结果：


{% raw %}
[4]({{U＃R·L}})：
{% endraw %}
 在“最终图像优化”下移动滑块，以比较像素空间中的优化与去相关空间中的优化。两个图像都是使用相同的目标创建的，仅在参数化方面有所不同。


让我们更详细地考虑这个变化。样式转移涉及三个图像：内容图像，样式图像和我们优化的图像。
 所有这三个都馈入CNN，并且样式转移目标基于这些图像如何激活CNN的差异。
 我们唯一的变化是我们如何参数化优化的图像。我们使用缩放的傅立叶变换，而不是根据像素(与其邻居高度相关)对其进行参数化。

{% raw %}
![]({{U＃R·L}})
{% endraw %}

我们的确切实施可以在随附的笔记本中找到。请注意，它还使用[transformation robustness](https://distill.pub/2017/feature-visualization#regularizer-playground-robust)，这并非风格转移的所有实现都使用。

---

{% raw %}
[3]({{U＃R·L}})
{% endraw %}

## [组合模式生成网络](https://distill.pub/2018/differentiable-parameterizations#section-xy2rgb)


到目前为止，我们已经使用像素或傅立叶分量探索了与我们通常认为的图像相对接近的图像参数化。
 在本节中，我们将探讨**(3)通过使用不同的参数化为优化过程添加额外约束**的可能性。
 更具体地说，我们将图像参数化为神经网络 - 特别是组合模式生成网络(CPPN)。

CPPN是将(x，y)(x，y)(x，y)位置映射到图像颜色的神经网络：


通过将CPPN应用于位置网格，可以制作任意分辨率的图像。
 CPPN网络的参数 - 权重和偏差 - 决定产生什么图像。
 根据为CPPN选择的体系结构，生成的图像中的像素是约束，以在一定程度上共享其邻居的颜色。


随机参数可以产生美学上有趣的图像，但我们可以通过学习CPPN的参数来产生更有趣的图像。
 通常这是通过进化来完成的;在这里，我们探索了反向传播某些目标函数的可能性，这种特征可视化目标。
 这很容易实现，因为CPPN网络是可微分的，因为卷积神经网络和目标函数也可以通过CPPN传播以相应地更新其参数。
 也就是说，CPPN是一种可微分的图像参数化 - 一种在任何神经艺术或可视化任务中参数化图像的通用工具。

使用CPPN作为图像参数化可以为神经艺术增添一种有趣的艺术品质，模糊地让人联想到光绘。光绘是一种艺术媒介，通过用棱镜和镜子操纵彩色光束来创建图像。这种技术的显着例子是[Stephen Knapp的作品](http://www.lightpaintings.com/)。

请注意，这里的光绘比喻相当脆弱：例如，轻组合是一个加法过程，而CPPN可以在层之间具有负加权连接。


所生成图像的视觉质量受所选CPPN的体系结构的严重影响。
 不仅网络的形状，即层数和过滤器的数量起作用，而且还有所选择的激活功能和标准化。例如，与浅层网络相比，更深层次的网络会产生更细粒度的细节。
 我们鼓励读者通过改变CPPN的架构来尝试生成不同的图像。这可以通过更改补充笔记本中的代码轻松完成。


CPPN生成的模式的演变是艺术文物本身。
 为了保持光绘的隐喻，优化过程对应于光束方向和形状的迭代调整。
 因为与例如像素参数化相比，迭代变化具有更全局的效果，所以在优化开始时仅主要模式是可见的。
 通过迭代地调整权重，我们的假想光束以这样的方式定位，即细节出现。


{% raw %}
![]({{U＃R·L}})
{% endraw %}
{% raw %}
[8]({{U＃R·L}})：
{% endraw %}
 培训期间CPPN的输出。 *通过悬停控制每个视频，或者如果您在移动设备上，则点击它。*


通过玩这个比喻，我们还可以创建一种新的动画，将上述图像之一变换为不同的动画。
 直观地说，我们从一幅灯光画开始，我们移动光束来创造一个不同的光束。
 实际上，通过内插两个模式的CPPN表示的权重来实现该结果。然后通过在给定内插CPPN表示的情况下生成图像来生成多个中间帧。
 和以前一样，参数的变化具有全局效果，并创建视觉上吸引人的中间帧。


{% raw %}
![]({{U＃R·L}})
{% endraw %}
{% raw %}
[9]({{U＃R·L}})：
{% endraw %}
 在两个学习点之间插值CPPN权重。


在本节中，我们提出了一个超出标准图像表示的参数化。
 神经网络(在这种情况下为CPPN)可用于参数化针对给定目标函数优化的图像。
 更具体地说，我们将特征可视化目标函数与CPPN参数化相结合，以创建具有独特视觉风格的无限分辨率图像。

---

{% raw %}
[4]({{U＃R·L}})
{% endraw %}

## [半透明图案的生成](https://distill.pub/2018/differentiable-parameterizations#section-rgba)


本文中使用的神经网络经过训练，可以接收2D RGB图像作为输入。
 是否可以使用相同的网络来合成超出此域**的**(4)的工件？
 事实证明，我们可以通过使我们的可微分参数化定义图像的*族*而不是单个图像，然后在每个优化步骤中从该族中采样一个或几个图像来实现。
 这很重要，因为我们将探索优化的许多对象比进入网络的图像具有更多的自由度。

具体来说，让我们考虑半透明图像的情况。除了RGB通道之外，这些图像还有一个alpha通道，它对每个像素的不透明度进行编码(在[0,1]范围内[伽马校正混合](https://en.wikipedia.org/wiki/Alpha_compositing#Composing_alpha_blending_with_gamma_correction)

其中IaI_aIa是图像III的alpha通道。


如果我们使用静态背景BGBGBG(例如黑色)，则透明度仅表示该背景直接有助于优化目标的像素位置。
 实际上，这相当于优化RGB图像并使其在颜色与背景匹配的区域变得透明!
 直观地说，我们希望透明区域与“这个区域的内容可能是任何东西”相对应。
 基于这种直觉，我们在每个优化步骤中使用不同的随机背景。
 
 我们尝试了从真实图像采样和使用不同类型的噪声。
 只要它们具有足够的随机性，不同的分布就不会对最终的优化产生有意义的影响。
 因此，为简单起见，我们使用平滑的2D高斯噪声。

{% raw %}
![]({{U＃R·L}})
{% endraw %}


默认情况下，优化半透明图像会使图像完全不透明，因此网络始终可以获得最佳输入。
 为了避免这种情况，我们需要通过鼓励一些透明度的目标来改变我们的目标。
 我们发现用以下方法替换原始目标是有效的：


{% raw %}
此新目标会自动平衡原始目标objold \ text {obj} _ {\ text {old}} objold并降低平均透明度。
{% endraw %}
 如果图像变得非常透明，它将聚焦于原始目标。如果它变得太不透明，它将暂时停止关注原始目标并专注于降低平均不透明度。


{% raw %}
[11]({{U＃R·L}})：
{% endraw %}
 用于不同层和单元的半透明图像的优化的示例。


特征可视化旨在通过创建最大化激活它们的图像来了解视觉模型中的神经元正在寻找什么。
 不幸的是，这些可视化无法使神经元对其所关注的内容产生严重影响
 优化整个通道时不会发生此问题，因为在这种情况下，每个像素都有多个靠近中心的神经元。结果，整个输入图像充满了这些神经元强烈关注的副本。
 例如，注意这些神经元可视化中心周围的几何结构：


事实证明，半透明图像的生成在特征可视化中很有用。
 特征可视化旨在通过创建最大化激活它们的图像来了解视觉模型中的神经元正在寻找什么。
 不幸的是，这些可视化无法区分图像的哪些区域强烈影响神经元的激活以及仅略微影响神经元激活的区域。
 优化整个通道的激活时不会发生此问题，因为在这种情况下，每个像素都有多个接近其居中的神经元。结果，整个输入图像充满了这些神经元强烈关注的副本。


理想情况下，我们希望我们的可视化方法能够区分重要性 - 一种表示图像的一部分无关紧要的自然方式是使其透明。
 因此，如果我们使用alpha通道优化图像并鼓励整个图像透明，则根据特征可视化目标不重要的图像部分应该变得透明。

---

{% raw %}
[5]({{U＃R·L}})
{% endraw %}

## [通过3D渲染实现高效纹理优化](https://distill.pub/2018/differentiable-parameterizations#section-featureviz-3d)


在上一节中，我们能够使用RGB图像的神经网络来创建半透明的RGBA图像。
 我们可以进一步推动这一点，创建**(4)其他类型的对象**甚至进一步从RGB输入中移除？
 在本节中，我们将探索优化** 3D对象**以实现特征可视化目标。
 我们使用3D渲染过程将它们转换为可以馈入网络的2D RGB图像，并通过渲染过程反向传播以优化3D对象的纹理。


我们的技术类似于Athalye等人的方法。用于创建真实世界的对抗性示例，因为我们依赖于目标函数的反向传播来随机抽取3D模型的视图。
 我们不同于现有的艺术纹理生成方法，因为我们在反向传播期间不修改对象的几何形状。
 通过从顶点的位置解开纹理的生成，我们可以为复杂的对象创建非常详细的纹理。

在我们描述我们的方法之前，我们首先需要了解3D对象在屏幕上的存储和呈现方式。对象的几何体通常保存为互连三角形的集合，称为**三角形网格**，或简称为网格。为了渲染逼真的模型，在网格上绘制**纹理**。纹理保存为通过使用所谓的** UV-mapping **应用于模型的图像。网格中的每个顶点cic_ici与纹理图像中的(ui，vi)(u_i，v_i)(ui，vi)坐标相关联。然后通过用图像的区域对每个三角形着色来渲染，即在屏幕上绘制模型，该区域由其顶点的(u，v)(u，v)(u，v)坐标界定。

您可以使用以下WebGL视图来熟悉这些概念。该视图显示了着名的斯坦福兔子的3D模型和相关的纹理。您可以通过旋转，缩放和平移来与模型进行交互。此外，您可以将对象展开为其二维纹理表示。此展开揭示了用于在纹理图像中存储纹理的UV贴图。请注意如何将纹理划分为多个补丁，以便对对象进行完整且不失真的覆盖。


创建3D对象纹理的一种简单的天真方式是以正常方式优化图像，然后将其用作在对象上绘制的纹理。
 但是，此方法生成的纹理不考虑底层的UV映射，因此会在渲染对象中创建各种可视化工件。
 首先，**接缝**在渲染纹理上是可见的，因为优化不知道底层的UV映射，因此，不会沿纹理的分割块一致地优化纹理。
 其次，所生成的图案在对象的不同部分上随机取向**(参见例如垂直和摆动图案)，因为它们在下面的UV映射中不一致地定向。
 最终生成的模式**不一致地缩放**，因为UV映射不会强制三角形区域与纹理中的映射三角形之间的一致比例。


{% raw %}
[13]({{U＃R·L}})：
{% endraw %}
 着名斯坦福兔子的3D模型。您可以通过旋转和缩放与模型进行交互。此外，您可以将对象展开为其二维纹理表示。此展开揭示了用于在纹理图像中存储纹理的UV贴图。请注意基于渲染的优化纹理如何划分为多个补丁，以便对对象进行完整且不失真的覆盖。


我们采取不同的方法。
 我们不是直接优化纹理，而是通过3D对象的渲染来优化纹理*，就像用户最终看到的那样。
 下图显示了拟议管道的概述：


我们通过使用傅立叶参数化随机初始化纹理来启动该过程。
在每次训练迭代中，我们采样随机摄像机位置，其朝向对象的边界框的中心，并且我们将纹理化对象渲染为图像。
然后，我们将所需目标函数的梯度，即神经网络中感兴趣的特征，反向传播到渲染图像。


但是，渲染图像的更新并不对应于我们旨在优化的纹理的更新。因此，我们需要进一步将更改传播到对象的纹理。
通过应用反向UV映射可以轻松实现传播，对于屏幕上的每个像素，我们知道它在纹理中的坐标。
通过修改纹理，在以下优化迭代期间，渲染图像将包含在先前迭代中应用的更改。


{% raw %}
[15]({{U＃R·L}})：
{% endraw %}
 通过优化特征可视化目标函数来生成纹理。
 纹理中的接缝几乎看不到，图案正确定向。


生成的纹理始终沿切割进行优化，从而消除接缝并为渲染对象强制执行统一的方向。
此外，由于功能优化被对象的几何形状解开，纹理的分辨率可以是任意高的。
在下一节中，我们将了解如何重用此框架来执行艺术样式转移到对象的纹理。

---

{% raw %}
[6]({{U＃R·L}})
{% endraw %}

## [通过3D渲染对纹理进行样式转换](https://distill.pub/2018/differentiable-parameterizations#section-style-transfer-3d)


现在我们已经建立了一个有效反向传播到UV映射纹理的框架，我们可以使用它来适应3D对象的现有样式传输技术。
与2D情况类似，我们的目标是使用用户提供的图像的样式重新绘制原始对象的纹理。
下图概述了该方法：


该算法的工作方式与上一节中介绍的算法类似，从随机初始化的纹理开始。
在每次迭代中，我们采样一个面向对象边界框中心的随机视点，我们渲染它的两个图像：一个具有原始纹理，*内容图像*，一个具有我们当前的纹理优化，*学习图像*。


在呈现*内容图像*和*学习图像*之后，我们针对由Gatys等人引入的样式转移目标函数进行优化。然后我们将参数化映射回UV映射纹理，如上一节中介绍的那样。
然后迭代该过程，直到在目标纹理中获得所需的内容和样式的混合。


{% raw %}
[17]({{U＃R·L}})：
{% endraw %}
 风格转移到各种3D模型。请注意，内容纹理中的可视地标(如眼睛)会在生成的纹理中正确显示。


由于每个视图都是独立优化的，因此强制优化会尝试在每次迭代时添加所有样式的元素。
例如，如果我们使用梵高的“星夜”绘画作为风格图像，每个视图中都会添加星星。
通过引入一种风格的“记忆”，我们发现我们获得了更令人愉悦的结果，例如上面提到的结果
以前的观点。为此，我们维持代表Gram矩阵的样式的移动平均值
在最近采样的观点上。在每次优化迭代中，我们根据这些平均矩阵计算样式损失，
而不是为该特定视图计算的那些。

 我们使用TensorFlow的`tf.stop_gradient`方法替换当前的Gram矩阵
 他们的移动平均线在前进传球，同时仍然传播正确的渐变
 到目前的Gram矩阵。


另一种方法，例如，
 需要在每一步采样场景的多个视点，
 增加内存需求。相反，我们的替代技巧也可以
 用于将样式转换应用于a上的高分辨率(> 10M像素)图像
 单一消费级GPU。


生成的纹理组合了所需样式的元素，同时保留了原始纹理的特征。
以梵高的星夜作为风格图像为例，创造了一个模型。
由此产生的纹理包含了梵高作品的韵律和活力。
然而，尽管风格图像主要是冷色调，但最终的毛皮具有温暖的橙色底色，因为它保留了原始纹理。
更有趣的是，当不同的风格被转移时，兔子的眼睛是如何保留的。
例如，当从梵高的绘画中获得风格时，眼睛会变成星状漩涡，而如果使用康定斯基的作品，它们会变成仍然与原始眼睛相似的抽象图案。

{% raw %}
![]({{U＃R·L}})
{% endraw %}

使用所提出的方法生成的纹理模型可以容易地与流行的3D建模软件或游戏引擎一起使用。为了表明这一点，我们将其中一个设计3D打印为真实世界的物理神器我们使用了[全彩砂岩](https://www.shapeways.com/materials.sandstone)材料。

---

{% raw %}
## [结论]({{U＃R·L}})
{% endraw %}


对于创意艺术家或研究人员来说，有很多方法可以参数化图像以进行优化。
 这不仅打开了截然不同的图像效果，还打开了动画和3D对象!
 我们认为本文探讨的可能性只是划伤了表面。
 例如，人们可以探索扩展3D对象纹理的优化以优化材料或反射 - 甚至可以去Kato等人的方向。并优化网格顶点位置。


本文重点介绍*可微分*图像参数化，因为它们易于优化并涵盖了广泛的可能应用。
 但是使用强化学习或进化策略来优化图像参数化当然是可能的，这些参数化不是可微分的，或者只是部分可区分的。
 使用不可微分的参数化可以为图像或场景生成开辟令人兴奋的可能性。


最后，我们认为参考化的深思熟虑可以用于其他优化问题，从而提供对结果的额外控制。
 例如，降维通常被视为点的位置上的优化问题;
 虽然每个点通常被参数化为一个简单的位置，但可以很容易地使用替代参数化。
 这可以加速降维，或者可以成为调整若干相关维数减少的工具。
 当然，人们确实非常仔细地考虑了他们在许多领域中优化对象的方式(例如，神经网络);我们认为值得更广泛地考虑。
 参数化的这种更一般的使用在某些方面类似于预处理和自然梯度。

### 致谢


我们非常感谢Justin Gilmer，Shan Carter，Dominik Roblek和Amit Patel。
 贾斯汀慷慨地介入处理本文的审查过程。
 Shan可以提供出色的设计见解和实施建议。
 Dominik Roblek使[3D打印的例子](https://distill.pub/2018/figure-style-transfer-3d-picture)成为可能，而Amit Patel在我们的[样式转移图](https://distill.pub/2018/figure-style-transfer-diagram)中搜寻了一个模糊的错误。


我们非常感谢那些花时间撰写深入评论的评论者，这些评论帮助我们大大改进了论文：庞伟，以及匿名审稿人2和匿名审稿人3。
 我们也感谢
 Matt Sharifi，Arvind Satyanarayan，Ian Johnson和Vincent Sitzmann
 他们的想法，评论，讨论和支持。


最后，许多开源工具使这项工作成为可能，我们对此表示感谢。
 特别是，我们所有的实验都基于[Tensorflow](https://www.tensorflow.org/)，
 我们的大多数交互式图都是用[Svelte](https://svelte.technology/)构建的，
 并且所有3D图表都使用[ThreeJS](https://threejs.org/)。

### 作者贡献

**研究：** Alex开发了透明度，CPPN参数化，3D参数化和非VGG样式传输的使用。 Chris开发了联合参数化用于对齐的方法，并且很少参与透明度和3D模型的开发。 Nicola和Ludwig对文章中提出的最终版本进行了改进。

**写作和图表：**该文本最初由Nicola起草，并由其他作者完善。交互式图表由路德维希和尼古拉设计。最终的笔记本电脑主要由Ludwig创建，基于早期的代码和Alex和Chris的笔记本电脑。

### 讨论和审查

{% raw %}
[评论1  -  Pang Wei Koh](https://github.com/distillpub/post--differentiable-parameterizations/issues/50)[评论2  - 匿名](https://github.com/distillpub/post--differentiable-parameterizations/issues/51)[评论3  - 匿名]({{u＃r＃l }})
{% endraw %}

### 参考


1.初始主义：深入研究神经网络[[HTML]](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)Mordvintsev，A.，Olah，C。和Tyka，M.，2015。谷歌研究博客。检索六月，第20卷(14)，第5页。
1.艺术风格的神经算法[[PDF]](http://arxiv.org/pdf/1508.06576.pdf)Gatys，L.A.，Ecker，A.S。和Bethge，M.，2015。CoRR，Vol abs/1508.06576。
1.特征可视化[[link]](https://distill.pub/2017/feature-visualization)Olah，C.，Mordvintsev，A。和Schubert，L.，2017。Distill。 [DOI：10.23915/distill.00007](https://doi.org/10.23915/distill.00007)
1.深度神经网络很容易被愚弄：对无法识别的图像的高置信度预测[[PDF]](http://arxiv.org/pdf/1412.1897.pdf)Nguyen，AM，Yosinski，J。和Clune，J.，2015。IEEE Con​​ference on计算机视觉和模式识别，CVPR 2015，波士顿，马萨诸塞州，美国，2015年6月7日至12日，第427-436页。
{% raw %}
1.综合强大的对抗性例子[[PDF]](https://arxiv.org/pdf/1707.07397.pdf)Athalye，A.，Engstrom，L.，Ilyas，​​A。和Kwok，K.，2017。arXiv preprint {{u＃ R·L}}。
{% endraw %}
1.多层网络的损耗表面[[PDF]](arXiv:1707.07397)Choromanska，A.，Henaff，M.，Mathieu，M.，Arous，G.B。和LeCun，Y.，2015年。人工智能和统计学，第192--204页。
{% raw %}
1.用于大规模图像识别的非常深的卷积网络[[PDF]](http://arxiv.org/pdf/1412.0233.pdf)Simonyan，K。和Zisserman，A.，2014。arXiv preprint {{u＃r＃l} }。
{% endraw %}
1.反卷积和棋盘格工件[[link]](http://arxiv.org/pdf/1409.1556.pdf)Odena，A.，Dumoulin，V。和Olah，C.，2016。Distill，Vol 1(10)，pp.e3 。 [DOI：10.23915/distill.00003](arXiv:1409.1556)
1.深度图像优先[[PDF]](http://distill.pub/2016/deconv-checkerboard)Ulyanov，D.，Vedaldi，A。和Lempitsky，V.S。，2017.CoRR，Vol abs/1711.10925。
1.组成模式生成网络：一种新颖的发展抽象[[PDF]](https://doi.org/10.23915/distill.00003)Stanley，KO，2007。遗传编程和可演化机器，第8卷(2)，第131页 - -162。斯普林格。
1. Javascript中的神经网络生成艺术[[link]](http://arxiv.org/pdf/1711.10925.pdf)Ha，D.，2015。
1.图像回归[[HTML]](http://eplex.cs.ucf.edu/papers/stanley_gpem07.pdf)Karpathy，A.，2014。
1.从潜在向量生成大图像[[link]](http://blog.otoro.net/2015/06/19/neural-network-generative-art)Ha，D.，2016。
1.计算机图形的人工进化[[HTML]](https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html)Sims，K.，1991.SIGGRAPH Comput。图。，第25卷(4)，第319-328页。 ACM。 [DOI：10.1145/127719.122752](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors)
1.神经三维网格渲染器[[PDF]](http://www.karlsims.com/papers/siggraph91.html)Kato，H.，Ushiku，Y。和Harada，T.，2017.arXiv preprint https://doi.org/10.1145/127719.122752 。
1.斯坦福兔子[[link]](http://arxiv.org/pdf/1711.07566.pdf。stanford.edu/data/3Dscanrep)Turk，G。和Levoy，M.，2005。斯坦福大学计算机图形实验室。
1.自然进化策略。 [[PDF]](arXiv:1711.07566)Wierstra，D.，Schaul，T.，Glasmachers，T.，Sun，Y.，Peters，J。和Schmidhuber，J.，2014。Journal of Journal of Journal机器学习研究，第15卷(1)，第949-980页。
1.进化策略作为强化学习的可扩展替代方法Salimans，T.，Ho，J.，Chen，X。和Sutskever，I.，2017.arXiv preprint http://graphics。
{% raw %}
1. Tensorflow：在异构分布式系统上进行大规模机器学习[[PDF]](http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf)Abadi，M.，Agarwal，A.，Barham，P.，Brevdo，E.，Chen ，Z.，Citro，C.，Corrado，GS，Davis，A.，Dean，J.，Devin，M.，Ghemawat，S.，Goodfellow，IJ，Harp，A.，Irving，G.，Isard，M 。，贾，Y。，J {\'{o}} zefowicz，R.，Kaiser，L.，Kudlur，M.，Levenberg，J.，Man {\'{e}}，D.，Monga，R 。，Moore，S.，Murray，DG，Olah，C.，Schuster，M.，Shlens，J.，Steiner，B.，Sutskever，I.，Talwar，K.，Tucker，PA，Vanhoucke，V.， Vasudevan，V.，Vi {\'{e}} gas，FB，Vinyals，O.，Warden，P.，Wattenberg，M.，Wicke，M.，Yu，Y。和Zheng，X.，2016.arXiv预印本arXiv:1703.03864。
{% endraw %}

### 更新和更正

如果您发现错误或想要建议更改，请[在GitHub上创建一个问题](https://github.com/distillpub/post--differentiable-parameterizations/issues/new)。

### 重用

除非另有说明，否则图表和文本根据知识共享署名[CC-BY 4.0](https://creativecommons.org/licenses/by/4.0)和[来源于GitHub](https://github.com/distillpub/post--differentiable-parameterizations)获得许可。从其他来源重复使用的数字不属于本许可证，可以通过其标题中的注释来识别：“图......来自......”。

### 引文

对于学术背景下的归因，请引用此作品

BibTeX引文

