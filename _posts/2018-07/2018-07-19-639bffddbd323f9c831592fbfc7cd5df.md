---
layout:     post
title:      Best (and Free!!) Resources to Understand Nuts and Bolts of Deep Learning
subtitle:   转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/uL6cB21jnr8/best-free-resources-understand-nuts-bolts-deep-learning.html
date:       2018-07-19
author:     Matt Mayo Editor
header-img: img/background1.jpg
catalog: true
tags:
    - https
    - learning
    - ai
    - deep
    - paralleldots
    - resources
    - stanford courses
    - pdf
    - cs
    - data
    - calculus
    - book
    - perspective
    - reviewed
    - reviews
    - blog
    - frameworks
    - adversarial
    - neural
    - differentiation
    - tutorials
    - criticisms
    - apis
    - prof
    - algorithms
    - quick
    - convolutional
    - convolutions
    - lstms
    - types
    - goodfellow
    - absolutely
    - viral
    - challenge
    - linear
    - models
    - derivation
    - derivatives
    - survey
    - understand
    - challenging business
    - boyd
    - generative
    - generate
    - started
    - starting
    - applied
    - apply
    - businesses
    - lecture
    - common
    - optimization
    - multiple tasks
    - backprop
    - carlo
---

**By Muktabh Mayank, [ParallelDots](https://blog.paralleldots.com/)**

![](https://blog.paralleldots.com/wp-content/uploads/2018/06/Deep-learning-comment-l-agregation-des-modeles-permet-de-prevoir-l-imprevisible_knowledge_standard.jpg)


The internet is filled with tutorials to get started with Deep Learning. You can choose to get started with the superb Stanford courses [CS221](http://cs231n.stanford.edu/) or [CS224](http://cs224d.stanford.edu/), [Fast AI courses](http://www.fast.ai/) or Deep Learning AI [courses](https://www.deeplearning.ai/) if you are an absolute beginner. All except Deep Learning AI are free and accessible from the comfort of your home. All you need is a good computer (preferably with a Nvidia GPU) and you are good to take your first steps into Deep Learning.

This blog is however not addressing the absolute beginner. Once you have a bit of intuition about how Deep Learning algorithms work, you might want to understand how things work below the hood. While most work in Deep Learning (the 10% apart from Data Munging viz 90% of total work) is adding layers like Conv2d, changing hyperparameters in different types of optimization strategies like ADAM or using batchnorm and other techniques just by writing one line commands in Python (thanks to the awesome frameworks available), a lot of the people might feel a deep desire to know what happens behind the scenes. This is the list of resources which might help you get to know what happens inside the hood when you (say) put a conv2d layer or call T.grad in Theano.

 

### General Treatise

 Deep Learning Book is of course the most famous and well-known [resource](http://www.deeplearningbook.org/). Other good resources are Professor Charniak’s [course](https://cs.brown.edu/courses/csci1460/assets/files/deep-learning.pdf) and [paper](https://arxiv.org/abs/1709.01412) which is a technical introduction to Deep Learning. There are other resources too which you might want to take up if you want to understand things from a particular perspective. For example, the [tutorial](https://arxiv.org/abs/1801.05894) was written from point of applied mathematicians and if you just want to start coding without going into any theory then read [here](https://arxiv.org/abs/1703.05298). One more recommended resource is Deep Learning course in PyTorch [here](https://documents.epfl.ch/users/f/fl/fleuret/www/dlc). This course talks about things bottom up and help you grab a bigger perspective.

 

### Issues about Backpropogation

 There are many a time, when people are not sure about “how is gradient descent and backpropogation the same thing ?” or “what exactly is the chain rule and backpropogation?”. To understand the basics, we might choose to read the original paper by Rumelhart, Hinton, and Williams where it all started. The paper is located [here](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf) and is a very simple to understand document.

Some other very useful resources one can read on top of this are Karpathy’s blog on backward [prop derivation](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b) and [this video](https://www.youtube.com/watch?v=gl3lfL-g5mA) explaining backprop’s derivation.

 

### Linear Algebra and other Maths

 Anyone would redirect someone aspiring to learn Linear Algebra to goto Prof. Strang’s [course](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010). This is probably the best resource on Linear Algebra on the planet. Similar is the case with Optimization course of Prof. Boyd [here](http://web.stanford.edu/~boyd/cvxbook) or Calculus on Manifolds book for vector calculus (You can find a pdf when you Google “Calculus on Manifolds”). However, one doesn’t need to go through the depth at which these resources look at their subjects to jump into Deep learning. A very quick way to get started is to take the quick refresher on all prerequisite Calculus for Deep Learning which is available [here](https://arxiv.org/abs/1802.01528). There is also this very good set of [lecture notes](http://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227BT/lectures.html) to just look at convex optimization used in Deep Learning. Another good resource is Sebastian Reuder’s paper [here](https://arxiv.org/abs/1609.04747). I also like these lecture notes to understand derivatives on [tensors](https://github.com/mtomassoli/tensor-differential-calculus/blob/master/tensor_diff_calc.pdf).

 

### Automatic Differentiation and Deep Learning Libraries

 Automatic Differentiation isn’t something that you absolutely need to know when you are doing Deep Learning. Most frameworks like Torch, Theano or tensorflow do it for you automatically. In most cases, you don’t even have to know how the differentiation is being done. That said, if you are determined to understand how Deep Learning frameworks work, you might want to understand how automatic differentiation works [here](https://arxiv.org/abs/1502.05767). Other good resources to understand how Deep Learning libraries function are can be found in this [blog](http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour) and [video](https://www.youtube.com/watch?v=Lo1rXJdAJ7w).

 

### Convolutional Neural Networks

 The most useful things you might need after you have done some courses which enable you to use basic convents is to understand how convolutions work on images. “What is output shape after you apply a certain type of convolutions on an input ?” , “How does stride affect convolutions ?”, “What is Batch Normalization ?” and stuff like that. The two best resources I have seen for these type of applied questions are the tutorial [here](https://arxiv.org/abs/1603.07285) and Ian Goodfellow’s talk [here](https://www.youtube.com/watch?v=Xogn6veSyxA). A more thorough [review](https://arxiv.org/abs/1803.08834) on Convnets is here if you want to get an idea. This [review on object detection](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9) is a very good resource in the topic.

 

### Deep Learning in NLP

 The Stanford 224 course I pointed out earlier in the blog is a very good starting point and you should be good enough for almost everything. There is also a course on youtube by Graham Neubig (which uses dynet) [here](https://www.youtube.com/watch?v=Sss2EA4hhBQ). There is also an [NLP book](https://u.cs.biu.ac.il/~yogo/nnlp.pdf) by Yoav Goldberg which you might like. (Newer) advances of NLP after this book was written are reviewed [here](https://arxiv.org/abs/1708.02709). There is also a very common question about whether to use convnets or RNNs (LSTMs/GRUs) on text. A good overview is [here](https://arxiv.org/pdf/1803.01271.pdf).

 

### Reinforcement Learning

 ![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/2000px-Reinforcement_learning_diagram.svg.png)


Sutton and Barto is the bible to get started with these methods. The book is free and is available [here](http://incompleteideas.net/book/the-book-2nd.html). A very good review of recent Deep Reinforcement Learning methods is available [here](https://arxiv.org/abs/1708.05866). There is this very interesting tutorial on Reinforcement Learning [here](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752).

A good review of Monte Carlo Tree Search (which is a part of AlphaGo algorithm by Deepmind apart from Deep Reinforcement Learning techniques) is [here](http://mcts.ai/pubs/mcts-survey-master.pdf). I, however, used [this](http://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search?utm_source=top.caibaojian.com/19271) quick tutorial to learn about them.

 

### Some other good reviews/tutorials

 **A good tutorial about GANs (Generative Adversarial Networks) and generative models in general is what Goodfellow gave in ICLR 2016. It can be found [here](https://www.youtube.com/watch?v=HGYYEUSm-0Q). Neural Networks have been used to transfer art (for example in Prisma app), a detailed survey of methods to do that can be found [here](https://arxiv.org/abs/1705.04058). Another good survey on Multi Task Learning (combining multiple tasks by same neural network) by Reuder is [here](https://arxiv.org/abs/1706.05098).**

 

### Criticisms

 Although Deep Learning works amazingly well on multiple problems, we know there will always be some places where they have not reached yet. Some good criticisms to read are Failures of gradient-based learning by [Shalev-Shwartz et al.](https://arxiv.org/abs/1703.07950) , this talk by Hinton which lists some difficulties for [convnets](https://www.youtube.com/watch?v=rTawFwUvnLE) and how convnets cannot decipher negatives of images [they train on](https://arxiv.org/pdf/1703.06857.pdf). Another criticism here went viral/controversial few days back is [this](https://arxiv.org/abs/1801.00631). There is also [this extensive report](https://arxiv.org/abs/1802.07228) on the malicious use of Deep Learning.

 

### Adversarial Examples

 This is a huge field of making artificial/real data points that can fool Convnets. I could have put it in in criticism sections but didn’t as

they are not a technical challenge for all applications and
I am not very well read on them. One very cool case to get started and interested is [here](https://www.labsix.org/physical-objects-that-fool-neural-nets) where they generate “adversarial objects” to fool neural networks.

You can also read about Machine Learning algorithms you should know to become a Data Scientist [here](https://blog.paralleldots.com/data-science/machine-learning/ten-machine-learning-algorithms-know-become-data-scientist).

We hope you liked the article. Please [Sign Up](http://user.apis.paralleldots.com/signing-up?utm_source=blog&utm_medium=chat&utm_campaign=paralleldots_blog) for a free ParallelDots account to start your AI journey. You can also check demo’s of PrallelDots AI APIs [here](https://www.paralleldots.com/text-analysis-apis).

 **Bio: [ParallelDots](https://blog.paralleldots.com/.)** is an applied AI research group. They work with enterprises globally to tackle challenging business problems and create the winners of tomorrow. They also provide AI consulting services to explore the "what, why, how and who"​ about deploying AI in businesses.

[Original](https://blog.paralleldots.com/data-science/deep-learning/best-and-free-resources-to-understand-of-deep-learning). Reposted with permission.

**Related:**



 
