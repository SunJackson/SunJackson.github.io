---
layout:     post
title:      Document worth reading： “Optimization Methods for Supervised Machine Learning： From Linear Models to Deep Learning”
subtitle:   转载自：https://advanceddataanalytics.net/2018/07/20/document-worth-reading-optimization-methods-for-supervised-machine-learning-from-linear-models-to-deep-learning/
date:       2018-07-20
author:     Michael Laux
header-img: img/background0.jpg
catalog: true
tags:
    - learning
    - optimization methods
    - models
    - nonconvex
    - logistic
    - stochastic
    - questions
    - networks
---

The goal of this tutorial is to introduce key models, algorithms, and open questions related to the use of optimization methods for solving problems arising in machine learning. It is written with an INFORMS audience in mind, specifically those readers who are familiar with the basics of optimization algorithms, but less familiar with machine learning. We begin by deriving a formulation of a supervised learning problem and show how it leads to various optimization problems, depending on the context and underlying assumptions. We then discuss some of the distinctive features of these optimization problems, focusing on the examples of logistic regression and the training of deep neural networks. The latter half of the tutorial focuses on optimization algorithms, first for convex logistic regression, for which we discuss the use of first-order methods, the stochastic gradient method, variance reducing stochastic methods, and second-order methods. Finally, we discuss how these approaches can be employed to the training of deep neural networks, emphasizing the difficulties that arise from the complex, nonconvex structure of these models. [Optimization Methods for Supervised Machine Learning: From Linear Models to Deep Learning](http://arxiv.org/abs/1706.10207v1)





### Like this:

Like Loading...


*Related*

