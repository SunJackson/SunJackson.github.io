---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://advanceddataanalytics.net/2018/07/26/if-you-did-not-already-know-433/
date:      2018-07-26
img:      3
author:      Michael Laux
tags:
    - labels
    - tasks
    - memory
    - memories
    - processes
---

[**Canonical Correlated AutoEncoder (C2AE)**](http://arxiv.org/abs/1707.00418v1) [![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
](https://www.google.de/search?q=Canonical Correlated AutoEncoder)Multi-label classification is a practical yet challenging task in machine learning related fields, since it requires the prediction of more than one label category for each input instance. We propose a novel deep neural networks (DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this task. Aiming at better relating feature and label domain data for improved classification, we uniquely perform joint feature and label embedding by deriving a deep latent space, followed by the introduction of label-correlation sensitive loss function for recovering the predicted label outputs. Our C2AE is achieved by integrating the DNN architectures of canonical correlation analysis and autoencoder, which allows end-to-end learning and prediction with the ability to exploit label dependency. Moreover, our C2AE can be easily extended to address the learning problem with missing labels. Our experiments on multiple datasets with different scales confirm the effectiveness and robustness of our proposed method, which is shown to perform favorably against state-of-the-art methods for multi-label classification. … 

[**Conditional Neural Process (CNP)**](http://arxiv.org/abs/1807.01613v1) [![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
](https://www.google.de/search?q=Conditional Neural Process)Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion. … 

[**Fast Weight Long Short-Term Memory**](http://arxiv.org/abs/1804.06511v1) [![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
](https://www.google.de/search?q=Fast Weight Long Short-Term Memory)Associative memory using fast weights is a short-term memory mechanism that substantially improves the memory capacity and time scale of recurrent neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs. In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories. We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties. … 

### Like this:
Like Loading...

*Related*

