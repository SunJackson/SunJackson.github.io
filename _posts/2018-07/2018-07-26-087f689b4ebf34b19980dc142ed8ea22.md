---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://advanceddataanalytics.net/2018/07/26/if-you-did-not-already-know-433/
date:      2018-07-26
img:      0
author:      Michael Laux
tags:
    - labels
    - tasks
    - memory
    - memories
    - processes
---

[**Canonical Correlated AutoEncoder(C2AE)**](http://arxiv.org/abs/1707.00418v1)[![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
](https://www.google.de/search?q=Canonical相关AutoEncoder)多标签分类是机器学习相关领域中一项实际但具有挑战性的任务，因为它需要为每个输入实例预测多个标签类别。我们提出了一种新的基于深度神经网络(DNN)的模型，Canonical Correlated AutoEncoder(C2AE)，用于解决此任务。针对更好的相关特征和标签域数据以改进分类，我们通过导出深潜空间来独特地执行联合特征和标签嵌入，随后引入标签相关敏感损失函数以恢复预测的标签输出。我们的C2AE是通过集成典型相关分析和自动编码器的DNN架构实现的，它允许端到端学习和预测，并具有利用标签依赖性的能力。此外，我们的C2AE可以轻松扩展，以解决丢失标签的学习问题。我们对具有不同尺度的多个数据集的实验证实了我们提出的方法的有效性和稳健性，其被证明对于用于多标签分类的最新方法表现良好。 ...


[**条件神经过程(CNP)**](http://arxiv.org/abs/1807.01613v1)[![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
](https://www.google.de/search?q=Conditional神经过程)深度神经网络在函数逼近方面表现优异，但它们通常从头开始训练每个新函数。另一方面，贝叶斯方法，例如高斯过程(GP)，利用先验知识在测试时快速推断新函数的形状。然而，GP的计算成本很高，而且很难设计合适的先验。在本文中，我们提出了一系列神经模型，即条件神经过程(CNPs)，它们结合了两者的优点。 CNP受到GP等随机过程灵活性的启发，但结构为神经网络，并通过梯度下降进行训练。 CNP仅在观察少数训练数据点后进行准确预测，然后扩展到复杂函数和大型数据集。我们展示了该方法在一系列规范机器学习任务中的性能和多功能性，包括回归，分类和图像完成。 ...


[**Fast Weight Long Short-Term Memory**](http://arxiv.org/abs/1804.06511v1)[![](https://aboutdataanalytics.files.wordpress.com/2015/01/google.png?w=529)
](https://www.google.de/search?q=Fast权重长短期记忆)使用快速权重的联想记忆是一种短期记忆机制，可以显着改善递归神经网络(RNN)的记忆容量和时间尺度。由于最近的研究仅对常规RNN引入了快速权重，因此不知道快速重量记忆是否有益于门控RNN。在这项工作中，我们报告了长期短期记忆(LSTM)网络和快速体重联想记忆之间的显着协同作用。我们表明，在学习关联检索任务中，这种组合可以带来更快的训练和更低的测试误差，在高记忆任务困难时性能提升最为突出。 ...

### 喜欢这个：

喜欢Loading ...

*有关*

