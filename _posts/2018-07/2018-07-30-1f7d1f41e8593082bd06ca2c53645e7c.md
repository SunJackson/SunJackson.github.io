---
layout:     post
catalog: true
title:      Whats new on arXiv
subtitle:      转载自：https://advanceddataanalytics.net/2018/07/30/whats-new-on-arxiv-721/
date:      2018-07-30
img:      2
author:      Michael Laux
tags:
    - models
    - modeling
    - modelling
    - modeled
    - networks
---
**Entropic Latent Variable Discovery**
We consider the problem of discovering the simplest latent variable that can make two observed discrete variables conditionally independent. This problem has appeared in the literature as probabilistic latent semantic analysis (pLSA), and has connections to non-negative matrix factorization. When the simplicity of the variable is measured through its cardinality, we show that a solution to this latent variable discovery problem can be used to distinguish direct causal relations from spurious correlations among almost all joint distributions on simple causal graphs with two observed variables. Conjecturing a similar identifiability result holds with Shannon entropy, we study a loss function that trades-off between entropy of the latent variable and the conditional mutual information of the observed variables. We then propose a latent variable discovery algorithm — LatentSearch — and show that its stationary points are the stationary points of our loss function. We experimentally show that LatentSearch can indeed be used to distinguish direct causal relations from spurious correlations.
**Survival of the Fittest Group: Factorial Analyses of Treatment Effects under Independent Right-Censoring**
This paper introduces new effect parameters for factorial survival designs with possibly right-censored time-to-event data. In the special case of a two-sample design it coincides with the concordance or Wilcoxon parameter in survival analysis. More generally, the new parameters describe treatment or interaction effects and we develop estimates and tests to infer their presence. We rigorously study the asymptotic properties by means of empirical process techniques and additionally suggest wild bootstrapping for a consistent and distribution-free application of the inference procedures. The small sample performance is discussed based on simulation results. The practical usefulness of the developed methodology is exemplified on a data example about patients with colon cancer by conducting one- and two-factorial analyses.
**On the use of Singular Spectrum Analysis**
Singular Spectrum Analysis (SSA) or Singular Value Decomposition (SVD) are often used to de-noise univariate time series or to study their spectral profile. Both techniques rely on the eigendecomposition of the correlation matrix estimated after embedding the signal into its delayed coordinates. In this work we show that the eigenvectors can be used to calculate the coefficients of a set of filters which form a filter bank. The properties of these filters are derived. In particular we show that their outputs can be grouped according to their frequency response. Furthermore, the frequency at the maximum of each frequency response and the corresponding eigenvalue can provide a power spectrum estimation of the time series. Two different applications illustrate how both characteristics can be applied to analyze wideband signals in order to achieve narrow-band signals or to infer their frequency occupation.
**Combining Restricted Boltzmann Machines with Neural Networks for Latent Truth Discovery**
Latent truth discovery, LTD for short, refers to the problem of aggregating multiple claims from various sources in order to estimate the plausibility of statements about entities. In the absence of a ground truth, this problem is highly challenging, when some sources provide conflicting claims and others no claims at all. In this work we provide an unsupervised stochastic inference procedure on top of a model that combines restricted Boltzmann machines with feed-forward neural networks to accurately infer the reliability of sources as well as the plausibility of statements about entities. In comparison to prior work our approach stands out (1) by allowing the incorporation of arbitrary features about sources and claims, (2) by generalizing from reliability per source towards a reliability function, and thus (3) enabling the estimation of source reliability even for sources that have provided no or very few claims, (4) by building on efficient and scalable stochastic inference algorithms, and (5) by outperforming the state-of-the-art by a considerable margin.
**Harmonic Adversarial Attack Method**
Adversarial attacks find perturbations that can fool models into misclassifying images. Previous works had successes in generating noisy/edge-rich adversarial perturbations, at the cost of degradation of image quality. Such perturbations, even when they are small in scale, are usually easily spottable by human vision. In contrast, we propose Harmonic Adversarial Attack Methods (HAAM), that generates edge-free perturbations by using harmonic functions. The property of edge-free guarantees that the generated adversarial images can still preserve visual quality, even when perturbations are of large magnitudes. Experiments also show that adversaries generated by HAAM often have higher rates of success when transferring between models. In addition, we find harmonic perturbations can simulate natural phenomena like natural lighting and shadows. It would then be possible to help find corner cases for given models, as a first step to improving them.
**High Dimensional Model Representation as a Glass Box in Supervised Machine Learning**
Prediction and explanation are key objects in supervised machine learning, where predictive models are known as black boxes and explanatory models are known as glass boxes. Explanation provides the necessary and sufficient information to interpret the model output in terms of the model input. It includes assessments of model output dependence on important input variables and measures of input variable importance to model output. High dimensional model representation (HDMR), also known as the generalized functional ANOVA expansion, provides useful insight into the input-output behavior of supervised machine learning models. This article gives applications of HDMR in supervised machine learning. The first application is characterizing information leakage in “big-data” settings. The second application is reduced-order representation of elementary symmetric polynomials. The third application is analysis of variance with correlated variables. The last application is estimation of HDMR from kernel machine and decision tree black box representations. These results suggest HDMR to have broad utility within machine learning as a glass box representation.
**Interpreting RNN behaviour via excitable network attractors**
Machine learning has become a basic tool in scientific research and for the development of technologies with significant impact on society. In fact, such methods allow to discover regularities in data and make predictions without explicit knowledge of the rules governing the system under analysis. However, a price must be paid for exploiting such a modeling flexibility: machine learning methods are usually black-box, meaning that it is difficult to fully understand what the machine is doing and how. This poses constraints on the applicability of such methods, neglecting the possibility to gather novel scientific insights from experimental data. Our research aims to open the black-box of recurrent neural networks, an important family of neural networks suitable to process sequential data. Here, we propose a novel methodology that allows to provide a mechanistic interpretation of their behaviour when used to solve computational tasks. The methodology is based on mathematical constructs called excitable network attractors, which are models represented as networks in phase space composed by stable attractors and excitable connections between them. As the behaviour of recurrent neural networks depends on training and inputs driving the autonomous system, we introduce an algorithm to extract network attractors directly from a trajectory generated by the neural network while solving tasks. Simulations conducted on a controlled benchmark highlight the relevance of the proposed methodology for interpreting the behaviour of recurrent neural networks on tasks that involve learning a finite number of stable states.
**Metric Embedding Autoencoders for Unsupervised Cross-Dataset Transfer Learning**
Cross-dataset transfer learning is an important problem in person re-identification (Re-ID). Unfortunately, not too many deep transfer Re-ID models exist for realistic settings of practical Re-ID systems. We propose a purely deep transfer Re-ID model consisting of a deep convolutional neural network and an autoencoder. The latent code is divided into metric embedding and nuisance variables. We then utilize an unsupervised training method that does not rely on co-training with non-deep models. Our experiments show improvements over both the baseline and competitors’ transfer learning models.
**Principal Filter Analysis for Guided Network Compression**
Principal Filter Analysis (PFA), is an elegant, easy to implement, yet effective methodology for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10 and CIFAR-100 PFA achieves a compression rate of 8x and 3x with an accuracy gain of 0.4% points and 1.4% points, respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.
**A Trace Lasso Regularized L1-norm Graph Cut for Highly Correlated Noisy Hyperspectral Image**
This work proposes an adaptive trace lasso regularized L1-norm based graph cut method for dimensionality reduction of Hyperspectral images, called as `Trace Lasso-L1 Graph Cut’ (TL-L1GC). The underlying idea of this method is to generate the optimal projection matrix by considering both the sparsity as well as the correlation of the data samples. The conventional L2-norm used in the objective function is sensitive to noise and outliers. Therefore, in this work L1-norm is utilized as a robust alternative to L2-norm. Besides, for further improvement of the results, we use a penalty function of trace lasso with the L1GC method. It adaptively balances the L2-norm and L1-norm simultaneously by considering the data correlation along with the sparsity. We obtain the optimal projection matrix by maximizing the ratio of between-class dispersion to within-class dispersion using L1-norm with trace lasso as the penalty. Furthermore, an iterative procedure for this TL-L1GC method is proposed to solve the optimization function. The effectiveness of this proposed method is evaluated on two benchmark HSI datasets.
**Dynamical Component Analysis (DyCA): Dimensionality Reduction For High-Dimensional Deterministic Time-Series**
Multivariate signal processing is often based on dimensionality reduction techniques. We propose a new method, Dynamical Component Analysis (DyCA), leading to a classification of the underlying dynamics and – for a certain type of dynamics – to a signal subspace representing the dynamics of the data. In this paper the algorithm is derived leading to a generalized eigenvalue problem of correlation matrices. The application of the DyCA on high-dimensional chaotic signals is presented both for simulated data as well as real EEG data of epileptic seizures.
**Resource-Size matters: Improving Neural Named Entity Recognition with Optimized Large Corpora**
This study improves the performance of neural named entity recognition by a margin of up to 11% in F-score on the example of a low-resource language like German, thereby outperforming existing baselines and establishing a new state-of-the-art on each single open-source dataset. Rather than designing deeper and wider hybrid neural architectures, we gather all available resources and perform a detailed optimization and grammar-dependent morphological processing consisting of lemmatization and part-of-speech tagging prior to exposing the raw data to any training process. We test our approach in a threefold monolingual experimental setup of a) single, b) joint, and c) optimized training and shed light on the dependency of downstream-tasks on the size of corpora used to compute word embeddings.
**Learning low dimensional word based linear classifiers using Data Shared Adaptive Bootstrap Aggregated Lasso with application to IMDb data**
In this article we propose a new supervised ensemble learning method called Data Shared Adaptive Bootstrap Aggregated (AdaBag) Lasso for capturing low dimensional useful features for word based sentiment analysis and mining problems. The literature on ensemble methods is very rich in both statistics and machine learning. The algorithm is a substantial upgrade of the Data Shared Lasso uplift algorithm. The most significant conceptual addition to the existing literature lies in the final selection of bag of predictors through a special bootstrap aggregation scheme. We apply the algorithm to one simulated data and perform dimension reduction in grouped IMDb data (drama, comedy and horror) to extract reduced set of word features for predicting sentiment ratings of movie reviews demonstrating different aspects. We also compare the performance of the present method with the classical Principal Components with associated Linear Discrimination (PCA-LD) as baseline. There are few limitations in the algorithm. Firstly, the algorithm workflow does not incorporate online sequential data acquisition and it does not use sentence based models which are common in ANN algorithms . Our results produce slightly higher error rate compare to the reported state-of-the-art as a consequence.
**Variational Option Discovery Algorithms**
We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent’s performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.
**Semantically Meaningful View Selection**
An understanding of the nature of objects could help robots to solve both high-level abstract tasks and improve performance at lower-level concrete tasks. Although deep learning has facilitated progress in image understanding, a robot’s performance in problems like object recognition often depends on the angle from which the object is observed. Traditionally, robot sorting tasks rely on a fixed top-down view of an object. By changing its viewing angle, a robot can select a more semantically informative view leading to better performance for object recognition. In this paper, we introduce the problem of semantic view selection, which seeks to find good camera poses to gain semantic knowledge about an observed object. We propose a conceptual formulation of the problem, together with a solvable relaxation based on clustering. We then present a new image dataset consisting of around 10k images representing various views of 144 objects under different poses. Finally we use this dataset to propose a first solution to the problem by training a neural network to predict a ‘semantic score’ from a top view image and camera pose. The views predicted to have higher scores are then shown to provide better clustering results than fixed top-down views.
**Open Source Automatic Speech Recognition for German**
High quality Automatic Speech Recognition (ASR) is a prerequisite for speech-based applications and research. While state-of-the-art ASR software is freely available, the language dependent acoustic models are lacking for languages other than English, due to the limited amount of freely available training data. We train acoustic models for German with Kaldi on two datasets, which are both distributed under a Creative Commons license. The resulting model is freely redistributable, lowering the cost of entry for German ASR. The models are trained on a total of 412 hours of German read speech data and we achieve a relative word error reduction of 26% by adding data from the Spoken Wikipedia Corpus to the previously best freely available German acoustic model recipe and dataset. Our best model achieves a word error rate of 14.38 on the Tuda-De test set. Due to the large amount of speakers and the diversity of topics included in the training data, our model is robust against speaker variation and topic shift.
**Selective Clustering Annotated using Modes of Projections**
**Beta Autoregressive Fractionally Integrated Moving Average Models**
**How to capitalize on a priori contrasts in linear (mixed) models: A tutorial**
Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. This is unfortunate as researchers typically have specific hypotheses about which condition means differ from each other. A priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be ‘tested instead of, rather than as a supplement to, the ordinary `omnibus’ F test’ (Hayes, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, Helmert, and polynomial contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the weight coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more precise confirmatory tests of experimental hypotheses than standard omnibus F-test.
**BSAS: Beetle Swarm Antennae Search Algorithm for Optimization Problems**
Beetle antennae search (BAS) is an efficient meta-heuristic algorithm. However, the convergent results of BAS rely heavily on the random beetle direction in every iterations. More specifically, different random seeds may cause different optimized results. Besides, the step-size update algorithm of BAS cannot guarantee objective become smaller in iterative process. In order to solve these problems, this paper proposes Beetle Swarm Antennae Search Algorithm (BSAS) which combines swarm intelligence algorithm with feedback-based step-size update strategy. BSAS employs k beetles to find more optimal position in each moving rather than one beetle. The step-size updates only when k beetles return without better choices. Experiments are carried out on building system identification. The results reveal the efficacy of the BSAS algorithm to avoid influence of random direction of Beetle. In addition, the estimation errors decrease as the beetles number goes up.
**Pull Message Passing for Nonparametric Belief Propagation**
We present a ‘pull’ approach to approximate products of Gaussian mixtures within message updates for Nonparametric Belief Propagation (NBP) inference. Existing NBP methods often represent messages between continuous-valued latent variables as Gaussian mixture models. To avoid computational intractability in loopy graphs, NBP necessitates an approximation of the product of such mixtures. Sampling-based product approximations have shown effectiveness for NBP inference. However, such approximations used within the traditional ‘push’ message update procedures quickly become computationally prohibitive for multi-modal distributions over high-dimensional variables. In contrast, we propose a ‘pull’ method, as the Pull Message Passing for Nonparametric Belief propagation (PMPNBP) algorithm, and demonstrate its viability for efficient inference. We report results using an experiment from an existing NBP method, PAMPAS, for inferring the pose of an articulated structure in clutter. Results from this illustrative problem found PMPNBP has a greater ability to efficiently scale the number of components in its mixtures and, consequently, improve inference accuracy.
**DeepLink: A Novel Link Prediction Framework based on Deep Learning**
Recently, link prediction has attracted more attentions from various disciplines such as computer science, bioinformatics and economics. In this problem, unknown links between nodes are discovered based on numerous information such as network topology, profile information and user generated contents. Most of the previous researchers have focused on the structural features of the networks. While the recent researches indicate that contextual information can change the network topology. Although, there are number of valuable researches which combine structural and content information, but they face with the scalability issue due to feature engineering. Because, majority of the extracted features are obtained by a supervised or semi supervised algorithm. Moreover, the existing features are not general enough to indicate good performance on different networks with heterogeneous structures. Besides, most of the previous researches are presented for undirected and unweighted networks. In this paper, a novel link prediction framework called ‘DeepLink’ is presented based on deep learning techniques. In contrast to the previous researches which fail to automatically extract best features for the link prediction, deep learning reduces the manual feature engineering. In this framework, both the structural and content information of the nodes are employed. The framework can use different structural feature vectors, which are prepared by various link prediction methods. It considers all proximity orders that are presented in a network during the structural feature learning. We have evaluated the performance of DeepLink on two real social network datasets including Telegram and irBlogs. On both datasets, the proposed framework outperforms several structural and hybrid approaches for link prediction problem.
**Automatic Short Answer Grading and Feedback Using Text Mining Methods**
Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and es- says, especially after the 1990s, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the Uni- versity of North Texas. We first apply standard data mining techniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We then consider a clustering approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer.
**Auto-Encoding Variational Neural Machine Translation**
We present a deep generative model of bilingual sentence pairs. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. Efficient training is done by amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling in all such scenarios.
**Simple Bayesian testing of scientific expectations in linear regression models**
Scientific theories can often be formulated using equality and order constraints on the relative effects in a linear regression model. For example, it may be expected that the effect of the first predictor is larger than the effect of the second predictor, and the second predictor is expected to be larger than the third predictor. The goal is then to test such expectations against competing scientific expectations or theories. In this paper a simple default Bayes factor test is proposed for testing multiple hypotheses with equality and order constraints on the effects of interest. The proposed testing criterion can be computed without requiring external prior information about the expected effects before observing the data. The method is implemented in R-package called `{\tt lmhyp}’ which is freely downloadable and ready to use. The usability of the method and software is illustrated using empirical applications from the social and behavioral sciences.
**Revealing the Unobserved by Linking Collaborative Behavior and Side Knowledge**
We propose a tensor-based model that fuses a more granular representation of user preferences with the ability to take additional side information into account. The model relies on the concept of ordinal nature of utility, which better corresponds to actual user perception. In addition to that, unlike the majority of hybrid recommenders, the model ties side information directly to collaborative data, which not only addresses the problem of extreme data sparsity, but also allows to naturally exploit patterns in the observed behavior for a more meaningful representation of user intents. We demonstrate the effectiveness of the proposed model on several standard benchmark datasets. The general formulation of the approach imposes no restrictions on the type of observed interactions and makes it potentially applicable for joint modelling of context information along with side data.
**Concept Tagging for Natural Language Understanding: Two Decadelong Algorithm Development**
Concept tagging is a type of structured learning needed for natural language understanding (NLU) systems. In this task, meaning labels from a domain ontology are assigned to word sequences. In this paper, we review the algorithms developed over the last twenty five years. We perform a comparative evaluation of generative, discriminative and deep learning methods on two public datasets. We report on the statistical variability performance measurements. The third contribution is the release of a repository of the algorithms, datasets and recipes for NLU evaluation.
**On the overfly algorithm in deep learning of neural networks**
In this paper we investigate the supervised backpropagation training of multilayer neural networks from a dynamical systems point of view. We discuss some links with the qualitative theory of differential equations and introduce the overfly algorithm to tackle the local minima problem. Our approach is based on the existence of first integrals of the generalised gradient system with build-in dissipation.
**Learnable: Theory vs Applications**
Two different views on machine learning problem: Applied learning (machine learning with business applications) and Agnostic PAC learning are formalized and compared here. I show that, under some conditions, the theory of PAC Learnable provides a way to solve the Applied learning problem. However, the theory requires to have the training sets so large, that it would make the learning practically useless. I suggest shedding some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners.
**Few Shot Learning with Simplex**
Deep learning has made remarkable achievement in many fields. However, learning the parameters of neural networks usually demands a large amount of labeled data. The algorithms of deep learning, therefore, encounter difficulties when applied to supervised learning where only little data are available. This specific task is called few-shot learning. To address it, we propose a novel algorithm for few-shot learning using discrete geometry, in the sense that the samples in a class are modeled as a reduced simplex. The volume of the simplex is used for the measurement of class scatter. During testing, combined with the test sample and the points in the class, a new simplex is formed. Then the similarity between the test sample and the class can be quantized with the ratio of volumes of the new simplex to the original class simplex. Moreover, we present an approach to constructing simplices using local regions of feature maps yielded by convolutional neural networks. Experiments on Omniglot and miniImageNet verify the effectiveness of our simplex algorithm on few-shot learning.
• Accuracy to Throughput Trade-offs for Reduced Precision Neural Networks on Reconfigurable Logic• DeepPhase: Surgical Phase Recognition in CATARACTS Videos• A Novel Color Edge Detection Algorithm Based on Quaternion Hardy Filter• Finding any Waldo: zero-shot invariant and efficient visual search• A Modality-Adaptive Method for Segmenting Brain Tumors and Organs-at-Risk in Radiation Therapy Planning• EchoFusion: Tracking and Reconstruction of Objects in 4D Freehand Ultrasound Imaging without External Trackers• Automated Characterization of Stenosis in Invasive Coronary Angiography Images with Convolutional Neural Networks• Dictionary Learning in Fourier Transform Scanning Tunneling Spectroscopy• Generic Camera Attribute Control using Bayesian Optimization• A post-processing method to improve the white matter hyperintensity segmentation accuracy for randomly-initialized U-net• Exploiting Spatial Correlation in Convolutional Neural Networks for Activation Value Prediction• A Capsule Network for Traffic Speed Prediction in Complex Road Networks• Several generalizations and variations of Chu-Vandermonde identity• Multimodal Classification with Deep Convolutional-Recurrent Neural Networks for Electroencephalography• Multi-Scale Gradual Integration CNN for False Positive Reduction in Pulmonary Nodule Detection• Sokoto Coventry Fingerprint Dataset• Non-local Low-rank Cube-based Tensor Factorization for Spectral CT Reconstruction• Multi-view Reconstructive Preserving Embedding for Dimension Reduction• Judging a Book by its Description : Analyzing Gender Stereotypes in the Man Bookers Prize Winning Fiction• Revisiting path-type covering and partitioning problems• Value of Information in Greedy Submodular Maximization• End-to-End Learning via a Convolutional Neural Network for Cancer Cell Line Classification• On a reduction of the weighted induced bipartite subgraph problem to the weighted independent set problem• Structured Point Cloud Data Analysis via Regularized Tensor Regression for Process Modeling and Optimization• Contributions to the development of the CRO-SL algorithm: Engineering applications problems• A conditional independence framework for coherent modularized inference• Discovering physical concepts with neural networks• Eigenvalue-free interval for threshold graphs• Characterization of semiglobal stability properties for discrete-time models of non-uniformly sampled nonlinear systems• Bootstrap percolation on the product of the two-dimensional lattice with a Hamming square• A combinatorial formula for certain binomial coefficients for Jack polynomials• Revisiting the effect of spatial resolution on information content based on classification results• A general metric for identifying adversarial images• A Reinforcement Learning Approach to Target Tracking in a Camera Network• On mean field games models for exhaustible commodities trade• On convergence of 1D Markov diffusions to heavy-tailed invariant density• Computing (R, S) policies with correlated demand• Message-passing neural networks for high-throughput polymer screening• Multirate 5G Downlink Performance Comparison for f-OFDM and w-OFDM Schemes with Different Numerologies• Integrative Multi-View Reduced-Rank Regression: Bridging Group-Sparse and Low-Rank Models• Tackling 3D ToF Artifacts Through Learning and the FLAT Dataset• Conditional Prior Networks for Optical Flow• Model Predictive Control of H5 Inverter for Transformerless PV Systems with Maximum Power Point Tracking and Leakage Current Reduction• Small Satellite Optical Communication Networks: Analytical Models• Performance Analysis of User-centric Virtual Cell Dense Networks over mmWave Channels• Perturbation Robust Representations of Topological Persistence Diagrams• Adapting control policies from simulation to reality using a pairwise loss• Characters Detection on Namecard with faster RCNN• W-TALC: Weakly-supervised Temporal Activity Localization and Classification• Fusion Network for Face-based Age Estimation• Understanding V2V Driving Scenarios through Traffic Primitives• Particle filters for applications in geosciences• Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency• Synthetically Trained Icon Proposals for Parsing and Summarizing Infographics• Task Recommendation in Crowdsourcing Based on Learning Preferences and Reliabilities• Permutations with orders coprime to a given integer• From Adversarial Training to Generative Adversarial Networks• Acceleration through Optimistic No-Regret Dynamics• On the feedback number of 3-uniform hypergraph• AXNet: ApproXimate computing using an end-to-end trainable neural network• IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks• Distributed leader election and computation of local identifiers for programmable matter• A maximum entropy network reconstruction of macroeconomic models• A Deep Learning Framework for Automatic Diagnosis in Lung Cancer• VIMCO: Variational Inference for Multiple Correlated Outcomes in Genome-wide Association Studies• Connected Subtraction Games on Subdivided Stars• Parallel Transport on the Cone Manifold of SPD Matrices for Domain Adaptation• Spectrum Matching in Licensed Spectrum Sharing• Adversarial Open-World Person Re-Identification• Faster Recovery of Approximate Periods over Edit Distance• Stochastic Geometry Modeling and Analysis of Finite Millimeter Wave Wireless Networks• Enhanced Machine Learning Techniques for Early HARQ Feedback Prediction in 5G• Hom complexes of graphs of diameter $1$• Location, Location, Location: Exploring Amazon EC2 Spot Instance Pricing Across Geographical Regions – Extended Version• Person Search in Videos with One Portrait Through Visual and Temporal Links• Global and local evaluation of link prediction tasks with neural embeddings• Invariant $\varphi$-minimal sets and total variation denoising on graphs• FARM: Functional Automatic Registration Method for 3D Human Bodies• ESCaF: Pupil Centre Localization Algorithm with Candidate Filtering• On a family of highly regular graphs by Brouwer, Ivanov, and Klin• Spectrum of the second variation• Alternating Path and Coloured Clustering• Internal observability of the wave equation in a triangular domain• Statistics of extreme ocean environments: Non-stationary inference for directionality and other covariate effects• ADMM for MPC with state and input constraints, and input nonlinearity• Universal trees grow inside separating automata: Quasi-polynomial lower bounds for parity games• CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping• The bd-model of ageing: from individual-based dynamics to evolutive differential inclusions• X2Face: A network for controlling face generation by using images, audio, and pose codes• Improving High Resolution Histology Image Classification with Deep Spatial Fusion Network• Protocol for an Observational Study on the Effects of Early-Life Participation in Contact Sports on Later-Life Cognition in a Sample of Monozygotic and Dizygotic Swedish Twins Reared Together and Twins Reared Apart• Smoothness of correlation functions in Liouville Conformal Field Theory• Towards an Embodied Semantic Fovea: Semantic 3D scene reconstruction from ego-centric eye-tracker videos• Diverse feature visualizations reveal invariances in early layers of deep neural networks• Temporal connectivity in finite networks with non-uniform measures• Power Minimizer Symbol-Level Precoding: A Closed-Form Sub-Optimal Solution• Construction of MDS Self-dual Codes over Finite Fields• Experimental Implementation of a Quantum Autoencoder via Quantum Adders• Learning associations between clinical information and motion-based descriptors using a large scale MR-derived cardiac motion atlas• Influence of Image Classification Accuracy on Saliency Map Estimation• Sign-alternating photoconductivity and magnetoresistance oscillations induced by terahertz radiation in HgTe quantum wells• Fair allocation of combinations of indivisible goods and chores• Infinite Mixture of Inverted Dirichlet Distributions• FPGA-Based CNN Inference Accelerator Synthesized from Multi-Threaded C Software• Quantized Hodgkin-Huxley Model for Quantum Neurons• Diagnosing Error in Temporal Action Detectors• End-to-end Deep Learning from Raw Sensor Data: Atrial Fibrillation Detection using Wearables• On the long time convergence of potential MFG• Region of Interest Detection in Dermoscopic Images for Natural Data-augmentation• Semi-convolutional Operators for Instance Segmentation• On coupling and vacant set level set percolation• Cluster expansions for Gibbs point processes• Connected Components at Scale via Local Contractions• Deep PDF: Probabilistic Surface Optimization and Density Estimation• An Algorithm for Learning Shape and Appearance Models without Annotations• Decompositions of complete multigraphs into stars of varying sizes• A small Griko-Italian speech translation corpus• Attention-based Active Visual Search for Mobile Robots• GPU Based Parallel Ising Computing for Combinatorial Optimization Problems in VLSI Physical Design

### Like this:
Like Loading...

*Related*

