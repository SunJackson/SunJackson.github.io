---
layout:     post
catalog: true
title:      Neural reinterpretations of movie trailers
subtitle:      转载自：http://blog.fastforwardlabs.com/2018/07/31/neural-reinterpretations-of-movie-trailers.html
date:      2018-07-31
img:      0
author:      Grant
tags:
    - trailers
    - scenes
    - films
    - movie
    - neural
---

In his latest project, artist and coder Mario Klingemann uses a neural network to match archival movie footage with the content of recent movie trailers. He regularly posts the resulting “neural reinterpretations” on his Twitter. The results are technically impressive. They’re also a fascinating view into how to explore the creative possibilities of a machine learning technique.

Looking through Klingemann’s tweets you can trace his explorations:

![](http://blog.fastforwardlabs.com/images/editor_uploads/2018-06-26-144731-Screen_Shot_2018_06_25_at_10_45_15_AM.png)


##### Mario Klingemann’s neural scene classifier grouping scenes it finds similar.

![](http://blog.fastforwardlabs.com/images/editor_uploads/2018-06-26-144942-Screen_Shot_2018_06_25_at_10_46_42_AM.png)


##### A neural reinterpretation of the Fight Club trailer, with the original footage on the left and the matched on the right.

The movie trailer reinterpretations are a great showcase for the technique for a couple of reasons:


Trailers are made up of short clips. This gives the algorithm lots of shots at finding interesting matches (every cut is a new example). If it was instead focused on a 2 minute long continuous scene, you wouldn’t get to see nearly as many matches. Also the fact that the cuts are often timed to the music makes the reinterpreted content appear more connected to the audio of the trailer.


Films have a built up vocabulary of what different shots mean, like a close-up of a face to signal intense feelings. Film-makers employ these patterns consciously. As film watchers, we may not think about scene types explicitly, but we do build up associations and expectations with different framing, movements, and styles. The side-by-side reinterpretations make this referential language more visible by showing us two examples at a time, helping us notice the similarity the machine has identified. We can then often extrapolate even further into “ah, right, that’s another one of those ‘vehicles rushing by’ shots” that you normally don’t consciously note. This takes the trailers from technical demos into artistic territory.


![](http://blog.fastforwardlabs.com/images/editor_uploads/2018-06-26-145050-Screen_Shot_2018_06_25_at_10_47_09_AM.png)


##### A still from “Learning to see: Gloomy Sunday” by Memo Atken

“Learning to see: Gloomy Sunday” by Memo Akten explores similarity in a different, fascinating way. He has a model trained on specific types of art that interpret his webcam photos and generate new images: for example, a sheet becomes waves. Like in the trailer reinterpretations, what takes this beyond technical demo is how suggestive the association can be. The machine’s ability to identify similarity between a sheet and a wave gives us an understanding that we can then apply outside of the context of the video. It’s a suggestive analogy that opens out so that the viewer can build upon it and make their own connections.
