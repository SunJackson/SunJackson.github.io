---
layout:     post
catalog: true
title:      Naive Bayes： A Baseline Model for Machine Learning Classification Performance
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/j9HEf9lK1Zo/naive-bayes-baseline-model-machine-learning-classification-performance.html
date:      2019-05-07
author:      Asel Mendis
tags:
    - playing
    - import
    - print
    - tennis
    - no_x
---
![](http://feedproxy.google.com/wp-content/uploads/scikitlearn-pandas-naivebayes.jpg)


 

### Bayes Theorem

 
![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(A)


The above equation represents Bayes Theorem in which it describes the probability of an event occurring P(A) based on our prior knowledge of events that may be related to that event P(B).

Lets explore the parts of Bayes Theorem:

- P(A|B) - **Posterior Probability**


- The conditional probability that event A occurs given that event B has occurred.


- The probability of event A.


- The probability of event B.


- The conditional probability of B occurring given event A has occurred.


*Now, lets explore the parts of Bayes Theorem through the eyes of someone conducting machine learning:*

- P(A|B) - **Posterior Probability**


- The conditional probability of the response variable (target variable) given the training data inputs.


- The probability of the response variable (target variable).


- The probability of the training data.


- The conditional probability of the training data given the response variable.

![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(c)


- P(c|x) - Posterior probability of the target/class (c) given predictors (x).

- P(c) - Prior probability of the class (target).

- P(x|c) - Probability of the predictor (x) given the class/target (c).

- P(x) - Prior probability of the predictor (x).


**Example of using Bayes theorem:**I'll be using the tennis weather dataset.





||outlook|temp|humidity|windy|play
|------
|0|sunny|hot|high|False|no|
|1|sunny|hot|high|True|no|
|2|overcast|hot|high|False|yes|
|3|rainy|mild|high|False|yes|
|4|rainy|cool|normal|False|yes|
|5|rainy|cool|normal|True|no|
|6|overcast|cool|normal|True|yes|
|7|sunny|mild|high|False|no|
|8|sunny|cool|normal|False|yes|
|9|rainy|mild|normal|False|yes|
|10|sunny|mild|normal|True|yes|
|11|overcast|mild|high|True|yes|
|12|overcast|hot|normal|False|yes|
|13|rainy|mild|high|True|no|

Lets take a look at how each category looks when inside a frequency table:





**What is the probability of playing tennis given it is rainy?**

- P(rain|play=yes)


- frequency of (outlook=rainy) when (play=yes) / frequency of (play=yes) = 3/9


- frequency of (play=yes) / total(play) = 9/14


- frequency of (outlook=rainy) / total(outlook) = 5/14

![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(play=yes)




The probability of playing tennis when it is rainy is **60%.**The process is very simple once you obtain the frequencies for each category.

Here is a simple function to help any newbies remember the parts of Bayes equation:



Here is a simple function to calculate the posterior probability for you, but you must be able to find each part of bayes equation yourself.



**Lets see another way to find the posterior probability this time using contingency tables in Python:**





||no|yes|rowtotal
|------
|overcast|0.000000|0.285714|0.285714|
|rainy|0.142857|0.214286|0.357143|
|sunny|0.214286|0.142857|0.357143|
|coltotal|0.357143|0.642857|1.000000|


To only get the column total



||no|yes|rowtotal
|------
|overcast|0.0|0.444444|0.285714|
|rainy|0.4|0.333333|0.357143|
|sunny|0.6|0.222222|0.357143|
|coltotal|1.0|1.000000|1.000000|


To only get the row total



||no|yes|rowtotal
|------
|overcast|0.000000|1.000000|1.0|
|rainy|0.400000|0.600000|1.0|
|sunny|0.600000|0.400000|1.0|
|coltotal|0.357143|0.642857|1.0|


These tables are all pandas dataframe objects. Therefore using pandas subsetting and the `bayesposterior` function I made, we can arrive at the same conclusion:



 

### Naive Bayes Algorithm

 

Naive Bayes is a supervised Machine Learning algorithm inspired by the Bayes theorem. It works on the principles of conditional probability. Naive Bayes is a classification algorithm for binary and multi-class classification. The Naive Bayes algorithm uses the probabilities of each attribute belonging to each class to make a prediction.

**Example**What is the probability of playing tennis when it is sunny, hot, highly humid and windy? So using the tennis dataset, we need to use the Naive Bayes method to predict the probability of someone playing tennis given the mentioned weather conditions.



|play|no|yes|All
|outlook|||
|------
|overcast|0|4|4|
|rainy|2|3|5|
|sunny|3|2|5|
|All|5|9|14|




pd.crosstab(tennis['temp'], tennis['play'], margins = True)




|play|no|yes|All
|temp|||
|------
|cool|1|3|4|
|hot|2|2|4|
|mild|2|4|6|
|All|5|9|14|




pd.crosstab(tennis['humidity'], tennis['play'], margins = True)




|play|no|yes|All
|humidity|||
|------
|high|4|3|7|
|normal|1|6|7|
|All|5|9|14|




pd.crosstab(tennis['windy'], tennis['play'], margins = True)




|play|no|yes|All
|windy|||
|------
|False|2|6|8|
|True|3|3|6|
|All|5|9|14|




pd.crosstab(index=tennis['play'],columns="count", margins=True)




|col_0|count|All
|play||
|------
|no|5|5|
|yes|9|9|
|All|14|14|

Now by using the above contingency tables, we will go through how the Naive Bayes algorithm calculates the posterior probability.

1. Calculate P(x|play=yes). In this case x refers to all the predictors 'outlook', 'temp', 'humidity' and 'windy'.


1. P(sunny|play=yes)→2/9

1. P(hot|play=yes)→2/9

1. P(high|play=yes)→3/9

1. P(True|play=yes)→3/9

![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(x)




1. P(sunny|play=no)→3/5

1. P(hot|play=no)→2/5

1. P(high|play=no)→4/5

1. P(True|play=no)→3/5

![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(x)




1. P(play=yes)→9/14

1. P(play=yes)→5/14



![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(play=yes)

![](https://latex.codecogs.com/gif.latex?\boldsymbol{\mathbf{P(play=no)








 

### Type of Naive Bayes Algorithm 

 Python's Scikitlearn gives the user access to the following 3 Naive Bayes models.

1. Gaussian 


1. The gaussian NB Alogorithm assumes all contnuous features (predictors) and all follow a Gaussian (Normal Distribution).


1. Multinomial 


1. Multinomial NB is suited for discrete data that have frequencies and counts. Spam Filtering and Text/Document Classification are two very well-known use cases.


1. Bernoulli


1. Bernoulli is similar to Multinomial except it is for boolean/binary features. Like the multinomial method it can be used for spam filtering and document classification in which binary terms (i.e. word occurrence in a document represented with True or False).



- Multinomial NB is suited for discrete data that have frequencies and counts. Spam Filtering and Text/Document Classification are two very well-known use cases.


Lets implement a Multinomial and Gaussian Model with Scikitlearn


