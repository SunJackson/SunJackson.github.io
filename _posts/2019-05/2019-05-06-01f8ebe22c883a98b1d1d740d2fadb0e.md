---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/05/06/distilled-news-1057/
date:      2019-05-06
author:      Michael Laux
tags:
    - data
    - quantum
    - learning
    - modeling
    - bayesian models
---

**Leveraging the best of both Python and R**

Data Science has become an integral part of every industry today. Right from Banking, to insurance to healthcare, humongous amount of data is being generated every second of the day. Therefore it has become imperative that we should be able to utilize this vast amount of data to generate actionable insights and work upon them. A lot of tools in the form of programming languages are available in the market today. Out of all those languages, Python and R appear to be leading the race. Both Python and R are being widely used in the data science world. Both the languages have a wide variety of tools which provide an excellent array of functions, extremely suitable for the data science scenario. Whereas Python is a general-purpose language used for a variety of applications, R is a programming language and environment for statistical computing and graphics.

**Bayesian models in R**

If there was something that always frustrated me was not fully understanding Bayesian inference. Sometime last year, I came across an article about a TensorFlow-supported R package for Bayesian analysis, called greta. Back then, I searched for greta tutorials and stumbled on this blog post that praised a textbook called Statistical Rethinking: A Bayesian Course with Examples in R and Stan by Richard McElreath. I had found a solution to my lingering frustration so I bought a copy straight away. I spent the last few months reading it cover to cover and solving the proposed exercises, which are heavily based on the rethinking package. I cannot recommend it highly enough to whoever seeks a solid grip on Bayesian statistics, both in theory and application. This post ought to be my most gratifying blogging experience so far, in that I am essentially reporting my own recent learning. I am convinced this will make the storytelling all the more effective.

**So, what is Artificial Intelligence? Firstly, it’s not as hard as it sounds**

In this article I will demystify the term Artificial Intelligence, I will reveal where and how it is used. Lastly, using basic programming techniques I will provide a simple proof of concept that AI can be applicable in uncomplicated business processes. Do not fear, you do not have to be a tech guru to understand this article.

**Demystifying Quantum Machine Learning**

Small quantum computers and larger special-purpose quantum simulators, annealers and so on seem to have potential use in machine learning and data analysis. However, the execution of quantum algorithms requires quantum hardware that is not yet available. On the hardware side, there have been great strides in several enabling technologies. Small-scale quantum computers with 50-100 qubits will be made widely available via quantum cloud computing (the ‘Qloud’). Special-purpose quantum information processors such as quantum simulators, quantum annealers, integrated photonic chips, nitrogen-vacancy centres (NV)-diamond arrays, qRAM, and made-to-order superconducting circuits will continue to advance in size and complexity. Quantum machine learning offers a suite of potential applications for small quantum computers complemented and enhanced by special-purpose quantum information processors digital quantum processors and sensors. These hardware challenges are technical in nature, and clear paths exist towards overcoming them. They must be overcome, however, if quantum machine learning is to become a ‘killer app’ for quantum computers. As noted previously, most of the quantum algorithms that have been identified face a number of caveats that limits their applicability.

**10 Machine Learning Methods that Every Data Scientist Should Know**

The ten methods described offer an overview – and a foundation you can build on as you hone your machine learning knowledge and skill:1. Regression2. Classification3. Clustering4. Dimensionality Reduction5. Ensemble Methods6. Neural Nets and Deep Learning7. Transfer Learning8. Reinforcement Learning9. Natural Language Processing10. Word Embeddings

**Practical Introduction to Market Basket Analysis – Asociation Rules**

Ever wondered why items are displayed in a particular way in retail/online stores. Why certain items are suggested to you based on what you have added to the cart? Blame it on market basket analysis or association rule mining.

**Process Mining (Part 3/3): More analysis and visualizations**

A week ago, Havard Business Review published an article on process mining and provided reasons for companies to adopt it. If you need a refresher on the concepts of process mining, you can refer to my first post. Conducting process mining is easy with R’s bupaR package. bupaR allows you to create a variety of visualizations as you analyse event logs. It includes visualizations of workflow on the ground which you can then compare them against theoretical models to discover deviations. However, there are some analysis and visualizations which are not included in bupaR. I will cover these in this last post on process mining. (The datasets used in this post are healthcare related.)

**The unstoppable rise of white box data**

Chris Taggart explains the benefits of ‘white box data’ and outlines the structural shifts that are moving the data world toward this model.

**How Microsoft Azure Machine Learning Studio Clarifies Data Science**

Simple to use, but serious data science knowledge still required: I’ve been dying to test drive one of the many recent tools on the market targeted at the ‘citizen data scientists’ like DataRobot, H20 Driverless AI and Microsoft’s new product in the cloud called Microsoft Azure Machine Learning Studio (Studio). These tools promise to accelerate the time to value of data science projects by simplifying machine learning model construction. Ultimately, this will allow data engineers, programmers, business analysts and others without PhDs to start chipping away at the massive modeling opportunity companies are eager to tap into but are limited in their ability to address due to data science skills shortage. So I opened up an Azure account and spent some hours building a few machine learning models from the ground up using their sample data. I’ll describe my experiences here to show you how easy this tool really is to use in the hopes that others can quickly grasp its strengths and weakness. I think I am a representative candidate to be conducting this review as I am not a working data scientist today. I however am a graduate student in data science at UC Berkeley, have a CS degree, have taken several graduate level statistics and machine learning courses and can program in Python.

**Careful! Looking at you model results too much can cause information leakage**

It’s always to use as much data as you can when building machine learning models. I think we all are aware of the issue of overfitting, which is essentially where the model you build replicates the training data results so perfectly its fitted to the training data and does not generalise to better represent the population the data comes to, with catastrophic results when you feed in new data and get very odd results.

**Kaplan Meier curves: an introduction**

Kaplan-Meier curves are widely used in clinical and fundamental research, but there are some important pitfalls to keep in mind when making or interpreting them. In this short post, I’m going to give a basic overview of how data is represented on the Kaplan Meier plot. The Kaplan-Meier estimator is used to estimate the survival function. The visual representation of this function is usually called the Kaplan-Meier curve, and it shows what the probability of an event (for example, survival) is at a certain time interval. If the sample size is large enough, the curve should approach the true survival function for the population under investigation.

**Natural Language Processing – Event Extraction**

The amount of text generated every day is mind-blowing. Millions of data feeds are published in the form of news articles, blogs, messages, manuscripts and countless more, and the ability to automatically organize and handle it is becoming indispensable. With improvements in neural network algorithms, significant computer power increase and easy access to comprehensive frameworks, Natural Language Processing has never been so explored. One of its common applications is called Event Extraction, which is the process of gathering knowledge about periodical incidents found in texts, automatically identifying information about what happened and when it happened.

**Shiny: A data scientist’s best friend**

One of the most important skills for any data scientist is the ability to clearly communicate results to a general audience. These are the people who need to understand the insights in order to take further action. Unfortunately, too many data science projects bog down in math and computation that’s impenetrable the general reader. This is why tools like Shiny are quickly becoming every data scientist’s best friend.

**Converting D3.js to PDF to PowerPoint**

After creating a beautiful visualization in D3.js I often get the same question from my colleagues in marketing: «Could you send me a PowerPoint of this?». I usually explain that this a programmed chart, similar to a website, and that it is unfortunately not possible to put this on a PowerPoint slide. So far my best recommendation was to use the Microsoft snipping tool. But I recently found a solution that does a surprisingly good job. So before we start, we need of course a D3.js visualization. For this I created a sunburst chart showing the annual net revenue of Mondelez in 2018. It’s a pretty interesting chart. As we can see, 43.1% of their net sales alone came from biscuits like e.g. Oreos. And North America alone accounts for 50% of these sales. That’s really a lot of biscuits.

**Joining Data Sources**

Most ‘data science’ in the real world involves creating a data set, a visualization, an application that requires pulling and joining data from very different sources to tell a cohesive story. Moving past toy data sets, let’s take a look one start-to-finish application of joining disparate data sources into one comprehensive map. Specifically, we’ll be answering the question: Which zip codes in Los Angeles County have the most Starbucks stores per capita? If, for example, it were your job to find the best place in LA County for a new Starbucks location, you might be very invested in the answer to this question.

**Using the ‘What-If Tool’ to investigate Machine Learning models.**

In this era of explainable and interpretable Machine Learning, one merely cannot be content with simply training the model and obtaining predictions from it. To be able to really make an impact and obtain good results, we should also be able to probe and investigate our models. Apart from that, algorithmic fairness constraints and bias should also be clearly kept in mind before going ahead with the model. Investigating a model requires asking a lot of questions and one needs to have an acumen of a detective to probe and look for issues and inconsistencies within the models. Also, such a task is usually complex requiring to write a lot of custom code. Fortunately, the What-If Tool has been created to address this issue making it easier for a broad set of people to examine, evaluate, and debug ML systems easily and accurately.





### Like this:

Like Loading...


*Related*

