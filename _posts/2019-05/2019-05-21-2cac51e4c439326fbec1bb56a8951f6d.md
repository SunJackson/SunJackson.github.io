---
layout:     post
catalog: true
title:      Unfolded ISTA and Orthogonality Regularization -implementation -
subtitle:      转载自：http://feedproxy.google.com/~r/blogspot/wCeDd/~3/w0VrllSaxio/unfolded-ista-and-orthogonality.html
date:      2019-05-21
author:      Igor (noreply@blogger.com)
tags:
    - blanche
    - unfolded
    - papers
    - convergences
    - unfolding sparse recovery
---




![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)














### 
Unfolded ISTA and Orthogonality Regularization -implementation -



Xiaohan sent me the following a few months ago:

Dear Igor,I'm a long-time fan of your blog and want to share our two recent NIPS papers with you. One is a theory paper on unfolding sparse recovery algorithms into deep networks (and a spotlight oral of NIPS'18); the other is an empirical exploration of applying orthogonality regularizations to training deep CNNs, with many techniques inspired by sparse optimization.The first paper proves the theoretical linear convergence (as upper bound) of unfolded ISTA networks (LISTA), and proposes two new structures (weight and threshold) to facilitate that fast converegnce and significantly boost performance. The work is done in collaboration with Jialin Liu (http://www.math.ucla.edu/~liujl11/) and Wotao Yin (http://www.math.ucla.edu/~wotaoyin/) in Math@UCLA.Preprint: https://arxiv.org/abs/1808.10038Github: https://github.com/xchen-tamu/linear-lista-cpss 

The second paper proposes several orthogonality regularizations on CNN weights, by penalizing the distance between the Gram matrix of weights and identity under different metrics. We show that orthogonality evidently accelerates and stabilizes the empirical training convergence, as well as improve as final accuracies. The mose powerful regularization was derived from Restrcited Isometry Property (RIP).

> 
The second paper proposes several orthogonality regularizations on CNN weights, by penalizing the distance between the Gram matrix of weights and identity under different metrics. We show that orthogonality evidently accelerates and stabilizes the empirical training convergence, as well as improve as final accuracies. The mose powerful regularization was derived from Restrcited Isometry Property (RIP).

Preprint: https://arxiv.org/abs/1810.09102Github: https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality

It would be great if you could distribute their informtion to potential interested audience on nuit-blanche.Best regards,--Xiaohan ChenDept. Computer Science & EngineeringTexas A&M University, College Station, TX, U.S.Webpage: http://people.tamu.edu/~chernxh/






![](https://2.bp.blogspot.com/-uMZ3W20zH-Q/XN1BUabnYCI/AAAAAAAAWek/ZZQTMk4DNAQCa_x-ecITpo0EIxZRMPLFQCLcBGAs/s320/lisatista.png)




![](https://3.bp.blogspot.com/-dE--qXwlSQs/XN1AJSyeIlI/AAAAAAAAWeU/MmD2PFn58n4M1SZosYVk0Lcj7n4YDccPACLcBGAs/s320/lista-cp.png)





> 
In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available: this https URL.
![](https://3.bp.blogspot.com/-h_IUAwNXzsw/XN1BAzzk0TI/AAAAAAAAWec/goIhS5He0TEKpMoXP5iNyhlSyFztobiIACLcBGAs/s400/validationcurvesorthogonality.png)





> 
This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available: this https URL.




![](https://lh5.googleusercontent.com/proxy/lTvIEwtKmZneqIlhRc-7Jt5-zR2SVwpq0oxc1C5q5ju7bjk9kxPYTkqbcJtV4vzhIkH1gpgMSB-t-HkR_3Yy08iC8e6svEhEE-jh72do=s0-d)
Liked this entry ? subscribe to Nuit Blanche's feed, there's more where that came from. You can also subscribe to Nuit Blanche by Email.

Other links:***Paris Machine Learning***: Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup< br/>

***About LightOn***: Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our Blog***About myself***: LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv





Printfriendly








![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






**Nuit Blanche** community:@NuitBlog, Facebook and Reddit

**Paris Machine Learning** Meetup.com, archives, LinkedIn , Facebook,@ParisMLGroup

**LightOn**





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)














![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)










![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Search Nuit Blanche











![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Subscribe by E-MAIL to Nuit Blanche





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Contact me:





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Popular Posts





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)










![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Pages





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Subscribe to the LinkedIn Matrix Factorization Group
![](http://1.bp.blogspot.com/--_UYuqPjBRk/TvsKQ-1QeNI/AAAAAAAAFQI/ePupIs-m6WU/s240/Jungle.jpg)






![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Subscribe to the LinkedIn Compressive Sensing group
![](http://2.bp.blogspot.com/_0ZCyAOBrUtA/SK8gEp_u-EI/AAAAAAAABv8/Itdy7gkjWYQ/S259/CS-group-logo.JPG)






![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)










![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Nuit Blanche QR code





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Readership Statistics

 ![](https://lh5.googleusercontent.com/proxy/CMLxtjtMPiKYWxTUUrdxxJA9zNyPYBre3PDDaRxsfWdgCb0CD6pFDtRtZ4wuJNRfqVdiPST_gcIqWj08mkMy40pn5pTotMJGr7wjhPQGXn4ExBECbYFWcelaFkc2=s0-d)
 and another set of ![](https://lh5.googleusercontent.com/proxy/Uem4SzOmCd2aB4NHUF_-YGhZkTKJuuAl00aZAv2jpn5dgJ4FOD7n2eWkYbYaFEj_VtYlG95XP78XuintkB7j8Foyxx7Qi29dwsPF6yC06oIrqOMtA1eeA4EJ1V8=s0-d)
 and 

![](https://lh3.googleusercontent.com/proxy/xC3Ivsf9XqvuH_1sovntt1UNvyMGyRGulYYnoEqffpmZUEjjVO30mKxYna0bQLaOX3tm3eUFeLVn7kXSh23SeTGSEi-HMBzUCbr_sYiJNTurrJ5s_UQUPnKE3EY=s0-d)




More than **838** readers receive every entries in their mailboxes while more than 

There are more detailed information in the following blog entries.

So far, this site has seen more than 





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Nuit Blanche Referenced in the Dead Tree World!





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## **If You Like It, Link To It**





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Another Blog List





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Search Pages I Link To





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






Previous Entries






![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)







Books Wish List

![](https://images-na.ssl-images-amazon.com/images/G/01/gifts/registries/wishlist/v2/web/wl-btn-74-b._V46774601_.gif)


## Books Wish List





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






Start-ups we like







![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Focused Interest





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)










![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Categories/Subjects of Interest





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)






## Other sites of interest / Blogroll





![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)





![](https://lh5.googleusercontent.com/proxy/L0EpTyDVt-v9uCVAnF1LLt1OsI_K_gHrhZXgavyRwv0OUlNuCxuztunuvRxN0iPTgM41PnK7YxpaMuKITRzdMAg4e0c=s0-d)






![](https://resources.blogblog.com/img/icon18_wrench_allbkg.png)





