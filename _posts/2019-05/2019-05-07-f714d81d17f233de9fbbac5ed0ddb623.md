---
layout:     post
catalog: true
title:      Document worth reading： “An Interdisciplinary Comparison of Sequence Modeling Methods for Next-Element Prediction”
subtitle:      转载自：https://analytixon.com/2019/05/07/document-worth-reading-an-interdisciplinary-comparison-of-sequence-modeling-methods-for-next-element-prediction/
date:      2019-05-07
author:      Michael Laux
tags:
    - sequences
    - learn sequence models
    - modeling
    - techniques
    - learning field
---

Data of sequential nature arise in many application domains in forms of, e.g. textual data, DNA sequences, and software execution traces. Different research disciplines have developed methods to learn sequence models from such datasets: (i) in the machine learning field methods such as (hidden) Markov models and recurrent neural networks have been developed and successfully applied to a wide-range of tasks, (ii) in process mining process discovery techniques aim to generate human-interpretable descriptive models, and (iii) in the grammar inference field the focus is on finding descriptive models in the form of formal grammars. Despite their different focuses, these fields share a common goal – learning a model that accurately describes the behavior in the underlying data. Those sequence models are generative, i.e, they can predict what elements are likely to occur after a given unfinished sequence. So far, these fields have developed mainly in isolation from each other and no comparison exists. This paper presents an interdisciplinary experimental evaluation that compares sequence modeling techniques on the task of next-element prediction on four real-life sequence datasets. The results indicate that machine learning techniques that generally have no aim at interpretability in terms of accuracy outperform techniques from the process mining and grammar inference fields that aim to yield interpretable models. An Interdisciplinary Comparison of Sequence Modeling Methods for Next-Element Prediction





### Like this:

Like Loading...


*Related*

