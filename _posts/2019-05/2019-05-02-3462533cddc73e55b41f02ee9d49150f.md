---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/05/02/distilled-news-1053/
date:      2019-05-02
author:      Michael Laux
tags:
    - processes
    - learning
    - symmetries
    - representations
    - gradient
---

**Hamiltonian Monte Carlo from scratch**

Markov chain Monte Carlo (MCMC) is a method used for sampling from posterior distributions. Hamiltonian Monte Carlo (HMC) is a variant that uses gradient information to scale better to higher dimensions, and which is used by software like PyMC3 and Stan. Some great references on MCMC in general and HMC in particular are• Christopher Bishop’s ‘Pattern Recognition and Machine Learning’ A classic machine learning textbook, and gives a good overview of motivating sampling, as well as a number of different sampling strategies.• Iain Murray’s lectures at the MLSS A good, approachable lecture, including intuition about how these algorithms are applied and implemented• Michael Betancourt’s ‘A Conceptual Introduction to Hamiltonian Monte Carlo’ A thorough, readable reference that is the main source here

**Exercises in automatic differentiation using autograd and jax**

This is a short note on how to use an automatic differentiation library, starting from exercises that feel like calculus, and ending with an application to linear regression using very basic gradient descent. I am using autograd here, though these experiments were originally done using jax, which adds XLA support, so everything can run on the GPU. It is strikingly easy to move from autograd to jax, but the random number generation is just weird enough that the following is run with autograd. I have included the equivalent jax code for everyting, though Automatic differentiation has found intense application in deep learning, but my interest is in probabilistic programming, and gradient-based Markov chain Monte Carlo in particular.

**What Process Mining Is, and Why Companies Should Do It**

There have long been a few fundamental challenges associated with business process management, at least for as long as the two of us have been involved with the field (forty years or so, for better or worse). Two of the most troublesome problems, in our view, are at least partially responsible for the fact that process management and improvement is, among many companies, a back-burner issue at the moment. But a relatively new and innovative technology, process mining, has the capability to solve both of the problems and to revitalize process management in firms where it has lain fallow for years. One problem involves the creation of ‘current state’ processes – a description of how a business process is being performed today. In business process reengineering, organizations are primarily interested in an improved ‘to be’ process, so often they have little interest in exploring ‘as is,’ or how the process is currently performed. But understanding the current process is critical to knowing whether it is worth investing in improvements, where performance problems exist, and how much variation there is in the process across the organization. As a result, some companies tend to either skip current process analysis altogether, adopt shortcuts to it, or pay consultants a lot of money to analyze the ‘as is’ process.

**Study shows many senior managers distrust big data**

A new Massey University study shows many top executives are failing to capitalise on the benefits of big data, preferring to rely on their own intuition.

**Node.js Docker workflow**

I’ve been using Docker for approximately a year now, and after some time getting used to I am now a huge fan of how it can improve the whole making of an application, from the development phase to the production phase.In this article I chose to talk about 3 parts of the making of an app that Docker can bring to a new level:1. Optimizing the production artifact2. Normalizing environments3. Improving integration and deliveryThese insights, adaptable to other stacks than Node.js, come mainly from experience on small to large scale application development, as I’m continuously trying to increase the realization flow in work and personal projects.

**This Google Experiment Destroyed Some of the Assumptions About Representation Learning**

Building knowledge in high dimensional datasets is one of the fundamental challenges of modern deep learning applications. As humans, we are extremely proficient of reasoning in a small number of dimensions but information represented in a large number of dimensions results mostly incomprehensible. One ability of humans cognition that proves helpful when understanding large dimensional datasets is our ability to decompose the world in smaller and somewhat disconnected pieces of knowledge. In the context of deep learning, the equivalent to that skill is known as disentangled representations. Recently, artificial intelligence(AI) researchers from Google published a paper that challenges the traditional understanding of disentangled representations. The idea of disentangled representations is that an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. This idea draws inspiration from physics such as world symmetries that state that any world can be modeled in different layers that are connected by symmetrical transformations. The definition of symmetrical transformation is one that leaves certain properties of the source object invariant. One of the ultimate representations of this idea is the Noether’s Theorem which states that that every conservation law is grounded in a corresponding continuous symmetry transformation. For example, the conservation of energy arises from the time translation symmetry, the conservation of momentum arises from the space translation symmetry, and the conservation of angular momentum arises due to the rotational symmetry. Disentangled representations attempt to decompose an environment into different representations that are easier to learn and virtually disconnected from each other. From the theoretical standpoint, disentangled representations are part of a broader branch of deep learning known as representation learning.





### Like this:

Like Loading...


*Related*

