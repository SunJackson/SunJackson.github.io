---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/05/01/distilled-news-1051/
date:      2019-05-01
author:      Michael Laux
tags:
    - learning
    - learned
    - data
    - cool kids like
    - spark
---

**Serving Vowpal Wabbit models on Kubernetes**

In this post I am going to showcase an example of building a Kubernetes service to bring to production Machine Learning models trained with a library that does not have a ‘native’ serving API. I chose Vowpal Wabbit because while not being as popular as other Machine Learning frameworks, it does often give very good results out of the box and requires minimal effort in terms of data preparation, optimizer selection and hyper-parameter tuning. I personally feel that learning Vowpal Wabbit (VW) is a good starting point for a (Junior) Data Scientist as a beginner can easily, quickly and iteratively prototype on quite large data-sets. On the other hand, from a Machine Learning Engineer perspective, serving a model trained with VW is an interesting task as the interface provided by the library is minimal and there is no official serving API. In this post I will try to walk you through the development of this service. There are several GitHub gists, and you can find the full repository here.

**Hyperparameter optimization in python. Part 0: Introduction.**

Hyperparameter optimization, or HPO as cool kids like to call it, is quickly becoming common knowledge in data science. Anything, with hyper in the name sounds cool enough, but what does it actually do and why should you care? Every machine learning algorithm has a certain number of parameters that you define before you start training. The number of layers, depth of a tree or the amount of regularization are just some examples of such (hyper)parameters. Once those are defined you can feed the data to your model, train it and evaluate its performance. Hyperparameter optimization is just the process of tweaking hyperparameters to achieve the highest performance under some time constraint.

**ColumnTransformer Meets Natural Language Processing**

How to combine several feature extraction mechanisms or transformations into a single transformer in a scikit-learn pipeline. Since published several articles on text classification, I have received inquiries on how to deal with mixed input feature types, for example, how to combine numeric, categorical and text features in a classification or regression model. Therefore, I decided to write a post using an example to answer this question. There are several different methods to append or combine different feature types. One method is using scipy.sparse.hstack that stack sparse matrices horizontally. However, I will introduce one method, the new hot baby on the block: ColumnTransformer function from sklearn. If you would like to try it out, you will need to upgrade your sklearn to 0.20.

**AdaNet – Adaptive Structural Learning of Artificial Neural Networks**

In the march toward Machine Learning ubiquity, AutoML is emerging as an important mechanism for making ML accessible to those without deep expertise in the field. Simply put, the aspiration of AutoML is to allow a ‘push-button’ experience, where the user provides data, pushes a button, and gets an ML model out the other end. In the case of neural networks, determining the network structure is an important challenge-e.g. how many layers, how many nodes per layer, how are the layers connected together? The typical advice is to just choose as large a model as you can get away with from the perspective of hardware capabilities and speed, and then use standard practices to prevent overfitting. The AdaNet paper from Google Research proposes a new algorithm that allows us to learn the optimal network structure. The theory they develop and apply in the paper was quite different from what I am used to reading, and I learned a valuable perspective by working through it. I’ll try to present my understanding of it here in a more accessible format with the hope that others might enjoy and benefit from their perspective.

**The Fundamental Problem of Machine Learning, Without Math**

Hello! Today, we’re going to cover partial stochastic deep neural gradients of convex isomorphisms. I’m just kidding; this is just a mess of large words, but reading papers can feel like this sometimes. So, I wanted to write about what I feel is the most important topic in machine learning and artificial intelligence in an accessible manner which doesn’t require prior knowledge or math skills. I’ve also thought about this problem, and I present my take, in the form of pretty pictures and cool colors. I hope, after 10 minutes of reading, you will have an intuitive understanding, all without bashing your head on a wall. If you haven’t been living under a rock for the past few years, you know that artificial intelligence is the next big thing. Cars are driving themselves, computers are beating humans at Go, and robots are taking our jobs. Soon, robots will overthrow the government and enslave our kids…

**Exploration in Reinforcement Learning**

Everyone is confronted with the same dilemma on a daily basis: should I keep doing what I do, or should I try something else. For example should I go to my preferred restaurant or should I try a new one, should I keep my current job or should I find a new one, etc… In Reinforcement Learning, this type of decision is called exploitation when you keep doing what you were doing, and exploration when you try something new. Naturally this raises a question about how much to exploit and how much to explore. However, there is a problem in exploration which is that we don’t really know what would be the outcome, it could be better than the current situation or it could be worse. Humans, try to get as much information as possible before making the move, for example before we try a new restaurant we read reviews or ask friends who already tried it. In Reinforcement Learning on the other hand, it is not possible to do that, but there are some techniques that will help figuring out the best strategy.

**Soft Computing**

In Rescue when Conventional Algorithms Fail. This article focuses on the basic differences between Soft Computing and Hard Computing.

**Google’s Eigenvector… or how a Random Surfer finds the most relevant Webpages**

Like most people you will have used a search engine lately, like Google. But have you ever thought about how it manages to give you the most fitting results? How does it order the results so that the best are on top? Read on to find out! The earliest search engines either had human curated indices, like Yahoo! or used some simple heuristic like the more often the keyword you were looking for was mentioned on a page the better, like Altavista – which led to all kinds of crazy effects like certain keywords being repeated thousands of times on webpages to make them more ‘relevant’. Now, most of those search engines are long gone because a new kid arrived on the block: Google! Google’s search engine results were much better than all of the competition and they became the dominant player in no time. How did they do that?

**Security Vulnerabilities of Neural Networks**

Neural networks are becoming more and more ubiquitous within the modern world, and they are often implemented without much consideration of their potential security flaws. This has led to a new field of cybersecurity which looks at the vulnerabilities of neural networks, and how we can protect them from being leveraged against us by hackers. Although this is a relatively serious matter, in this article, I will take a humorous approach to outline the vulnerabilities of neural networks which I hope the reader will find both fun and informative.

**Graph Theory and Deep Learning know-hows**

Machine learning is all the rage today, and once the science catches up with the hype, it will likely become a normality in our lives. One of the ways we are reaching for the next step is with a new form of deep learning; Geometric Deep Learning. Read about the inspiration and ideas here. The focus of this series is on how we can use Deep Learning on on graphs The two prerequisites needed to understand Graph Learning is in the name itself; Graph Theory and Deep Learning. This is all you need to know to understand the nature of, and build a high-level intuition for these two ideas.

**Building machine learning models in Apache Spark using SCALA in 6 steps**

When dealing with building machine learning models, Data scientists spend most of the time on 2 main tasks when building machine learning models:1. Pre-processing and CleaningThe major portion of time goes in to collecting, understanding, and analysing, cleaning the data and then building features. All the above steps mentioned are very important and critical to build successful machine learning model.2. IterationsThe optimization algorithms and finalizing the model accesses the data over multiple iterationsNow, when it comes to big data, we need a tool which can efficiently pre-process and iterate on large data sets and this is where spark is useful. As compared with only MapReduce, Spark uses MapReduce where the intermediate results can be passed directly to the next stage in the pipeline. Spark also has provision to cache the data for in memory operations. This makes spark well suited for machine learning algorithms.The next part of the article uses the data set from UCI data repository to build the machine learning model in Scala. I have assumed that the user has access to cluster where Scala and spark is already installed. Spark can also work on local system but I highly recommend applying and trying out these codes on a cluster to realize the true power Spark and Scala. I have used shell command everywhere.

**Estimating Non-linear Correlation in R**

In this post, we will learn about using a nonlinear correlation estimation function in R. We will also look at a few examples.

**Explainable AI’: who takes the decisions for us?**

In March 2016, AlphaGo, the software developed by Google to play Go, defeated the Korean champion Lee Sedol obtaining an historical result (it is the first time that a computer beats a 9-dan of the game of Go) and generating a lot of interest and echo among the practitioners in Artificial Intelligence (AI). A lot has already been said across the academic and popular press to explain the scale of the event, the GO game is notoriously ‘computationally complex’ to deal with for a computer, much more complex than chess, it is intractable with techniques that have been successful in the historical victories of Deep Blue (IBM) against the chess champion Kasparov in the late 90s. But for our purposes, it is the 37th move of the second match of AlphaGo against Lee Sedol to make the difference, it was the decisive move for the victory of the computer, a move then has been commented by GO champion Fan Hui: ‘It’s not a human move, I’ve never seen a man playing such a move’. Nobody has succeeded to understand or to explain the strategy behind this winning move, to give it a justification.

**Rendering elegant stock trading agents using Matplotlib and Gym**

We are going to be extending the code we wrote in the last tutorial to render an insightful visualization of the environment using Matplotlib. If you haven’t read my first article on Creating custom Gym environments from scratch, you should stop here and read that first. If you are unfamiliar with the matplotlib library, don’t worry. We will be going over every line so you can create your own custom visualizations of your gym environments. As always, the code for this tutorial will be available on my Github.

**Variational Autoencoder In Finance**

This article explores the use of a variational autoencoder to reduce the dimensions of financial time series with Keras and Python. We will further detect similarities between financial instruments in different markets and will use the results obtained to construct a custom index.

**Predicting the performance of deep learning models**

It’s widely acknowledged that the recent successes of Deep Learning rest heavily upon the availability of huge amounts of data. Vision was the first domain in which the promise of DL was realised, probably because of the availability of large datasets such as ImageNet. The recent surge of simulators for RL further illustrates that as we push further to apply these techniques to real-world problems, data scarcity quickly becomes the bottleneck.

**Simple way to deploy machine learning models to cloud**

The Machine learning world currently sees Data Scientists (DS) performing one or both of the following 2 prominent roles:1. Where a DS receives a data dump, applies some Machine learning algo on the data and reports back the results in the form of some presentation or report.2. Where the DS creates a usable piece of software for the stakeholders to consume the machine learning models.In this blog post, I’m attempting to display an example approach to the second aspect of a DS’s job i.e., creating some software that can be used by the stakeholders. Specifically, we would create a web-service that can be queried to obtain the predictions from a machine learning model. The post is mostly intended for machine learning practitioners who would like to go beyond only developing models.





### Like this:

Like Loading...


*Related*

