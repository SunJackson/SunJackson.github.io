---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://analytixon.com/2019/05/05/if-you-did-not-already-know-722/
date:      2019-05-05
author:      Michael Laux
tags:
    - models
    - modeling
    - teachers
    - learning
    - predictions
---

**JointGAN** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
A new generative adversarial network is developed for joint distribution matching. Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain. The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning. From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented. … 

**Random Energy Model** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
In statistical physics of disordered systems, the random energy model is a toy model of a system with quenched disorder. It concerns the statistics of a system of N particles, such that the number of possible states for the systems grow as {\displaystyle 2^N, while the energy of such states is a Gaussian stochastic variable. The model has an exact solution. Its simplicity makes this model suitable for pedagogical introduction of concepts like quenched disorder and replica symmetry.![](https://aboutdataanalytics.files.wordpress.com/2015/04/link.png?w=529)
 Random Energy Models, Optimal Learning Machines and Beyond … 

**Born-Again Network (BAN)** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student’s compactness. %we desire a compact model with performance close to the teacher’s. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction. … 





### Like this:

Like Loading...


*Related*

