---
layout:     post
catalog: true
title:      Big Data-4： Webserver log analysis with RDDs, Pyspark, SparkR and SparklyR
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/NxYQkVm0uWE/
date:      2019-05-11
author:      Tinniam V Ganesh
tags:
    - path status
    - lines
    - import
    - images
    - plot counts
---





*â€œThereâ€™s something so paradoxical about pi. On the one hand, it represents order, as embodied by the shape of a circle, long held to be a symbol of perfection and eternity. On the other hand, pi is unruly, disheveled in appearance, its digits obeying no obvious rule, or at least none that we can perceive. Pi is elusive and mysterious, forever beyond reach. Its mix of order and disorder is what makes it so bewitching. â€� *

From Infinite Powers by Steven Strogatz

Anybody who wants to be â€œanybodyâ€� in Big Data must necessarily be able to work on both large structured and unstructured data. Log analysis is critical in any enterprise which is usually unstructured. As I mentioned in my previous post Big Data: On RDDs, Dataframes,Hive QL with Pyspark and SparkR-Part 3 RDDs are typically used to handle unstructured data. Spark has the Dataframe abstraction over RDDs which performs better as it is optimized with the Catalyst optimization engine. Nevertheless, it is important to be able to process with RDDs. This post is a continuation of my 3 earlier posts on Big Data namely

1. Big Data-1: Move into the big league:Graduate from Python to Pyspark2. Big Data-2: Move into the big league:Graduate from R to SparkR3. Big Data: On RDDs, Dataframes,Hive QL with Pyspark and SparkR-Part 3

This post uses publicly available Webserver logs from NASA. The logs are for the months Jul 95 and Aug 95 and are a good place to start unstructured text analysis/log analysis. I highly recommend parsing these publicly available logs with regular expressions. It is only when you do that the truth of Jamie Zawinskiâ€™s pearl of wisdom

*â€œSome people, when confronted with a problem, think â€œI know, Iâ€™ll use regular expressions.â€� Now they have two problems.â€� â€“ *Jamie Zawinksi

hits home. I spent many hours struggling with regex!!

For this post for the RDD part, I had to refer to Dr. Fisseha Berhaneâ€™s blog post Webserver Log Analysis and for the Pyspark part, to the Univ. of California Specialization which I had done 3 years back Big Data Analysis with Apache Spark. Once I had played around with the regex for RDDs and PySpark I managed to get SparkR and SparklyR versions to work.

The notebooks used in this post have been published and are available at

1. logsAnalysiswithRDDs

1. logsAnalysiswithPyspark

1. logsAnalysiswithSparkRandSparklyR


You can also download all the notebooks from Github at WebServerLogsAnalysis

An essential and unavoidable aspect of Big Data processing is the need to process unstructured text.Web server logs are one such area which requires Big Data techniques to process massive amounts of logs. The Common Log Format also known as the NCSA Common log format, is a standardized text file format used by web servers when generating server log files. Because the format is standardized, the files can be readily analyzed.

A publicly available webserver logs is the NASA-HTTP Web server logs. This is good dataset with which we can play around to get familiar to handling web server logs. The logs can be accessed at NASA-HTTP

**Description** These two traces contain two monthâ€™s worth of all HTTP requests to the NASA Kennedy Space Center WWW server in Florida.

**Format** The logs are an ASCII file with one line per request, with the following columns:

-host making the request. A hostname when possible, otherwise the Internet address if the name could not be looked up.

-timestamp in the format â€œDAY MON DD HH:MM:SS YYYYâ€�, where DAY is the day of the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is the time of day using a 24-hour clock, and YYYY is the year. The timezone is -0400.

-request given in quotes.

-HTTP reply code.

-bytes in the reply.

## 1 Parse Web server logs with RDDs

### 1.1 Read NASA Web server logs

Read the logs files from NASA for the months Jul 95 and Aug 95

from pyspark import SparkContext, SparkConf

Check the logs to identify the parsing rules required for the logs

i=0

ix-stp-fl2-19.ix.netcom.com â€“ â€“ [03/Aug/1995:23:03:09 -0400] â€œGET /images/faq.gif HTTP/1.0â€� 200 263slip183-1.kw.jp.ibm.net â€“ â€“ [04/Aug/1995:18:42:17 -0400] â€œGET /shuttle/missions/sts-70/images/DSC-95EC-0001.gif HTTP/1.0â€� 200 107133piweba4y.prodigy.com â€“ â€“ [05/Aug/1995:19:17:41 -0400] â€œGET /icons/menu.xbm HTTP/1.0â€� 200 527ruperts.bt-sys.bt.co.uk â€“ â€“ [07/Aug/1995:04:44:10 -0400] â€œGET /shuttle/countdown/video/livevideo2.gif HTTP/1.0â€� 200 69067dal06-04.ppp.iadfw.net â€“ â€“ [07/Aug/1995:21:10:19 -0400] â€œGET /images/NASA-logosmall.gif HTTP/1.0â€� 200 786p15.ppp-1.directnet.com â€“ â€“ [10/Aug/1995:01:22:54 -0400] â€œGET /images/KSC-logosmall.gif HTTP/1.0â€� 200 1204

### 1.3 Write the parsing rule for each of the fields

- host

- timestamp

- path

- status

- content_bytes


### 1.21 Get IP address/host name

This regex is at the start of the log and includes any non-white characted

import re



Out[3]: [â€˜in24.inetnebr.comâ€™, â€˜uplherc.upl.comâ€™, â€˜uplherc.upl.comâ€™]



















1.22 Get timestamp
Get the time stamp

rslt=(rdd.map(lambda line: re.search(â€˜(\S+ -\d{4})â€™,line)

[(â€˜[01/Aug/1995:00:00:01 -0400â€™,),(â€˜[01/Aug/1995:00:00:07 -0400â€™,),(â€˜[01/Aug/1995:00:00:08 -0400â€™,)]


1.23 HTTP request


Get the HTTP request sent to Web server \w+ {GET}

# Get the REST call with â€� â€œ

[(â€˜/shuttle/missions/sts-68/news/sts-68-mcc-05.txtâ€™,),(â€˜/â€™,),(â€˜/images/ksclogo-medium.gifâ€™,)]

### 1.23Get HTTP response status

Get the HTTP response to the request

Out[6]: [(â€˜200â€™,), (â€˜304â€™,), (â€˜304â€™,)]

## 1.24 Get content size

Get the HTTP response in bytes

rslt=(rdd.map(lambda line: re.search(â€˜^.*\s(\d*)$â€™,line)

Out[7]: [(â€˜1839â€™,), (â€˜0â€™,), (â€˜0â€™,)]



















1.24 Putting it all together
Now put all the individual pieces together into 1 big regular expression and assign to the groups

1. Host 2. Timestamp 3. Path 4. Status 5. Content_size







[(â€˜in24.inetnebr.comâ€™,â€˜ -â€˜,â€˜ â€˜,â€˜-â€˜,â€˜[01/Aug/1995:00:00:01 -0400]â€™,â€˜â€�GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0â€³â€˜,â€˜/shuttle/missions/sts-68/news/sts-68-mcc-05.txtâ€™,â€˜200 1839â€™,â€˜1839â€™),(â€˜uplherc.upl.comâ€™,â€˜ -â€˜,â€˜ â€˜,â€˜-â€˜,â€˜[01/Aug/1995:00:00:07 -0400]â€™,â€˜â€�GET / HTTP/1.0â€³â€˜,â€˜/â€™,â€˜304 0â€™,â€˜0â€™),(â€˜uplherc.upl.comâ€™,â€˜ -â€˜,â€˜ â€˜,â€˜-â€˜,â€˜[01/Aug/1995:00:00:08 -0400]â€™,â€˜â€�GET /images/ksclogo-medium.gif HTTP/1.0â€³â€˜,â€˜/images/ksclogo-medium.gifâ€™,â€˜304 0â€™,â€˜0â€™)]

### 1.25 Add a log parsing function


1.26 Check for parsing failure


Check how many lines successfully parsed with the parsing function

Out of a total of 3461613 logs, 38768 failed to parseOut[10]:[(â€˜gw1.att.com â€“ â€“ [01/Aug/1995:00:03:53 -0400] â€œGET /shuttle/missions/sts-73/news HTTP/1.0â€� 302 -â€˜,0),(â€˜js002.cc.utsunomiya-u.ac.jp â€“ â€“ [01/Aug/1995:00:07:33 -0400] â€œGET /shuttle/resources/orbiters/discovery.gif HTTP/1.0â€� 404 -â€˜,0),(â€˜pipe1.nyc.pipeline.com â€“ â€“ [01/Aug/1995:00:12:37 -0400] â€œGET /history/apollo/apollo-13/apollo-13-patch-small.gifâ€� 200 12859â€™,0)]

### 1.26 The above rule is not enough to parse the logs

It can be seen that the single rule only parses part of the logs and we cannot group the regex separately. There is an error â€œAttributeError: â€˜NoneTypeâ€™ object has no attribute â€˜groupâ€™â€� which shows up

#rdd.map(lambda line: re.search(â€˜^(\S+)((\s)(-))+\s(\[\S+ -\d{4}\])\s(â€œ\w+\s+([^\s]+)\s+HTTP.*â€�)\s(\d{3}\s(\d*)$)â€™,line[0]).group()).take(4)

File â€œ/databricks/spark/python/pyspark/util.pyâ€�, line 99, in wrapperreturn f(*args, **kwargs)File â€œâ€�, line 1, in AttributeError: â€˜NoneTypeâ€™ object has no attribute â€˜groupâ€™

at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:490)

### 1.27 Add rule for parsing failed records

One of the issues with the earlier rule is the content_size has â€œ-â€� for some logs

import re

### 1.28 Parse records which fail

Parse the records that fails with the new rule

failed2=rdd.map(lambda line: parse_failed(line)).filter(lambda line: line[1]==1)

Out[13]:[(â€˜gw1.att.com â€“ â€“ [01/Aug/1995:00:03:53 -0400] â€œGET /shuttle/missions/sts-73/news HTTP/1.0â€� 302 -â€˜,1),(â€˜js002.cc.utsunomiya-u.ac.jp â€“ â€“ [01/Aug/1995:00:07:33 -0400] â€œGET /shuttle/resources/orbiters/discovery.gif HTTP/1.0â€� 404 -â€˜,1),(â€˜tia1.eskimo.com â€“ â€“ [01/Aug/1995:00:28:41 -0400] â€œGET /pub/winvn/release.txt HTTP/1.0â€� 404 -â€˜,1),(â€˜itws.info.eng.niigata-u.ac.jp â€“ â€“ [01/Aug/1995:00:38:01 -0400] â€œGET /ksc.html/facts/about_ksc.html HTTP/1.0â€� 403 -â€˜,1),(â€˜grimnet23.idirect.com â€“ â€“ [01/Aug/1995:00:50:12 -0400] â€œGET /www/software/winvn/winvn.html HTTP/1.0â€� 404 -â€˜,1)]






1.28 Add both rules


Add both rules for parsing the log.

**Note it can be shown that even with both rules all the logs are not parse.Further rules may need to be added**

import re


1.29 Group the different regex to groups for handling



1.30 Parse the logs and map the groups


parsed_rdd = rdd.map(lambda line: parse_log2(line)).filter(lambda line: line[1] == 1).map(lambda line : line[0])

parsed_rdd2 = parsed_rdd.map(lambda line: map2groups(line))

## 2. Parse Web server logs with Pyspark


2.1Read data into a Pyspark dataframe


import os

+â€”â€”â€”â€”â€”â€”â€”+â€”â€”â€”â€”â€”â€”â€”â€”â€“+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“+â€”â€”+â€”â€”â€”â€”+|host |timestamp |path |status|content_size|+â€”â€”â€”â€”â€”â€”â€”+â€”â€”â€”â€”â€”â€”â€”â€”â€“+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“+â€”â€”+â€”â€”â€”â€”+|199.72.81.55 |01/Jul/1995:00:00:01 -0400|/history/apollo/ |200 |6245 ||unicomp6.unicomp.net |01/Jul/1995:00:00:06 -0400|/shuttle/countdown/ |200 |3985 ||199.120.110.21 |01/Jul/1995:00:00:09 -0400|/shuttle/missions/sts-73/mission-sts-73.html |200 |4085 ||burger.letters.com |01/Jul/1995:00:00:11 -0400|/shuttle/countdown/liftoff.html |304 |0 ||199.120.110.21 |01/Jul/1995:00:00:11 -0400|/shuttle/missions/sts-73/sts-73-patch-small.gif|200 |4179 |+â€”â€”â€”â€”â€”â€”â€”+â€”â€”â€”â€”â€”â€”â€”â€”â€“+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“+â€”â€”+â€”â€”â€”â€”+only showing top 5 rows

bad_rows_df = split_df.filter(split_df[â€˜hostâ€™].isNull() |

### 2.3Check no of rows which do not have digits

We have already seen that the content_type field has â€˜-â€˜ instead of digits in RDDs

#bad_content_size_df = base_df.filter(~ base_df[â€˜valueâ€™].rlike(râ€™\d+$â€™))

### 2.4 Add â€˜*â€™ to identify bad rows

To identify the rows that are bad, concatenate â€˜*â€™ to the content_size field where the field does not have digits. It can be seen that the content_size has â€˜-â€˜ instead of a valid number

from pyspark.sql.functions import lit, concat

+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”+|concat(value, *) |+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”+|dd15-062.compuserve.com â€“ â€“ [01/Jul/1995:00:01:12 -0400] â€œGET /news/sci.space.shuttle/archive/sci-space-shuttle-22-apr-1995-40.txt HTTP/1.0â€� 404 -*||dynip42.efn.org â€“ â€“ [01/Jul/1995:00:02:14 -0400] â€œGET /software HTTP/1.0â€� 302 -* ||ix-or10-06.ix.netcom.com â€“ â€“ [01/Jul/1995:00:02:40 -0400] â€œGET /software/winvn HTTP/1.0â€� 302 -* ||ix-or10-06.ix.netcom.com â€“ â€“ [01/Jul/1995:00:03:24 -0400] â€œGET /software HTTP/1.0â€� 302 -* |+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”+

### 2.5 Fill NAs with 0s

# Replace all null content_size values with 0.

cleaned_df = split_df.na.fill({â€˜content_sizeâ€™: 0})

## 3. Webserver logs parsing with SparkR

library(SparkR)

1 199.72.81.55 â€“ â€“ [01/Jul/1995:00:00:01 -0400] â€œGET /history/apollo/ HTTP/1.0â€� 200 62452 unicomp6.unicomp.net â€“ â€“ [01/Jul/1995:00:00:06 -0400] â€œGET /shuttle/countdown/ HTTP/1.0â€� 200 39853 199.120.110.21 â€“ â€“ [01/Jul/1995:00:00:09 -0400] â€œGET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0â€� 200 40854 burger.letters.com â€“ â€“ [01/Jul/1995:00:00:11 -0400] â€œGET /shuttle/countdown/liftoff.html HTTP/1.0â€� 304 05 199.120.110.21 â€“ â€“ [01/Jul/1995:00:00:11 -0400] â€œGET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0â€� 200 41796 burger.letters.com â€“ â€“ [01/Jul/1995:00:00:12 -0400] â€œGET /images/NASA-logosmall.gif HTTP/1.0â€� 304 07 burger.letters.com â€“ â€“ [01/Jul/1995:00:00:12 -0400] â€œGET /shuttle/countdown/video/livevideo.gif HTTP/1.0â€� 200 08 205.212.115.106 â€“ â€“ [01/Jul/1995:00:00:12 -0400] â€œGET /shuttle/countdown/countdown.html HTTP/1.0â€� 200 39859 d104.aa.net â€“ â€“ [01/Jul/1995:00:00:13 -0400] â€œGET /shuttle/countdown/ HTTP/1.0â€� 200 398510 129.94.144.152 â€“ â€“ [01/Jul/1995:00:00:13 -0400] â€œGET / HTTP/1.0â€� 200 7074host timestamp1 199.72.81.55 [01/Jul/1995:00:00:01 -04002 unicomp6.unicomp.net [01/Jul/1995:00:00:06 -04003 199.120.110.21 [01/Jul/1995:00:00:09 -04004 burger.letters.com [01/Jul/1995:00:00:11 -04005 199.120.110.21 [01/Jul/1995:00:00:11 -04006 burger.letters.com [01/Jul/1995:00:00:12 -04007 burger.letters.com [01/Jul/1995:00:00:12 -04008 205.212.115.106 [01/Jul/1995:00:00:12 -04009 d104.aa.net [01/Jul/1995:00:00:13 -040010 129.94.144.152 [01/Jul/1995:00:00:13 -0400path status content_size1 /history/apollo/ 200 62452 /shuttle/countdown/ 200 39853 /shuttle/missions/sts-73/mission-sts-73.html 200 40854 /shuttle/countdown/liftoff.html 304 05 /shuttle/missions/sts-73/sts-73-patch-small.gif 200 41796 /images/NASA-logosmall.gif 304 07 /shuttle/countdown/video/livevideo.gif 200 08 /shuttle/countdown/countdown.html 200 39859 /shuttle/countdown/ 200 398510 / 200 7074

## 4 Webserver logs parsing with SparklyR

Installing package into â€˜/databricks/spark/R/libâ€™

#install.packages(â€œsparklyrâ€�)

## 5 Hosts

### 5.1 RDD


5.11 Parse and map to hosts to groups


parsed_rdd = rdd.map(lambda line: parse_log2(line)).filter(lambda line: line[1] == 1).map(lambda line : line[0])


5.12Plot counts of hosts


import pandas as pd import matplotlib.pyplot as plt df=pd.DataFrame(rslt,columns=[â€˜hostâ€™,â€˜countâ€™]) sns.barplot(x=â€˜hostâ€™,y=â€˜countâ€™,data=df) plt.subplots_adjust(bottom=0.6, right=0.8, top=0.9) plt.xticks(rotation=â€œverticalâ€�,fontsize=8) display()![](https://gigadom.files.wordpress.com/2019/05/rdd-1.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/rdd-1.png?w=456)


### 5.2 PySpark


5.21 Compute counts of hosts


df= (cleaned_df

+â€”â€”â€”â€”â€”â€”â€“+â€”â€“+| host|count|+â€”â€”â€”â€”â€”â€”â€“+â€”â€“+|piweba3y.prodigyâ€¦.|21988||piweba4y.prodigyâ€¦.|16437||piweba1y.prodigyâ€¦.|12825|| edams.ksc.nasa.gov |11964|| 163.206.89.4 | 9697|+â€”â€”â€”â€”â€”â€”â€“+â€”â€“+only showing top 5 rows






5.22 Plot count of hosts


import matplotlib.pyplot as plt

![](https://gigadom.files.wordpress.com/2019/05/pyspark-1.png?w=456)


### 5.3 SparkR


5.31 Compute count of hosts


c <- SparkR::select(a,a$host)

 host noHosts

### 5.32 Plot count of hosts

![](https://gigadom.files.wordpress.com/2019/05/sparkr-1.png?w=456)


### 5.4 SparklyR


5.41 Compute count of Hosts


df <- sdf %>% select(host,timestamp,path,status,content_size)

### 5.42 Plot count of hosts

p <-ggplot(data=df2, aes(x=host, y=noHosts,fill=host)) + geom_bar(stat=â€œidentityâ€�)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(â€˜Hostâ€™) + ylab(â€˜Countâ€™)

p

![](https://gigadom.files.wordpress.com/2019/05/sparklyr-1.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/sparklyr-1.png?w=456)


## 6 Paths

#### 6.1 RDD


6.11 Parse and map to hosts to groups


parsed_rdd = rdd.map(lambda line: parse_log2(line)).filter(lambda line: line[1] == 1).map(lambda line : line[0])

![](https://i0.wp.com/s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png?w=456&ssl=1)




[(â€˜â€�GET /images/NASA-logosmall.gif HTTP/1.0â€³â€˜, 207520),(â€˜â€�GET /images/KSC-logosmall.gif HTTP/1.0â€³â€˜, 164487),(â€˜â€�GET /images/MOSAIC-logosmall.gif HTTP/1.0â€³â€˜, 126933),(â€˜â€�GET /images/USA-logosmall.gif HTTP/1.0â€³â€˜, 126108),(â€˜â€�GET /images/WORLD-logosmall.gif HTTP/1.0â€³â€˜, 124972),(â€˜â€�GET /images/ksclogo-medium.gif HTTP/1.0â€³â€˜, 120704),(â€˜â€�GET /ksc.html HTTP/1.0â€³â€˜, 83209),(â€˜â€�GET /images/launch-logo.gif HTTP/1.0â€³â€˜, 75839),(â€˜â€�GET /history/apollo/images/apollo-logo1.gif HTTP/1.0â€³â€˜, 68759),(â€˜â€�GET /shuttle/countdown/ HTTP/1.0â€³â€˜, 64467)]

#### 6.12 Plot counts of HTTP Requests

df=pd.DataFrame(rslt,columns=[â€˜pathâ€™,â€˜countâ€™]) sns.barplot(x=â€˜pathâ€™,y=â€˜countâ€™,data=df) plt.subplots_adjust(bottom=0.7, right=0.8, top=0.9) plt.xticks(rotation=â€œverticalâ€�,fontsize=8)

display()

![](https://gigadom.files.wordpress.com/2019/05/rdd-2.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/rdd-2.png?w=456)


### 6.2 Pyspark

#### 6.21 Compute count of HTTP Requests



Out[20]:+â€”â€”â€”â€”â€”â€”â€“+â€”â€”+| path| count|+â€”â€”â€”â€”â€”â€”â€“+â€”â€”+|/images/NASA-logoâ€¦|208362||/images/KSC-logosâ€¦|164813||/images/MOSAIC-loâ€¦|127656||/images/USA-logosâ€¦|126820||/images/WORLD-logâ€¦|125676|+â€”â€”â€”â€”â€”â€”â€“+â€”â€”+only showing top 5 rows


6.22 Plot count of HTTP Requests


import matplotlib.pyplot as plt

import pandas as pd import seaborn as sns df1=df.toPandas() df2 = df1.head(10) df2.count() sns.barplot(x=â€˜pathâ€™,y=â€˜countâ€™,data=df2)

plt.subplots_adjust(bottom=0.7, right=0.8, top=0.9) plt.xlabel(â€œHTTP Requestsâ€�) plt.ylabel(â€˜Countâ€™) plt.xticks(rotation=90,fontsize=8)

display()

![](https://gigadom.files.wordpress.com/2019/05/pyspark-2.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/pyspark-2.png?w=456)


 

### 6.3 SparkR

#### 6.31Compute count of HTTP requests


3.14 Plot count of HTTP Requests


![](https://gigadom.files.wordpress.com/2019/05/sparkr-2.png?w=456)


### 6.4 SparklyR

#### 6.41 Compute count of paths

# Source: spark [?? x 2]

#### 6.42 Plot count of Paths

![](https://gigadom.files.wordpress.com/2019/05/sparkly-2.png?w=456)


### 7.1 RDD

#### 7.11 Compute count of HTTP Status

parsed_rdd = rdd.map(lambda line: parse_log2(line)).filter(lambda line: line[1] == 1).map(lambda line : line[0])

![](https://i0.wp.com/s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png?w=456&ssl=1)


Out[22]:

[(â€˜200â€™, 3095682),(â€˜304â€™, 266764),(â€˜302â€™, 72970),(â€˜404â€™, 20625),(â€˜403â€™, 225),(â€˜500â€™, 65),(â€˜501â€™, 41)]






1.37 Plot counts of HTTP response statusâ€™


df=pd.DataFrame(rslt,columns=[â€˜statusâ€™,â€˜countâ€™]) sns.barplot(x=â€˜statusâ€™,y=â€˜countâ€™,data=df) plt.subplots_adjust(bottom=0.4, right=0.8, top=0.9) plt.xticks(rotation=â€œverticalâ€�,fontsize=8)

display()

![](https://gigadom.files.wordpress.com/2019/05/rdd-3.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/rdd-3.png?w=456)


### 7.2 Pyspark


7.21 Compute count of HTTP status


status_count=(cleaned_df

+â€”â€”+â€”â€”-+|status| count|+â€”â€”+â€”â€”-+| 200|3100522|| 304| 266773|| 302| 73070|| 404| 20901|| 403| 225|| 500| 65|| 501| 41|| 400| 15|| null| 1|

### 7.22 Plot count of HTTP status

Plot the HTTP return status vs the counts

df1=status_count.toPandas()

df2 = df1.head(10) df2.count() sns.barplot(x=â€˜statusâ€™,y=â€˜countâ€™,data=df2) plt.subplots_adjust(bottom=0.5, right=0.8, top=0.9) plt.xlabel(â€œHTTP Statusâ€�) plt.ylabel(â€˜Countâ€™) plt.xticks(rotation=â€œverticalâ€�,fontsize=10) display()

![](https://gigadom.files.wordpress.com/2019/05/pyspark-3.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/pyspark-3.png?w=456)


### 7.3 SparkR


7.31 Compute count of HTTP Response status


library(SparkR)


3.16 Plot count of HTTP Response status


library(ggplot2)

![](https://gigadom.files.wordpress.com/2019/05/sparkr-3.png?w=456)


#### 7.4 SparklyR

#### 7.41 Compute count of status

# Source: spark [?? x 2]

#### 7.42 Plot count of status

p <-ggplot(data=df2, aes(x=status, y=noStatus,fill=status)) + geom_bar(stat=â€œidentityâ€�)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(â€˜Statusâ€™) + ylab(â€˜Countâ€™) p

![](https://gigadom.files.wordpress.com/2019/05/sparkly-3.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/sparkly-3.png?w=456)


### 8.1 RDD

#### 8.12 Compute count of content size

df=pd.DataFrame(rslt,columns=[â€˜content_sizeâ€™,â€˜countâ€™]) sns.barplot(x=â€˜content_sizeâ€™,y=â€˜countâ€™,data=df) plt.subplots_adjust(bottom=0.4, right=0.8, top=0.9) plt.xticks(rotation=â€œverticalâ€�,fontsize=8) display()

![](https://gigadom.files.wordpress.com/2019/05/rdd-4.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/rdd-4.png?w=456)


### 8.2 Pyspark


8.21 Compute count of content_size


size_counts=(cleaned_df

#### 8.22 Plot counts of content size

Plot the path access versus the counts

df1=size_counts.toPandas()

df2 = df1.head(10) df2.count() sns.barplot(x=â€˜content_sizeâ€™,y=â€˜countâ€™,data=df2) plt.subplots_adjust(bottom=0.5, right=0.8, top=0.9) plt.xlabel(â€œcontent_sizeâ€�) plt.ylabel(â€˜Countâ€™) plt.xticks(rotation=â€œverticalâ€�,fontsize=10) display()

![](https://gigadom.files.wordpress.com/2019/05/pyspark-4.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/pyspark-4.png?w=456)


### 8.3 SparkR

### 8.31 Compute count of content size

 content_size numContentSize

8.32 Plot count of content sizes

p <-ggplot(data=df1, aes(x=content_size, y=numContentSize,fill=content_size)) + geom_bar(stat=â€œidentityâ€�) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(â€˜Content Sizeâ€™) + ylab(â€˜Countâ€™)

p

![](https://gigadom.files.wordpress.com/2019/05/sparkr-4.png?w=456)
![](https://gigadom.files.wordpress.com/2019/05/sparkr-4.png?w=456)


### 8.4 SparklyR


8.41Compute count of content_size


df <- sdf %>% select(host,timestamp,path,status,content_size)

# Source: spark [?? x 2]

#### 8.42 Plot count of content_size


*Related*








---
