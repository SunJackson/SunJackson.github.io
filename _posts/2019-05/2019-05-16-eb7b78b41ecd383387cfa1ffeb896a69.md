---
layout:     post
catalog: true
title:      Large-Scale Evolution of Image Classifiers
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/rbXrHM13uuc/large-scale-evolution-image-classifiers.html
date:      2019-05-16
author:      Matt Mayo Editor
tags:
    - mashayekhi
    - morteza
    - edited
    - proposed method
    - matching shapes
---

**By Morteza Mashayekhi (Edited by Susan Chang & Serena McDonnell)**

Deep neural networks excel in many difficult tasks, given large amounts of training data and enough processing power. The neural network architecture is an important factor in achieving a highly accurate model, and normally state-of-the-art results are obtained by building up on many years of effort, by hundreds of researchers (see examples in Szegedy et al., 2015, (He et al., 2016)[https://arxiv.org/abs/1512.03385] for more details). Techniques to automatically discover these neural network architectures are, therefore, very much desirable. As part of the AutoML project at Google, this paper was published in ICML 2017 (Real et al., 2017), with the aim of discovering deep neural network (DNN) architectures without any human participation. The proposed method is simple and is able to generate a fully trained network requiring no post-processing. While previous work in Morse et al. 2016, He et al. 2016 focused on small neural networks, these small networks are not scalable, as it’s deep neural networks that are needed for achieving state-of-the-art in image classification.

 

### Overall approach

 The heart of the proposed method is an evolutionary algorithm (EA). An EA is a population-based optimization algorithm that, inspired by biological evolution, evolves a population of candidate individuals, by repeatedly performing operations such as reproduction, mutation, recombination (crossing-over), and selection. A fitness function determines the quality of each individual. The authors employ a modified version of the EA, described above such that a population of simple neural network architectures and their hyper-parameters are evolved while the weights are adjusted using back propagation.

 

### Initial conditions

 Each evolutionary experiment begins with a population of simple models, consisting of a single-layer with no convolutions and a learning rate of 0.1. This poor initial condition forces evolution to make the discoveries itself, rather than allowing the experimenter to “rig” the experiment by hand picking initial conditions.

 

### Evolution process

 Next, the models are evolved, using only mutation operations. The authors define a set of 11 types of mutation operations, all of which are designed to be very similar to what a human network designer does to improve an architecture.

The operators include: altering the learning rate, “identity” (keep training), weight reset, insert/remove convolutions, add/remove skip connections, insert a one-to-one/identity connection, and alterations of stride size, number of channels, and filter size.

While the authors explored the use of several recombination operations, they found that these recombinations did not improve their results. The fitness of each individual is calculated as the accuracy of the discovered neural network on a validation set.

Each individual architecture plus a learning rate form the individual’s DNA, which is implemented in TensorFlow. The vertices in this graph represent activations, such as ReLU and Linear functions. The edges represent either an identity function or a convolution layer containing mutable numerical parameters which define the convolution operation. During evolution, when multiple edges are incident on a vertex, they each must be of the same size and have the same number of channels for their activations. If not, this inconsistency is resolved by choosing one incoming edge to be the primary input, reshaping the rest using zeroth-order interpolation for the size, and truncating/padding for the number of channels. Mutation on a numerical parameter chooses the new value at random around the existing value. This makes it possible for the evolutionary process to choose any number within this range that gives better accuracy.

At each step of the evolutionary process, two individuals are randomly selected from the population by a computer, referred to as a worker. The individual with lower fitness is killed right away. A new child (a new DNN architecture) is created by applying a mutation operation (randomly selected from the predefined set of mutations) to the surviving individual. The child is trained, its fitness value is computed, and becomes “alive” as it’s added back to the population. If a child’s layer has matching shapes with that of its parent, the weights are preserved. This helps to retain some of the learned weights during the evolutionary process instead of learning them from the scratch. This process is repeated for a long time (about 12 days based on the experiments).
![](https://aisc.a-i.science/static/post-assets/large-scale-evolution-of-image-classifiers/figure1.png)

