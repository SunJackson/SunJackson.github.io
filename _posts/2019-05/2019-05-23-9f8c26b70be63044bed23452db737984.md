---
layout:     post
catalog: true
title:      Comparing Frequentist, Bayesian and Simulation methods and conclusions
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/MKd5qbIA1nY/
date:      2019-05-23
author:      Posts on R Lover ! a programmer
tags:
    - male_
    - timings
    - testing
    - tested
    - data
---





So, a programmer, a frequentist, and a bayesian walk into a bar. No this postisn’t really on the path to some politically incorrect stereotypical humor. Juttrying to make it fun and catch your attention. As the title implies this postis really about applying the differing viewpoints and methodologies inherent inthose approaches to statistics. To be honest I’m not even going to spend a lottime on the details of methods, I want to focus on the conclusions each of thesepeople would draw after analyzing the same data using their favored methods.This post isn’t really so much about how they would proceed but more about whatthey would conclude at the end of the analysis. I make no claim about which ofthese fine people have the best way of doing things although I was raised afrequentist, I am more and more enamored of bayesian methods and while my taglineis accurate “R Lover !a programmer” I will admit a love for making computers dothe hard work for me so if that involves a **little** programming, I’m all forit.

### Background

Last week I saw a nice post from AnindyaMozumdaron the R Bloggers feed. The topic material wasfun for me (analyzing the performance of male 100m sprinters and the fastestman on earth), as well as exploring the concepts in Allen B. Downey’s book*Think Stats: Probability and Statistics forProgrammers* , which is atmy very favorite price point, free download, and Creative Commons licensed. Thepost got me interested in following up a bit and thinking things through “outloud” as a blog post.

**This post is predicated on your having readhttps://www.radmuzom.com/2019/05/18/fun-with-statistics-is-usain-bolt-really-the-fastest-man-on-earth/.Please do. I’ll wait until you come back.**

Welcome back. I’m going to try and repeat as little as possible from that blogpost just make comparisons. So to continue our little teaser… So, aprogrammer, a frequentist, and a bayesian walk into a bar and start arguingabout whether Usain Bolt really is the fastest man on earth. The programmer hastold us how they would go about answering the question. The answer was:

> 
There is only a 1.85% chance of seeing a difference as large as the observeddifference if there is actually no difference between the (median) timings ofUsain Bolt and Asafa Powell.


and was derived by counting observations across 10,000 simulations of the datausing the `infer` package and looking at differences between median timings. Ournull hypothesis was there is no “real” difference difference between Bolt andPowell even though our data has a median for Bolt of 9.90 and median for Powellof 9.95. That is after all a very small difference. But our simulation allows usto reject that null hypothesis and favor the alternative that the difference isreal.

Should we be confident that we are 100% – 1.85% = 98% likely to be correct? NO!as Downey notes:

> 
For most problems, we only care about the order of magnitude: if the p-valueis smaller that 1/100, the effect is likely to be real; if it is greater than1/10, probably not. If you think there is a difference between a 4.8%(significant!) and 5.2% (not significant!), you are taking it too seriously.


Can we say that Bolt will win a race with Powell 98% of time? Again a resoundingNO! We’re 98% certain that the *“true”* difference in their medians is .05seconds? NOPE.

### Back to the future

Okay we’ve heard the programmer’s story at our little local bar. It’s time tolet our frequentist have their moment in the limelight. Technically the bestterm would be Neyman-Pearson Frequentist but we’re not going to stand onformality. It is nearly a century old and stands as an “improvement” on Fisher’ssignificance testing. A nice little summary here onWikipedia.

I’m not here to belabor the nuances but frequentist methods are among theoldest and arguably the most prevalent method in many fields. They are often thefirst method people learned in college and sometimes the only method. They stilldrive most of the published research in many fields although other methods aretaking root.

Before the frequentist can tell their tale though let’s make sure they have thesame data as in the earlier post. Let’s load all the libraries we’re going touse and very quickly reproduce the process Anindya Mozumdar went through toscrape and load the data. We’ll have a `tibble` named `male_100` that containsthe requisite data and we’ll confirm that the summary for the top 6 runners meanand median are identical. Note that I am suppressing messages as the librariesload since R 3.6.0 has gotten quite chatty in this regard.

```
library(rvest)
library(readr)
library(dplyr)
library(ggplot2)
library(ggstatsplot)
library(BayesFactor)

male_100_html <- read_html("http://www.alltime-athletics.com/m_100ok.htm")
male_100_pres <- male_100_html %>%
 html_nodes(xpath = "//pre")
male_100_htext <- male_100_pres %>%
 html_text()
male_100_htext <- male_100_htext[[1]]

male_100 <- readr::read_fwf(male_100_htext, skip = 1, n_max = 3178,
 col_types = cols(.default = col_character()),
 col_positions = fwf_positions(
 c(1, 16, 27, 35, 66, 74, 86, 93, 123),
 c(15, 26, 34, 65, 73, 85, 92, 122, 132)
 ))

male_100 <- male_100 %>%
 select(X2, X4) %>% 
 transmute(timing = X2, runner = X4) %>%
 mutate(timing = gsub("A", "", timing),
 timing = as.numeric(timing)) %>%
 filter(runner %in% c("Usain Bolt", "Asafa Powell", "Yohan Blake",
 "Justin Gatlin", "Maurice Greene", "Tyson Gay")) %>%
 mutate_if(is.character, as.factor) %>%
 droplevels
male_100
```

```
## # A tibble: 520 x 2
## timing runner 
## 
## 1 9.58 Usain Bolt 
## 2 9.63 Usain Bolt 
## 3 9.69 Usain Bolt 
## 4 9.69 Tyson Gay 
## 5 9.69 Yohan Blake 
## 6 9.71 Tyson Gay 
## 7 9.72 Usain Bolt 
## 8 9.72 Asafa Powell 
## 9 9.74 Asafa Powell 
## 10 9.74 Justin Gatlin
## # … with 510 more rows
```

```
male_100$runner <- forcats::fct_reorder(male_100$runner, male_100$timing)

male_100 %>%
 group_by(runner) %>%
 summarise(mean_timing = mean(timing)) %>%
 arrange(mean_timing)
```

```
## # A tibble: 6 x 2
## runner mean_timing
## 
## 1 Usain Bolt 9.90
## 2 Asafa Powell 9.94
## 3 Tyson Gay 9.95
## 4 Justin Gatlin 9.96
## 5 Yohan Blake 9.96
## 6 Maurice Greene 9.97
```

```
male_100 %>%
 group_by(runner) %>%
 summarise(median_timing = median(timing)) %>%
 arrange(median_timing)
```

```
## # A tibble: 6 x 2
## runner median_timing
## 
## 1 Usain Bolt 9.9 
## 2 Asafa Powell 9.95
## 3 Yohan Blake 9.96
## 4 Justin Gatlin 9.97
## 5 Maurice Greene 9.97
## 6 Tyson Gay 9.97
```

Most of the code above is simply shortened from the original post. The onlything that is completely new is `forcats::fct_reorder(male_100$runner, male_100$timing)` which takes the `runner` factor and reorders it according tothe median by runner. This doesn’t matter for the calculations we’ll do but itwill make the plots look nicer.

### Testing, testing!

One of the issues with a frequentist approach compared to a programmers approachis that there are a lot of different tests you could choose. But in this casewearing my frequentist hat there really are only two choices. A Oneway ANOVA orthe Kruskall Wallis which uses ranks and eliminates some assumptions.

This also gives me a chance to talk about a great package that supports bothfrequentists and bayesian methods and completely integrates visualizing yourdata with analyzing your data, which IMHO is the only way to go. The package is`ggstatsplot`. Full disclosureI’m a minor contributor to the package but please know that the true hero of thepackage is Indrajeet Patil. It’s stable,mature, well tested and well maintained try it out.

So let’s assume we want to run a classic Oneway ANOVA first (Welch’s method sowe don’t have to assume equal variances across groups). Assuming that theomnibuds F test is significant lets say we’d like to look at the pairwisecomparisons and adjust the p values for multiple comparison using Holm. We’re abig fan of visualizing the data by runner and of course we’d like to plot thingslike the mean and median and the number of races per runner. We’d of course liketo know our effect size we’ll stick with eta squared we’d like it as elegant aspossible.

Doing this analysis using frequentist methods in `R` is not difficult. HeckI’ve even blogged about it myself it’s so“easy”. The benefit of `ggbetweenstats` from `ggstatsplot` is that it prettymuch allows you to do just about everything in one command. Seamlessly mixingthe plot and the results into one output. We’re only going to scratch thesurface of all the customization possibilities.

```
ggbetweenstats(data = male_100, 
 x = runner, 
 y = timing,
 type = "p",
 var.equal = FALSE,
 pairwise.comparisons = TRUE,
 partial = FALSE,
 effsize.type = "biased",
 point.jitter.height = 0, 
 title = "Parametric (Mean) testing assuming unequal variances",
 ggplot.component = ggplot2::scale_y_continuous(breaks = seq(9.6, 10.4, .2), 
 limits = (c(9.6,10.4))),
 messages = FALSE
 )
```

![](https://ibecav.netlify.com/post/2019-05-23-comparing-frequentist-bayesian-and-simulation-methods-and-conclusions_files/figure-html/frequentist1-1.png)
![](https://ibecav.netlify.com/post/2019-05-23-comparing-frequentist-bayesian-and-simulation-methods-and-conclusions_files/figure-html/frequentist1-1.png)


Our conclusion is similar to that drawn by simulation. We can clearly reject thenull that all these runners have the same mean time. Using Games-Howell andcontrolling for multiple comparisons with Holm, however, we can only showsupport for the difference between Usain Bolt and Maurice Green. There isinsufficient evidence to reject the null for all the other possible pairings.(You can actually tell `ggbetweenstats` to show the p value for all the pairingsbut that gets cluttered quickly).

From a frequentist perspective there are a whole set of non-parametric teststhat are available for use. They typically make fewer assumptions about the datawe have and often operate by exchanging the ranks of the outcome variable(`timing`) rather than using the number.

The only thing we need to change in our input to the function is `type = "np"` and the `title`.

```
ggbetweenstats(data = male_100, 
 x = runner, 
 y = timing,
 type = "np",
 var.equal = FALSE,
 pairwise.comparisons = TRUE,
 partial = FALSE,
 effsize.type = "biased",
 point.jitter.height = 0, 
 title = "Non-Parametric (Rank) testing",
 ggplot.component = ggplot2::scale_y_continuous(breaks = seq(9.6, 10.4, .2), 
 limits = (c(9.6,10.4))),
 messages = FALSE
 )
```

![](https://ibecav.netlify.com/post/2019-05-23-comparing-frequentist-bayesian-and-simulation-methods-and-conclusions_files/figure-html/frequentist2-1.png)
![](https://ibecav.netlify.com/post/2019-05-23-comparing-frequentist-bayesian-and-simulation-methods-and-conclusions_files/figure-html/frequentist2-1.png)


Without getting overly consumed by the exact numbers note the very similarresults for the overall test, but that we now also are more confident aboutwhether the difference between Usain Bolt and Justin Gaitlin. I highlight thatbecause there is a common misconception that non-parametric tests are always lesspowerful (sensitive) than their parametric cousins.

### Asking the question differently (see Learning Statistics with R )

> 
Much of the credit for this section goes to Danielle Navarro (bookdown translation: Emily Kothe) in Learning Statistics with R


It usually takes several lessons or even an entire semester to teach thefrequentist method, because null hypothesis testing is a very elaboratecontraption that people (well in my experience very smart undergraduatestudents) find very hard to master. In contrast, the Bayesian approach tohypothesis testing *“feels”* far more intuitive. Let’s apply it to our currentscenario.

We’re at the bar the three of us wondering whether Usain Bolt is really thefastest or whether all these individual data points really are just a randommosaic of data noise. Both the programmer and the frequentist set the testing upconceptually the same way. Can we use the data to reject the null that all therunners are the same. Convinced they’re not all the same they applied the samegeneral procedure to reject (or not) the hypothesis that any pair was the samefor example Bolt versus Powell (for the record I’m not related to either). Theydiffer in computational methods and assumptions but not in overarching method.

At the end of their machinations they have no ability to talk about how likely(probable) it is that runner 1 will beat runner 2. Often times that’s exactlywhat you really want to know. There are two hypotheses that we want to compare,a null hypothesis h0 that all the runners run equally fast and an alternativehypothesis h1 that they don’t. Prior to looking at the data while we’resitting at the bar we have no real strong belief about which hypothesis is true(odds are 1:1 in our naive state). We have our data and we want it to inform ourthinking. Unlike frequentist statistics, Bayesian statistics allow us to talkabout the probability that the null hypothesis is true (which is a complete **nono** in a frequentist context). Better yet, it allows us to calculate theposterior probability of the null hypothesis, using Bayes’ rule and our data.

In practice, most Bayesian data analysts tend not to talk in terms of the rawposterior probabilities. Instead, we/they tend to talk in terms of the posteriorodds ratio. Think of it like betting. Suppose, for instance, the posteriorprobability of the null hypothesis is 25%, and the posterior probability of thealternative is 75%. The alternative hypothesis **h1** is three times as probable as thenull **h0**, so we say that the odds are 3:1 in favor of the alternative.

At the end of the Bayesian’s efforts they can make what feel like very naturalstatements of interest, for example, “The evidence provided by our datacorresponds to odds of 42:1 that these runners are not all equally fast.

Let’s try it using ggbetweenstats…

```
ggbetweenstats(data = male_100, 
 x = runner, 
 y = timing,
 type = "bf",
 var.equal = FALSE,
 pairwise.comparisons = TRUE,
 partial = FALSE,
 effsize.type = "biased",
 point.jitter.height = 0, 
 title = "Bayesian testing",
 messages = FALSE
 )
```

![](https://ibecav.netlify.com/post/2019-05-23-comparing-frequentist-bayesian-and-simulation-methods-and-conclusions_files/figure-html/bayes1-1.png)
![](https://ibecav.netlify.com/post/2019-05-23-comparing-frequentist-bayesian-and-simulation-methods-and-conclusions_files/figure-html/bayes1-1.png)


Yikes! Not what I wanted to see in the bar. The pairwise comparisons have goneaway (we’ll get them back) and worse yet what the heck does loge(BF10) = 2.9mean? I hate log conversions I was promised a real number like 42:1! Who’sCauchy why is he there at .0.707?

Let’s break this down. loge(BF10) = 2.9 is also `exp(2.9)` or about 18 sothe good news is the odds are better than 18:1 that the runners are not equallyfast. Since rounding no doubt loses some accuracy lets use the `BayesFactor`package directly and get a more accurate answer before we round `anovaBF(timing ~ runner, data = as.data.frame(male_100), rscaleFixed = .707)` is what we wantwhere `rscaleFixed = .707` ensures we have the right Cauchy value.

```
anovaBF(timing ~ runner, data = male_100, rscaleFixed = .707)
```

```
## Bayes factor analysis
## --------------
## [1] runner : 19.04071 ±0.01%
## 
## Against denominator:
## Intercept only 
## ---
## Bayes factor type: BFlinearModel, JZS
```

Okay that’s better so to Bayesian thinking the odds are 19:1 against the fact that they all run about the same speed, or 19:1 they run at different speeds.

Hmmm. One of the strengths/weaknesses of the Bayesian approach is that peoplecan have their own sense of how strong 19:1 is. I like those odds. One of thereally nice things about the Bayes factor is the numbers are inherentlymeaningful. If you run the data and you compute a Bayes factor of 4, it meansthat the evidence provided by your data corresponds to betting odds of 4:1 infavor of the alternative. However, there have been some attempts to quantify thestandards of evidence that would be considered meaningful in a scientificcontext. One that is widely used is from Kass and Raftery (1995). (**N.B. there are others and I have deliberately selected one of the most conservative standards. See for example https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12888-018-1761-4/MediaObjects/12888_2018_1761_Fig1_HTML.png**)

|------
|1 – 3|Negligible evidence|
|3 – 20|Positive evidence|
|20 -150|Strong evidence|
|>150|Very strong evidence|

Okay we have “positive evidence” and we can quantify it, that’s good. But whatabout all the pairwise comparisons? Can we take this down to all theindividual pairings? I’m on the edge of my bar stool here. What are the oddsBolt really is faster than Powell? Can we quantify that without somehow breakingthe multiple comparisons rule?

The short answer is yes we can safely extend this methodology to incorporatepairwise comparisons. We shouldn’t abuse the method and we should fit our modelwith the best possible prior information but in general, as simulatedhere,

> 
With Bayesian inference (and the correct prior), though, this problemdisappears. Amazingly enough, you don’t have to correct Bayesian inferences formultiple comparisons.


With that in mind let’s build a quick little function that will allow us to passa data source and two names and run a Bayesian t-test via `BayesFactor::ttestBF`to compare two runners. `ttestBF` returns a lot of info in a custom object sowe’ll use the `extractBF` function to grab it in a format where we can pluck outthe actual BF10

```
compare_runners_bf <- function(df, runner1, runner2) {
 
 ds <- df %>%
 filter(runner %in% c(runner1, runner2)) %>%
 droplevels %>% 
 as.data.frame
 zzz <- ttestBF(formula = timing ~ runner, data = ds)
 yyy <- extractBF(zzz)
 xxx <- paste0("The evidence provided by the data corresponds to odds of ", 
 round(yyy$bf,0), 
 ":1 that ", 
 runner1, 
 " is faster than ",
 runner2 )
 return(xxx)
}
```

Now that we have a function we can see what the odds are that Bolt is fasterthan the other 5 and print them one by one

```
compare_runners_bf(male_100, "Usain Bolt", "Asafa Powell")
```

```
## [1] "The evidence provided by the data corresponds to odds of 5:1 that Usain Bolt is faster than Asafa Powell"
```

```
compare_runners_bf(male_100, "Usain Bolt", "Tyson Gay")
```

```
## [1] "The evidence provided by the data corresponds to odds of 5:1 that Usain Bolt is faster than Tyson Gay"
```

```
compare_runners_bf(male_100, "Usain Bolt", "Justin Gatlin")
```

```
## [1] "The evidence provided by the data corresponds to odds of 21:1 that Usain Bolt is faster than Justin Gatlin"
```

```
compare_runners_bf(male_100, "Usain Bolt", "Yohan Blake")
```

```
## [1] "The evidence provided by the data corresponds to odds of 8:1 that Usain Bolt is faster than Yohan Blake"
```

```
compare_runners_bf(male_100, "Usain Bolt", "Maurice Greene")
```

```
## [1] "The evidence provided by the data corresponds to odds of 1355:1 that Usain Bolt is faster than Maurice Greene"
```

Okay now I feel like we’re getting somewhere with our bar discussions. Should Ifeel inclined to make a little wager on say who buys the next round of drinks asa Bayesian I have some nice useful information. I’m not rejecting a nullhypothesis I’m casting the information I have as a statement of the odds I thinkI have of “winning”.

But of course this isn’t the whole story so please read on…

### Who’s Cauchy and why does he matter?

Earlier I made light of the fact that the output from `ggbetweenstats` hadrCauchy = 0.707 and `anovaBF` uses `rscaleFixed = .707`. Now we need to spenda little time actually understanding what that’s all about. Cauchy isAugustin-Louis Cauchy andthe reason that’s relevant is that BayesFactor makes use of his distribution asa default. I’m not evengoing to try and take you into the details of the math but it is important wehave a decent understanding of what we’re doing to our data.

The BayesFactorpackagehas a few built-in “default” named settings. They all have the same shape; theonly differ by their scale, denoted by r. The three named defaults are medium =0.707, wide = 1, and ultrawide = 1.414. “Medium”, is the default. The scalecontrols how large, on average, the expected true effect sizes are. For aparticular scale 50% of the true effect sizes are within the interval (−r,r).For the default scale of “medium”, 50% of the prior effect sizes are within therange (−0.7071,0.7071). Increasing r increases the sizes of expected effects;decreasing r decreases the size of the expected effects.

![](https://3.bp.blogspot.com/-_bGk9XuZfFs/Uy3aW1mqg-I/AAAAAAAAJYM/ZPgEtVA6MKI/s1600/unnamed-chunk-6.png)

