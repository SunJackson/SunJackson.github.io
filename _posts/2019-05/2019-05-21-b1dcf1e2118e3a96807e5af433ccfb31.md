---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/05/21/distilled-news-1075/
date:      2019-05-21
author:      Michael Laux
tags:
    - models
    - data
    - learning
    - learned
    - features
---

**Using Mixed-Effects Models For Linear Regression**

Mixed-effects regression models are a powerful tool for linear regression models when your data contains global and group-level trends. This article walks through an example using fictitious data relating exercise to mood to introduce this concept. R has had an undeserved rough time in the news lately, so this post will use R as a small condolence to the language, though a robust framework exist in Python as well. Mixed-effect models are common in political polling analysis where national-level characteristics are assumed to occur at a state-level while state-level sample sizes may be too small to drive those characteristics on their own. They are also common in scientific experiments where a given effect is assumed to be present among all study individuals which needs to be teased out from a specific effect on a treatment group. In a similar vein, this framework can be helpful in pre/post studies of interventions.

**Applying the Universal Machine Learning Workflow to the UCI Mushroom Dataset**

This post is intended to demonstrate the universal machine learning workflow as stated by Francois Chollet in Deep Learning with Python.

**How To Find Probability From Probability Density Plots**

To understand the distribution of data by knowing the actual probability within a range.

**An Inutitive Understanding to Fader Networks**

A fader network is a neural network that gives you control over a set of attributes of a data item. In this blog, we will talk about fader networks controlling attributes on images as they are the most intuitive to understand.

**LSTM Autoencoder for Extreme Rare Event Classification in Keras**

Here we will learn the details of data preparation for LSTM models, and build an LSTM Autoencoder for rare-event classification.

**DeepFool – A simple and accurate method to fool Deep Neural Networks.**

Deep Neural Networks achieve state of the art performances in many tasks but fail miserably on slightly perturbed images, perturbed in a meaningful way (and not randomly).The DeepFool paper have the following major contributions:1.Simple and accurate method for computing the robustness of different classifiers to adversarial perturbations.2.Experiments showing• DeepFool computes a more optimal adversarial perturbation• Adversarial Training significantly increases the robustness.

**Predicting Wine Quality with Gradient Boosting Machines**

This article gives a tutorial for training a Gradient Boosting Machine (GBM) model that predicts the quality of a wine using only the price and a written description of the wine. In many real Data Science projects, it is important to take the projects into the next phase by deploying a live model that others can use to get real-time predictions. In this article, I demonstrate the model training and deployment process using AWS SageMaker to create a live endpoint.

**The Hitchhiker’s Guide to Feature Extraction**

Good Features are the backbone of any machine learning model. And good feature creation often needs domain knowledge, creativity, and lots of time. In this post, I am going to talk about:• Various methods of feature creation- Both Automated and manual• Different Ways to handle categorical features• Longitude and Latitude features• Some kaggle tricks• And some other ideas to think about feature creation.This post is about useful feature engineering methods and tricks that I have learned and end up using often.

**Image Classification with Tensorflow 2.0**

In this article, we will address one of the most common AI problems. Here is the scenario: you’re surfing the web at a bar enjoying a beer and some chicken wings when you start to wonder ‘could I write an image classifier using Tensorflow 2.0 and Transfer Learning?’ This article will show an end to end process for accomplishing this. Note, I will not be going into how the model works, that would be a whole other article. This article will show you how to get up and running with Tensorflow 2.0. Once we have that we can go back and play with the params that might make the code more efficient.

**An illustrated guide to AI for the handsome corporate executive**

A lot of companies are making headways with machine learning (ML) these days. The leaps in image recognition and natural language processing enabled by recent advancements in deep learning and massive increases in available data have put AI* on the corporate radar almost everywhere. ML has outperformed humans and enabled companies to process service requests at a scale quite simply not possible with a 100% human service desk. It has seen successful applications in domains ranging from fraud detection to medical image analysis, from machine translation to personalised recommendations. Since quite a number of the headlines in ML are made by the GAFA lot, it is only natural that you as an executive or service manager are looking towards them when you start out exploring applications of artificial intelligence for your business. Besides the obvious issues in following this approach – your company is probably still a largely offline endeavour – there are some less-discussed issues I want to address in this post.

**Handtrack.js: Hand Tracking Interactions in the Browser using Tensorflow.js and 3 lines of code.**

A while ago, I was really blown away by results from an experiment using TensorFlow object detection api to track hands in an image. I made the trained model and source code available, and since then it has been used to prototype some rather interesting usecases (a tool to help kids spell, extensions to predict sign language, hand ping pong, etc). However, while many individuals wanted to experiment with the trained model, a large number still had issues setting up Tensorflow (installation, TF version issues, exporting graphs, etc). Luckily, Tensorflow.js addresses several of these installations/distribution issues, as it is optimized to run in the standardized environment of browsers. To this end, I created Handtrack.js as a library to allow developers quickly prototype hand/gesture interactions powered by a trained hand detection model.

**Finding Bayesian Legos – Part 2**

Last time on Finding Bayesian Legos, my dear friend Joe and I were trying to find a way to help Joe gain confidence that he has indeed picked-up all of the Legos® his children left behind. My first proposal was that we determine how good Joe is at picking up Legos by performing experiments on Joe to determine the probability he would pick a single Lego in any given sweep of the room where the Legos have been left laying. Joe said, ‘No go on this plan! I really dislike picking up Legos. Can’t you find a better way?’

**Review: MobileNetV2 – Light Weight Model (Image Classification)**

In this story, MobileNetV2, by Google, is briefly reviewed. In the previous version MobileNetV1, Depthwise Separable Convolution is introduced which dramatically reduce the complexity cost and model size of the network, which is suitable to Mobile devices, or any devices with low computational power. In MobileNetV2, a better module is introduced with inverted resiaul structure. Non-linearities in narrow layers are removed this time. With MobileNetV2 as backbone for feature extraction, state-of-the-art performances are also achieved for object detection and semantic segmentation. This is a paper in 2018 CVPR with more than 200 citations. (Sik-Ho Tsang @ Medium)

**Text-based Graph Convolutional Network – Bible Book Classification**

In this article, I will walk you through the details of text-based Graph Convolutional Network (GCN) and its implementation using PyTorch and standard libraries. The text-based GCN model is an interesting and novel state-of-the-art semi-supervised learning concept that was proposed recently (expanding upon the previous GCN idea by Kipf et al. on non-textual data) which is able to very accurately infer the labels of some unknown textual data given related known labeled textual data. At the highest level, it does so by embedding the entire corpus into a single graph with documents (some labelled and some unlabelled) and words as nodes, with each document-word & word-word edges having some predetermined weights based on their relationships with each other (eg. Tf-idf). A GCN is then trained on this graph with documents nodes that have known labels, and the trained GCN model is then used to infer the labels of unlabelled documents.

**ML Approaches for Time Series**

In this post I play around with some Machine Learning techniques to analyze time series data and explore their potential use in this case of scenarios. In this first post only the first point of the index is developed. The rest have a separate post which can be accessed from the index.

**OCR for Scanned Numbers using Google’s AutoML Vision**





### Like this:

Like Loading...


*Related*

