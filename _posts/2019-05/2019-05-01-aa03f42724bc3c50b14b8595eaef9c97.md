---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/05/02/distilled-news-1052/
date:      2019-05-01
author:      Michael Laux
tags:
    - organizing data
    - difference
    - learning
    - learned
    - learns
---

**Organizing data science experiments with PLynx**

Continuous improvement, promoting innovative solutions and understanding complex domain are essential parts of challenges Data Scientists face today. On top of it they deal with various engineering problems starting with data collection and transformation to deploying and monitoring. Engineers and data scientists developed various tools and frameworks to address the core challenges of conducting data experiments in a reproducible way. Notebooks have proved to be great for one-shot analysis. On the other hand, they have countless flaws when it comes to production. Notebooks are often not reproducible, do not encourage reusing the same code, don’t support parallelization out of box, don’t work well with version control tools, etc. Similarly to engineers, data scientists managed to use existing tools and good practices such as Makefiles. They describe computational graph, where each step is a script. The users are still responsible for many engineering details such as parallel execution in the cloud or storing and accessing data. Other data scientists adopted the tools that normally data engineers would use. Apache Airflow, for example, is a very popular framework for building data pipelines. Unfortunately Data Science does not often work in the same way. Highly reliable execution is not as important as flexibility and ability to try different experiments. Using reliable data pipelines in Data Science can bring incremental improvements, however there is usually far more to gain from other activities like integrating new data sources or using additional workflows.

**Bias-variance dilemma?**

The Bias-Variance dilemma is relevant for supervised machine learning. It’s a way to diagnose an algorithm performance by breaking down its prediction error. There are three types of prediction errors: bias, variance, and irreducible error.

**Fast food, causality and R packages, part 1**

I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read here (PDF warning). The gist of the paper is to try to answer the following question: Do increases in minimum wages reduce employment? According to Card and Krueger’s paper from 1994, no. The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to $5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted the employment in fast food restaurants and found, against what economic theory predicted, an increase and not a decrease in employment. The authors used a method called difference-in-differences to asses the impact of the minimum wage increase. This result was and still is controversial, with subsequent studies finding subtler results. For instance, showing that there is a reduction in employment following an increase in minimum wage, but only for large restaurants (see Ropponen and Olli, 2011). Anyways, this blog post will discuss how to create a package using to distribute the data. In a future blog post, I will discuss preparing the data to make it available as a demo dataset inside the package, and then writing and documenting functions.

**Learn how to Build and Deploy a Chatbot in Minutes using Rasa (IPL Case Study!)**

Have you ever been stuck at work while a pulsating cricket match was going on? You need to meet a deadline but you just can’t concentrate because your favorite team is locked in a fierce battle for a playoff spot. Sounds familiar? I’ve been in this situation a lot in my professional career and checking my phone every 5 minutes was not really an option! Being a data scientist, I looked at this challenge from the lens of an NLP enthusiast. Building a chatbot that could fetch me the scores from the ongoing IPL (Indian Premier League) tournament would be a lifesaver. So I did just that! Using the awesome Rasa stack for NLP, I built a chatbot that I could use on my computer anytime. No more looking down at the phone and getting distracted. And the cherry on top? I deployed the chatbot to Slack, the popular platform for de facto team communications. That’s right – I could check the score anytime without having to visit any external site. Sounds too good an opportunity to pass up, right? In this article, I will guide you on how to build your own Rasa chatbot in minutes and deploy it in Slack. With the ICC Cricket World Cup around the corner, this is a great time to get your chatbot game on and feed your passion for cricket without risking your job.

**Determining Number of Clusters in One Picture**

If you want to determine the optimal number of clusters in your analysis, you’re faced with an overwhelming number of (mostly subjective) choices. Note that there’s no ‘best’ method, no ‘correct’ k, and there isn’t even a consensus as to the definition of what a ‘cluster’ is. With that said, this picture focuses on three popular methods that should fit almost every need: Silhouette, Elbow, and Gap Statistic.

**Concept of Cross-Validation in R**

One of the most brilliantly simple and compelling ideas in all of statistics: to estimate how well your model will do on new data, take your data set and divide it into two parts at random. Fit the model to one part and then evaluate its prediction on the other; average over a couple of splits into training and testing sets. In this blog, we will study in depth the concept of cross-validation. We will further study about different cross-validation techniques too!

**Visualising Bias and Unbiasedness**

A question on X validated led me to wonder at the point made by Christopher Bishop in his Pattern Recognition and Machine Learning book about the MLE of the Normal variance being biased.

**Self-driving cars: bigger road safety, less privacy**

Cars are the most ubiquitous transport method nowadays. They offer comfort, freedom, availability, various expenses and a slim-but-not-insignificant probability of being involved in an accident. Casualties are diminishing, yet cars remain as one of the deadliest transport systems, only surpassed mainly by motorcycles. They are also an untenable economic burden, costing between 1% and 3% of each country’s GDP according to the World Health Organization. Joint efforts by governments and car manufacturers are striving to reduce this rate significantly: the European Commission, for instance, has developed several strategies, a Cooperative Intelligent Transport Systems among others, which focuses on reducing human error and creating environmentally friendly solutions.

**The easy way to work with CSV, JSON, and XML in Python**

Python’s superior flexibility and ease of use are what make it one of the most popular programming language, especially for Data Scientists. A big part of that is how simple it is to work with large datasets. Every technology company today is building up a data strategy. They’ve all realised that having the right data: insightful, clean, and as much of it as possible, gives them a key competitive advantage. Data, if used effectively, can offer deep, beneath the surface insights that can’t be discovered anywhere else. Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python!

**A step towards general NLP with Dynamic Memory Networks**

An obstacle for general NLP is that different tasks (such as text classification, sequence tagging and text generation) require different sequential architectures. One way to deal with this problem is to view these different tasks as question-answering problems. So, for example, the model could be asked what the sentiment for a piece of text is (traditionally a text classification problem) and the answer could be one of ‘positive’, ‘negative’ or ‘neutral’. The paper ‘Ask Me Anything: Dynamic Memory Networks for Natural Language Processing’ introduces a new, modularised architecture for question-answering. For complex question-answering problems, the memory component of LSTMs and GRUs can serve as a bottleneck. It is difficult to accumulate all relevant information in the memory component in one pass, and hence, a key idea behind the paper is to allow the model access to the data as many times as required. Although the architecture looks extremely complex at first glance, it can be broken down into a number of simple components.

**Meaning Representation and SRL: assuming there is some meaning**

A meaning representation can be understood as a bridge between subtle linguistic nuances and our common-sense non-linguistic knowledge about the world. It can be seen as a formal structure capturing the meaning of linguistic input. While doing this, we assume that any given linguistic structure has some stuff/information that can be used to express the state of the world. How do we realize that someone has praised or insulted us? That’s where the need for meaning representation arises. We understand this by breaking the linguistic input into meaningful structure and linking that to our knowledge of the real world. In this case, it can be our knowledge about a specific person, our knowledge about previous experiences and relationship with that person, our knowledge of that particular moment and many more.

**Deep Reinforcement Learning and Monte Carlo Tree Search With Connect 4**

In the previous article I wrote about how to implement a reinforcement learning agent for a Tic-tac-toe game using TD(0) algorithm. I implemented 2 kinds of agents. The first is a tabular reinforcement learning agent which means that the value function is stored as a table that contains all the values of every state that exists in the game for the optimal policy (which is learned during the algorithm iterations). It is possible to store all the values because the game has less than 6000 states. The second implementation was for a deep reinforcement learning agents, which means that instead of saving all the values in a table, we train a neural network to predict the value for a given state. This method is more general and robust because the network can learn similarities like translation or reflection in the state space. I would like in this article to take it one step further and try to implement an agent that learns how to play the game Connect 4. The game contains a board with 7 columns and 6 rows. Each player in his turn chooses one of the column and drop one colored disk from the top which falls to the lowest empty spot. The first one to have 4 disks with the same color in a horizontal, vertical or diagonal line wins. Connect 4 is far more complex than Tic-Tac-Toe because it has more than 10¹4 states. In this article I will describe 2 different approaches. The first approach is the famous deep Q learning algorithm or DQL, and the second is a Monte Carlo Tree Search (or MCTS).

**What Is a Hazard Function in Survival Analysis?**

One of the key concepts in Survival Analysis is the Hazard Function. But like a lot of concepts in Survival Analysis, the concept of ‘hazard’ is similar, but not exactly the same as, its meaning in everyday English. Since it’s so important, though, let’s take a look.

**Six Types of Survival Analysis and Challenges in Learning Them**

Survival analysis isn’t just a single model. It’s a whole set of tests, graphs, and models that are all used in slightly different data and study design situations. Choosing the most appropriate model can be challenging. In this article I will describe the most common types of tests and models in survival analysis, how they differ, and some challenges to learning them. What they share is all of these allow you to test how predictor variables predict an outcome variable that measures the time until an event. They are all based on a few central concepts that are important in any time-to-event analysis, including censoring, survival functions, the hazard function, and cumulative hazards.

**A Recipe for Training Neural Networks**

Some few weeks ago I posted a tweet on ‘the most common neural net mistakes’, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a webinar :)). Clearly, a lot of people have personally encountered the large gap between ‘here is how a convolutional layer works’ and ‘our convnet achieves state of the art results’. So I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.





### Like this:

Like Loading...


*Related*

