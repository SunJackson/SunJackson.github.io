---
layout:     post
catalog: true
title:      POWER to the People
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/S2fmUYXCbY8/
date:      2019-05-14
author:      rOpenSci - open tools for open science
tags:
    - data
    - models
    - modelling
    - modelled
    - get_power
---





NASA generates and provides heaps of data to the scientific community. Not allof it is looking out at the stars. Some of it is looking back at us here onEarth. NASA’s Earth science program observes, understands and models theEarth system. We can use these data to discover how our Earth is changing,to better predict change, and to understand the consequences for life on Earth.

The Earth science program includes thePrediction of Worldwide Energy Resource (POWER)project, which was initiated to improve upon the current renewable energy dataset and to create new data sets from new satellite systems. The POWER projecttargets three user communities: 1) Renewable Energy (SSE), 2) SustainableBuildings (SB) and 3) Agroclimatology (AG) and covers 140+ differentparameters.

### How Did This Package Happen?

I first became acquainted with the POWER data when I joined the GIS labat the International Rice Research Institute(IRRI) in 2011. We commonly used theagroclimatology data from POWER to map and model rice projects. In mostof the work I did, I used it with the EPIRICE model, but it was also usedfor other crop modelling. Since then I have used the POWER data in projects withEPIRICE myself and have worked with other researchers who use it for cropsimulation modelling exercises,. The data were a great resource foragricultural modelling, even if it was a bit coarse at 1˚ x 1˚ (covering roughly12225 km2 or 4721 mi2 per grid cell at the equator, moreas you approach the poles), the full data set had global coverage, offering datawhere we often needed it in areas that lacked good weather station coverage.

Because I had used the data and I knew plenty of others used the data; in 2017 Istarted writing nasapower, to interface with the POWER website and runqueries to get the data from the server but only for agricultural weather data(AG community), as this was my main (really only) interest. This created asimplified procedure for downloading the data in place of using the point andclick interface of the website for repeated queries. I submitted it forreview with rOpenSciand was happy with the feedback I received from the reviewers, quickly makingthe suggested changes.

### What Happened Next

However, in late 2017 very shortly after the package was reviewed, thePOWER team announced that the POWER data would be getting a makeover.The data would be served at 0.5˚ x 0.5˚ and use a newAPI to access them. Thiscomplicated things. I had never worked with an official API before. Ihad only worked with FTP servers, which are at best a very rudimentaryAPI, but easy to work with in R. So I went back to the reviewers andeditor and suggested that I needed to rewrite the whole package due tothe major changes that were coming. The editor, Scott Chamberlain,readily agreed and over the next several months I learned how to writean R package that is an API client, a proper API client.

Thankfully rOpenSci has a great package, crul, that makes this much easier.Even better, Maëlle Salmon had written ablog post on how to use it! So Igot down to business writing the new version of nasapower using crul.

I quickly found out that the easy part was using crul; the hard partwas validating user inputs. I decided early in the process of writingthe package to validate the users’ input on the client side before evenquerying the server to make sure that only well-formed requests weresent. That way I could provide the user with feedback on what may havebeen entered incorrectly to make it easier for them to correct ratherthan relying on the server’s response.

There are over 140 different parameters for three “communities”(AG, SSE and SB), that the POWER data provides. One of the first issues Iencountered was how to validate the users’ requests against what was available.Thankfully Ben Raymond found a JSON file thatwas served up for the API documentation that I was able to use tocreate an internal list objectto check against, which made the parameters and communities easy validate.

Formatting dates and checking those given all the different ways we enter themproved to be another challenge entirely taking nearly 100 lines of code.

Along with learning how to write an API client package, one of themethods I used in this package that I had not made full use of beforewas lists. Internally in that 100+ lines of code there are severalinter-related checks and values that are provided for the query. Byusing lists I was able to return more than one value from a functionafter it was checked and validated and then provide that to the functionI created to generate the request that crul sends, *e.g.*, thelongitude, latitude and whether it is a single point orregion.By doing this I was able to simplify the parameters that the user had toenter when constructing a query in their R session because the APIdictates by default many values that cannot co-occur.

### Are We Doing it Right?

There were a few hang-ups along the way. At one point the POWER teammade a change to the API response while the package was in review.Initially it provided an option for just a CSV file that was properlyrectangular. Suddenly it changed overnight with no warning to offering aCSV file with information in a “header” as they called it. To me aheader in a CSV file is just column names. This was more. This wasmetadata about the data itself. The quickest way to deal with this wasto simply read only the CSV file portion of the returned file. However,I thought it might be useful to users to include the metadata that POWERwas now providing in this format as well. So I learned something elsenew, how to modify the S3 `print()` method function to include themetadata in the console along with the weather or climate data.

Later, in early 2019 I ran into an issue with the method I’d used tovalidate user inputs for `community` when building the query that wassent. Daniel Reis Pereira reportedthat for some reason nasapower was failing to download 2 m wind data,necessary for calculating evapotranspiration for crop modelling. Lookinginto the JSON file I used as suggested by Ben Raymond, I found the issue. ThePOWER team did not list the AG community as having this data available. Icontacted them thinking is was a mistake in the file since the data are inindeed available. After some back and forth with the NASA-POWER team I neverreally was clear on how I should handle this from their perspective aside fromtheir suggestion of just looking at their webpage to see what was available.Additionally, when I checked, it appeared that the data were available for allthree communities. So I ended up dropping the check for `community` from theuser input validations because the community specified only affects the unitsthat the data are reported in, you can see the POWER documentation for more onthis. I still check user inputs for the correct parameter specification andtemporal matches, but assume all communities offer the same data. It’s notoptimal but until the POWER team can deliver a more robust way for us to checkagainst their records it will have to do.

### Finally, It (Mostly?) Works!

The package is relatively simple as far as functionality. It onlyfetches data and reformats it for use in R or crop modelling software.

### The Three Functions

The first function, `get_power()`, is the main function that is used andoffers the greatest flexibility. Say you want to know the maximum andminimum temperatures for Death Valley in California, USA every day forthe last 35 years, you can easily do this with nasapower usingcoordinates for the valley floor.

If you are not sure what the parameters are that you need to get thedata you can always have a look at the list of available parametersusing `?parameters`. That shows that T2M_MIN and T2M_MAXare the minimum and maximum temperatures at 2 meters. We will use thoseto construct the query.

```
library(nasapower)

dv <- get_power(community = "AG",
 lonlat = c(-117.2180, 36.7266),
 dates = c("1983-01-01", "2018-12-31"),
 temporal_average = "DAILY",
 pars = c("T2M_MIN", "T2M_MAX"))

dv

```

```
## NASA/POWER SRB/FLASHFlux/MERRA2/GEOS 5.12.4 (FP-IT) 0.5 x 0.5 Degree Daily Averaged Data 
## Dates (month/day/year): 01/01/1983 through 12/31/2018 
## Location: Latitude 36.7266 Longitude -117.218 
## Elevation from MERRA-2: Average for 1/2x1/2 degree lat/lon region = 1192.53 meters Site = na 
## Climate zone: na (reference Briggs et al: http://www.energycodes.gov) 
## Value for missing model data cannot be computed or out of model availability range: -99 
## 
## Parameters:
## T2M_MIN MERRA2 1/2x1/2 Minimum Temperature at 2 Meters (C) ;
## T2M_MAX MERRA2 1/2x1/2 Maximum Temperature at 2 Meters (C) 
## 
## # A tibble: 13,149 x 9
## LON LAT YEAR MM DD DOY YYYYMMDD T2M_MIN T2M_MAX
## 
## 1 -117. 36.7 1983 1 1 1 1983-01-01 -4.73 7.24
## 2 -117. 36.7 1983 1 2 2 1983-01-02 -3.33 7.72
## 3 -117. 36.7 1983 1 3 3 1983-01-03 0.05 11.8
## 4 -117. 36.7 1983 1 4 4 1983-01-04 1.27 14.9
## 5 -117. 36.7 1983 1 5 5 1983-01-05 2.55 15.9
## 6 -117. 36.7 1983 1 6 6 1983-01-06 2.63 17.1
## 7 -117. 36.7 1983 1 7 7 1983-01-07 3.2 17.2
## 8 -117. 36.7 1983 1 8 8 1983-01-08 1.96 18.3
## 9 -117. 36.7 1983 1 9 9 1983-01-09 0.7 14.0
## 10 -117. 36.7 1983 1 10 10 1983-01-10 1.3 17.6
## # … with 13,139 more rows

```

In the metadata header you can see information about where the datacomes from and what dates have been queried and returned as well as theelevation data.

The `tibble` contains a few columns not in the original data, but thatcan make it easier to work within R. The original data only include YEARand DOY. Looking at the data returned, there are:


LON = the queried longitude as a double;


LAT = the queried latitude as a double;


YEAR = the queried year as a double;


MM = the queried month as a double,


DD = the queried day as a double,


DOY = the day of year or Julian date as an integer,


YYYYMMDD = the full date as a date object and the requested parameters,


T2M_MIN = the minimum temperature at 2 meters above the Earth’s surface as a double, and


T2M_MAX = the maxiumum temperature at 2 meters above the Earth’s surface as a double.


#### Visualising the Data

To visualise these data I will use ggplot2, but first I need to gather the datainto long format using tidyr’s `gather()`.

```
library(tidyr)
library(ggplot2)
library(hrbrthemes)

dv_long <-
 tidyr::gather(dv, key = "T2M",
 value = "Degrees",
 c("T2M_MIN", "T2M_MAX"))

ggplot(dv_long, aes(x = YYYYMMDD, y = Degrees,
 colour = Degrees)) +
 geom_line() +
 xlab("Year") +
 ggtitle(label = "Death Valley, CA Daily Max and Min Temperatures",
 sub = "Source: NASA POWER") +
 scale_colour_viridis_c() +
 theme_ipsum()

```


![](https://i2.wp.com/ropensci.org/img/blog-images/2019-05-14-nasapower/graph_t-1.png?w=456&ssl=1)
![](https://i2.wp.com/ropensci.org/img/blog-images/2019-05-14-nasapower/graph_t-1.png?w=456&ssl=1)
Figure 1: Daily temperature extremes at 2 meters above the Earth’s surface for the grid cell covering Death Valley, California, USA for the time-period from 1983 to 2018.


That is quite a swing in air temperatures from well over 40˚ C to wellbelow 0˚ C throughout the year. I was going to put together a comparisonwith station data using GSODR butinstead found a good reason why you might want to use nasapower to getPOWER data. When I checked for stations nearby this specified point,there were two in the GSOD database, 746190-99999 and 999999-53139.However, neither one of them offers weather data for this time period.The station, 746190-99999, only offers data from 1982 to 1992 and999999-53139 only provides data from 2004 to 2019. This nicelyillustrates one of the advantages of the POWER data, it is available forany point on the globe. One of the weaknesses you might have noticed ifyou looked at the elevation data in the metadata header is that it showsthe elevation of Death Valley is over 1000 m above sea level when weknow that it is 86 m below sea level. This is due to the fact that thedata are the average of a 0.5˚ by 0.5˚ area or roughly3000 km2 as opposed to a single point that a station canprovide.

#### Crop Modelling From Space

This can be advantageous as well, however. Some agricultural scientists workwith models that predict crop yields in response to changes to the crop,weather, farmer inputs or even climate change. Crop yield modelling oftenuses daily weather data covering large relatively continuous areas thatare less affected by things like elevation, so for these purposes, thePOWER data can be very useful. Another reason the POWER data are usefulcould be that the same data set is available for many areas, makingmodel output comparisons easier.

If you do any crop modelling work you are likely familiar with theDecision Support System for Agrotechnology TransferDSSAT platform, , . The new POWER APIprovides properly formatted ICASA files,which are the format that DSSAT uses. Naturally I took advantage of this andadded a function, `create_icasa()`, to download and save ICASA files for use incrop simulations.

But, being in Toowoomba, Queensland, I had to acknowledge another cropsimulation model, the Agricultural Production Systems sIMulatorAPSIM. APSIM was developed here and has similarfunctionality to DSSAT. However, the POWER API did not offer properly formattedAPSIM .met files. So, wrote a function, `create_met()`, that takes advantage ofthe POWER data API and the R APSIM package to generate the proper weather.met files since many APSIM users, use R in their modelling pipeline, *e.g.*,APSIMBatch and apsimr.

Both of these functions simply download data and write the values todisk in a specialised file format that these crop modelling platformsuse, therefore I have declined to illustrate their usage in this blogpost.

### There Is More Than Just Daily Data for Single Cells

#### Retrieving Climatology

Daily weather data are not the only data offered by this API. Two other optionsexist for the `temporal_average` parameter, INTERANNUAL and CLIMATOLOGY.INTERANNUAL data provide monthly averages for the same 0.5˚ x 0.5˚ grid as thedaily data for the time-period the user specifies, while CLIMATOLOGY provides0.5˚ x 0.5˚ gridded data of a thirty year time period from January 1984 toDecember 2013.

The CLIMATOLOGY data are the only way to get the entire surface in one query,but single cell and regional data are also available for this temporal average.

```
library(raster)

```

```
## Loading required package: sp

##
## Attaching package: 'raster'

## The following object is masked from 'package:tidyr':
##
## extract

```

```
library(viridisLite)

global_t2m <-
 get_power(
 community = "AG",
 pars = "T2M",
 temporal_average = "CLIMATOLOGY",
 lonlat = "GLOBAL"
 )

# view the tibble
global_t2m

```

```
## NASA/POWER SRB/FLASHFlux/MERRA2/GEOS 5.12.4 (FP-IT) 0.5 x 0.5 Degree Climatologies 
## 22-year Additional Solar Parameter Monthly & Annual Climatologies (July 1983 - June 2005), 30-year Meteorological and Solar Monthly & Annual Climatologies (January 1984 - December 2013) 
## Location: Global 
## Value for missing model data cannot be computed or out of model availability range: -99 
## Parameter(s): 
## T2M MERRA2 1/2x1/2 Temperature at 2 Meters (C) 
## 
## Parameters:
## NA;
## NA;
## T2M MERRA2 1/2x1/2 Temperature at 2 Meters (C) 
## 
## # A tibble: 259,200 x 16
## LON LAT PARAMETER JAN FEB MAR APR MAY JUN JUL AUG
## 
## 1 -180. -89.8 T2M -29.0 -40.7 -52.9 -57.8 -59.1 -59.6 -61.3 -61.8
## 2 -179. -89.8 T2M -29.0 -40.7 -52.9 -57.8 -59.1 -59.6 -61.3 -61.8
## 3 -179. -89.8 T2M -29.0 -40.7 -52.9 -57.8 -59.1 -59.6 -61.3 -61.8
## 4 -178. -89.8 T2M -29.0 -40.7 -52.9 -57.8 -59.1 -59.6 -61.3 -61.8
## 5 -178. -89.8 T2M -29.0 -40.7 -52.9 -57.8 -59.1 -59.6 -61.3 -61.8
## 6 -177. -89.8 T2M -28.9 -40.7 -52.9 -57.9 -59.1 -59.6 -61.3 -61.8
## 7 -177. -89.8 T2M -28.9 -40.7 -52.9 -57.9 -59.1 -59.6 -61.3 -61.8
## 8 -176. -89.8 T2M -28.9 -40.7 -53.0 -57.9 -59.1 -59.6 -61.3 -61.8
## 9 -176. -89.8 T2M -28.9 -40.7 -53.0 -57.9 -59.1 -59.6 -61.3 -61.8
## 10 -175. -89.8 T2M -28.9 -40.7 -53.0 -57.9 -59.1 -59.6 -61.3 -61.8
## # … with 259,190 more rows, and 5 more variables: SEP , OCT ,
## # NOV , DEC , ANN 

```

```
# map only annual average temperatures by converting the 15th column to a raster
# object
T2M_ann <- rasterFromXYZ(
 global_t2m[c(1:2, 16)],
 crs = "+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")

# how many unique values
n <- length(unique(global_t2m$ANN))

# plot the annual average temperature using viridis
plot(T2M_ann, col = viridis(n = n), xlab = "Longitude", ylab = "Latitude")

```


![](https://i2.wp.com/ropensci.org/img/blog-images/2019-05-14-nasapower/global-T2M-1.png?w=456&ssl=1)
![](https://i2.wp.com/ropensci.org/img/blog-images/2019-05-14-nasapower/global-T2M-1.png?w=456&ssl=1)
Figure 2: Global 30-year annual meteorological (January 1984 – December 2013) average temperature at 2 meters above the Earth’s surface modelled from satellite derived data. You can mostly make out the outlines of the continents and especially the mountain ranges such as the Andes and Rocky Mountains to the left and the Tibetan plateau at about 100˚ longitude (x-axis) and 45˚ latitude (y-axis).


#### Retrieving Regional Data

If your interests cover a large area, it is possible to retrieve an area ofcells, rather than a single cell in a query. They can not be more than 100points in total for an area of 4.5˚ x 4.5˚. Regional coverage is simplyspecified by providing a bounding box as “lower left (lon, lat)” and “upperright (lon, lat)” coordinates, *i.e.*, `lonlat = c(xmin, ymin, xmax, ymax)` inthat order for a given region, *e.g.*, a bounding box for the south-westerncorner of Australia:

`lonlat = c(112.5, -55.5, 115.5, -50.5)`.

```
regional_t2m <-
 get_power(
 community = "AG",
 pars = "T2M",
 temporal_average = "CLIMATOLOGY",
 lonlat = c(112.5, -55.5, 115.5, -50.5)
 )

# view the tibble
regional_t2m

```

```
## NASA/POWER SRB/FLASHFlux/MERRA2/ 0.5 x 0.5 Degree Climatologies 
## 22-year Additional Solar Parameter Monthly & Annual Climatologies (July 1983 - June 2005), 30-year Meteorological and Solar Monthly & Annual Climatologies (January 1984 - December 2013) 
## Location: Regional 
## Elevation from MERRA-2: Average for 1/2x1/2 degree lat/lon region = na meters Site = na 
## Climate zone: na (reference Briggs et al: http://www.energycodes.gov) 
## Value for missing model data cannot be computed or out of model availability range: -99 
## 
## Parameters:
## T2M MERRA2 1/2x1/2 Temperature at 2 Meters (C) 
## 
## # A tibble: 77 x 16
## LON LAT PARAMETER JAN FEB MAR APR MAY JUN JUL AUG
## 
## 1 113. -55.2 T2M 2.98 3.38 3.13 2.4 1.65 1.01 0.41 -0.05
## 2 113. -55.2 T2M 3.07 3.47 3.22 2.5 1.75 1.11 0.51 0.05
## 3 114. -55.2 T2M 3.14 3.55 3.32 2.6 1.85 1.21 0.61 0.15
## 4 114. -55.2 T2M 3.2 3.62 3.39 2.69 1.94 1.29 0.69 0.22
## 5 115. -55.2 T2M 3.26 3.69 3.46 2.76 2.01 1.37 0.77 0.28
## 6 115. -55.2 T2M 3.3 3.73 3.51 2.82 2.06 1.43 0.83 0.34
## 7 116. -55.2 T2M 3.32 3.77 3.54 2.86 2.1 1.49 0.88 0.39
## 8 113. -54.8 T2M 3.12 3.51 3.3 2.57 1.82 1.17 0.59 0.12
## 9 113. -54.8 T2M 3.2 3.6 3.39 2.68 1.92 1.26 0.69 0.21
## 10 114. -54.8 T2M 3.27 3.7 3.49 2.78 2.02 1.36 0.78 0.3
## # … with 67 more rows, and 5 more variables: SEP , OCT ,
## # NOV , DEC , ANN 

```

```
# map only annual average temperatures by converting the 15th column to a raster
# object
T2M_ann_regional <- rasterFromXYZ(
 regional_t2m[c(1:2, 16)],
 crs = "+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")

# how many unique values
n <- length(unique(regional_t2m$ANN))

# plot the annual average temperature using viridis
plot(T2M_ann_regional, col = viridis(n = n), xlab = "Longitude", ylab = "Latitude")

```


![](https://i2.wp.com/ropensci.org/img/blog-images/2019-05-14-nasapower/regional-t2m-1.png?w=456&ssl=1)
![](https://i2.wp.com/ropensci.org/img/blog-images/2019-05-14-nasapower/regional-t2m-1.png?w=456&ssl=1)
Figure 3: Regional 30-year annual meteorological (January 1984 – December 2013) average temperature at 2 meters above the Earth’s surface modelled from satellite derived data for the south-western coastal area of Australia, illustrating the maximum allowable cells in a regional query.


As you can see, because the data are georeferenced it is easy to usethem in R’s spatial packages including sf and raster.Emerson Del Ponte,(@edelponte), used the data in a talk at theInternational Congress of Plant Pathology in August 2018, “Can Rainfallbe a Useful Predictor of Epidemic Risk Across Temporal and SpatialScales?”,see slides23,24and25for maps created using nasapower and the POWER data for two states inBrazil. These maps took a bit more work to query the API and generate,but I plan to add an example vignette detailing how this can be done ina future release.

### Conclusion

Even though the package took me well over one year to write and work outall of the bugs and has only three functions, I learned quite a bit fromthe experience. The new API really does improve the ability for adeveloper to write a client to interface with the data. Now thenasapower package is able to access all of the data available whereasthe initial version was only able to work with the “AG” community data.

While nasapower does not redistribute the data or provide it in anyway, if you use the data, we encourage users to follow the requests ofthe POWER Project Team.

> 
When POWER data products are used in a publication, we request thefollowing acknowledgement be included: “These data were obtained fromthe NASA Langley Research Center POWER Project funded through the NASAEarth Science Directorate Applied Science Program.”


The approach I have used mostly works, but there has been at least oneexample where data are listed as being available in the POWER databasebut querying by the apparently proper `pars` does not work, see Issue#34. Because ofissues like this, in what appear to be edge cases, I suggest checkingthe web interface if you experience difficulties querying data.Hopefully the API itself has settled out a bit and will not have thesudden changes that I experienced. The POWER team have been supportiveof the package, and I have received feedback and interaction from otherusers along the way that helped me by finding and reporting bugs.

### Acknowledgements

I would like to recognise the valuable comments from Emerson Del Ponte andPaul Melloy on this blog post.Most of all, I appreciate the rOpenSci review process withvaluable contributions from HazelKavılı, AlisonBoyer and of course my ever-patienteditor and third reviewer, ScottChamberlain. I welcome further feedback forissues and suggestions,they have only made this package better. If you are interested in this orwant to know more about how to use the package, I encourage you tobrowse the vignette and other on-line documentation,https://ropensci.github.io/nasapower/articles/nasapower.html.

https://CRAN.R-project.org/package=APSIM.


*Related*








---
