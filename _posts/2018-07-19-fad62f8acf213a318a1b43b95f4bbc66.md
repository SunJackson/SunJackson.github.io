---
layout:     post
title:      New approximate nearest neighbor benchmarks
subtitle:   转载自：https://erikbern.com/2018/06/17/new-approximate-nearest-neighbor-benchmarks.html
date:       2018-07-19
author:     未知
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - https
    - benchmarks
    - algorithms
    - mrpt
    - nmslib
    - pynndescent
    - angular
    - neighbors
    - iâ
    - vector
    - search
    - graph
    - hnsw
    - annoy
    - approximate nearest neighbor
    - stars
    - starring
    - discretion
    - word
    - projects
    - millions
    - wellâ
    - applicable
    - application
    - cpu
    - ngt
---

As some of you may know, one of my side interests is approximate nearest neighbor algorithms. Iâ€™m the author of [Annoy](https://github.com/spotify/annoy), a library with 3,500+ stars on Github as of today. It offers fast approximate search for nearest neighbors with the additional benefit that you can load data super fast from disk using mmap. I built it at Spotify to use for music recommendations where itâ€™s still used to power millions (maybe billions) of music recommendations every day.

Approximate nearest neighbor search is very useful when you have a large dataset of millions of datapoints and you learn some kind of vector representation of these items. Word2vec might be the most well known example of this, but thereâ€™s plenty of other examples. For an introduction of this topic, check out an older series of blog posts: [Nearest neighbor methods and vector models](https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1.html).

Anyway, at some point I got a bit tired of reading papers of various algorithms claiming to be the fastest and most accurate, so I built a benchmark suite called [ann-benchmarks](https://github.com/erikbern/ann-benchmarks). It pits a number of algorithms in a brutal showdown. I recently Dockerized it and wrote about it [previously on this blog](https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html). So why am I blogging about it just three months later? Wellâ€¦thereâ€™s a lot of water under the bridge in the world of approximate nearest neighbors, so I decided to re-run the benchmarks and publish new results. I will probably do this a few times every year, at my own questionable discretion.

## Changes

There were several new libraries added to this benchmark:

- [NGT-Panng](https://github.com/yahoojapan/NGT) from Yahoo! Japan, a graph-based search structure

- [pynndescent](https://github.com/lmcinnes/pynndescent) which is also a graph-based search algorithm, in fact based on the same paper as k-graph

- [MRPT](https://github.com/teemupitkanen/mrpt) which is based on random projects, like Annoy.


On top of that, hnsw are included in three different flavor, one as a part of [NMSLIB](https://github.com/nmslib/nmslib), one as a part of [FAISS](https://github.com/nmslib/nmslib) (from Facebook) and one as a part of [hnswlib](https://github.com/nmslib/hnsw). I also dropped a few slow or semi-broken algorithms.

Another change this time was that Iâ€™m enforcing single-CPU queries. This made the benchmarks marginally slower, but I think itâ€™s the most â€œfairâ€� way to compare. I think batching is not always applicable for real world application. Previously, I used a thread pool to saturate all CPUs on the instance, but there was some concern that this might affect certain algorithms in different ways. So I used Dockerâ€™s ability to tie the container to a single CPU.

## Results

Without further ado, hereâ€™s the results for the latest run. For the glove-100-angular dataset:

![](https://erikbern.com/assets/ann-benchmarks-2018-06/glove-100-angular.png)


sift-128-euclidean

![](https://erikbern.com/assets/ann-benchmarks-2018-06/sift-128-euclidean.png)


nytimes-256-angular

![](https://erikbern.com/assets/ann-benchmarks-2018-06/nytimes-256-angular.png)


gist-960-euclidean

![](https://erikbern.com/assets/ann-benchmarks-2018-06/gist-960-euclidean.png)


## Results: summarized

By now, youâ€™re probably squinting at charts to figure out which library is the best. To save you the pain, Iâ€™m just going to summarize it into a somewhat subjective list:

1. hnsw(nmbslib)

1. hnswlib

1. hnsw(faiss)

1. kgraph

1. NGT-panng

1. pynndescent

1. SW-graph(nmslib)

1. annoy

1. flann

1. BallTree(nmslib)

1. mrpt

1. rpforest


The various flavors of hnsw are all at the top, but thatâ€™s partly because they were all built by the same person, [Yury Malkov](https://github.com/yurymalkov) with [a paper](https://arxiv.org/abs/1603.09320) describing the approach.

pynndescent and kgraph are both based on [the same paper](http://wwwconference.org/proceedings/www2011/proceedings/p577.pdf) so itâ€™s not surprising their performance is fairly similar.

For some reason, MRPT would crash when I ran it on angular data, and I gave up after some time investigating it. Hopefully next benchmark will feature MRPT for angular data as well.

Thereâ€™s more goodies! [Martin AumÃ¼ller](https://github.com/maumueller) and [Alexander Faithfull](https://github.com/ale-f) have contributed code to export all the results to a website. [I put it up on a temporary URL](http://vectors.erikbern.com/) for you to enjoy.

Thatâ€™s it! [ann-benchmarks](https://github.com/erikbern/ann-benchmarks) ~~currently has almost 500 stars on Github, so Iâ€™d love it if you can pay it a visit and who knowsâ€¦ starring a repo just takes a second. Just saying!~~ just passed 500 stars on Github, meaning itâ€™s a legitimate project now! ğŸ�‰
