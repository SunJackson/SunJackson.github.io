---
layout:     post
title:      brutally short intro to theano word embeddings
subtitle:   转载自：http://matpalm.com/blog/2015/03/28/theano_word_embeddings
date:       2018-07-19
author:     未知
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - embeddings
    - theano
    - http
    - unit
    - training cost
    - writing
    - write
    - loss
    - items
    - numpy
    - indexing
    - t_e
    - linear algebra
    - connected hidden
    - operations
    - multi
    - labels
---

one thing in theano i couldn't immediately find examples for was a simple embedding lookup table, a critical component for anything with NLP. turns out that it's just one of those things that's so simple no one bothered writing it down :/ 

tl;dr : you can just use [numpy indexing](http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html) and everything just works.

consider the following theano.tensor example of 2d embeddings for 5 items. each row represents a seperate embeddable item.

to pick a subset of the embeddings it's as simple as just using [indexing](http://deeplearning.net/software/theano/library/tensor/basic.html#indexing).
for example to get the third & first embeddings it's ...

if we want to concatenate them into a single vector (a common operation when we're feeding up to, say, a densely connected hidden layer), 
it's a [reshape](http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.reshape)

all the required multi dimensional operations you need for batching just work too..

eg. if we wanted to run a batch of size 2 with the first batch item being the third & first embeddings and the second batch item being the 
fourth & fourth embeddings we'd do the following...

this type of packing of the data into matrices is crucial to enable linear algebra libs and GPUs to really fire up.

consider the following as-simple-as-i-can-think-up "network" that uses embeddings; 

given 6 items we want to train 2d embeddings such that the first two items have the same embeddings, the third and fourth have the same embeddings and the last two
have the same embeddings. additionally we want all other combos to have different embeddings.

the *entire* theano code (sans imports) is the following..

first we initialise the embedding matrix as before

the "network" is just a dot product of two embeddings ...

... where the training cost is a an L1 penality against the "label" of 1.0 for the pairs we want to have the same embeddings and 
0.0 for the ones we want to have a different embeddings.

we can generate training examples by randomly picking two elements and assigning label 1.0 for the pairs 0 & 1, 2 & 3 and 4 & 6 (and 0.0 otherwise) and every once in awhile
write them out to a file.

plotting this shows the convergence of the embeddings (labels denote initial embedding location)...

![](http://matpalm.com/blog/imgs/2015/twe/dp_embeddings.png)


0 & 1 come together, as do 2 & 3 and 4 & 5. ta da!

## a note on dot products

it's interesting to observe the effect of this (somewhat) arbitrary cost function i picked.

for the pairs where we wanted the embeddings to be same the cost function, \( |1 - a \cdot b | \), is minimised when the dotproduct is 1 and this happens when the vectors 
are the same and have unit length. you can see this is case for pairs 0 & 1 and 4 & 5 which have come together and ended up on the unit circle. but what about 2 & 3? 
they've gone to the origin and the dotproduct of the origin with itself is 0, so it's *maximising* the cost, not minimising it! why?

it's because of the other constraint we added. for all the pairs we wanted the embeddings to be different the cost function, \( |0 - a \cdot b | \), is minimised when
the dotproduct is 0. this happens when the vectors are orthogonal. both 0 & 1 and 4 & 5 can be on the unit sphere and orthogonal but for them to be both orthogonal
to 2 & 3 *they* have to be at the origin. since my loss is an L1 loss (instead of, say, a L2 squared loss) the pair 2 & 3 is overall better at the origin because it 
gets more from minimising this constraint than worrying about the first.

the pair 2 & 3 has come together not because we were training embeddings to be the same but because we were also training them to be different. 
this wouldn't be a problem if we were using 3d embeddings since they could all be both on the unit sphere and orthogonal at the same time.

you can also see how the points never fully converge. in 2d with this loss it's impossible to get the cost down to 0 so they continue to get bumped around. in 3d, as
just mentioned, the cost can be 0 and the points would converge.

## a note on inc_subtensor optimisation

there's one non trivial optimisation you can do regarding your embeddings that relates to how sparse the embedding update is. 
in the above example we have 6 embeddings in total and, even though we only update 2 of them at a time, we are calculating the 
gradient with respect to the *entire* t_E matrix. the end result is that we calculate (and apply) a gradient that for the majority of rows is just zeros.

you can imagine how much sparser things are when you've got 1M embeddings and are only updating <10 per example :/

rather than do all this wasted work we can be a bit more explicit about both how we want the gradient calculated and updated 
by using [inc_subtensor](http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.inc_subtensor)

and of course you should only do this once you've [proven it's the slow part...](http://matpalm.com/blog/2015/02/22/the_curse_of_gpufromhost)

[for code, see this gist](https://gist.github.com/matpalm/bf2e71564e87e6c36081)
