---
layout:     post
title:      Hyperopt - A bayesian Parameter Tuning Framework
subtitle:   转载自：http://mlwhiz.github.io/blog/2017/12/28/hyperopt_tuning_ml_model/
date:       2018-07-19
author:     Rahul Agarwal
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - hyperopt
    - function
    - spaces
    - parameter
    - kaggle competitions
    - hyperparams
    - values
    - search
    - coursera
    - amazed
    - maximize
---

Recently I was working on a in-class competition from the ["How to win a data science competition"](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) Coursera course. Learned a lot of new things from that about using [XGBoost for time series prediction](http://mlwhiz.com/blog/2017/12/26/How_to_win_a_data_science_competition) tasks.

The one thing that I tired out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed. Left the machine with hyperopt in the night. And in the morning I had my results. It was really awesome and I did avoid a lot of hit and trial.

## What really is Hyperopt?

From the site:

> 
Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.


What the above means is that it is a optimizer that could minimize/maximize the loss function/accuracy(or whatever metric) for you.

All of us are fairly known to cross-grid search or random-grid search. Hyperopt takes as an input a space of hyperparams in which it will search, and moves according to the result of past trials.

To know more about how it does this, take a look at this [paper](https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf) by J Bergstra.
Here is the [documentation](https://github.com/hyperopt/hyperopt/wiki/FMin) from github.

## How?

Let me just put the code first. This is how I define the objective function. The objective function takes space(the hyperparam space) as the input and returns the loss(The thing you want to minimize.Or negative of the thing you want to maximize)

(X,y) and (Xcv,ycv) are the train and cross validation dataframes respectively.

We have defined a hyperparam space by using the variable `space` which is actually just a dictionary. We could choose different distributions for different parameter values.

We use the `fmin` function from the hyperopt package to minimize our `fn` through the `space`.



## Finally:

Running the above gives us pretty good hyperparams for our learning algorithm.

In fact I bagged up the results from multiple hyperparam settings and it gave me the best score on the LB.

If you like this and would like to get more information about such things, subscribe to the mailing list on the right hand side.

Also I would definitely recommend this [course](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) about winning Kaggle competitions by Kazanova, Kaggle rank 3 . Do take a look.
