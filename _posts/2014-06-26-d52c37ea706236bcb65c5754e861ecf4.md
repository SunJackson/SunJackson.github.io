---
layout:     post
title:      Law of Large Numbers and Central Limit Theorem
subtitle:   转载自：http://bugra.github.io/work/notes/2014-06-26/law-of-large-numbers-central-limit-theorem/
date:       2014-06-26
author:     Bugra Akyildiz
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - epsilon
    - average
    - x_n
    - size
    - samples
    - sampled
    - mu
    - expected
    - expectation
    - overline
    - rvert
---

Law of Large Numbers states that:

sample average($X_n$):
$$ \overline{X_n} \rightarrow \frac{1}{n}(X_1 + X_2 \ldots + X_n) $$ 
(converges to the expected average as the sample size goes to infinity):

$ \overline{X_n} \rightarrow \mu $ where $ n \rightarrow \infty $

More formally, assume that $ X_1, X_2, \ldots, X_n $ are independent and identically distributed random variables with mean $\mu$. Let $\overline{X_n}$ represents the average of $n$ variables. Then, for any $\epsilon \ge 0$, the following must hold true:
$$ \lim_{n \rightarrow \infty} P(\lvert \overline{X_n} - \mu \rvert \le \epsilon) = 1 $$

Intuitively, this makes perfect sense. When I flip a fair coin, the mean of flip coins(1 and 0's) will be closers to 0.5 as the number of trials increase. For a small number of trials, the observations may not support the expectation value.(For edge case, when the observation is 1, the difference is actually the largest.) Whole frequentist approaches and even the definition or to reason about probability of some event has connections to law of large numbers. When one has a coin and want to represent the physical events of getting heads or tails in random variables, she actually does not know if 10 heads will happen out of 20 trials. What she knows is that for a large number of trials, the getting head and tails will be equal to each other or at least very close for a fair coin.

> In a nutshell, average of independent (many) samples will converge to the mean of the underlying distribution that the observations are sampled from.

