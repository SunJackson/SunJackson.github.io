---
layout:     post
catalog: true
title:      Concept of Cross-Validation in R
subtitle:      转载自：https://dimensionless.in/concept-of-cross-validation-in-r/
date:      2019-04-29
author:      Kartik Singh
tags:
    - training
    - models
    - modelling
    - sets
    - setting
---

### Introduction

One of the most brilliantly simple and compelling ideas in all of statistics: to estimate how well your model will do on new data, take your data set and divide it into two parts at random. Fit the model to one part and then evaluate its prediction on the other; average over a couple of splits into training and testing sets.

In this blog, we will study in depth the concept of cross-validation. We will further study about different cross-validation techniques too!

###  

### What is Cross-Validation?

Cross-validation is an extension of the training, validation, and holdout (TVH) process that minimizes the sampling bias of machine learning models. With cross-validation, we still have our holdout data, but we use several different portions of the data for validation rather than using one fifth for the holdout, one for validation, and the remainder for training

So all in all, for a prediction problem, a model generally has a data set of known data, called the training data set, and a set of unknown data against which the model is tested, known as the test data set. The target is to have a data set for testing the model in the training phase and then provide insight on how the specific model adapts to an independent data set. A round of cross-validation comprises the partitioning of data into complementary subsets, then performing analysis on one subset. After this, the analysis is validated on other subsets (testing sets). To reduce variability, many rounds of cross-validation are performed using many different partitions and then an average of the results are taken. Cross-validation is a powerful technique in the estimation of the model performance technique.

###  

### Why Do We Need Cross-Validation?

The purpose of cross-validation is to assess how your prediction model performs with an unknown dataset. We shall look at it from a layman’s point of view.

You are learning how to drive a car. Now, anyone can drive a car on an empty road. The real test is how you drive in demanding traffic. It is why the trainers train you on roads that have traffic so that you get used to it.

Therefore, when it is time for you actually to drive your car, you are prepared to do so without the trainer sitting by your side to guide you. You are ready to handle any situation, the like of which you might not have encountered before.

###  

### Goal

In this blog, we will be learning about different cross-validation techniques and will be implementing them in R. For this purpose, we will train a simple regression model in R. We will be evaluating its performance through different cross-validation techniques. 

###  

### Data Generation

In this section, we will generate data to train a simple linear regression model. We will evaluate our linear regression model with different cross-validation techniques in the next section.



||gen_sim_data = function(sample_size){  x = runif(n = sample_size, min = -1, max = 1)   y = rnorm(n = sample_size, mean = x ^ 3, sd = 0.25)   data.frame(x, y)}|

{  x = runif(n = sample_size, min = -1, max = 1)

   data.frame(x, y)

###  

### Different Types of Cross-Validation

####  

#### 1. The Validation set Approach

A validation set is a set of data for training artificial intelligence with the goal of finding and optimizing the best model to solve a given problem. Validation sets are also known as dev sets. Training sets make up the majority of the total data, averaging 60 per cent. In testing, the models have parameters in a process that is known as adjusting weights.

The validation set makes up about 20 per cent of the bulk of data used. The validation set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it.

Testing sets make up 20 per cent of the bulk of the data. These sets are ideal data and results with which to verify the correct operation of an AI. The test set is the input data grouped together with verified correct outputs, generally by human verification. This ideal setting is for testing the results and assess the performance of the final model.



||set.seed(42)sim_data = gen_sim_data(sample_size = 200)sim_idx  = sample(1:nrow(sim_data), 160)sim_trn  = sim_data[sim_idx, ]sim_val  = sim_data[-sim_idx, ] fit = lm(y ~ poly(x, 10), data = sim_trn)|

sim_data = gen_sim_data(sample_size = 200)

sim_trn  = sim_data[sim_idx, ]

 

####  

#### 2. Leave Out One Cross-Validation

Leave-one-out cross-validation is K-fold cross-validation to its logical extreme, with K equal to N, the number of data points in the set. That means that N separate times, we need to run the function approximator on all the data except for one point. Then, we make a prediction for that point. As before the average error is computed and used to evaluate the model. The evaluation given by leave-one-out cross-validation error (LOO-XVE) is good, but at first pass, it seems very expensive to compute. Fortunately, locally weighted learners can make LOO predictions just as easily as they make regular predictions. That means computing the LOO-XVE takes no more time than computing the residual error and it is a much better way to evaluate models. 



||# Define training controltrain.control = trainControl(method = "LOOCV") # Train the modelmodel = train(y ~., data = sim_train, method = "lm", trControl = train.control) # Summarize the resultsprint(model)|

train.control = trainControl(method = "LOOCV")

# Train the model

 

####  

#### 3. k-fold Cross-Validation

K-fold cross-validation is one way to improve the holdout method. The data set is divided into *k* subsets, and the holdout method is repeated *k* times. Each time, one of the *k* subsets is used as the test set and the other *k-1* subsets are put together to form a training set. Then the average error across all *k* trials is computed. Furthermore, the advantage of this method is that it matters less how the data gets divided. Also, every data point gets to be in a test set exactly once and gets to be in a training set *k-1* times.

The variance of the resulting estimate is reduced as “*k”* is increased. Moreover, the disadvantage of this method is that the training algorithm has to be rerun from scratch *k* times, which means it takes *k* times as much computation to make an evaluation. Additionally, a variant of this method is to randomly divide the data into a test and training set *k* different times. The advantage of doing this is that you can independently choose how large each test set is and how many trials you average over.



||### KFOLD## Define training control set.seed(123)train.control = trainControl(method = "cv", number = 10) # Train the modelmodel = train(y ~., data = sim_data, method = "lm",trControl = train.control) # Summarize the resultsprint(model)|

## Define training control

set.seed(123)

 

model = train(y ~., data = sim_data, method = "lm",trControl = train.control)

# Summarize the results

####  

#### 4. Adversarial Validation

The general idea is to check the degree of similarity between training and tests in terms of feature distribution: if they are difficult to distinguish, the distribution is probably similar and the usual validation techniques should work. It does not seem to be the case, so we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0/1 labels (0 — train, 1-test) and evaluating a binary classification task.

####  

#### 5. Stratified k-fold Cross Validation

Stratification is the process of rearranging the data to ensure each fold is a good representative of the whole. For example in a binary classification problem where each class comprises 50% of the data, it is best to arrange the data such that in every fold, each class comprises around half the instances. Furthermore, the intuition behind this relates to the bias of most classification algorithms. Also, they tend to weight each instance equally which means overrepresented classes get too much weight (e.g. optimizing F-measure, Accuracy or a complementary form of error). Stratification is not so important for an algorithm that weights each class equally



||data(labels); examples.index = 1:nrow(L); examples.name = rownames(L); positives= which(L[,3]==1); x = do.stratified.cv.data.single.class(examples.index, positives, kk=5, seed=23); y = do.stratified.cv.data.single.class(examples.name, positives, kk=5, seed=23);  do.stratified.cv.data.over.classes(L, examples.index, kk=5, seed=23); |

examples.index = 1:nrow(L); 

positives= which(L[,3]==1); 

y = do.stratified.cv.data.single.class(examples.name, positives, kk=5, seed=23); 

do.stratified.cv.data.over.classes(L, examples.index, kk=5, seed=23); 

###  

### Limitations of Cross-Validation

We have seen what cross-validation in **machine learning** is and understood the importance of the concept. It is a vital aspect of machine learning, but it has its limitations.

- In an ideal world, the cross-validation will yield meaningful and accurate results. However, the world is not perfect. You never know what kind of data the model might encounter in the future.


- Usually, in predictive modelling, the structure you study evolves over a period. Hence, you can experience differences between the training and validation sets. Let us consider you have a model that predicts stock values.


- Here is another example where the limitation of the cross-validation process comes to the fore. You develop a model for predicting the individual’s risk of suffering from a particular ailment. However, you train the model using data from a study involving a specific section of the population. The moment you apply the model to the general population, the results could vary a lot.


###  

### Summary

In this blog, we had a look at cross-validation and it’s importance. Also, we had a small peek at different types of cross-validation techniques. Furthermore, we did look at the implementations of respective techniques in R.

**Follow this link**, if you are looking to **learn data science online!**

You can **follow this link for our Big Data course!**

Additionally, if you are having an interest in **learning Data Science, click here to start the Online Data Science Course**

Furthermore, if you want to read more about data science, read our **Data Science Blogs**






 
