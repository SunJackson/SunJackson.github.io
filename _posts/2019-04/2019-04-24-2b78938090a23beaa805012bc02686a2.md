---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/04/24/distilled-news-1046/
date:      2019-04-24
author:      Michael Laux
tags:
    - modeling
    - data
    - learns
    - generative models
    - machine learning
---

**Process Mining (Part 1/3): Introduction to bupaR package**

The digitalization of healthcare is more than just electronic medical records. It has also allowed each instance a clinician conducts an activity for a patient to be stored as a log. These logs stored in the healthcare institution’s information system are known as event logs.

**Process Mining (Part 2/3): More on bupaR package**

We will use a pre-loaded dataset sepsis from the bupaR package. This event log is based on real life management of sepsis from the point of admission to discharge.

**Quick Example of Latent Profile Analysis in R**

Latent Profile Analysis (LPA) tries to identify clusters of individuals (i.e., latent profiles) based on responses to a series of continuous variables (i.e., indicators). LPA assumes that there are unobserved latent profiles that generate patterns of responses on indicator items. Here, I will go through a quick example of LPA to identify groups of people based on their interests/hobbies. The data comes from the Young People Survey, available freely on Kaggle.com.

**What Is Latent Class Analysis?**

One of the most common – and one of the trickiest – challenges in data analysis is deciding how to include multiple predictors in a model, especially when they’re related to each other. Here’s an example. Let’s say you are interested in studying the relationship between work spillover into personal time as a predictor of job burnout. You have 5 categorical yes/no variables that indicate whether a particular symptom of work spillover is present (see below). While you could use each individual variable, you’re not really interested if one in particular is related to the outcome. Perhaps it’s not really each symptom that’s important, but the idea that spillover is happening. One possibility is to count up the number of items to which each respondent said yes. This variable will measure the degree to which spillover is happening. In many studies, this is just what you need. But it doesn’t tell you something important – whether there are certain combinations that generally co-occur, and is it these combinations that affect burnout? In other words, what if it’s not just the degree of spillover that’s important, but the type?

**Tchap**

French government’s open source secure encrypted chat tool, built off the open source Riot.

**SpaceSheet – Interactive Latent Space Exploration through a Spreadsheet Interface**

Generative models capture properties and relationships of images in a generic vector space representation called a latent space. Latent spaces can be sampled to create novel images and perform semantic operations consistent with the principles inferred from the training set. Designers can use representations learned by generative models to express design intent, enabling more effective design experimentation. We present the SpaceSheet, a general-purpose spreadsheet interface designed to support the experimentation and exploration of latent spaces.

**Review: SqueezeNet (Image Classification)**

In this story, SqueezeNet, by DeepScale, UC Berkeley and Stanford University, is reviewed. With equivalent accuracy, smaller CNN architectures offer at least three advantages1. Smaller Convolutional Neural Networks (CNNs) require less communication across servers during distributed training.2. Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car.3. Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory.

**Extracting knowledge from knowledge graphs using Facebook Pytorch BigGraph.**

Machine learning gives us the ability to train a model, which can convert data rows into labels in such a way that similar data rows are mapped to similar or the same label. For example, we are building SPAM filter for email messages. We have a lot of email messages, some of which are marked as SPAM and some as INBOX. We can build a model, which learns to identify the SPAM messages. The messages to be marked as SPAM will be in some way similar to those, which are already marked as SPAM. The concept of similarity is vitally important for machine learning. In the real world, the concept of similarity is very specific to the subject matter and it depends on our knowledge. Majority of mathematical models, on the other hand, assume that the concept of similarity is defined. Typically, we represent our data as multidimensional vectors and measure the distance between vectors.

**Data Augmentation library for text**

In previous story, you understand different approaches to generate more training data for your NLP task model. In this story, we will learn how can you it do it with just a few line codes.

**Probabilistic Programming Journal 1: Modeling event change**

Our goal is to understand whether our data has changed over time. The data we are working with is related to text’s. The time period where we observed and collected data is for a total 74 days, the data loaded is in the form of a vector that contains observations of texts per day. We generate a complimentary vector that records the day for each point in the vector. This can be seen in the code above where we use the numpy arrange function. Our goal is to determine whether there were changes in trends. This task done by a human would be pretty challenging because eyeballing the data does not help determining the changes at all. So we will use some probabilistic modeling to tackle this issue. How can we model this data? Well one of the choices we have is using a Poisson model. Why? Because this type of model works well with count data.

**Democratising Machine learning with H2O**

I have been experimenting with H2O for quite some time and found it really seamless and intuitive for solving ML problems. Seeing it perform so well on Leaderboard, I thought it was time that I wrote an article on the same to make it easy for others to make a transition into the world of H2O.

**Autoencoders: Deep Learning with TensorFlow’s Eager API**

Deep Learning has revolutionized the Machine Learning scene in the last years. Can we apply it to image compression? How well can a Deep Learning algorithm reconstruct pictures of kittens? What’s an autoencoder? Today we’ll find the answers to all of those questions.

**Visualizing Twitter interactions with NetworkX**

Social media is used every day for many purposes: expressing opinions about different topics such as products and movies, advertising an event, a service or a conference, among other things. But what is most interesting about social media, and particularly for this post, about Twitter, is that it creates connections; networks that can be studied to understand how people interact or how news and opinions get spread.

**Building a Collaborative Filtering Recommender System with ClickStream Data**

How to implement a recommendation algorithm based on prior implicit feedback.

**Building a private, local photo search app using machine learning**

This is it. This is the best goddamn thing I’ve ever done. I don’t normally like to brag but I’m so freak’n proud of myself for this one that I feel like I need to share it. They said it wasn’t possible (no one actually said that), they said it couldn’t be done (lots of people said it could be done), but I did it and it works GREAT!

**Software development best practices in a deep learning environment**

Deep learning systems are now being used extensively in many environments. They differ from traditional software systems in the manner through which output is generated: decisions which are made to produce results are learned from training data, rather than being hand-coded, as is the case with traditional software systems. This has led people to wrongly believe that software development is not as necessary for machine learning systems, since the algorithms, which are at the heart of any substantial system, are learned by the system itself. Furthermore, state-of-the-art machine learning (ML) is advancing so rapidly that data scientists are focusing primarily on short term goals like model precision. Given that the deep learning world will have changed within a matter of months, why waste time properly engineering your system today? Currently, the lie of the land in deep learning engineering is that a system is experimented together until some precision goal is achieved, and then is put into production.

**Introducing Meta Reward Learning**

Reinforcement learning has been at the center of some of the biggest artificial intelligence(AI) breakthroughs of the last five years. In mastering games like Go, Quake III or StarCraft, reinforcement learning models demonstrated that they can surpass human performance and create unique long-term strategies never explored before. Part of the magic of reinforcement learning relies on regularly rewarding the agents for actions that lead to a better outcome. That models works great in dense reward environments like games in which almost every action correspond to a specific feedback but what happens if that feedback is not available? In reinforcement learning this is known as sparse rewards environments and, unfortunately, it’s a representation of most real-world scenarios. Recently, researchers from Google published a new paper proposing a technique for achieving generalization with reinforcement learning that operate in sparse reward environments.





### Like this:

Like Loading...


*Related*

