---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/04/11/distilled-news-1032/
date:      2019-04-11
author:      Michael Laux
tags:
    - bert
    - data
    - algorithms
    - images
    - learning
---

**Simple Neural Network on MCUs**

Edge computing is one of those things where you have the nails and are still looking for a hammer. In an earlier post, I wrote about Why Machine Learning on the Edge is critical. Pete Warden has also shared interesting insights in Why The Future of Machine Learning is Tiny. There will be many exciting technologies coming out to accelerate the development in this space. Today, we are going to look at how to deploy a neural network (NN) on a microcontroller (MCU) with uTensor.

**Generating critical scenarios using Anomaly Detection**

Say you are developing a self driving car or the software for one(which I agree is a pretty big assumption) and your first prototype is ready which you want to test thoroughly. One way is to run a lot of test cars and check how the system is responding to its environments which is known as validation of a system. But this can be be very capital intensive and while you should be doing it, there is an easier or lets say cheaper way to do it, which is through simulation which can generate huge amounts of data very quickly. In both cases, you get plenty of scenarios on which you can test your system but does it make sense to subject the self driving software to loads and loads of regular conditions? We need to test the software on critical scenarios which is the real test for an autonomous system but critical scenarios are generated less than 1% of the time. Here, I propose a technique called anomaly detection which can be used to filter critical scenarios from huge chunks of data.

**Dimensionality Reduction toolbox in python**

In recent years, the volume of data has exploded by more than 80%. This has led to the emergence of many models of Machine Learning, since it is easier to train these models with an important dataset. However, the weight of the data can have a significant impact on the time of implementation of an algorithm since the complexity increases with the size of the data. In parallel, dimensionality can complicate the viewing of information contained in a database. Dimensionality reduction is the process of reducing the total number of features in our feature set using strategies like feature selection or feature extraction.

**Sentiment analysis for text with Deep Learning**

I started working on a NLP related project with twitter data and one of the project goals included sentiment classification for each tweet. However when I explored the available resources such as NLTK sentiment classifier and other resource available in python, I was disappointed by the performance of these models. At most I would get about 60% to 70% accuracy on binary classification (i.e only positive or negative class) tasks. Hence I started researching about ways to increase my model performance. One of the obvious choices was to build a deep learning based sentiment classification model.

**Some examples of applying BERT in specific domain**

Several new pre-trained contextualized embeddings are released in 2018. New state-of-the-art results is changing every month. BERT is one of the famous model. In this story, we will extend BERT to see how we can apply BERT on different domain problem.

**Dissecting BERT Part 1: The Encoder**

This is Part 1/2 of Dissecting BERT written by Miguel Romero and Francisco Ingham. Each article was written jointly by both authors. If you already understand the Encoder architecture from Attention is All You Need and you are interested in the differences that make BERT awesome, head on to BERT Specifics. In this blog post, we are going to examine the Encoder architecture in depth (see Figure 1) as described in Attention Is All You Need. In BERT Specifics we will dive into the novel modifications that make BERT particularly effective.

**Convolutional Neural Networks – Simplified**

Take a moment to observe and look at your surroundings. Even if you are sitting still on your chair or lying on your bed, your brain is constantly trying to analyze the dynamic world around you. Without your conscious effort your brain is continuously making predictions and acting upon them.

**Top 5 Interesting Applications of GANs for Every Machine Learning Enthusiast!**

The term ‘GAN’ was introduced by the Ian Goodfellow in 2014 but the concept has been around since as far back as 1990 (pioneered by Jürgen Schmidhuber). But it was only after Goodfellow’s paper on the subject that they gained popularity in the community. And since then, there’s been no looking back for GANs! In fact, GANs are now ubiquitous. Data scientists and deep learning researchers use this technique to generate photorealistic images, change facial expressions, create computer game scenes, visualize designs, and more recently, even generate awe-inspiring artwork!

**Implicit Self-Regularization in Deep Neural Networks**

This video is a recap of our February 2019 Americas TWiML Online Meetup. In this month’s community segment, we discuss the first TWiML Live – The OpenAI Panel discussion, the last Americas meetup, and the OpenAI’s GPT-2 language model. In our presentation segment, Charles Martin, PhD, leads a discussion on ‘Implicit Self-Regularization in Deep Neural Networks’ with the paper on ‘Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning’ by himself and Michael W. Mahoney.

**XGBoost Algorithm: Long May She Reign!**

XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured/tabular data, decision tree based algorithms are considered best-in-class right now. Please see the chart below for the evolution of tree-based algorithms over the years.

**Earth mover’s distance**

The way semantic search engines retrieve results from their ontology is by computing the closest / most similar documents to the query. In order to get the top most similar documents, the algorithm computes the distance between the query and many documents. When the similarities are obtained, the documents are ranked accordingly and the ones at the top are retrieved.

**A Simple CNN: Multi Image Classifier**

Computer vision and neural networks are the hot new IT of machine learning techniques. With advances of neural networks and an ability to read images as pixel density numbers, numerous companies are relying on this technique for more data. For example, speed camera uses computer vision to take pictures of license plate of cars who are going above the speeding limit and match the license plate number with their known database to send the ticket to. Although this is more related to Object Character Recognition than Image Classification, both uses computer vision and neural networks as a base to work. A more realistic example of image classification would be Facebook tagging algorithm. When you upload an album with people in them and tag them in Facebook, the tag algorithm breaks down the person’s picture pixel location and store it in the database. Because each picture has its own unique pixel location, it is relatively easy for the algorithm to realize who is who based on previous pictures located in the database. Of course the algorithm can make mistake from time to time, but the more you correct it, the better it will be at identifying your friends and automatically tag them for you when you upload. However, the Facebook tag algorithm is built with artificial intelligence in mind. This means that the tagging algorithm is capable of learning based on our input and make better classifications in the future. We will not focus on the AI aspect, but rather on the simplest way to make an image classification algorithm. The only difference between our model and Facebook’s will be that ours cannot learn from it’s mistake unless we fix it. However, for a simple neural network project, it is sufficient.

**Comparing Text Summarization Techniques**

Hey everyone! First off, I want to say thanks for stopping by to read my post. This is my first post on Medium so I’m excited to gather your feedback. The motivation behind this post was to provide an overview of various Text Summarization approaches while providing the tools and guidance necessary to build one yourself. Hope you enjoy!Text Summarization is an increasingly popular topic within NLP and, with the recent advancements in modern deep learning, we are consistently seeing newer, more novel approaches. The goal of this article is to compare the results of a few approaches that I experimented with:1. Sentence Scoring based on Word Frequency2. TextRank using Universal Sentence Encoder3. Unsupervised Learning using Skip-Thought Vectors





### Like this:

Like Loading...


*Related*

