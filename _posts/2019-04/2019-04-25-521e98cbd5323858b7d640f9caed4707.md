---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/04/25/distilled-news-1048/
date:      2019-04-25
author:      Michael Laux
tags:
    - data
    - cloud
    - networks
    - media
    - learning
---

**Hopfield Networks are useless. Here’s why you should learn them.**

Hopfield networks were invented in 1982 by J.J. Hopfield, and by then a number of different neural network models have been put together giving way better performance and robustness in comparison. To my knowledge, they are mostly introduced and mentioned in textbooks when approaching Boltzmann Machines and Deep Belief Networks, since they are built upon Hopfield’s work. Nevertheless, they provide such a different and alternative perspective on learning systems compared to the current state of deep learning that they are worth understanding, if anything, for the sake of it. Let’s find out how they work.

**Recombinant Programming: A New Kind of Programming**

I want to discuss with you today four different kinds of programming. This framework should help you better understand the uniqueness of Deep Learning as a new way of programming. I characterize Deep Learning as a technique for programming massively parallel computer systems to perform useful tasks. We will explore here why this is revolutionary and is a new kind of programming. In most introductory computer programming classes, we are introduced to the idea of imperative programming. All programming can be understood in the abstract sense as a kind of specification. Imperative programming is a specification that tells a computer the exact and detailed sequence of steps to perform. These also will include conditions to test, processes to execute and alternative paths to follow (i.e., conditions, functions, and loops). All of the more popular languages we have heard of (i.e., JavaScript, Java, Python, C etc.) are all imperative languages. When a programmer writes are program, he formulates in his mind the exact sequence of a task that needs to be composed to arrive at a solution. Von Neumann computer architectures are intrinsically capable of interpreting and executing imperative code.

**Ant Media Server**

Ant Media Server is an open source media server that supports:• Ultra Low Latency Adaptive One to Many WebRTC Live Streaming in Enterprise Edition• Adaptive Bitrate for Live Streams (WebRTC, MP4, HLS) in Enterprise Edition• SFU in One to Many WebRTC Streams in Enterprise Edition• Live Stream Publishing with RTMP and WebRTC• WebRTC to RTMP Adapter• IP Camera Support• Recording Live Streams (MP4 and HLS)• Restream to Social Media Simultaneously(Facebook and Youtube in in Enterprise Edition)• One-Time Token Control in Enterprise Edition• Object Detection in Enterprise Edition

**Fusing data and design to supercharge innovation – in products and processes**

Often in business, the biggest breakthroughs come when organizations connect different, and seemingly disparate, ideas and approaches. Diversity of perspective has been shown time and again to foster fresh thinking to solve tough problems. Such is the promise for organizations that can effectively bring together data and design. Data are often prized for their indifference to intuition, stoic reflection of the facts, and ability to shatter our assumptions – veritable superpowers when put into the hands of decision makers. While data have always been an important business input, recent advances in artificial intelligence (AI) and other analytics are only increasing the number of organizational arenas in which decision makers can activate data’s superpowers, from hiring to product development to customer engagement. Separately, design thinking has spread like wildfire across industries after some iconic brands and born-digital companies (think Apple and Google) demonstrated the revenues and customer satisfaction it could drive. Harnessing qualitative insights, creativity, and a relentless focus on end-user needs, the approach is typically aimed at product and service innovation.

**Minimizing Cloud Data Processing Costs**

Data-driven organizations are leveraging the unbounded elastic nature of the cloud to process petabytes of data stored in cloud data lakes. This often leads to high cloud costs that can run into millions, if checks and balances are not in place.With this comprehensive guide, data leaders can understand how 3 basic levers – Cost control, Traceability, and Predictability – are crucial for effective financial governance. You’ll also be able to:• Learn how financial governance works for cloud data lakes.• Explore available cloud service provider tools, third-party cloud management tools, and the new breed of cloud-native data platforms.• Understand how to control cloud costs while still meeting rising business demand and SLAs.Download this free ebook today to learn more!

**Features and machine learning systems for structured and sequential data**

Modern web and communication technology relies heavily on sequential and structured data for its process execution and communication protocols. Due to its complex properties, a manual analysis and detection of problems on this data is too time-consuming and expensive, and hence not feasible. As a consequence, features and automatic learning systems on this type of data are highly sought after. To address these issues, the thesis proposes features and systems for learning on structured, sequential and temporal data, both in abstract and in concrete form, with a focus on analyses in the fields of IT security and Quality of Service, on the data domains of analysis data of malware binaries and JavaScript code, as well as on mobile network communication data. The proposed features and feature combinations cover various statistical, non-behavioral and behavioral, stateless, stateful, structural and temporal concepts, and are used individually and in a complementary manner, e.g. via hierarchical or ensemble approaches. The proposed learning systems are evaluated against competitive approaches, where they outperform commonly used and state-of-the-art methods, including approaches using neural networks. Specific practically relevant aspects are also addressed in depth, like high levels of automation to extend the scope of the system application, different re-training procedures, or the calibration of metrics relevant for the specific domain. To improve the interpretability of the system processes and their results and to increase the system reliability and its level of trust, different visualization approaches are proposed, focussing on interpretable and transparent feature projections and relevance analyses. These additional discussions on the proposed ideas further support a potential adaptation of the proposed ideas to concrete application scenarios.

**Can Blockchain Tame AI’s Dark Creative Impulses?**

How can we trust what we see and hear on our digital devices? Beyond the tiresome meme of fake news, something more fundamental has been sneaking up on us for a few years without generating the sort of alarm it deserves. These days it’s hard to hear any alarms over the general din of the contemporary news cycle, but media people and marketers must take heed of technology that comes under the academic heading of ‘generative AI’ or, more specifically, generative content or image synthesis. This is the practice of training and using computers to generate realistic video, audio and images – usually of people doing and saying things they never did or said – often for ‘entertainment,’ but surely for darker purposes as well. You may have seen it flare-up in the notorious trend of so-called #Deepfakes, which erupted on Reddit last year before being banned on Reddit and a number of other sites. But popular apps like FakeApp (based on Google’s popular TensorFlow platform) have made widespread public access to this technology inevitable. Further research in generative adversarial networks (GANs), as well, assure us that the capabilities of AI to produce audio-visual evidence that’s all but indistinguishable from reality are similarly unavoidable. The anxiety we feel today over polarized discord regarding what’s real and what’s fake pales in comparison to the society we face when we literally can’t distinguish between real and synthetic evidence. Science, law, and government all depend on this ability, as does our personal security, but commerce and marketing have a special role to play as they provide the economic force that powers today’s global media distribution platforms. More on that in a minute.

**Machine Learning and Deep Link Graph Analytics: A Powerful Combination**

Machine learning (ML) – an aspect of artificial intelligence (AI) that allows software to accurately identify patterns and predict outcomes – has become a hot industry topic. With ever-increasing advances in data analysis, storage, and computing power in the last few years, machine learning has been playing an increasingly important role in enterprise applications such as fraud prevention, personalized recommendation, predictive analytics, and so on. Applying graph database capabilities to ML and AI apps is relatively new, however. That’s surprising in light of the fact that Google’s Knowledge Graph, which first popularized the concept of finding relationships within data to yield more relevant and precise information, dates back to 2012. Also, it’s a natural fit: Graphs are ideal for storing, connecting, and making inferences from complex data. The main reason that graphs have not played an important role in ML is that legacy graph databases cannot deliver what is really needed for machine learning: deep link graph analytics for large datasets. Let’s take a deeper dive into how graphs can help machine learning and how they are related to deep link graph analytics for Big Data.

**Translating the Kenyan Sign Language with Deep Learning**

In July 2016, I met Hudson – a brilliant deaf developer who had just built an app to teach his deaf community sexual health. We became fast friends. Since then, my circle of deaf friends and acquaintances has grown. I even got a sign name. In the deaf community, it’s easier to assign someone a sign from a unique characteristic rather than finger spelling their name every time. But there was a problem, for a long time I could not communicate well enough with this growing circle of deaf friends and acquaintances, because I had not learned sign language.

**Key Kubernetes Commands**

Kubernetes is the premier technology for deploying and manage large apps. In this article, we’ll get up and running with K8s on your local machine. Then you’ll Deploy your first app. Finally, you’ll see the top K8s commands to know. If you’re new to Kubernetes, check out my previous article to learn fundamental K8s concepts. Let’s set sail!

**Siamese Dream – A Beginners Guide to Tracking with Siamese Neural Networks**

You’re sitting on your couch, dog at your feet, one hand completely immersed in a bag of White Cheddar Smartfood. Netflix is on the TV and you’re just passing minute 48 of the IMDB rated 6.4/10, thriller Surveillance. You watch as the main character tries to evade the watching eye of the police. Poring over endless frames of CCTV surveillance footage, they follow every move of the antagonist without touching a single button. You can’t help but think. They can’t actually do this, can they? The answer to that question is yes, they can. This example depicts one of the many possible applications within the research of the popular scientific discipline of computer vision. A growing area of research, that has seen an explosion of use cases and promising algorithms. Such algorithms include siamese neural networks, that can implement these as seen on TV results.

**Probability is not Predictability**

Suppose you are watching the nightly news (or more likely your phone just tells you) that tomorrow there is a 30% chance of rain. Tomorrow, do you carry an umbrella? It’s a seemingly benign question, but the way we interpret probabilities has real ramifications for how we make decisions and business and in life. It In the following, I’ll break down what probabilities and statistics like this mean at their core, and how we make decisions around this uncertainty.

**TensorFlow 2.0 in 5 Minutes (tutorial)**

At the TensorFlow Dev Summit 2019, Google introduced the alpha version of TensorFlow 2.0. The new version, was redesigned with a focus on developer productivity, simplicity, and ease of use. There are multiple changes in TensorFlow 2.0 to make its users more productive. It includes many API changes, such as reordering arguments, removing redundant APIs, renaming symbols, and changing default values for parameters. In this article I will try to summarise some of the few changes worth noticing.





### Like this:

Like Loading...


*Related*

