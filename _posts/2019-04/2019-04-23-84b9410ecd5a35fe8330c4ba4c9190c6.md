---
layout:     post
catalog: true
title:      If you did not already know
subtitle:      转载自：https://analytixon.com/2019/04/23/if-you-did-not-already-know-709/
date:      2019-04-23
author:      Michael Laux
tags:
    - learning
    - learned
    - trees
    - dndt
    - methods
---

**Hamming Distance** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. … 

**Lifelong Learning (LL)** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction. … 

**Deep Neural Decision Tree (DNDT)** ![](https://analytixon.files.wordpress.com/2015/01/google.png?w=529)
Deep neural networks have been proven powerful at processing perceptual data, such as images and audio. However for tabular data, tree-based models are more popular. A nice property of tree-based models is their natural interpretability. In this work, we present Deep Neural Decision Trees (DNDT) — tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting. We evaluate DNDT on several tabular datasets, verify its efficacy, and investigate similarities and differences between DNDT and vanilla decision trees. Interestingly, DNDT self-prunes at both split and feature-level. … 





### Like this:

Like Loading...


*Related*

