---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/04/29/distilled-news-1050/
date:      2019-04-29
author:      Michael Laux
tags:
    - data
    - learning
    - businesses
    - visualize
    - visually
---

**Deep Learning Tutorial: Build an Image Recognizer on your dataset in less than 10 minutes**

This tutorial is a concise step-by-step guide for building and training an Image Recognizer on any image dataset of your choice. In this tutorial, you will learn how to:• Scrape images from Google Images and create your own dataset• Build and train an image recognizer on your dataset• Visualize and adequately interpret classification results• Test model with new imagesTo run this notebook, you can simply open it with Google Colab here. Once in Colab, make sure to change the following to enable GPU backend, Runtime -> Change runtime type -> Hardware Accelerator -> GPU

**Counting Pizza: Metrics for Machine Learning**

Visually understanding machine learning metrics. It’s important to understand the right metrics to make business decisions using machine learning. Classification says whether something is something else: say whether an image contains a pizza slice. Precision and recall deal with quantifiable results for classification that translate into business models. There’s no requirement to understand how machine learning actually works. Anyone that touches a business model which involves machine learning should understand these metrics. Founders, management, business development, and investors included. It’s important for a decision maker to give the data science team the appropriate metric for the business model. And I hope to show you that anyone who can count can understand which metric to use. Say you want to diagnose a rare disease.• How often are you correct when you say that a patient has a disease?• How many cases of the disease do you miss?Answers to questions like these fuel a business model. Precision and recall are the two respective answers. This article aims to give intuition for these important metrics. And intuition often comes best when we can put an image to a concept. So let’s consider a visual example: how many pizza slices are in the picture below?

**How will the Data Scientist’s job change through automated machine learning?**

Automated machine learning is a fundamental shift to machine learning and data science. Data science as it stands today, is resource-intensive, expensive and challenging. It requires skills which are in high demand. Automated Machine learning may not quite lead to the beach lifestyle for the data scientist – but automated machine learning will fundamentally change the job of a data scientist. It’s an irony that AI / ML could replace many jobs – and the first it seems is that of the data scientist himself! But we have been there before. In the 90s and 2000 we had CASE tools. Managers loved them because they were supposed to replace those expensive Programmers. That has not happened. So, would automated machine learning be any different?

**Visualizing Relationships between Loss Functions and Gradient Descent**

Gradient Descent working on Loss/Activation functions. This article assumes you have prior knowledge on training Neural Net and seeks to demystify the relationships between Loss function, Gradient Descent and Backpropagation through visualisation.

**Graphs in Spreadsheets**

In this tutorial, you’ll learn how to create visualizations to display data and gain more meaningful insights with spreadsheets. Even though graphs do not need any introduction, but to put it simply, graphs are a graphical representation of various points of a domain. Graphs help you view and analyze your data in a more realistic and simplified way. Graphs, Plots, and Charts can all be used interchangeably since they all mean more or less the same. A graph can tell you a lot about your data and makes it easy to understand much like A picture is worth a thousand words!

**How I Built Animated Plots in R to Analyze my Fitness Data (and you can too!)**

We are currently in the midst of a global fitness revolution. Most of the people I know are geeking out over the latest gadget in the market that will help them achieve their fitness goals. My focus, however, is slightly different. I can’t help it – I’m a data scientist after all! All these fitness trackers, bands, even our smartphones – they all store our health data via certain applications, like Healthkit on iOS, Google Fit on Android, etc. We are literally a few touches away from accessing our health data – distance covered, steps-taken, calories burned, heart rate, etc.

**Naive Bayes in One Picture**

Naive Bayes is a deceptively simple way to find answers to probability questions that involve many inputs. For example, if you’re a website owner, you might be interested to know the probability that a visitor will make a purchase. That question has a lot of ‘what-ifs’, including time on page, pages visited, and prior visits. Naive Bayes essentially allows you to take the raw inputs (i.e. historical data), sort the data into more meaningful chunks, and input them into a formula.

**Concept Drift and Model Decay in Machine Learning**

Concept drift is a drift of labels with time for the essentially the same data. It leads to the divergence of decision boundary for new data from that of a model built from earlier data/labels. Scoring randomly sampled new data can detect the drift allowing us to trigger the expensive re-label/re-train tasks on an as needed basis…

**A Complete Guide to Learn R**

R Programming Technology is an open source programming language. Also, the R programming language is the latest cutting-edge tool. R Basics is the hottest trend. Moreover, the R command line interface (C.L.I) consists of a prompt, usually the > character.

**Artificial and Human Intelligence**

Prescriptive Analytics is the area of Artificial Intelligence dedicated to prescribe best possible next actions. It relies on a set of techniques which I will illustrate using a simple and familiar problem everyone knows: packing luggage. My wife and kids might not know it, but they are using Business Rules (BR), Machine Learning (ML), and Decision Optimization (DO).

**What is the Difference Between Hadoop and Spark?**

Hadoop and Spark are software frameworks from Apache Software Foundation that are used to manage ‘Big Data’. There is no particular threshold size which classifies data as ‘big data’, but in simple terms, it is a data set that is too high in volume, velocity or variety such that it cannot be stored and processed by a single computing system. Big Data market is predicted to rise from $27 billion (in 2014) to $60 billion in 2020 which will give you an idea of why there is a growing demand for big data professionals. The increasing need for big data processing lies in the fact that 90% of the data was generated in the past 2 years and is expected to increase from 4.4 zb (in 2018) to 44 zb in 2020. Let’s see what Hadoop is and how it manages such astronomical volumes of data.

**What is AI Bias? , How it affects?**

Bias trouble in AI – Definition – Bias, is very much diverse which keeps varying in a different context(domain). So now, will come to point, What is AI Bias? AI systems are only as good as the data we put into them. But, what if the AI algorithm is trained with bad data containing implicit racial, gender, or ideological biases. For Example: Say you’re training an image recognition system to identify any country presidents. The historical data reveals a pattern of males, so the algorithm concludes that only males are presidents. It won’t recognise a female in that role, even though it’s a probable outcome in future elections. It’s important to remember, though, that AI can cement misconceptions in how a task is addressed, or magnify existing inequalities. This can happen even when no one told the algorithm explicitly to treat anyone differently. Now, How to deal with data-driven bias at an early stage of Data Engineering?

**How a Data Catalog Connects the What and the Why**

Businesses today increasingly turn to big data and advanced analytics for their most troubling questions. The belief that if you get lots of data, a system to handle it (hardware, software), and a few smart people, the data will magically reveal insights. But having more data doesn’t usually mean you have better data. A lot of bad data (unreliable reporting, missing observations) means a lot of noise which has to be filtered down. Second, having more data doesn’t help you determine causality between variables of interest. Data can tell you what is happening, but it will rarely tell you why. To effectively bring together the what and the why – a problem and its cause, in order to find a probable solution – leaders need to combine the advanced capabilities of big data and analytics with tried-and-true qualitative approaches such as human collaboration, interviewing teams, conducting focus groups, and in-depth observation. What can serve as a platform to do all this human collaboration? A data catalog.

**Hyperparameter optimization in python. Part 1: Scikit-Optimize.**

In this blog series, I am comparing python HPO libraries. Before reading this post, I would highly advise that you read Part 0: Introduction where I:• talked about what HPO is,• selected libraries to compare,• selected the evaluation criteria,• defined an example problem for the HPO.Code for this blog post and other parts of the series is available on github while all the experiments with scripts, hyperparameters, charts, and results (that you can download) are available for you on Neptune.Without further ado, let’s dive in, shall we?





### Like this:

Like Loading...


*Related*

