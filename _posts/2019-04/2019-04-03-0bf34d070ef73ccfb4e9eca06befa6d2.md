---
layout:     post
catalog: true
title:      Let’s get it right
subtitle:      转载自：https://analytixon.com/2019/04/03/lets-get-it-right-25/
date:      2019-04-03
author:      Michael Laux
tags:
    - biases
    - source bias
    - developers
    - developing
    - developments
---

***Article***: ***Aequitas***

An open source bias audit toolkit for machine learning developers, analysts, and policymakers to audit machine learning models for discrimination and bias, and make informed and equitable decisions around developing and deploying predictive risk-assessment tools.

***Article***: ***Skater***

Skater is a unified framework to enable Model Interpretation for all forms of model to help one build an Interpretable machine learning system often needed for real world use-cases(** we are actively working towards to enabling faithful interpretability for all forms models). It is an open source python library designed to demystify the learned structures of a black box model both globally(inference on the basis of a complete data set) and locally(inference about an individual prediction).

***Article***: ***AI Fairness 360***

The AI Fairness 360 toolkit (AIF360) is an open source software toolkit that can help detect and remove bias in machine learning models. It enables developers to use state-of-the-art algorithms to regularly check for unwanted biases from entering their machine learning pipeline and to mitigate any biases that are discovered. AIF360 enables AI developers and data scientists to easily check for biases at multiple points along their machine learning pipeline, using the appropriate bias metric for their circumstances. It also provides a range of state-of-the-art bias mitigation techniques that enable the developer or data scientist to reduce any discovered bias. These bias detection techniques can be deployed automatically to enable an AI development team to perform systematic checking for biases similar to checks for development bugs or security violations in a continuous integration pipeline.

***Paper***: ***The Algorithmic Automation Problem: Prediction, Triage, and Human Effort***

In a wide array of areas, algorithms are matching and surpassing the performance of human experts, leading to consideration of the roles of human judgment and algorithmic prediction in these domains. The discussion around these developments, however, has implicitly equated the specific task of prediction with the general task of automation. We argue here that automation is broader than just a comparison of human versus algorithmic performance on a task; it also involves the decision of which instances of the task to give to the algorithm in the first place. We develop a general framework that poses this latter decision as an optimization problem, and we show how basic heuristics for this optimization problem can lead to performance gains even on heavily-studied applications of AI in medicine. Our framework also serves to highlight how effective automation depends crucially on estimating both algorithmic and human error on an instance-by-instance basis, and our results show how improvements in these error estimation problems can yield significant gains for automation as well.

***Article***: ***Should We Adapt Infrastructure for Future Technology?***

As companies attempt to introduce self-driving cars, flying taxis, cargo drones, and delivery robots to the public sphere, they are constantly battling to adapt to the inconsistent world we’ve already created. So the question arises, should we modify our own world to better suit the autonomous technology of the future?

***Article***: ***De-biasing language***

In a recent paper, Hila Gonen and Yoav Goldberg argue that methods for de-biasing language models aren’t effective; they make bias less apparent, but don’t actually remove it. De-biasing might even make bias more dangerous by hiding it, rather than leaving it out in the open. The toughest problems are often the ones you only think you’ve solved. Language models are based on ‘word embeddings,’ which are essentially lists of word combinations derived from human language. There are some techniques for removing bias from word embeddings. I won’t go into them in detail, but for the sake of argument, imagine taking the texts of every English language newspaper and replacing ‘he,’ ‘she,’ and other gender-specific words with ‘they’ or ‘them.’ (Real techniques are more sophisticated, of course.) What Gonen and Goldberg show is that words still cluster the same way: stereotypically female professions still turn up as closely related (their example is nurse, caregiver, receptionist, and teacher).

***Book***: ***A Construction Manual for Robots’ Ethical Systems – Requirements, Methods, Implementations***

This book will help researchers and engineers in the design of ethical systems for robots, addressing the philosophical questions that arise and exploring modern applications such as assistive robots and self-driving cars. The contributing authors are among the leading academic and industrial researchers on this topic and the book will be of value to researchers, graduate students and practitioners engaged with robot design, artificial intelligence and ethics.

***Article***: ***The AI Ethics Deficit – 94% of IT Leaders Call for More Attention to Responsible and Ethical AI Development***

The results of a new study on AI ethics was released. According to research conducted by Vason Bourne on behalf of SnapLogic, studied the views and perspectives of IT decision-makers (ITDMs) across industries, asking key questions such as: who bears primary responsibility to ensure AI is developed ethically and responsibly, will global expert consortiums impact the future development of AI, and should AI be regulated and, if so, by whom?





### Like this:

Like Loading...


*Related*

