---
layout:     post
catalog: true
title:      Beyond Thunderspline： Mad Max： Affine Spline Insights into Deep Learning/ From Hard to Soft： Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference
subtitle:      转载自：http://feedproxy.google.com/~r/blogspot/wCeDd/~3/3XkdAg2wS58/beyond-thunderspline-mad-max-affine.html
date:      2019-04-29
author:      Igor (noreply@blogger.com)
tags:
    - vq distance
    - masos
    - nonlinearities
    - nonlinearity
    - progress
---


The following set of papers shows that most DNNs that have been working so far have specific kinds of nonlinearities that can be better understood thanks to the splines. Yes, peeeoppple, we are going back to the splines!



![](https://3.bp.blogspot.com/--k7UzeBZ3Ag/XMFawgKhSdI/AAAAAAAAWWY/kD8KnPAMwuQ1CeUcLiyfkdJ_isFKFkOsQCLcBGAs/s400/matchedfiltertemplate.png)









From the first paper, I like the fact that Breiman has an influence on something related to DNNs: 

> 
Leveraging a result from Breiman [1993], we prove in Appendix D.7 that the composition of
just two MASOs has a universal approximation property. 


Here is something of interest:




> 
5. DNs are Template Matching Machines
We now take a deeper look into the featurization/prediction process of (4.9) in order to bridge DNs and
classical optimal classification theory via matched filters (aka “template matching”). We will focus on
classification for concreteness.


Followed eventually by:




> 
Visual inspection of the figures highlights that, as
we progress through the layers of the DN, similar images become closer in VQ distance but further in
Euclidean distance. In particular, the similarity of the first 3 panels in (a)–(c) of the figures, which replicate the same experiment but using a DN trained with correct labels, a DN trained with incorrect labels,
and a DN that is not trained at all, indicates that the early convolution layers of the DN are partitioning
the training images based more on their visual features than their class membership.


Here are the three papers: 







> 
We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space that is implicitly induced by a MASO directly links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up new geometric avenue to study how DNs organize signals in a hierarchical fashion. To validate the utility of the VQ interpretation, we develop and validate a new distance metric for signals and images that quantifies the difference between their VQ encodings. (This paper is a significantly expanded version of A Spline Theory of Deep Learning from ICML 2018.)




> 
We build a rigorous bridge between deep networks (DNs) and approximation theory via spline
functions and operators. Our key result is that a
large class of DNs can be written as a composition
of max-affine spline operators (MASOs), which
provide a powerful portal through which to view
and analyze their inner workings. For instance,
conditioned on the input signal, the output of a
MASO DN can be written as a simple affine transformation of the input. This implies that a DN
constructs a set of signal-dependent, class-specific
templates against which the signal is compared
via a simple inner product; we explore the links to
the classical theory of optimal classification via
matched filters and the effects of data memorization. Going further, we propose a simple penalty
term that can be added to the cost function of any
DN learning algorithm to force the templates to
be orthogonal with each other; this leads to significantly improved classification performance and
reduced overfitting with no change to the DN architecture. The spline partition of the input signal
space opens up a new geometric avenue to study
how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a
new distance metric for signals that quantifies the
difference between their partition encodings



![](https://1.bp.blogspot.com/-qelNE4E5r50/XMFkX6dcjgI/AAAAAAAAWWk/HuUYfhuLHUUdBSG_AizlpRwtBgf4PvGrgCLcBGAs/s400/orthogonalityfilters.png)

![](https://1.bp.blogspot.com/-p0tPZ16GDA8/XMFlDYB0IAI/AAAAAAAAWWs/qkaBpxIWqgEV1ZF9Yg0sm_BDOp277vMXwCLcBGAs/s320/nonlinearitiesmaso.png)





> 
Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the r\^{o}le played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as {\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means. While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax. {\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).} We show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ``hard'' VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ``soft'' VQ inference problems. We further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference. A prime example of a $\beta$-VQ DN nonlinearity is the {\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation. Finally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.

















Other links:
