---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/04/29/distilled-news-1049/
date:      2019-04-29
author:      Michael Laux
tags:
    - statistical data
    - models
    - modeling
    - learning
    - causality
---

**The Three Pillars of Robust Machine Learning: Specification Testing, Robust Training and Formal Verification**

Building machine learning systems differs from traditional software development in many aspects of its lifecycle. Established software methodologies for testing, debugging and troubleshooting result simply impractical when applied to machine learning models. While the behavior of traditional software components like websites, mobile apps or APIs is exclusively dictated by its code, machine learning models evolve their knowledge over time depending on specific datasets. How to define and write robust machine learning agents is one of the existential challenges for the entire space. Recently, artificial intelligence(AI) researchers from DeepMind have been exploring some ideas about When we think about writing robust software, we immediately relate to two code that behaves according to a predefined set of specifications. In the case of machine learning there is no established definition of correct specifications or robust behavior. The accepted practice is to train a machine learning model using a specific dataset and test it using a different dataset. That approach is incredibly efficient achieving above average behaviors in both datasets but is not always efficient when comes to edge cases. A classic example of these challenges are seeing in image classification models that can be completely disrupted by introducing small variations in the input dataset that are completely imperceptible to the human eye.

**Causal inference in randomized clinical trials**

Correct interpretation of statistical data requires caution in implying causality , a caution contrasting with the purpose of most clinical trials whose major objective is the opposite, to assign causality. A typical example showing that association does not imply causation is given in Fig 1. How can we reconcile these opposing considerations? In this brief review we provide basic definitions of causal inference and discuss why treatment effect, a common clinical trial endpoint after an intervention, should not be interpreted as implying causation. We provide a concise guide on how to conduct statistical analyses to obtain results where causal interpretation may be reasonable. We also introduce classical causal methods for randomized trials and discuss methods to use covariate information to improve efficiency and methods to deal with non-compliance. Lastly, we introduce recent advanced research in causal inference of survival effects including methods for time-varying treatment experiments and high-dimensional covariate information.

**PySnooper**

PySnooper is a poor man’s debugger. You’re trying to figure out why your Python code isn’t doing what you think it should be doing. You’d love to use a full-fledged debugger with breakpoints and watches, but you can’t be bothered to set one up right now. You want to know which lines are running and which aren’t, and what the values of the local variables are. Most people would use print lines, in strategic locations, some of them showing the values of variables. PySnooper lets you do the same, except instead of carefully crafting the right print lines, you just add one decorator line to the function you’re interested in. You’ll get a play-by-play log of your function, including which lines ran and when, and exactly when local variables were changed. What makes PySnooper stand out from all other code intelligence tools? You can use it in your shitty, sprawling enterprise codebase without having to do any setup. Just slap the decorator on, as shown below, and redirect the output to a dedicated log file by specifying its path as the first argument.

**Analyzing earthquakes in USA to determine the possibly risky to quakes hotels.**

Ever wondered if there was a prediction system that could accurately predict the earthquakes far before it could hit the grounds beneath our feet, the fatalities and the destruction of several skyscrapers, homes, hotels would have drastically decrease and so we could have escaped from one of the greatest calamity of nature and the quakes would no longer become a reason behind the demise of a single people. Suppose, you went to a beach city in the pacific coastal part of USA and checked in a hotel. Now, you’re enjoying the sea view and suddenly you feel a vulnerable tremor and the hotel, possibly a skyscraper collapsed like a toy and now you’re wondering if you could knew which hotels in these earthquake heavy places needs to be scrutinized and upgraded with anti-quake devices, you wouldn’t have chosen this hotel for staying by only being influenced by the user ratings. But it’s possible by the advancement of today’s very powerful Data Science tools to identify the places where the chances of the tremors are highly possible and by that place, one could easily fetch the data of the nearby business venues including business venues, apartments, or just anything to measure the risk factors and the structural strengths to equip them with the quake-resistant devices and to upgrade them, leading to securing the residents and the visitors.

**Unsupervised Learning Project: Creating Customer Segments**

**Beginner’s Guide to Object Detection Algorithms**

Within the field of Deep Learning, the sub-discipline called ‘Object Detection’ involves processes such as identifying the objects through a picture, video or a webcam feed.

**Parsing and Visualizing Transcriptomics Data in Shiny**

The genome represents the history book and instruction set of all organisms on Earth. Over the last century, genetic analysis has provided deep insight into the underpinnings of biological processes and the molecular etiology of disease. A crucial principle to understanding how genes function is that defects can arise due to mutation of the genes themselves, but also in the with the way that they are regulated and the amount of gene products they produce. Messenger RNA (mRNA) copies of DNA genes are copies in a process called ‘transcription’. These mRNA are then used in turn as templates to produce proteins in the cell in a process called ‘translation’. The number of copies of the mRNA copies transcribed from genes and the number of copies of protein translated from mRNA are referred to as the degree to which these molecules are ‘expressed’.

**Logistic Regression on MNIST with PyTorch**

**Data science, big data and statistics**

This article analyzes how Big Data is changing the way we learn from observations. We describe the changes in statistical methods in seven areas that have been shaped by the Big Data-rich environment: the emergence of new sources of information; visualization in high dimensions; multiple testing problems; analysis of heterogeneity; automatic model selection; estimation methods for sparse models; and merging network information with statistical models. Next, we compare the statistical approach with those in computer science and machine learning and argue that the convergence of different methodologies for data analysis will be the core of the new field of data science. Then, we present two examples of Big Data analysis in which several new tools discussed previously are applied, as using network information or combining different sources of data. Finally, the article concludes with some final remarks.

**Automatically Generate Hotel Descriptions with LSTM**

How to create a generative model for text using LSTM recurrent neural networks in Python with Keras. In order to build a content based recommender system, I collected hotel descriptions for 152 hotels in Seattle. I was thinking of some other ways to torture this good quality clean data set. Hey! why not train my own text-generating neural network of hotel descriptions? That is, create a language model for generating natural language text (i.e. hotel descriptions) by implement and training a word-based Recurrent Neural Network. The objective of this project is to generate new hotel descriptions, given some input text. I do not expect results to be accurate, as long as the predicted text are coherent, I will be happy.

**Why you should pick your hypotheses BEFORE looking at your data**

While sitting in statistics class my prof would often repeat the importance of deciding the hypotheses one wants to test before looking at the data. He would scoff at the idea of data-mining and claim anyone that formed their hypotheses after looking at data was doomed to failure. He never said why.

**Setting up continuous multi-platform R package building, checking and testing with R-Hub, Docker and GitLab CI/CD for free, with a working example**

In the previous post, we looked at how to easily automate R analysis, modeling, and development work for free using GitLab’s CI/CD. Together with the fantastic R-hub project, we can use GitLab CI/CD to do much more. In this post, we will take it to the next level by using R-hub to test our development work on many different platforms such as multiple Linux setups, MS Windows and MacOS. We will also show how to automate and continuously execute those multiplatform checks using GitLab CI/CD integration and Docker images. For those too busy to read, we also provide a working example implementation in a public GitLab repository.

**Zilic: Detect any disease with machine learning**

Using machine learning to detect any disease with a single test. Machine learning has transformed healthcare. It’s being used to diagnose lung cancer, pneumonia, and other diseases. Machine learning is more accurate and faster at diagnosis than real doctors. Despite all this innovation, there’s still a huge problem. Errors in diagnosis still contribute to 1 in 10 patient deaths in hospitals. To figure out what’s wrong, let’s go back to the future.





### Like this:

Like Loading...


*Related*

