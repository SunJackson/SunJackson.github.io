---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/04/18/distilled-news-1042/
date:      2019-04-18
author:      Michael Laux
tags:
    - learning
    - learns
    - models
    - brands
    - morphnet
---

**Why Machine Learning Models Crash And Burn In Production**

One magical aspect of software is that it just keeps working. If you code a calculator app, it will still correctly add and multiply numbers a month, a year, or 10 years later. The fact that the marginal cost of software approaches zero has been a bedrock of the software industry’s business model since the 1980s. This is no longer the case when you are deploying machine learning (ML) models. Making this faulty assumption is the most common mistake of companies taking their first artificial intelligence (AI) products to market. The moment you put a model in production, it starts degrading.

**Towards Zero-Overhead Reproducibility: Docker Support for ML Training**

**Why software projects take longer than you think – a statistical model**

Anyone who built software for a while knows that estimating how long something is going to take is hard. It’s hard to come up with an unbiased estimate of how long something will take, when fundamentally the work in itself is about solving something. One pet theory I’ve had for a really long time, is that some of this is really just a statistical artifact.

**MorphNet: Towards Faster and Smaller Neural Networks**

MorphNet optimizes a neural network through a cycle of shrinking and expanding phases. In the shrinking phase, MorphNet identifies inefficient neurons and prunes them from the network by applying a sparsifying regularizer such that the total loss function of the network includes a cost for each neuron. However, rather than applying a uniform cost per neuron, MorphNet calculates a neuron cost with respect to the targeted resource. As training progresses, the optimizer is aware of the resource cost when calculating gradients, and thus learns which neurons are resource-efficient and which can be removed.

**Building a Flask API to Automatically Extract Named Entities Using SpaCy**

How to use the Named Entity Recognition module in spaCy to identify people, organizations, or locations in text, then deploy a Python API with Flask.

**Unsupervised Learning: Dimensionality Reduction**

As stated in previous articles, unsupervised learning refers to a kind of machine learning algorithms and techniques that are trained and fed with unlabeled data. In other words, we do not know the correct solutions or the values of the target variable beforehand. The main goal of these types of algorithms is to study the intrinsic and hidden structure of the data in order to get meaningful insights, segment the datasets in similar groups or to simplify them. Throughout this article, we are going to explore some of the algorithms and techniques most commonly used to reduce the dimensionality of datasets.

**Introduction to LSTM Units While Playing Jazz**

Long short-term memory (LSTM) units allow to learn very long sequences. It is a more general and robust version of the gated recurrent unit (GRU), which will not be addressed in this post. In this post, we will learn how an LSTM unit works, and we will apply it to generate some jazz music.

**8 Useful R Packages for Data Science You Aren’t Using (But Should!)**

I’m a big fan of R – it’s no secret. I have relied on it since my days of learning statistics back in university. In fact, R is still my go-to language for machine learning projects. Three things primarily attracted me to R:• The easy-to-understand and use syntax• The incredible RStudio tool• R packages!R offers a plethora of packages for performing machine learning tasks, including ‘dplyr’ for data manipulation, ‘ggplot2’ for data visualization, ‘caret’ for building ML models, etc.

**Calculating the Semantic Brand Score with Python**

The Semantic Brand Score (SBS) is a novel metric designed to assess the importance of one or more brands, in different contexts and whenever it is possible to analyze textual data, even big data. The advantage with respect to some traditional measures is that the SBS do not relies on surveys administered to small samples of consumers. The measure can be calculated on any source of text documents, such as newspaper articles, emails, tweets, posts on online forums, blogs and social media. The idea is to capture insights and honest signals through the analysis of big textual data. Spontaneous expressions of consumers, or other brand stakeholders, can be collected from the places where they normally appear – for example a travel forum, if studying the importance of museum brands. This has the advantage of reducing the biases induced by the use of questionnaires, where interviewees know that they are being observed. The SBS can also be adapted to different languages and to study the importance of specific words, or set of words, not necessarily ‘brands’.

**Deep embedding’s for categorical variables (Cat2Vec)**

In this blog I am going to take you through the steps involved in creating a embedding for categorical variables using a deep learning network on top of keras. The concept was originally introduced by Jeremy Howard in his fastai course. Please see the link for more details.

**Face Recognition using Artificial Intelligence**

Face can be considered as the unique identity of an individual. People across the world have unique faces and facial features. It plays a major role for interacting with other people in society. Considering these facts, facial recognition is implemented in the real world. What is a Facial Recognition System? In simple words a Facial Recognition System can be defined as a technology which can identify or verify a person from a digital image or video source by comparing and analyzing patterns based on the person’s facial contours.

**Chatbots aren’t as difficult to make as You Think**

Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they intimidate someone very new to the field. We struggle with a lot of questions before we even begin to start working on them. Are they hard to create? What technologies should I know before working on them? In the end, we end up discouraged reading through so many posts on the internet and effectively accomplish nothing.

**Breaking the curse of small data sets in Machine Learning: Part 2**

This is Part 2 of the series Breaking the curse of small datasets in Machine Learning. In Part 1, I have discussed how the size of the data set impacts traditional Machine Learning algorithms and a few ways to mitigate those issues. In Part 2, I will discuss how deep learning model performance depends on data size and how to work with smaller data sets to get similar performances.

**Facing the ARIMA Model against Neural Networks**

The purpose of this small project is to go through the ARIMA model to evaluate its performance in a univariate dataset. Also, its performance will be compared with other techniques that are currently available to create predictions in time series using neural networks. This post consists of different methods for forecasting time series. However, none of these methods is perfect as there is no perfect way to predict the future, so these results should be taken with care and always with the advice of an expert.





### Like this:

Like Loading...


*Related*

