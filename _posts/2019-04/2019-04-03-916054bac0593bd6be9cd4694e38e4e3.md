---
layout:     post
catalog: true
title:      A text mining function for websites
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/P12koH-By3Q/
date:      2019-04-03
author:      experimentalbehaviour
tags:
    - websites
    - articles
    - css
    - html_document
    - links
---





For one of my projects I needed to download text from multiple websites. In this case, I used `rvest` and `dplyr`. Accessing the information you want can be relatively easy if the sources come from the same websites, but pretty tedious when the websites are heterogenous. The reason is how the content is kept in the HTML of the website (Disclaimer: I am not an expert *at all* on HTML or anything website related). Assume that you want to extract the title, author information, publish date, and of course the main article text. You can identify the location of that information via Cascading Style Sheets (CSS) or XML Path Language (XPath). As soon as you have the CSS or XPath locations, you can access it in R. The following text will walk you through an example and provide the relevant code.



## Where is the information I need?

Assume you want to get the relevant information from an article from The Guardian. Open this website in your browser. I recommend using Google Chrome because I will use a handy tool called SelectorGadgetwhich allows you to easily find the CSS or XPath information via point and click. You know exactly what you want on that website, i.e. title, author information, main text and publishing date. But how to get that into R? We’ll start by loading the HTML page into R using `rvest`.

```
library(rvest)
url <- "https://www.theguardian.com/environment/2015/jan/08/mayors-failure-clean-up-londons-air-pollution-risks-childrens-health"

try(html_document <- read_html(url))

print(html_document) 
```

```
#
#
```

## How can I retrieve the information I need?

The following code shows you how to access the information found with the above means in R. First, with the XPath method, then with the CSS method. At the end, we will construct a data frame with that information.

```
library(dplyr)


title_xpath <- "//h1[contains(@class, 'content__headline')]"
title_text <- html_document %>%
 html_node(xpath = title_xpath) 


title_text <- title_text %>%
 html_text(trim = T) 
 

author_css <- ".tone-colour span" 
author_text <- html_document %>%
 html_node(css = author_css) %>%
 html_text(trim = T) 


body_xpath <- "//div[contains(@class, 'content__article-body')]//p" 


body_text <- html_document %>%
 html_nodes(xpath = body_xpath) %>%
 html_text(trim = T) %>%
 paste0(collapse = "\n")
 

date_xpath <- "//time" 

library(lubridate) 
date_text <- html_document %>%
 html_node(xpath = date_xpath) %>%
 html_attr(name = "datetime") %>% 
 as.Date() %>% 
 parse_date_time(., "ymd", tz = "UTC") 
 

article <- data.frame(
 url = url,
 date = date_text,
 title = title_text,
 author = author_text,
 body = body_text
 )

print(as_tibble(article))
```

```
#
#
```

The next step would be to wrap this code in a function, in order to be able to run it for multiple The Guardian articles.

```

scrape_guardian_article <- function(url) {
try(html_document <- read_html(url))
 title_xpath <- "//h1[contains(@class, 'content__headline')]"
title_text <- html_document %>%
 html_node(xpath = title_xpath)

title_text <- title_text %>%
 html_text(trim = T) 
 
author_css <- ".tone-colour span" 
author_text <- html_document %>%
 html_node(css = author_css) %>%
 html_text(trim = T) 

body_xpath <- "//div[contains(@class, 'content__article-body')]//p" 
body_text <- html_document %>%
 html_nodes(xpath = body_xpath) %>%
 html_text(trim = T) %>%
 paste0(collapse = "\n")

date_xpath <- "//time" 
library(lubridate) 
date_text <- html_document %>%
 html_node(xpath = date_xpath) %>%
 html_attr(name = "datetime") %>% 
 as.Date() %>% 
 parse_date_time(., "ymd", tz = "UTC") 
 
article <- data.frame(
 url = url,
 date = date_text,
 title = title_text,
 author = author_text,
 body = body_text
 )
return(article)
}


articles <- data.frame()
links <- c("https://www.theguardian.com/environment/2015/jan/08/mayors-failure-clean-up-londons-air-pollution-risks-childrens-health", "https://www.theguardian.com/world/2016/dec/07/marshall-islands-natives-return-mass-exodus-climate-change", "https://www.theguardian.com/environment/2016/dec/14/queenslands-largest-solar-farm-plugs-into-the-grid-a-month-early")

for (i in 1:length(links)) { 
 cat("Downloading", i, "of", length(links), "URL:", links[i], "\n")
 article <- scrape_guardian_article(links[i]) 
 articles <- rbind(articles, article) 
}
```

```
#
#
```

```
print(as_tibble(articles))
```

```
#
#
```

You can also modify the function to use it with `lapply`. To do that, use the following code modifications

```
# Change the return code in the functin defined above to:
articles <- rbind(articles, article)

# Run the function over vector of links
text_df <- as.data.frame(lapply(links, scrape_guardian_article))
```

## How can I adapt this to other websites?

Unfortunately, the above code won’t work for every website, probably not even for all The Guardian websites, because these websites are built differently. Main text will be stored just in `p` sometimes, whereas you will be more elaborated CSS path specifications on others. Depending on the number of different websites you want to scrape, it can be pretty tedious to write a function with the adequate CSS or XPath specifiers for everyone. However, as of now I do not know of a better way to do this (I would be grateful for tips and tricks, though). I tried using `RSelenium`, which sets up a server, navigates to the respective website and clicks or copies whatever you specify. However, in this case the algorithm cannot know perfectly what kind of information you want. Maybe there are machine learning methods that allow an algorithm to learn based on text on how to best identify the main text of a website, its title, etc. This sounds like a really interesting method. However, I am not yet aware of any such approaches.


*Related*








---
