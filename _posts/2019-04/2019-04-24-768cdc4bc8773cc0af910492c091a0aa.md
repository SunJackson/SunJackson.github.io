---
layout:     post
catalog: true
title:      Attention Craving RNNS： Building Up To Transformer Networks
subtitle:      转载自：http://feedproxy.google.com/~r/kdnuggets-data-mining-analytics/~3/x-6NULK4uik/attention-craving-rnn-building-transformer-networks.html
date:      2019-04-24
author:      Asel Mendis
tags:
    - next_h
    - h_
    - all_h
    - steps
    - rnns
---


  
 





---

**By William Falcon, PhD Researcher, AI researcher and AI writer for Forbes**
![](https://i.ibb.co/G7nfLsS/parks-and-recreation.gif)


Adding attention to your neural networks is a bit like wanting to take an afternoon nap at work. You know it’s better for you, everyone wants to do it, but **everyone’s too scared to**.

My goal today is to assume nothing, explain the details with animations, and make math great again (MMGA? ugh…)

**Here we’ll cover:**

1. Short RNN review.

1. Short sequence to sequence model review.

1. Attention in RNN's.

1. Improvements to attention.

1. Transformer network introduction.


 

### Recurrent Neural Networks (RNN) 

  

RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTM's (Hochreiter et al, 1997) and GRUs (Cho et al, 2014). For a deep tutorial, check out Chris Colah’s tutorial.

**Let’s look at machine translation for a concrete example of an RNN.**

1. Imagine we have an RNN with 56 hidden units.



rnn_cell = rnn_cell(input_dim=100, output_dim=56)


1. We have a word “NYU” which is represented by the integer 12 meaning it’s the 12th word in the vocab I created.



# 'NYU' is the 12th word in my vocab
word = 'NYU'
word = VOCAB[word]

print(word)
# 11



Except we don’t feed an integer into the RNN, we use a higher dimensional representation which we currently obtain through embeddings. An embedding lets us map a sequence of discrete tokens into continuous space (Bengio et al, 2003).

An RNN cell takes in two inputs, a word **x**, and a hidden state from the previous time step **h**. At every time step, it outputs a new **h**.
![](https://i.ibb.co/CB3zRyY/rnn-cell.gif)


**RNN CELL: next_h= f(x, prev_h).**



**Tip: For the first step h is normally just zeros.*

This is important: RNN *cell* is **DIFFERENT** from an RNN.

There’s a **MAJOR** point of confusion in RNN terminology. In deep learning frameworks like Pytorch and Tensorflow, the RNN CELL is the unit that performs this computation:

the RNN *NETWORK* for loops the cell over the time steps



**Here’s an illustration of an RNN moving the same RNN cell over time:**
![](https://i.ibb.co/LdL7CM5/rnn-move-rnn-cell.gif)


**The RNN moves the RNN cell over time. For attention, we’ll use ALL the h’s produced at each timestep**



 

### Sequence To Sequence Models (Seq2Seq) 

  

Now you’re a pro at RNNs, but let’s take it easy for a minute.
![](https://i.ibb.co/sjyyvJ8/anna-kendrick.gif)


Chill



RNNs can be used as blocks into larger deep learning systems.

One such system is a Seq2Seq model introduced by Bengio’s group (Cho et al, 2014) and Google (Sutskever et al, 2014), which can be used to translate a sequence to another. You can frame a lot of problems as translation:

1. Translate English to Spanish.

1. Translate a video sequence into another sequence.

1. Translate a sequence of instructions into programming code.

1. Translate user behavior into future user behavior

1. …

1. The only limit is your creativity!


A seq2seq model is nothing more than 2 RNNs, an encoder (E), and decoder (D).



The seq2seq model has 2 major steps:

**Step 1: *Encode* a sequence:**
![](https://i.ibb.co/LdL7CM5/rnn-move-rnn-cell.gif)


Encoding



**Step 2: Decode to generate a“translation.”**

This part gets really involved. The encoder in the previous step processed the full sequence at once (ie: it was a vanilla RNN).

In this second step, we run the decoder RNN one step at a time to generate predictions autoregressively (this is fancy for using the output of the previous step as the input to the next step).

There are two major ways of doing the decoding:

**Option 1: Greedy Decoding**

1. Run 1 step of the decoder.

1. Pick the highest probability output.

1. Use this output as the input to the next step




It’s called greedy because we always go with the highest probability next word.

**Option 2: Beam Search**

There’s a better technique called Beam Search, which considers multiple paths through the decoding process. Colloquially, a beam search of width 5 means we consider 5 possible sequences with the maximum log likelihood (math talk for 5 most probable sequences).

At a high-level, instead of taking the highest probability prediction, we keep the top k (beam size = k). Notice below, at each step we have 5 options (5 with the highest probability).
![](https://i.ibb.co/fS8K2Xn/beam-search.png)




This youtube video has a detailed beam search tutorial!

So, the full seq2seq process with greedy decoding as an animation to translate *“NYU NLP is awesome”* into Spanish looks like this:
![](https://i.ibb.co/PCW8Wb8/seq-2-seq-rnn-encoder-decoder.gif)


**Seq2Seq is made up of 2 RNNs an encoder and decoder**



**This model has various parts:**

1. Blue RNN is the encoder.

1. Red RNN is the decoder

1. The blue rectangle on top of the decoder is a fully connected layer with a softmax. This picks the most likely next word.






