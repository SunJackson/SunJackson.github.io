---
layout:     post
catalog: true
title:      How to Write a Spark Application in Scala
subtitle:      转载自：https://dimensionless.in/how-to-write-a-spark-application-in-scala/
date:      2019-04-15
author:      Kartik Singh
tags:
    - spark
    - steps
    - data developers
    - scala
    - org
---

### Introduction

The most difficult thing for big data developers today is choosing a programming language for big data applications. Also, Python and R programming, are the languages of choice among data scientists for building machine learning models whilst Java remains the go-to programming language for developing Hadoop applications. But, with the advent of various big data frameworks like Apache Kafka and Apache Spark- Scala programming language has gained prominence amid big data developers.

According to a recent survey, 71% of Spark users also use Scala programming language — making it the de facto language for Spark programming. Additionally, in this blog, we will understand how to implement spark application using Scala

### 

### What is Spark?

Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. Also, it is based on Hadoop MapReduce and it extends the MapReduce model to efficiently use it for more types of computations, which includes interactive queries and stream processing. But the main feature of Spark is its **in-memory cluster computing** that increases the processing speed of an application.

Moreover, Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries and streaming. Apart from supporting all these workloads in a respective system, it reduces the management burden of maintaining separate tools.

### 

### What is Scala?

Scala programming is a general-purpose computer language that supports the object-oriented, and functional style of programming on a larger scale. Also, the Scala language is a strong static type of programming language and is similar to the Java programming language. Also, one of the best similarities of Scala and Java is that you can code Scala just the same way that you code Java. It is also possible to use a lot of the Java libraries within Scala along with many of the third party libraries as well

Sacla name is after its feature of ‘scalability’ which separates it from other programming languages. Scala expresses common programming patterns in a more elegant, concise and type-safe manner. Scala is a pure object-oriented programming language (in the sense that every value is an object) which provides the features of functional languages (in the sense that every function is a value).

### 

### Spark+Scala Overview

At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. But, the main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. Furthermore, RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.

### 

### Understanding Map and Reduce

MapReduce is a Distributed Data Processing Algorithm, introduced by Google in it’s MapReduce Tech Paper. Moreover, the algorithm is mainly inspired by Functional Programming model. It is mainly useful to process a huge amount of data in a parallel, reliable and efficient way in cluster environments. Furthermore, the algorithm uses a Divide and Conquers technique to process a large amount of data. It divides input task into smaller and manageable sub-tasks to execute them in parallel.

MapReduce Algorithm uses the following three main steps:

1. Map Function

1. Shuffle Function

1. Reduce Function


#### 

#### Map Function

Map Function is the first step in MapReduce Algorithm. It takes input tasks (say DataSets. I have given only one DataSet in below diagram.) and divides them into smaller sub-tasks. Then perform required computation on each sub-task in parallel.

This step performs the following two sub-steps:

- Splitting step takes input DataSet from Source and divides into smaller Sub-DataSets.

- Mapping step takes those smaller Sub-DataSets and performs required action or computation on each Sub-DataSet.


The output of this Map Function is a set of key and value pairs as <Key, Value>

#### 

#### Shuffle Function

It is is the second step in MapReduce Algorithm. We can call Shuffle Function also as “Combine Function”.

It performs the following two sub-steps:

It takes a list of outputs coming from “Map Function” and performs these two sub-steps on each and every key-value pair.

- Merging step combines all key-value pairs which have same keys (that is grouping key-value pairs by comparing “Key”). This step returns <Key, List<Value>>.

- Sorting step takes input from Merging step and sort all key-value pairs by using Keys. This step also returns <Key, List<Value>> output but with sorted key-value pairs.


Finally, Shuffle Function returns a list of <Key, List<Value>> sorted pairs to next step.

#### 

#### Reduce Function

It is the final step in MapReduce Algorithm. This performs only one step: Reduce step. It takes a list of <Key, List<Value>> sorted pairs from Shuffle Function and performs reduce operation

### 

### Understanding RDD

Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.

Formally, an RDD is a read-only, partitioned collection of records. We can create RDDs through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.

There are two ways to create RDDs − parallelizing an existing collection in your driver program or referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.

Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations. Let us first discuss how MapReduce operations take place and why they are not so efficient.

### 

### Goal

Our goal in this article will be to understand the implementation of spark using Scala. Below, we will implement two Scala codes to understanding it’s functioning. First will be the word count example. Here, we will be counting the frequency of words present in the document. Our second, task will be a utilising rich ml library of spark. We will implement our first decision tree classifier here!

### Setting up Spark and Scala

Our first task is to install the requirements before we can start actually implementing.

#### Installing Java







||sudo apt-get install openjdk-8-jdk|


![](https://dimensionless.in/wp-content/uploads/2019/04/scala_install_java.png)
![](https://dimensionless.in/wp-content/uploads/2019/04/scala_install_java.png)


Setting up Scala


#### 



||sudo apt-get install scala|


![](https://dimensionless.in/wp-content/uploads/2019/04/scala_install_scala.png)
![](https://dimensionless.in/wp-content/uploads/2019/04/scala_install_scala.png)


Installing Spark


#### 



||### Download the Spark from <a src="https://spark.apache.org/downloads.html" target="_blank" rel="noreferrer noopener">https://spark.apache.org/downloads.html</a>tar -xvf <a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz" target="_blank" rel="noreferrer noopener">spark-2.4.1-bin-hadoop2.7.tgz</a>|

tar -xvf <a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz" target="_blank" rel="noreferrer noopener">spark-2.4.1-bin-hadoop2.7.tgz</a>



Setting up OpenSSH


#### Setting up OpenSSH



||sudo apt-get install openssh-server|



Editing the bashrc file for spark shell


#### Editing the bashrc file for spark shell



||sudo nano ~/.bashrc export PATH = $PATH: /usr/local/spark/bin|

export PATH = $PATH: /usr/local/spark/bin



Running the spark shell


#### Running the spark shell


![](https://dimensionless.in/wp-content/uploads/2019/04/scala_spark_cluster.png)
![](https://dimensionless.in/wp-content/uploads/2019/04/scala_spark_cluster.png)


Understanding Word Count Example

Step 1: Create Spark Session
We use spark session as an entry point for dataset API.

### 

#### Step 1: Create Spark Session

#### Step 2: Read Data and Convert to Dataset

We read data using read.text API which is similar to *textFile* API of RDD. The *as[String]* part of code assigns the needed schema for the dataset

#### Step 3: Split and Group by Word

Dataset mimics a lot of RDD API like a map, groupByKey etc. The below code we are splitting lines to get words and group them by words

One thing you may observe we don’t create a key/value pair. The reason is unlike RDD, dataset works in row level abstraction. Each value has treated a row with multiple columns and any column can act as a key for grouping like in the database.

#### Step 4: Count

Once we have grouped, we can count each word using the count method. It’s similar to *reduceByKey* of RDD.

#### Step 5: Print Results

Finally, once we count, we need to print the result. As with RDD, all the above API’s are lazy. We need to call an action to trigger computation. In the dataset, the show is one of those actions. It shows the first 20 results. If you want a complete result, you can use *collect* API.

Following is the implementation for the word count example



{% raw %}
|123456789101112131415161718192021|package com.oreilly.learningsparkexamples.mini.scalaimport org.apache.spark._import org.apache.spark.SparkContext._ object WordCount {def main(args: Array[String]) {val inputFile = args(0)val outputFile = args(1)val conf = new SparkConf().setAppName("wordCount")// Create a Scala Spark Context.val sc = new SparkContext(conf)// Load our input data.val input = sc.textFile(inputFile)// Split up into words.val words = input.flatMap(line => line.split(" "))// Transform into word and count.val counts = words.map(word => (word, 1)).reduceByKey{case (x, y) => x + y}// Save the word count back out to a text file, causing evaluation.counts.saveAsTextFile(outputFile)}}|
{% endraw %}

2


4


6


8


10


12


14


16


18


20


import org.apache.spark._

 

def main(args: Array[String]) {

val outputFile = args(1)

// Create a Scala Spark Context.

// Load our input data.

// Split up into words.

// Transform into word and count.

// Save the word count back out to a text file, causing evaluation.

}

 

### 

### Making a Decision Tree Classifier using Spark and Scala

In this section, we will be implementing a decision tree classifier using spark’s mllib library. MLlib is Spark’s scalable machine learning library consisting of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction. You can have a look at below code for the implementation of a decision tree classifier. You can refer to comments to understand about the functionalities to which every code chunk relates to



{% raw %}
|12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273|package org.apache.spark.examples.mlimport org.apache.spark.ml.Pipelineimport org.apache.spark.ml.classification.DecisionTreeClassificationModelimport org.apache.spark.ml.classification.DecisionTreeClassifierimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer} import org.apache.spark.sql.SparkSession object DecisionTreeClassificationExample {def main(args: Array[String]): Unit = {val spark = SparkSession.builder.appName("DecisionTreeClassificationExample").getOrCreate()// Load the data stored in LIBSVM format as a DataFrame.val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt") // Index labels, adding metadata to the label column.// Fit on whole dataset to include all labels in index.val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(data)// Automatically identify categorical features, and index them.val featureIndexer = new VectorIndexer().setInputCol("features").setOutputCol("indexedFeatures").setMaxCategories(4) // features with > 4 distinct values are treated as continuous..fit(data) // Split the data into training and test sets (30% held out for testing).val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3)) // Train a DecisionTree model.val dt = new DecisionTreeClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures") // Convert indexed labels back to original labels.val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels) // Chain indexers and tree in a Pipeline.val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, dt, labelConverter)) // Train model. This also runs the indexers.val model = pipeline.fit(trainingData) // Make predictions.val predictions = model.transform(testData) // Select example rows to display.predictions.select("predictedLabel", "label", "features").show(5) // Select (prediction, true label) and compute test error.val evaluator = new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")val accuracy = evaluator.evaluate(predictions)println(s"Test Error = ${(1.0 - accuracy)}") val treeModel = model.stages(2).asInstanceOf[DecisionTreeClassificationModel]println(s"Learned classification tree model:\n ${treeModel.toDebugString}")// $example off$ spark.stop()}}|
{% endraw %}

2


4


6


8


10


12


14


16


18


20


22


24


26


28


30


32


34


36


38


40


42


44


46


48


50


52


54


56


58


60


62


64


66


68


70


72


import org.apache.spark.ml.Pipeline

import org.apache.spark.ml.classification.DecisionTreeClassifier

import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}

import org.apache.spark.sql.SparkSession

object DecisionTreeClassificationExample {

val spark = SparkSession

.appName("DecisionTreeClassificationExample")

// Load the data stored in LIBSVM format as a DataFrame.

 

// Fit on whole dataset to include all labels in index.

.setInputCol("label")

.fit(data)

val featureIndexer = new VectorIndexer()

.setOutputCol("indexedFeatures")

.fit(data)

// Split the data into training and test sets (30% held out for testing).

 

val dt = new DecisionTreeClassifier()

.setFeaturesCol("indexedFeatures")

// Convert indexed labels back to original labels.

.setInputCol("prediction")

.setLabels(labelIndexer.labels)

// Chain indexers and tree in a Pipeline.

.setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))

// Train model. This also runs the indexers.

 

val predictions = model.transform(testData)

// Select example rows to display.

 

val evaluator = new MulticlassClassificationEvaluator()

.setPredictionCol("prediction")

val accuracy = evaluator.evaluate(predictions)

 

println(s"Learned classification tree model:\n ${treeModel.toDebugString}")

 

}



Summary
One of Scala’s biggest attractions is its close relationship with the cluster-computing framework Spark. Like Hadoop, we can use Spark for processing massive datasets across clusters of commodity hardware. But where Hadoop relies on the venerable MapReduce paradigm, Spark uses a different process that copies most of its operations into the system’s RAM. By handling these operations “in memory,” Spark can achieve much higher speeds than MapReduce. This speed gain makes Spark uniquely well suited to streaming data analytics.

### Summary

**Follow this link**, if you are looking to **learn more about data science online!**

You can **follow this link for our Big Data course!**

Additionally, if you are having an interest in **learning Data Science, click here to start Best Online Data Science Courses**

Furthermore, if you want to read more about data science, you can **read our blogs here**
