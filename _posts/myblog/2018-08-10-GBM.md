---
layout:     post
title:      GBM
subtitle:   深度学习，GBM
date:       2018-08-10
author:     SunJackson
category:   深度学习
catalog: true
tags:
    - 深度学习
    - Deep Learning
    - GBM
---


# GBM(gradient boosting machine)

## 参数详解:


### 树参数
- min_samples_split
  - 定义了树中一个节点所需要用来分裂的最少样本数。
  - 可以避免过度拟合(over-fitting)。如果用于分类的样本数太小，模型可能只适用于用来训练的样本的分类，而用较多的样本数则可以避免这个问题。
  - 但是如果设定的值过大，就可能出现欠拟合现象(under-fitting)。因此我们可以用CV值（离散系数）考量调节效果。

- min_samples_leaf
  - 定义了树中终点节点所需要的最少的样本数。
  - 同样，它也可以用来防止过度拟合。
  - 在不均等分类问题中(imbalanced class problems)，一般这个参数需要被设定为较小的值，因为大部分少数类别（minority class）含有的样本都比较小。
- min_weight_fraction_leaf
  - 和上面min_ samples_ leaf很像，不同的是这里需要的是一个比例而不是绝对数值：终点节点所需的样本数占总样本数的比值。
  - 2和3只需要定义一个就行了
- max_depth
  - 定义了树的最大深度。
  - 它也可以控制过度拟合，因为分类树越深就越可能过度拟合。
  - 当然也应该用CV值检验。
- max_leaf_ nodes
  - 定义了决定树里最多能有多少个终点节点。
  - 这个属性有可能在上面max_ depth里就被定义了。比如深度为n的二叉树就有最多2^n个终点节点。
  - 如果我们定义了max_ leaf_ nodes，GBM就会忽略前面的max_depth。
- max_features
  - 决定了用于分类的特征数，是人为随机定义的。
  - 根据经验一般选择总特征数的平方根就可以工作得很好了，但还是应该用不同的值尝试，最多可以尝试总特征数的30%-40%.
  - 过多的分类特征可能也会导致过度拟合。
 

### boosting参数

- learning_rate 
    - 这个参数决定着每一个决定树对于最终结果（步骤2.4）的影响。GBM设定了初始的权重值之后，每一次树分类都会更新这个值，而learning_ rate控制着每次更新的幅度。
    - 一般来说这个值不应该设的比较大，因为较小的learning rate使得模型对不同的树更加稳健，就能更好地综合它们的结果。
- n_estimators 
    - 定义了需要使用到的决定树的数量（步骤2）
    - 虽然GBM即使在有较多决定树时仍然能保持稳健，但还是可能发生过度拟合。所以也需要针对learning rate用CV值检验。
- subsample
    - 训练每个决定树所用到的子样本占总样本的比例，而对于子样本的选择是随机的。
    - 用稍小于1的值能够使模型更稳健，因为这样减少了方差。
    - 一把来说用~0.8就行了，更好的结果可以用调参获得。

### 第三类参数

- loss
    - 指的是每一次节点分裂所要最小化的损失函数(loss function)
    - 对于分类和回归模型可以有不同的值。一般来说不用更改，用默认值就可以了，除非你对它及它对模型的影响很清楚。


- init
    - 它影响了输出参数的起始化过程
    - 如果我们有一个模型，它的输出结果会用来作为GBM模型的起始估计，这个时候就可以用init


- random_state 
    - 作为每次产生随机数的随机种子
    - 使用随机种子对于调参过程是很重要的，因为如果我们每次都用不同的随机种子，即使参数值没变每次出来的结果也会不同，这样不利于比较不同模型的结果。
    - 任一个随即样本都有可能导致过度拟合，可以用不同的随机样本建模来减少过度拟合的可能，但这样计算上也会昂贵很多，因而我们很少这样用


- verbose
    - 决定建模完成后对输出的打印方式： 
        - 0：不输出任何结果（默认）
        - 1：打印特定区域的树的输出结果
        - >1：打印所有结果


- warm_ start 
    - 这个参数的效果很有趣，有效地使用它可以省很多事
    - 使用它我们就可以用一个建好的模型来训练额外的决定树，能节省大量的时间，对于高阶应用我们应该多多探索这个选项。


- presort 
    - 决定是否对数据进行预排序，可以使得树分裂地更快。
    - 默认情况下是自动选择的，当然你可以对其更改


## 如何调参

简易思路如下：

- step 1： 选择一个相对来说稍微高一点的learning rate，一般默认的值是0.1；

- step 2： 基于当前的learning rate，调整决策数数量，即基学习器的数量；

- step 3： 基于当前的learning rate和基学习器数量，调整决策树的参数；

    - step 3.1： 调节max_depth 和 num_samples_split；

    - step 3.2： 调节min_samples_leaf；

    - step 3.3：调节max_features；

- step 4： 调整子样本比例subsample，再降低learning rate并响应地提高基学习器个数；
