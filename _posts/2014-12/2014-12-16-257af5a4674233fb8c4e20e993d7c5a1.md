---
layout:     post
title:      The NIPS Experiment
subtitle:   转载自：http://inverseprobability.com/2014/12/16/the-nips-experiment
date:       2014-12-16
author:     未知
header-img: img/background2.jpg
catalog: true
tags:
    - reviewing
    - reviewer
    - scicast
    - question
    - nips
---

Just back from NIPS where it was really great to see the results of all the work everyone put in. I really enjoyed the program and thought the quality of all presented work was really strong. Both Corinna and I were particularly impressed by the work that put in by oral presenters to make their work accessible to such a large and diverse audience.

We also released some of the figures from the NIPS experiment, and there was a lot of discussion at the conference about what the result meant.

As we announced at the conference the consistency figure was 25.9%. I just wanted to confirm that in the spirit of openness that we’ve pursued across the entire conference process Corinna and I will provide a full write up of our analysis and conclusions in due course!

Some of the comment in the existing debate is missing out some of the background information we’ve tried to generate, so I just wanted to write a post that summarises that information to highlight its availability.

### Scicast Question

With the help of [Nicolo Fusi](http://nicolofusi.com/), [Charles Twardy](http://blog.scicast.org/tag/charles-twardy) and the entire Scicast team we launched [a Scicast question](https://scicast.org/#!/questions/1083/trades/create/power) a week before the results were revealed. The comment thread for that question already had [an amount of interesting comment](https://scicast.org/#!/questions/1083/comments/power) before the conference. Just for informational purposes before we began reviewing Corinna forecast this figure would be 25% and I forecast it would be 20%. The box plot summary of predictions from Scicast is below.

![](http://inverseprobability.com/assets/forecast.png)


There was also an amount of debate at the conference about what the results mean, a few attempts to answer this question (based only on the inconsistency score and the expected accept rate for the conference) are available here in this little [Facebook discussion](https://www.facebook.com/photo.php?fbid=10152999318811042&set=a.488500766041.291374.552771041&type=1&theater) and on [this blog post](http://mrtz.org/blog/the-nips-experiment).

### Background Information on the Process

Just to emphasise previous posts on this year’s conference see below:

1. [NIPS Decision Time](http://inverseprobability.com/2014/09/13/nips-decision-time)

1. [Reviewer Calibration for NIPS](http://inverseprobability.com/2014/08/02/reviewer-calibration-for-nips)

1. [Reviewer Recruitment and Experience](http://inverseprobability.com/2014/07/24/nips-reviewer-recruitment-and-experience)

1. [Paper Allocation for NIPS](http://inverseprobability.com/2014/06/28/paper-allocation-for-nips)


### Software on Github

And finally there is a [large amount of code available on a github site](https://github.com/sods/conference) for allowing our processes to be recreated. A lot of it is tidied up, but the last sections on the analysis are not yet done because it was always my intention to finish those when the experimental results are fully released.
