---
layout:     post
catalog: true
title:      pcLasso： a new method for sparse regression
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/_UURAYxfs-4/
date:      2019-01-14
author:      kjytay
tags:
    - pclasso
    - matrix
    - code
    - fits
    - components
---





I’m excited to announce that my first package has been accepted to CRAN! The package `pcLasso` implements ***principal components lasso***, a new method for sparse regression which I’ve developed with Rob Tibshirani and Jerry Friedman. In this post, I will give a brief overview of the method and some starter code. (For an in-depth description and elaboration of the method, please see our arXiv preprint. For more details on how to use the package, please see the package’s vignette.)

Let’s say we are in the standard supervised learning setting, with design matrix ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+p%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+p%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and response ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Let the singular value decomposition (SVD) of ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 be ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D+%5Cmathbf%7BUDV%5ET%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D+%5Cmathbf%7BUDV%5ET%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
, and let the diagonal entries of ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BD%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BD%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 be ![](https://s0.wp.com/latex.php?latex=d_1+%5Cgeq+d_2+%5Cgeq+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d_1+%5Cgeq+d_2+%5Cgeq+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Principal components lasso solves the optimization problem

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta%5E%2A+%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5Cbeta%5ET+%5Cmathbf%7BVZV%5ET%7D+%5Cbeta%2C+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta%5E%2A+%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5Cbeta%5ET+%5Cmathbf%7BVZV%5ET%7D+%5Cbeta%2C+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)



where ![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Ctheta&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ctheta&bg=ffffff&%23038;fg=333333&%23038;s=0)
 are non-negative hyperparameters, and ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 is the diagonal matrix with entries ![](https://s0.wp.com/latex.php?latex=d_1%5E2+-+d_1%5E2%2C+d_1%5E2+-+d_2%5E2%2C+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d_1%5E2+-+d_1%5E2%2C+d_1%5E2+-+d_2%5E2%2C+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
. The predictions this model gives for new data ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_%7Bnew%7D%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_%7Bnew%7D%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 are ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_%7Bnew%7D%7D%5Cbeta%5E%2A&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_%7Bnew%7D%7D%5Cbeta%5E%2A&bg=ffffff&%23038;fg=333333&%23038;s=0)
.

This optimization problem seems a little complicated so let me try to motivate it. Notice that if we replace ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 with the identity matrix, since ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 is orthogonal the optimization problem reduces to

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta%5E%2A+%26%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5Cbeta%5ET+%5Cmathbf%7BVV%5ET%7D+%5Cbeta+%5C%5C+%26%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5C%7C%5Cbeta%5C%7C_2%5E2%2C+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta%5E%2A+%26%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5Cbeta%5ET+%5Cmathbf%7BVV%5ET%7D+%5Cbeta+%5C%5C+%26%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5C%7C%5Cbeta%5C%7C_2%5E2%2C+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)



which we recognize as the optimization problem that elastic net solves. So we are doing something similar to elastic net.

To be more specific: we can think of ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=333333&%23038;s=0)
 as the coordinates of the coefficient vector in the standard basis ![](https://s0.wp.com/latex.php?latex=e_1+%3D+%5Cbegin%7Bpmatrix%7D+1+%26+0+%26+%5Cdots+%5Cend%7Bpmatrix%7D%5ET%2C+e_2+%3D+%5Cbegin%7Bpmatrix%7D+0+%26+1+%26+0+%26+%5Cdots+%5Cend%7Bpmatrix%7D%5ET%2C+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=e_1+%3D+%5Cbegin%7Bpmatrix%7D+1+%26+0+%26+%5Cdots+%5Cend%7Bpmatrix%7D%5ET%2C+e_2+%3D+%5Cbegin%7Bpmatrix%7D+0+%26+1+%26+0+%26+%5Cdots+%5Cend%7Bpmatrix%7D%5ET%2C+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Then ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%5ET%7D%5Cbeta&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%5ET%7D%5Cbeta&bg=ffffff&%23038;fg=333333&%23038;s=0)
 would be the coordinates of this same coefficient vector, but where the basis comprises the principal component (PC) directions of the design matrix ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Since we have the matrix ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
, with entries increasing down its diagonal, instead of the identity matrix sandwiched between ![](https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BV%5ET%7D%5Cbeta%29%5ET&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BV%5ET%7D%5Cbeta%29%5ET&bg=ffffff&%23038;fg=333333&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%5ET%7D%5Cbeta&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%5ET%7D%5Cbeta&bg=ffffff&%23038;fg=333333&%23038;s=0)
 in the quadratic penalty, it means that we are doing shrinkage in the principal components space in a way that (i) leaves the component along the first PC direction unchanged, and (ii) shrinks components along larger PC directions more to 0.

This method extends easily to groups (whether overlapping or non-overlapping). Assume that our features come in ![](https://s0.wp.com/latex.php?latex=K&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=K&bg=ffffff&%23038;fg=333333&%23038;s=0)
 groups. For each ![](https://s0.wp.com/latex.php?latex=k+%3D+1%2C+%5Cdots%2C+K&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=k+%3D+1%2C+%5Cdots%2C+K&bg=ffffff&%23038;fg=333333&%23038;s=0)
, let ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_k%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_k%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 represent the reduced design matrix corresponding to group ![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=333333&%23038;s=0)
, and let its SVD be ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_k%7D+%3D+%5Cmathbf%7BU_k+D_k+V_k%5ET%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX_k%7D+%3D+%5Cmathbf%7BU_k+D_k+V_k%5ET%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Let the diagonal entries of ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BD_k%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BD_k%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 be ![](https://s0.wp.com/latex.php?latex=d_%7Bk1%7D+%5Cgeq+d_%7Bk2%7D+%5Cgeq+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d_%7Bk1%7D+%5Cgeq+d_%7Bk2%7D+%5Cgeq+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
, and let ![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ_k%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BZ_k%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
 be the diagonal matrix with diagonal entries ![](https://s0.wp.com/latex.php?latex=d_%7Bk1%7D%5E2+-+d_%7Bk1%7D%5E2%2C+d_%7Bk1%7D%5E2+-+d_%7Bk2%7D%5E2%2C+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=d_%7Bk1%7D%5E2+-+d_%7Bk1%7D%5E2%2C+d_%7Bk1%7D%5E2+-+d_%7Bk2%7D%5E2%2C+%5Cdots&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Let ![](https://s0.wp.com/latex.php?latex=%5Cbeta_k&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta_k&bg=ffffff&%23038;fg=333333&%23038;s=0)
 be the reduced coefficient vector corresponding to the features in group ![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=333333&%23038;s=0)
. Then pcLasso solves the optimization problem

![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta%5E%2A+%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5Csum_%7Bk%3D1%7D%5EK+%5Cbeta_k%5ET+%5Cmathbf%7BV_kZ_kV_k%5ET%7D+%5Cbeta_k.+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cbeta%5E%2A+%3D+%5Cunderset%7B%5Cbeta+%5Cin+%5Cmathbb%7BR%7D%5Ep%7D%7B%5Ctext%7Bargmin%7D%7D+%5Cquad+%5Cfrac%7B1%7D%7B2n%7D%5C%7C+%5Cmathbf%7By%7D+-+%5Cmathbf%7BX%7D%5Cbeta+%5C%7C_2%5E2+%2B+%5Clambda+%5C%7C+%5Cbeta%5C%7C_1+%2B+%5Cfrac%7B%5Ctheta%7D%7B2%7D+%5Csum_%7Bk%3D1%7D%5EK+%5Cbeta_k%5ET+%5Cmathbf%7BV_kZ_kV_k%5ET%7D+%5Cbeta_k.+%5Cend%7Baligned%7D&bg=ffffff&%23038;fg=333333&%23038;s=0)



Now for some basic code. Let’s make some fake data:

Just like `glmnet` in the `glmnet` package, the `pcLasso` function fits the model for a sequence of ![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
 values which do not have to be user-specified. The user however, does have to specify the ![](https://s0.wp.com/latex.php?latex=%5Ctheta&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ctheta&bg=ffffff&%23038;fg=333333&%23038;s=0)
 parameter:

We can use the generic `predict` function to obtain predictions this fit makes on new data. For example, the following code extracts the predictions that pcLasso makes on the 5th ![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
 value for the first 3 rows of our training data:

The code above assumes that all our features belong to one big group. If our features come in groups, pcLasso can take advantage of that by specifying the `groups` option. `groups` should be a list of length ![](https://s0.wp.com/latex.php?latex=K&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=K&bg=ffffff&%23038;fg=333333&%23038;s=0)
, with `groups[[k]]` being a vector of column indices which belong to group ![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=k&bg=ffffff&%23038;fg=333333&%23038;s=0)
. For example, if features 1-5 belong to one group and features 6-10 belong to another group:

The function `cv.pcLasso` fits pcLasso and picks optimal ![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&%23038;fg=333333&%23038;s=0)
 values via cross-validation. The output of the `cv.pcLasso` function can also be used to predict on new data:

The vignette contains significantly more detail on how to use this package. If you spot bugs, have questions, or have features that you would like to see implemented, get in touch with us!


*Related*








---
