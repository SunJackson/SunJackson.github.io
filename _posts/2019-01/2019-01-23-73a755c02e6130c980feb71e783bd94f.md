---
layout:     post
catalog: true
title:      How to prepare data for NLP (text classification) with Keras and TensorFlow
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/4ti8q4Dn-1g/
date:      2019-01-23
author:      Dr. Shirin Glander
tags:
    - text
    - embeddings
    - word
    - keras_
    - col_double
---





In the past, I have written and taught quite a bit about image classification with Keras (e.g. here). Text classification isn’t too different in terms of using the Keras principles to train a sequential or function model. You can even use Convolutional Neural Nets (CNNs) for text classification.

What is very different, however, is how to prepare raw text data for modeling. When you look at the IMDB example from the Deep Learning with R Book, you get a great explanation of how to train the model. But because the **preprocessed** IMDB dataset comes with the `keras` package, it isn’t so straight-forward to use what you learned on your own data.

## How can a computer work with text?

As with any neural network, we need to convert our data into a numeric format; in Keras and TensorFlow we work with tensors. The IMDB example data from the `keras` package has been preprocessed to a list of integers, where every integer corresponds to a word arranged by descending word frequency.

So, how do we make it from raw text to such a list of integers? Luckily, Keras offers a few convenience functions that make our lives much easier.

```
library(keras)
library(tidyverse)
```

## Data

In the example below, I am using a Kaggle dataset: Women’s e-commerce cloting reviews. The data contains a text review of different items of clothing, as well as some additional information, like rating, division, etc. I will use the review title and text in order to classify whether or not the item was liked. I am creating the response variable from the rating: every item rates with 5 stars is considered “liked” (1), the rest as “not liked” (0). I am also combining review title and text.

```
clothing_reviews <- read_csv("/Users/shiringlander/Documents/Github/ix_lime_etc/Womens Clothing E-Commerce Reviews.csv") %>%
 mutate(Liked = ifelse(Rating == 5, 1, 0),
 text = paste(Title, `Review Text`),
 text = gsub("NA", "", text))
```

```
## Parsed with column specification:
## cols(
## X1 = col_double(),
## `Clothing ID` = col_double(),
## Age = col_double(),
## Title = col_character(),
## `Review Text` = col_character(),
## Rating = col_double(),
## `Recommended IND` = col_double(),
## `Positive Feedback Count` = col_double(),
## `Division Name` = col_character(),
## `Department Name` = col_character(),
## `Class Name` = col_character()
## )
```

```
glimpse(clothing_reviews)
```

```
## Observations: 23,486
## Variables: 13
## $ X1 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …
## $ `Clothing ID` 767, 1080, 1077, 1049, 847, 1080, 858,…
## $ Age 33, 34, 60, 50, 47, 49, 39, 39, 24, 34…
## $ Title NA, NA, "Some major design flaws", "My…
## $ `Review Text` "Absolutely wonderful - silky and sexy…
## $ Rating 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, 5,…
## $ `Recommended IND` 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,…
## $ `Positive Feedback Count` 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2, 2…
## $ `Division Name` "Initmates", "General", "General", "Ge…
## $ `Department Name` "Intimate", "Dresses", "Dresses", "Bot…
## $ `Class Name` "Intimates", "Dresses", "Dresses", "Pa…
## $ Liked 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,…
## $ text " Absolutely wonderful - silky and sex…
```

Whether an item was liked or not will be the response variable or label for classification of the reviews.

```
clothing_reviews %>%
 ggplot(aes(x = factor(Liked), fill = Liked)) +
 geom_bar(alpha = 0.8) +
 guides(fill = FALSE)
```

![](https://i0.wp.com/shirinsplayground.netlify.com/post/2019-01-21_text_classification_keras_data_prep_files/figure-html/unnamed-chunk-3-1.png?w=450&ssl=1)
![](https://i0.wp.com/shirinsplayground.netlify.com/post/2019-01-21_text_classification_keras_data_prep_files/figure-html/unnamed-chunk-3-1.png?w=450&ssl=1)


## Embeddings

These padded word index matrices now need to be converted into something that gives information about the features (i.e. words) in a way that can be used for learning. Currently, the state-of-the-art for text models are word embeddings or word vectors, which are learned from the text data. Word embeddings encode the context of words in relatively few dimensions while maximizing the information that is contained in these vectors. Basically, word embeddings are values that are learned by a neural net just as weights are learned by a multi-layer perceptron.

Word embedding vectors represent the words and their contexts; thus, words with similar meanings (synonyms) or with close semantic relationships will have more similar embeddings. Moreover, word embeddings should reflect how words are related to each other. For example, the embeddings for “man” should be to “king” as “woman” is to “queen”.

In our model below, we want to learn the word embeddings from our (padded) word vectors and directly use these learned embeddings for classification. This part can now be the same as in the Keras examples for LSTMs and CNNs

```
model <- keras_model_sequential() %>% 
 layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
 layer_dropout(0.2) %>%
 layer_conv_1d(
 filters, kernel_size, 
 padding = "valid", activation = "relu", strides = 1
 ) %>%
 layer_global_max_pooling_1d() %>%
 layer_dense(hidden_dims) %>%
 layer_dropout(0.2) %>%
 layer_activation("relu") %>%
 layer_dense(1) %>%
 layer_activation("sigmoid") %>% compile(
 loss = "binary_crossentropy",
 optimizer = "adam",
 metrics = "accuracy"
)
```

```
hist <- model %>%
 fit(
 x_train,
 y_train,
 batch_size = batch_size,
 epochs = epochs,
 validation_split = 0.3
 )
```

```
plot(hist)
```

![](https://i2.wp.com/shirinsplayground.netlify.com/post/2019-01-21_text_classification_keras_data_prep_files/figure-html/unnamed-chunk-15-1.png?w=450&ssl=1)
![](https://i2.wp.com/shirinsplayground.netlify.com/post/2019-01-21_text_classification_keras_data_prep_files/figure-html/unnamed-chunk-15-1.png?w=450&ssl=1)


## Alternative preprocessing functions

The above example follows the IMDB example from the Keras documentation, but there are alternative ways to preprocess your text for modeling with Keras:

```
one_hot_results <- texts_to_matrix(tokenizer, text, mode = "binary")
dim(one_hot_results)
```

```
## [1] 23486 1000
```

```
hashing_results <- text_hashing_trick(text[1], n = 100)
hashing_results
```

```
## [1] 88 75 18 90 7 90 23
```

## Pretrained embeddings

Here, we have learned word embeddings from our word vectors and directly used the output of the embedding layers as input for additional layers in our neural net. Because learning embeddings takes time and computational power, we could also start with pre-trained embeddings, particulary if we don’t have a whole lot of training data. You can find an example for how to use GloVe embeddings here.

---

## Session info

```
sessionInfo()
```

```
## R version 3.5.2 (2018-12-20)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Mojave 10.14.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats graphics grDevices utils datasets methods base 
## 
## other attached packages:
## [1] bindrcpp_0.2.2 forcats_0.3.0 stringr_1.3.1 dplyr_0.7.8 
## [5] purrr_0.2.5 readr_1.3.1 tidyr_0.8.2 tibble_2.0.1 
## [9] ggplot2_3.1.0 tidyverse_1.2.1 keras_2.2.4 
## 
## loaded via a namespace (and not attached):
## [1] reticulate_1.10 tidyselect_0.2.5 xfun_0.4 reshape2_1.4.3 
## [5] haven_2.0.0 lattice_0.20-38 colorspace_1.4-0 generics_0.0.2 
## [9] htmltools_0.3.6 yaml_2.2.0 base64enc_0.1-3 utf8_1.1.4 
## [13] rlang_0.3.1 pillar_1.3.1 withr_2.1.2 glue_1.3.0 
## [17] readxl_1.2.0 modelr_0.1.2 bindr_0.1.1 plyr_1.8.4 
## [21] tensorflow_1.10 cellranger_1.1.0 munsell_0.5.0 blogdown_0.10 
## [25] gtable_0.2.0 rvest_0.3.2 evaluate_0.12 labeling_0.3 
## [29] knitr_1.21 tfruns_1.4 fansi_0.4.0 broom_0.5.1 
## [33] Rcpp_1.0.0 backports_1.1.3 scales_1.0.0 jsonlite_1.6 
## [37] hms_0.4.2 digest_0.6.18 stringi_1.2.4 bookdown_0.9 
## [41] grid_3.5.2 cli_1.0.1 tools_3.5.2 magrittr_1.5 
## [45] lazyeval_0.2.1 crayon_1.3.4 whisker_0.3-2 pkgconfig_2.0.2 
## [49] zeallot_0.1.0 Matrix_1.2-15 xml2_1.2.0 lubridate_1.7.4 
## [53] rstudioapi_0.9.0 assertthat_0.2.0 rmarkdown_1.11 httr_1.4.0 
## [57] R6_2.3.0 nlme_3.1-137 compiler_3.5.2
```


*Related*








---
