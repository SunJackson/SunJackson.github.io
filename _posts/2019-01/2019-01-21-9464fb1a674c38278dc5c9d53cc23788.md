---
layout:     post
catalog: true
title:      Regression with Keras
subtitle:      转载自：https://www.pyimagesearch.com/2019/01/21/regression-with-keras/
date:      2019-01-21
author:      Adrian Rosebrock
tags:
    - keras
    - importing
    - imports
    - datasets
    - lines
---

![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_header-300x182.jpg)


In this tutorial, you will learn how to perform regression using Keras and Deep Learning. You will learn how to train a Keras neural network for regression and continuous value prediction, specifically in the context of house price prediction.

Todayâ€™s post kicks off a 3-part series on **deep learning, regression, and continuous value prediction.**

Weâ€™ll be studying Keras regression prediction in the context of house price prediction:

- **Part 1:** Today weâ€™ll be training a Keras neural network to **predict house prices based on categorical and numerical attributes** such as the number of bedrooms/bathrooms, square footage, zip code, etc.

- **Part 2:** Next week weâ€™ll train a Keras **Convolutional Neural Network to predict house prices based on input images of the houses themselves** (i.e., frontal view of the house, bedroom, bathroom, and kitchen).

- **Part 3:** In two weeks weâ€™ll define and train a neural network that ***combines* our categorical/numerical attributes with our images,** leading to better, more accurate house price prediction than the attributes or images alone.


Unlike classification (which predicts ***labels***), regression enables us to predict ***continuous values***.

For example, classification may be able to predict one of the following values: *{cheap, affordable, expensive}*.

Regression, on the other hand, will be able to predict an exact dollar amount, such as *â€œThe estimated price of this house is $489,121â€�*.

In many real-world situations, such as house price prediction or stock market forecasting, applying regression rather than classification is *critical* to obtaining good predictions.

**To learn how to perform regression with Keras, *just keep reading!***

## Regression with Keras

In the first part of this tutorial, weâ€™ll briefly discuss the difference between classification and regression.

Weâ€™ll then explore the house prices dataset we’re using for this series of Keras regression tutorials.

From there, weâ€™ll configure our development environment and review our project structure.

Along the way, we will learn how to use Pandas to load our house price dataset and define a neural network that for Keras regression prediction.

Finally, weâ€™ll train our Keras network and then evaluate the regression results.

### Classification vs. Regression
![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_classification_vs_reg.png)


**Figure 1:** Classification networks predict labels *(top)*. In contrast, regression networks can predict numerical values *(bottom)*. We’ll be performing regression with Keras on a housing dataset in this blog post.

Typically on the PyImageSearch blog, we discuss Keras and deep learning in the context of classification â€” predicting a label to characterize the contents of an image or an input set of data.

**Regression**, on the other hand, enables us to predict continuous values. Letâ€™s again consider the task of house price prediction.

As we know, classification is used to predict a class label.

For house price prediction we may define our categorical labels as:



||labels = {very cheap, cheap, affordable, expensive, very expensive}|

If we performed classification, our model could then learn to predict one of those five values based on a set of input features.

However, those labels are just that — categories that represent a potential range of prices for the house but do *nothing* to represent the *actual cost* of the home.

**In order to predict the actual cost of a home, we need to perform regression.**

Using regression we can train a model to predict a continuous value.

For example, while classification may only be able to predict a label, regression could say:

*â€œBased on my input data, I estimate the cost of this house to be $781,993.â€�*

**Figure 1** above provides a visualization of performing both classification and regression.

In the rest of this tutorial, youâ€™ll learn how to train a neural network for regression using Keras.

### The House Prices Dataset
![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_dataset.png)


**Figure 2:** Performing regression with Keras on the house pricing dataset (Ahmed and Moustafa) will ultimately allow us to predict the price of a house given its image.

The dataset weâ€™ll be using today is from 2016 paper, *House price estimation from visual and textual features*, by Ahmed and Moustafa.

**The dataset includes both *numerical/categorical* attributes along with *images* for 535 data points,** making it and excellent dataset to study for regression and mixed data prediction.

The house dataset includes **four numerical and categorical attributes:**

1. Number of bedrooms

1. Number of bathrooms

1. Area (i.e., square footage)

1. Zip code


These attributes are stored on disk in CSV format.

Weâ€™ll be loading these attributes from disk later in this tutorial using 
pandas , a popular Python package used for data analysis.

**A total of four images are also provided for each house:**

1. Bedroom

1. Bathroom

1. Kitchen

1. Frontal view of the house


**The end goal of the houses dataset is to predict the *price* of the home itself.**

In todayâ€™s tutorial, weâ€™ll be working with *just* the numerical and categorical data.

Next weekâ€™s blog post will discuss working with the image data.

And finally, two weeks from now weâ€™ll *combine* the numerical/categorical data with the images to obtain our best performing model.

But before we can train our Keras model for regression, we first need to configure our development environment and grab the data.

### Configuring Your Development Environment
![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_logos.png)


**Figure 3:** To perform regression with Keras, we’ll be taking advantage of several popular Python libraries including Keras + TensorFlow, scikit-learn, and pandas.

For this 3-part series of blog posts, you’ll need to have the following packages installed:

- **NumPy**

- **scikit-learn**

- **pandas**

- **Keras** with the **TensorFlow** backend (CPU or GPU)

- **OpenCV** (for the next two blog posts in the series)


Luckily most of these are easily installed with pip, a Python package manager.

Let’s install the packages now, ideally into a virtual environment as shown (you’ll need to create the environment):



||$ workon house_prices$ pip install numpy$ pip install scikit-learn$ pip install pandas$ pip install tensorflow # or tensorflow-gpu|

$ pip install numpy

$ pip install pandas

Notice that I haven’t instructed you to install OpenCV yet. The OpenCV install can be slightly involved — especially if you are compiling from source. Let’s look at our options:

1. **Compiling from source** gives us the full install of OpenCV and provides access to optimizations, patented algorithms, custom software integrations, and more. The good news is that all of my **OpenCV install tutorials** are meticulously put together and updated regularly. With patience and attention to detail, you can compile from source just like I and many of my readers do.

1. **Using pip to install OpenCV** is hands-down the fastest and easiest way to get started with OpenCV and essentially just checks prerequisites and places a precompiled binary that will work on most systems into your virtual environment site-packages. Optimizations may or may not be active. The big caveat is that the maintainer has elected not to include patented algorithms for fear of lawsuits. There’s nothing wrong with using patented algorithms for educational and research purposes, but you should use alternative algorithms commercially. Nevertheless, the pip method is a great option for beginners just remember that you don’t have the full install.


Pip is sufficient for this 3-part series of blog posts. You can install OpenCV in your environment via:



||$ workon house_prices$ pip install opencv-contrib-python|

$ pip install opencv-contrib-python

Please reach out to me if you have any difficulties getting your environment established.

### Downloading the House Prices Dataset

Before you download the dataset, go ahead and grab the source code to this post by using ***“Downloads”***section.

From there, unzip the file and navigate into the directory:



||$ cd path/to/downloaded/zip$ unzip keras-regression.zip$ cd keras-regression|

$ unzip keras-regression.zip

From there, you can download the House Prices Dataset using the following command:



||$ git clone https://github.com/emanhamed/Houses-dataset|

When we are ready to train our Keras regression network youâ€™ll then need to supply the path to the 
Houses-dataset directory via command line argument.

### Project structure

Now that you have the dataset, go ahead and use the 
tree command with the same arguments shown below to print a directory + file listing for the project:



||$ tree --dirsfirst --filelimit 10.â”œâ”€â”€ Houses-datasetâ”‚ â”œâ”€â”€ Houses Dataset [2141 entries]â”‚ â””â”€â”€ README.mdâ”œâ”€â”€ pyimagesearchâ”‚ â”œâ”€â”€ __init__.pyâ”‚ â”œâ”€â”€ datasets.pyâ”‚ â””â”€â”€ models.pyâ””â”€â”€ mlp_regression.py 3 directories, 5 files|

.

â”‚ â”œâ”€â”€ Houses Dataset [2141 entries]

â”œâ”€â”€ pyimagesearch

â”‚ â”œâ”€â”€ datasets.py

â””â”€â”€ mlp_regression.py

3 directories, 5 files

The dataset downloaded from GitHub now resides in the 
Houses-dataset/ folder.

The 
pyimagesearch/ directory is actually a module included with the code ***“Downloads”*** where inside, you’ll find:


datasets.py : Our script for loading the numerical/categorical data from the dataset

models.py : Our Multi-Layer Perceptron architecture implementation

These two scripts will be reviewed today. Additionally, we’ll be reusing both 
datasets.py and 
models.py (with modifications) in the next two tutorials to keep our code organized and reusable.

The regression + Keras script is contained in 
mlp_regression.py which we’ll be reviewing it as well.

### Loading the House Prices Dataset
![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_pandas.png)


**Figure 4:** We’ll use Python and `pandas` to read a CSV file in this blog post.

Before we can train our Keras regression model we first need to load the numerical and categorical data for the houses dataset.

Open up the 
datasets.py file an insert the following code:



||# import the necessary packagesfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.preprocessing import MinMaxScalerimport pandas as pdimport numpy as npimport globimport cv2import os def load_house_attributes(inputPath): # initialize the list of column names in the CSV file and then # load it using Pandas cols = ["bedrooms", "bathrooms", "area", "zipcode", "price"] df = pd.read_csv(inputPath, sep=" ", header=None, names=cols)|

from sklearn.preprocessing import LabelBinarizer

import pandas as pd

import glob

import os

def load_house_attributes(inputPath):

 # load it using Pandas

 df = pd.read_csv(inputPath, sep=" ", header=None, names=cols)

We begin by importing libraries and modules from scikit-learn, pandas, NumPy and OpenCV. OpenCV will be used next week as weâ€™ll be adding the ability to load images to this script.

On **Line 10**, we define the 
load_house_attributes function which accepts the path to the input dataset.

Inside the function we start off by defining the names of the columns in the CSV file (**Line 13**). From there, we use pandas’ function, 
read_csv to load the CSV file into memory as a date frame ( 
df ) on **Line 14**.

Below you can see an example of our input data, including the number of bedrooms, number of bathrooms, area (i.e., square footage), zip code, code, and finally the target price our model should be trained to predict:



||   bedrooms  bathrooms  area  zipcode     price0         4        4.0  4053    85255  869500.01         4        3.0  3343    36372  865200.02         3        4.0  3923    85266  889000.03         5        5.0  4022    85262  910000.04         3        4.0  4116    85266  971226.0|

0         4        4.0  4053    85255  869500.0

2         3        4.0  3923    85266  889000.0

4         3        4.0  4116    85266  971226.0

Let’s finish up the rest of the 
load_house_attributes function:



|161718192021222324252627282930313233| # determine (1) the unique zip codes and (2) the number of data # points with each zip code zipcodes = df["zipcode"].value_counts().keys().tolist() counts = df["zipcode"].value_counts().tolist()  # loop over each of the unique zip codes and their corresponding # count for (zipcode, count) in zip(zipcodes, counts): # the zip code counts for our housing dataset is *extremely* # unbalanced (some only having 1 or 2 houses per zip code) # so let's sanitize our data by removing any houses with less # than 25 houses per zip code if count < 25: idxs = df[df["zipcode"] == zipcode].index df.drop(idxs, inplace=True)  # return the data frame return df|

17


19


21


23


25


27


29


31


33


 # points with each zip code

 counts = df["zipcode"].value_counts().tolist()

 # loop over each of the unique zip codes and their corresponding

 for (zipcode, count) in zip(zipcodes, counts):

 # unbalanced (some only having 1 or 2 houses per zip code)

 # than 25 houses per zip code

 idxs = df[df["zipcode"] == zipcode].index

 

 return df

In the remaining lines, we:

- Determine the unique set of zip codes and then count the number of data points with each unique zip code (**Lines 18 and 19**).

- Filter out zip codes with low counts (**Line 28**). For some zip codes we only have one or two data points, making it extremely challenging, if not impossible, to obtain accurate house price estimates.

- Return the data frame to the calling function (**Line 33**).


Now let’s create the 
process_house_attributes function used to preprocess our data:



||def process_house_attributes(df, train, test): # initialize the column names of the continuous data continuous = ["bedrooms", "bathrooms", "area"]  # performin min-max scaling each continuous feature column to # the range [0, 1] cs = MinMaxScaler() trainContinuous = cs.fit_transform(train[continuous]) testContinuous = cs.transform(test[continuous])|

 # initialize the column names of the continuous data

 

 # the range [0, 1]

 trainContinuous = cs.fit_transform(train[continuous])

We define the function on **Line 35**. The 
process_house_attributes function accepts three parameters:


df : Our data frame generated by pandas (the previous function helps us to drop some records from the data frame)

train : Our training data for the House Prices Dataset

test : Our testing data.

Then on **Line 37**, we define the columns of our our continuous data, including bedrooms, bathrooms, and size of the home.

We’ll take these values and use scikit-learn’s 
MinMaxScaler to scale the continuous features to the range *[0, 1]* (**Lines 41-43**).

Now we need to pre-process our categorical features, namely the zip code:



|45464748495051525354555657| # one-hot encode the zip code categorical data (by definition of # one-hot encoing, all output features are now in the range [0, 1]) zipBinarizer = LabelBinarizer().fit(df["zipcode"]) trainCategorical = zipBinarizer.transform(train["zipcode"]) testCategorical = zipBinarizer.transform(test["zipcode"])  # construct our training and testing data points by concatenating # the categorical features with the continuous features trainX = np.hstack([trainCategorical, trainContinuous]) testX = np.hstack([testCategorical, testContinuous])  # return the concatenated training and testing data return (trainX, testX)|

46


48


50


52


54


56


 # one-hot encoing, all output features are now in the range [0, 1])

 trainCategorical = zipBinarizer.transform(train["zipcode"])

 

 # the categorical features with the continuous features

 testX = np.hstack([testCategorical, testContinuous])

 # return the concatenated training and testing data

First, we’ll one-hot encode the zip codes (**Lines 47-49**).

Then we’ll concatenate the *categorical features* with the *continuous features* using NumPy’s 
hstack function (**Lines 53 and 54**), returning the resulting training and testing sets as a tuple (**Line 57**).

Keep in mind that now *both* our categorical features and continuous features are *all* in the range *[0, 1].*

### Implementing a Neural Network for Regression
![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_arch.png)


**Figure 5:** Our Keras regression architecture. The input to the network is a datapoint including a home’s # Bedrooms, # Bathrooms, Area/square footage, and zip code. The output of the network is a single neuron with a linear activation function. Linear activation allows the neuron to output the predicted price of the home.

Before we can train a Keras network for regression, we first need to define the architecture itself.

Today weâ€™ll be using a simple Multilayer Perceptron (MLP) as shown in **Figure 5**.

Open up the 
models.py file and insert the following code:



|123456789101112131415161718192021222324|# import the necessary packagesfrom keras.models import Sequentialfrom keras.layers.normalization import BatchNormalizationfrom keras.layers.convolutional import Conv2Dfrom keras.layers.convolutional import MaxPooling2Dfrom keras.layers.core import Activationfrom keras.layers.core import Dropoutfrom keras.layers.core import Densefrom keras.layers import Flattenfrom keras.layers import Inputfrom keras.models import Model def create_mlp(dim, regress=False): # define our MLP network model = Sequential() model.add(Dense(8, input_dim=dim, activation="relu")) model.add(Dense(4, activation="relu"))  # check to see if the regression node should be added if regress: model.add(Dense(1, activation="linear"))  # return our model return model|

2


4


6


8


10


12


14


16


18


20


22


24


from keras.models import Sequential

from keras.layers.convolutional import Conv2D

from keras.layers.core import Activation

from keras.layers.core import Dense

from keras.layers import Input

 

 # define our MLP network

 model.add(Dense(8, input_dim=dim, activation="relu"))

 

 if regress:

 

 return model

First, we’ll import all of the necessary modules from Keras (**Lines 2-11**). Weâ€™ll be adding a Convolutional Neural Network to this file in *next weekâ€™s tutorial*, hence the additional imports that arenâ€™t utilized here today.

Let’s define the MLP architecture by writing a function to generate it called 
create_mlp .

The function accepts two parameters:


dim : Defines our input dimensions

regress : A boolean defining whether or not our regression neuron should be added

We’ll go ahead and start construction our MLP with a 
dim-8-4 architecture (**Lines 15-17**).

If we are performing regression, we add a 
Dense layer containing a single neuron with a linear activation function (**Lines 20 and 21**). Typically we use ReLU-based activations, but since we are performing regression we need a linear activation.

Finally, our 
model is returned on **Line 24**.

### Implementing our Keras Regression Script

Itâ€™s now time to put all the pieces together!

Open up the 
mlp_regression.py file and insert the following code:



||# import the necessary packagesfrom keras.optimizers import Adamfrom sklearn.model_selection import train_test_splitfrom pyimagesearch import datasetsfrom pyimagesearch import modelsimport numpy as npimport argparseimport localeimport os # construct the argument parser and parse the argumentsap = argparse.ArgumentParser()ap.add_argument("-d", "--dataset", type=str, required=True, help="path to input dataset of house images")args = vars(ap.parse_args())|

from keras.optimizers import Adam

from pyimagesearch import datasets

import numpy as np

import locale

 

ap = argparse.ArgumentParser()

 help="path to input dataset of house images")

We begin by importing necessary packages, modules, and libraries.

Namely, we’ll need the 
Adam optimizer from Keras, 
train_test_split from scikit-learn, and our 
datasets + 
models functions from the 
pyimagesearch module.

Additionally, we’ll use math features from NumPy for collecting statistics when we evaluate our model.

The 
argparse module is for parsing command line arguments.

Our script requires just one command line argument 
--dataset (**Lines 12-15**). You’ll need to provide the 
--dataset switch and the actual path to the dataset when you go to run the training script in your terminal.

Let’s load the house dataset attributes and construct our training and testing splits:



||# construct the path to the input .txt file that contains information# on each house in the dataset and then load the datasetprint("[INFO] loading house attributes...")inputPath = os.path.sep.join([args["dataset"], "HousesInfo.txt"])df = datasets.load_house_attributes(inputPath) # construct a training and testing split with 75% of the data used# for training and the remaining 25% for evaluationprint("[INFO] constructing training/testing split...")(train, test) = train_test_split(df, test_size=0.25, random_state=42)|

# on each house in the dataset and then load the dataset

inputPath = os.path.sep.join([args["dataset"], "HousesInfo.txt"])

 

# for training and the remaining 25% for evaluation

(train, test) = train_test_split(df, test_size=0.25, random_state=42)

Using our handy 
load_house_attributes function, and by passing the 
inputPath to the dataset itself, our data is loaded into memory (**Lines 20 and 21**).

Our training (75%) and testing (25%) data is constructed via **Line 26** and scikit-learn’s 
train_test_split method.

Let’s scale our house pricing data:



||# find the largest house price in the training set and use it to# scale our house prices to the range [0, 1] (this will lead to# better training and convergence)maxPrice = train["price"].max()trainY = train["price"] / maxPricetestY = test["price"] / maxPrice|

# scale our house prices to the range [0, 1] (this will lead to

maxPrice = train["price"].max()

testY = test["price"] / maxPrice

As stated in the comment, scaling our house prices to the range *[0, 1]* will allow our model to more easily train and converge. Scaling the output targets to *[0, 1]* will reduce the range of our output predictions (versus [0, 
maxPrice ]) and make it not only ***easier and faster to train our network*** but enable our model to ***obtain better results as well.***

Thus, we grab the maximum price in the training set (**Line 31**), and proceed to scale our training and testing data accordingly (**Lines 32 and 33**).

Let’s process the house attributes now:



||# process the house attributes data by performing min-max scaling# on continuous features, one-hot encoding on categorical features,# and then finally concatenating them togetherprint("[INFO] processing data...")(trainX, testX) = datasets.process_house_attributes(df, train, test)|

# on continuous features, one-hot encoding on categorical features,

print("[INFO] processing data...")

Recall from the 
datasets.py script that the 
process_house_attributes function:

- Pre-processes our categorical and continuous features.

- Scales our continuous features to the range *[0, 1]* via min-max scaling.

- One-hot encodes our categorical features.

- Concatenates the categorical and continuous features to form the final feature vector.


Now let’s go ahead and fit our MLP model to the data:



||# create our MLP and then compile the model using mean absolute# percentage error as our loss, implying that we seek to minimize# the absolute percentage difference between our price *predictions*# and the *actual prices*model = models.create_mlp(trainX.shape[1], regress=True)opt = Adam(lr=1e-3, decay=1e-3 / 200)model.compile(loss="mean_absolute_percentage_error", optimizer=opt) # train the modelprint("[INFO] training model...")model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200, batch_size=8)|

# percentage error as our loss, implying that we seek to minimize

# and the *actual prices*

opt = Adam(lr=1e-3, decay=1e-3 / 200)

 

print("[INFO] training model...")

 epochs=200, batch_size=8)

Our 
model is initialized with the 
Adam optimizer (**Lines 45 and 46**) and then compiled (**Line 47**). Notice that we’re using *mean absolute percentage error* as our loss function, indicating that we seek to minimize the *mean percentage difference* between the predicted price and the actual price.

The actual training process is kicked off on **Lines 51 and 52.**

After training is complete we can evaluate our model and summarize our results:



|54555657585960616263646566676869707172737475|# make predictions on the testing dataprint("[INFO] predicting house prices...")preds = model.predict(testX) # compute the difference between the *predicted* house prices and the# *actual* house prices, then compute the percentage difference and# the absolute percentage differencediff = preds.flatten() - testYpercentDiff = (diff / testY) * 100absPercentDiff = np.abs(percentDiff) # compute the mean and standard deviation of the absolute percentage# differencemean = np.mean(absPercentDiff)std = np.std(absPercentDiff) # finally, show some statistics on our modellocale.setlocale(locale.LC_ALL, "en_US.UTF-8")print("[INFO] avg. house price: {}, std house price: {}".format( locale.currency(df["price"].mean(), grouping=True), locale.currency(df["price"].std(), grouping=True)))print("[INFO] mean: {:.2f}%, std: {:.2f}%".format(mean, std))|

55


57


59


61


63


65


67


69


71


73


75


print("[INFO] predicting house prices...")

 

# *actual* house prices, then compute the percentage difference and

diff = preds.flatten() - testY

absPercentDiff = np.abs(percentDiff)

# compute the mean and standard deviation of the absolute percentage

mean = np.mean(absPercentDiff)

 

locale.setlocale(locale.LC_ALL, "en_US.UTF-8")

 locale.currency(df["price"].mean(), grouping=True),

print("[INFO] mean: {:.2f}%, std: {:.2f}%".format(mean, std))

**Line 56** instructs Keras to make predictions on our testing set.

Using the predictions, we compute the:

1. Difference between *predicted* house prices and the *actual* house prices (**Line 61**).

1. Percentage difference (**Line 62**).

1. Absolute percentage difference (**Line 63**).


From there, on **Lines 67 and 68**, we calculate the mean and standard deviation of the absolute percentage difference.

The results are printed via **Lines 72-75**.

Regression with Keras wasn’t so tough, now was it?

Let’s train the model and analyze the results!

### Keras Regression Results
![](https://www.pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_predicted.png)


**Figure 6:** For today’s blog post, our Keras regression model takes four numerical inputs, producing one numerical output: the predicted value of a home.

To train our own Keras network for regression and house price prediction make sure you have:

1. Configured your development environment according to the guidance above.

1. Used the ***â€œDownloadsâ€�*** section of this tutorial to download the source code.

1. Downloaded the house prices dataset based on the instructions in the *“The House Prices Datasetâ€�* section above.


From there, open up a terminal and supply the following command (making sure the 
--dataset command line argument points to where you downloaded the house prices dataset):



|161718192021222324252627282930313233343536373839404142434445464748495051525354555657|$ python mlp_regression.py --dataset Houses-dataset/Houses\ Dataset/[INFO] loading house attributes...[INFO] constructing training/testing split...[INFO] processing data...[INFO] training model...Train on 271 samples, validate on 91 samplesEpoch 1/200271/271 [==============================] - 0s 680us/step - loss: 84.0388 - val_loss: 61.7484Epoch 2/200271/271 [==============================] - 0s 110us/step - loss: 49.6822 - val_loss: 50.4747Epoch 3/200271/271 [==============================] - 0s 112us/step - loss: 42.8826 - val_loss: 43.5433Epoch 4/200271/271 [==============================] - 0s 112us/step - loss: 38.8050 - val_loss: 40.4323Epoch 5/200271/271 [==============================] - 0s 112us/step - loss: 36.4507 - val_loss: 37.1915Epoch 6/200271/271 [==============================] - 0s 112us/step - loss: 34.3506 - val_loss: 35.5639Epoch 7/200271/271 [==============================] - 0s 111us/step - loss: 33.2662 - val_loss: 37.5819Epoch 8/200271/271 [==============================] - 0s 108us/step - loss: 32.8633 - val_loss: 30.9948Epoch 9/200271/271 [==============================] - 0s 110us/step - loss: 30.4942 - val_loss: 30.6644Epoch 10/200271/271 [==============================] - 0s 107us/step - loss: 28.9909 - val_loss: 28.8961...Epoch 195/200271/271 [==============================] - 0s 111us/step - loss: 20.8431 - val_loss: 21.4466Epoch 196/200271/271 [==============================] - 0s 109us/step - loss: 22.2301 - val_loss: 21.8503Epoch 197/200271/271 [==============================] - 0s 112us/step - loss: 20.5079 - val_loss: 21.5884Epoch 198/200271/271 [==============================] - 0s 108us/step - loss: 21.0525 - val_loss: 21.5993Epoch 199/200271/271 [==============================] - 0s 112us/step - loss: 20.4717 - val_loss: 23.7256Epoch 200/200271/271 [==============================] - 0s 107us/step - loss: 21.7630 - val_loss: 26.0129[INFO] predicting house prices...[INFO] avg. house price: $533,388.27, std house price: $493,403.08[INFO] mean: 26.01%, std: 18.11%|

17


19


21


23


25


27


29


31


33


35


37


39


41


43


45


47


49


51


53


55


57


[INFO] loading house attributes...

[INFO] processing data...

Train on 271 samples, validate on 91 samples

271/271 [==============================] - 0s 680us/step - loss: 84.0388 - val_loss: 61.7484

271/271 [==============================] - 0s 110us/step - loss: 49.6822 - val_loss: 50.4747

271/271 [==============================] - 0s 112us/step - loss: 42.8826 - val_loss: 43.5433

271/271 [==============================] - 0s 112us/step - loss: 38.8050 - val_loss: 40.4323

271/271 [==============================] - 0s 112us/step - loss: 36.4507 - val_loss: 37.1915

271/271 [==============================] - 0s 112us/step - loss: 34.3506 - val_loss: 35.5639

271/271 [==============================] - 0s 111us/step - loss: 33.2662 - val_loss: 37.5819

271/271 [==============================] - 0s 108us/step - loss: 32.8633 - val_loss: 30.9948

271/271 [==============================] - 0s 110us/step - loss: 30.4942 - val_loss: 30.6644

271/271 [==============================] - 0s 107us/step - loss: 28.9909 - val_loss: 28.8961

Epoch 195/200

Epoch 196/200

Epoch 197/200

Epoch 198/200

Epoch 199/200

Epoch 200/200

[INFO] predicting house prices...

[INFO] mean: 26.01%, std: 18.11%

As you can see from our output, our initial mean absolute percentage error starts off as high as 84% and then quickly drops to under 30%.

By the time we finish training we can see our network starting to overfit a bit. Our training loss is as low as ~21%; however, our validation loss is at ~26%.

Computing our final mean absolute percentage error we obtain a final value of **26.01%.**

**What does this value mean?**

Our final mean absolute percentage error implies, that on average, our network will be ~26% off in its house price predictions with a standard deviation of ~18%.

### Limitations of the House Price Dataset

Being 26% off in a house price prediction is a good start but is certainly *not* the type of accuracy we are looking for.

That said, this prediction accuracy can also be seen as a *limitation* of the house price dataset itself.

Keep in mind that the dataset only includes four attributes:

1. Number of bedrooms

1. Number of bathrooms

1. Area (i.e., square footage)

1. Zip code


Most other house price datasets include *many* more attributes.

For example, the Boston House Prices Dataset includes a total of *fourteen attributes* which can be leveraged for house price prediction (although that dataset does have some racial discrimination).

The Ames House Dataset *includes over 79 different attributes* which can be used to train regression models.

When you think about it, the fact that we are able to even obtain 26% mean absolute percentage error without the knowledge of an expert real estate agent is fairly reasonable given:

1. There are only 535 total houses in the dataset (we only used **362 total houses** for the purpose of this guide).

1. We only have **four attributes** to train our regression model on.

1. The attributes themselves, while important in describing the home itself, **do little to characterize the area surrounding the house.**

1. **The house prices are incredibly varied** with a mean of $533K and a standard deviation of $493K (based on our filtered dataset of 362 homes).


**With all that said, learning how to perform regression with Keras is an important skill!**

In the next two posts in this series Iâ€™ll be showing you how to:

1. Leverage the images provided with the house price dataset to train a CNN on them.

1. Combine our numerical/categorical data with the house images, leading to a model that outperforms all of our previous Keras regression experiments.


## Summary

In this tutorial, you learned how to use the Keras deep learning library for regression.

Specifically, we used Keras and regression to predict the price of houses based on four numerical and categorical attributes:

- Number of bedrooms

- Number of bathrooms

- Area (i.e., square footage)

- Zip code


Overall our neural network obtained a mean absolute percentage error of 26.01%, implying that, on average, our house price predictions will be off by 26.01%.

That raises the questions:

- How can we *better* our house price prediction accuracy?

- What if we *leveraged images* for each house? Would that improve accuracy?

- Is there some way to *combine* both our categorical/numerical attributes with our image data?


To answer these questions youâ€™ll need to stay tuned for the remaining to tutorials in this Keras regression series.

**To download the source code to this post (and be notified when the next tutorial is published here on PyImageSearch), *just enter your email address in the form below.***

## Downloads:
