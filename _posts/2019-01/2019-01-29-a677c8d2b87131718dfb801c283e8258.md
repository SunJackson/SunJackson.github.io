---
layout:     post
catalog: true
title:      Web Scraping Google Sheets with RSelenium
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/f4zZKIppUNk/
date:      2019-01-29
author:      Matt.0
tags:
    - tables
    - â page
    - remdr
    - dates
    - rselenium
---





![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*eBZrWn2oyPO3oAIm-v5uPg.jpeg?w=456&ssl=1)
![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*eBZrWn2oyPO3oAIm-v5uPg.jpeg?w=456&ssl=1)
Photo by freestocks.org on Unsplash
I love to learn new things and one of ways I learn best is by doing. Also itâ€™s been said that you never fully understand a topic until you are able to explain it , I think blogging is a low barrier to explaining things.

Someone I met at a local data science meetup in MontrÃ©al wanted help web scraping to get team standings from the PuzzledPint. I jumped at the opportunity because I knew this would be my opportunity to finally learn **RSelenium!**

![](https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*txFP2X4BWaCm0fefpSzqbg.jpeg?w=456&ssl=1)


### Static Scraping vs. Dynamic Scraping

Static scraping ignores JavaScript. It fetches web pages from the server without the help of a browser. You get exactly what you see in â€œview page sourceâ€�, and then you slice and dice it. If the content youâ€™re looking for is available, you need to go no further. However, if the content is something like an `iframe`, you need dynamic scraping.

Dynamic scraping uses an actual browser (or a headless browser) and lets JavaScript do its thing. Then, it queries the DOM to extract the content itâ€™s looking for. Sometimes you need to automate the browser by simulating a user to get the content you need. In order for me to get the same details for the remaining posts, i would need to first navigate to the next page, which involves clicking the Next button at the bottom of the search results page.

### Setting up RSelenium with Docker

RSelenium provides **R** bindings for the Selenium Webdriver API. Selenium is a project focused on automating web browsers.

You need to follow the installation instructions for Docker Toolbox on Windows or Ubuntu.

![](https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*XvJ0GDWOAEHNApZvw-dOVQ.png?w=456&ssl=1)


Docker is used to run applications by using containers. Containers are simply a bundle of libraries and other dependencies in one package. You can think of it like a virtual machine, but rather than creating a whole OS it allows applications to use the same Linux kernel with only the *things* not already running on the host computer. Basically, it gives a significant performance boost and reduces the size of the application. Moreover, you can rest assured that the application will run on any other Linux machine regardless of any customized settings that machine might have that could differ from the machine used for writing and testing the code.

Youâ€™ll also need to install TightVNC which will allow you to see how youâ€™re manipulating the web page in real-time with RSelenium.

Next follow the instructions to create a Docker container running a selenium server and its own firefox.

![](https://i1.wp.com/cdn-images-1.medium.com/max/844/1*rMlftpSVS9BHbh9UBwCgsw.png?w=456&ssl=1)


> *Note: Once youâ€™ve set up the docker container (and everytime you restart your computer or start-up fresh again) open the Docker Quickstart Terminal and run the following command.*

Now that youâ€™ve booted your **Docker Quickstart Terminal** go into **R**and connect to a running server.

Navigate to the page using Rselenium.

Okay letâ€™s get a live screen shot of the site in the **Viewer** tab of RStudio.

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*tzaElAcPM2Mp-8tiVvHHXg.jpeg?w=456&ssl=1)


Keep in mind that this is just a static screen shot. Your going to want to use **TightVNC** to get a live view of your interactions while your developing your pipeline so you can see how your interacting with the website.

> Itâ€™s important to be watching TightVNC as you use the â€¦$highlightElement() in-between your â€¦$findElement() and â€¦$switchToFrame()/â€¦$clickElement() commands so that you actually know your selecting the appropriate things!

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*v4u3CxR8Z3e5PSwnzEpjKQ.jpeg?w=456&ssl=1)


Open the **TighVNC Viewer** and enter the port number; in this case 192.168.99.100 and enter that in the **Remote Host:** field. Click **Connect** and for the password enter the word: secret.

> Note: If TightVNC ever stops working (on Windows 10 it did often) and gives you the Error: â€œNO CONNECTION COULD BE MADE BECAUSE THE TARGET MACHINE ACTIVELY REFUSED ITâ€� then follow the steps for â€œDebugging Using VNCâ€� here.

### Accessing Elements in the DOM

Web pages are a set of nest objects (together, they are known as the **Document Object Model** or **DOM** for short). Itâ€™s a cross-platform and language-independent convention for representing and interacting with objects in HTML, XHTML and XML documents. Interacting with the DOM will be very important for us with RSelenium.

Hadley Wickham recommends using Selectorgadget, a Chrome extension, to help identify the web page elements you need. And he recommends this page for learning more about selectors.

For example by using SelectorGadget you can the table youâ€™re interested in. In this case it says itâ€™s an **iframe**. To isolate just the MontrÃ©al standings weâ€™ll click another box to only select the one of interest: iframe:nth-child(68).

![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*RDfMiT9bM80mW7xMeD-jPQ.jpeg?w=456&ssl=1)


In the context of a web browser, a frame is a part of a web page or browser window which displays content independent of its container, with the ability to load content independently. In this case the website is pulling in data from another source to display these tables interactively apart from the main standings page. Luckily for me they are all from Google Sheets so this will make it *much* easier for me. Unfortunately, you canâ€™t find the links for these sheets with `selectorgadget`. You will need to take a closer look at the source code using the Developerâ€™s Tool called `Inspector` in either Chrome of Firefox. If you have Windows and Firefox you would click **Open Menu** then Web Developer > Inspector or just Ctrl+Shift+c. Then I used the search box to look for the link (src=) for MontrÃ©al.

![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*yEL28XbgOFIF0yTR5SmAMg.jpeg?w=456&ssl=1)


> For me it was a big pain to actually find what I was looking for as sometimes the highlight looks like what you want but itâ€™s not. For example:

![](https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*8Bu0acQcB_CpRmdDhOKLtA.jpeg?w=456&ssl=1)


In the end I guess to figure out HTML it involves gradual â€œ**Denial and Error**â€� attempts.

![](https://i1.wp.com/cdn-images-1.medium.com/max/614/1*CP-fXLA8NMeNbl81qjJ2wQ.jpeg?w=456&ssl=1)


### Webscraping Google Sheets with RSelenium

#### Legal Disclaimer

Itâ€™s worth mentioning that administrators may want to protect certain parts of their website for a number of reasons, such as â€œ*indexing of an unannounced site, traversal of parts of the site which require vast resources of the server, recursive traversal of an infinite URL space, etc.*â€�

Therefore, one should always check if they have permission. One way to do this, is to use the robotstxt package to check if your web-bot has permission to access certain parts of a web-page.

If it says **TRUE** on the specific page you have permission. Alternatively, just go to the robots.txt file on the url of the main page to get a broader sense of what is (and isnâ€™t) allowed.

![](https://i1.wp.com/cdn-images-1.medium.com/max/360/1*Y5y6dhHtDd_6U0m9L8KNNg.jpeg?w=456&ssl=1)


#### Method # 1

Sometimes websites can be composed using frames. These are in effect seperate webpages which are brought together in a frameset. We will need to jump back-and-forth between these frames.

Now that we have the first month we can create a for loop for the rest of the dates. First letâ€™s switch back the outer frame and select the elements we will be manipulating.

So I know thereâ€™s going to be many tables, but just how many? We can check this use this via length(webElems).

![](https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*yQhjpdvfaEJ95OvaNY-C8A.jpeg?w=456&ssl=1)


There is actually 49 tables in total but since we started on the first one above there is only 48 links. Rather than hard-coding 1:48 itâ€™s better to do it via code as there will be more tables added in the future.

The problem here is that the for loop eventually fails at the end when it tries to click the right arrow but itâ€™s as far to the right as it can goâ€Šâ€”â€Štherefore it wonâ€™t download the last few tables (~5). Typically one would handle such conditions with something like:

and it usually works fine, including when using selenium. The issue encountered here however is clicking on the arrow once would always bring me to the *last* visible sheetsâ€Šâ€”â€Šskipping everything in middle. Therefore my work around to get the remaining sheets was this:

#### Method # 2

Big thanks to Nate on SO for pointing out an alternative solution that solves the task of *scrapping the tables *but**not** the task of exception handling in the above sense.
