---
layout:     post
catalog: true
title:      Window Aggregate operator in batch mode in SQL Server 2019
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/W00i_mN236U/
date:      2019-01-18
author:      tomaztsql
tags:
    - select
    - â
    - sets
    - rows
    - median
---





So this came as a surprise, when working on calculating simple statistics on my dataset, in particular min, max and median. First two are trivial. The last one was the one, that caught my attention.

While finding the fastest way on calculating the median (statistic: median) for given dataset, I have stumbled upon an interesting thing. While WINDOW function was performing super slow and calling R or Python using sp_execute_xternal_script outperform window function as well, it raised couple of questions.

But first, I created a sample table and populate it sample rows:

 

Query generated â€“ in my case â€“ little over 13 million records, just enough to test the performance.

So starting with calculating Median, but sorting first half and second half of rows respectively, the calculation time was surprisingly long:

Before and after each run, I cleaned the stored execution plan. The execution on 13 million rows took â€“ on my laptop â€“ around 45 seconds.

Next query, for median calculation was a window function query.

To my surprise, the performance was even worse, and at this time, I have to say, I was running this on SQL Server 2017 with CU7. But luckily, I had a SQL Server 2019 CTP 2.0 also installed and here, with no further optimization the query ran little over 1 second.

So the difference between the versions was enormous. I could replicate the same results by switching the database compatibility level from 140 to 150, respectively.

The answer was found in execution plan. When running window function under 140 compatibility level, execution plan decides to create nested loop two times, for both groups of upper and lower 50% of the dataset.![](https://tomaztsql.files.wordpress.com/2019/01/comp140_1.png?w=456)
![](https://tomaztsql.files.wordpress.com/2019/01/comp140_1.png?w=456)


This plan is is somehow similar to understanding of 50% of upper and lower dataset but with only one nested loop:

![](https://tomaztsql.files.wordpress.com/2019/01/itzik1.png?w=456)
![](https://tomaztsql.files.wordpress.com/2019/01/itzik1.png?w=456)


Difference is that when running the window function calculation of median on SQL Server version 2017, the query optimizer decides to take row execution mode for built-in window function with WITHIN GROUP.

![](https://tomaztsql.files.wordpress.com/2019/01/comp140_2.png?w=456)
![](https://tomaztsql.files.wordpress.com/2019/01/comp140_2.png?w=456)


This was, as far as I knew, not an issue since SQL Server 2016, where batch mode operator for window aggregation was already used.

When switching to compatibility level 150 and running the same window function, the execution plan is, as expected:

![](https://tomaztsql.files.wordpress.com/2019/01/comp150_1.png?w=456)
![](https://tomaztsql.files.wordpress.com/2019/01/comp150_1.png?w=456)


And window aggregate uses batch mode:

![](https://tomaztsql.files.wordpress.com/2019/01/comp150_2.png?w=456)
![](https://tomaztsql.files.wordpress.com/2019/01/comp150_2.png?w=456)


When calculating Median using R:

or Python:

both are executing and returning the results in about 5 seconds. So no bigger difference between R and Python when handling 13 million rows for calculating simple statistics.

To wrap up, If you find yourself in situation, where you need to calculate â€“ as in my case â€“ Median or any statistics, using window function within group, R or Python would be the fastest solutions, following T-SQL. Unless, you have the ability to use SQL Server 2019, T-SQL is your best choice.

Code and the plans, used in this blog post are available, as always at Github.


*Related*








---
