---
layout:     post
catalog: true
title:      10 Tips for Choosing the Optimal Number of Clusters
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/ETKeO7hu37c/
date:      2019-01-27
author:      Matt.0
tags:
    - clustering
    - clusters
    - methods
    - library
    - mammals_scaled
---





![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*nh_06G9VBvDKHbFHpyFayA.jpeg?w=456&ssl=1)
![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*nh_06G9VBvDKHbFHpyFayA.jpeg?w=456&ssl=1)
Photo by Pakata Goh on Unsplash
Clustering is one of the most common unsupervised machine learning problems. Similarity between observations is defined using some inter-observation distance measures or correlation-based distance measures.

There are 5 classes of clustering methods:

> + Hierarchical Clustering+ Partitioning Methods (k-means, PAM, CLARA)+ Density-Based Clustering+ Model-based Clustering+ Fuzzy Clustering

My desire to write this post came mainly from reading about the **clustree**package, the **dendextend** documentation, and the Practical Guide to Cluster Analysis in R book written by Alboukadel Kassambara author of the **factoextra******package.

### Data Set

I will be using a lesser known data set from the **cluster** package: all.mammals.milk.1956, one which I havenâ€™t looked at before.

This small dataset contains a list of 25 mammals and the constituents of their milk (water, protein, fat, lactose, ash percentages) from John Hartigan, Clustering Algorithms, Wiley, 1975.

First letâ€™s load the required packages.

Now load the data.

Letâ€™s explore and visualize the data.

All the variables are expressed as numeric. What about the statistical distribution?

![](https://i1.wp.com/cdn-images-1.medium.com/max/397/1*_LTOQcmFOfw6gAhAw6QmpA.png?w=456&ssl=1)


![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*i76oVZvwpPxZCYdovmanIw.png?w=456&ssl=1)


Whatâ€™s the relationship between the different attributes? Use `corrplot()` to create correlation matrix.

![](https://i2.wp.com/cdn-images-1.medium.com/max/415/1*P5xSIYPsSttLh5Rb5uXwUQ.png?w=456&ssl=1)


When you have variables which are measured in different scales it is useful to scale the data.

Dimensionality reduction can help with data visualization (*e.g. *PCA method).

![](https://i1.wp.com/cdn-images-1.medium.com/max/415/1*kCLvb6sYp-QyfCifw7Kyow.png?w=456&ssl=1)


These are the **5 PCs that capture 80% of the variance**. The scree plot shows that **PC1 captured ~ 75% of the variance**.

![](https://i1.wp.com/cdn-images-1.medium.com/max/415/1*6JIZBX699nUPNq-0I9A0Bg.png?w=456&ssl=1)


![](https://i2.wp.com/cdn-images-1.medium.com/max/415/1*O9lhD0BkpKtAv0wZlV6ZzA.png?w=456&ssl=1)


![](https://i0.wp.com/cdn-images-1.medium.com/max/415/1*wdGFx8hVFmij0gGHckY_Dw.png?w=456&ssl=1)


From these visualizations itâ€™s apparrent that water and lactose tend to increase together and that protein, ash and fat increase together; the two groups being inversely related.

### NaÃ¯ve K-means clustering

Partitioning clustering methods, like k-means and Partitioning Around Medoids (PAM), require that you specify the number of clusters to be generated.

k-means clusters is probably one of the most well known partitioning methods. The idea behind k-means clustering consists of defining clusters the **total within-cluster variation** , which measures the compactness of the clusters is minimized.

We can compute k-means in R with the **kmeans()** function:

The above example would group the data into two clusters, **centers = 2**, and attempt multiple initial configurations, reporting on the best one. For example, as this algorithm is sensitive to the initial positions of the cluster centroids adding **nstart = 30** will generate 30 initial configurations and then average all the centroid results.

Because the number of clusters (**k**) needs to be set before we start it can be advantageous to examine several different values of **k**.

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*7Lfl-pqYn44gypN1qlQMmg.png?w=456&ssl=1)


Although this visual assessment tells us where delineations occur between clusters, it does not tell us what the optimal number of clusters is.

### Determining Optimal Number of Clusters

A variety of measures have been proposed in the literature for evaluating clustering results. The term **clustering validation** is used to design the procedure of evaluating the results of a clustering algorithm. There are more than thirty indices and methods for identifying the optimal number of clusters so Iâ€™ll just focus on a few here including the very neat **clustree** package.

#### The â€œElbowâ€� Method

Probably the most well known method, the elbow method, in which the sum of squares at each number of clusters is calculated and graphed, and the user looks for a change of slope from steep to shallow (an elbow) to determine the optimal number of clusters. This method is inexact, but still potentially helpful.

![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*zQUkkjBgQBEWigJVNJc9DA.png?w=456&ssl=1)


The Elbow Curve method is helpful because it shows how increasing the number of the clusters contribute separating the clusters in a meaningful way, not in a marginal way. The bend indicates that additional clusters beyond the third have little value. (See [here] for a more mathematically rigorous interpretation and implementation of this method).

#### The Gap Statistic

The gap statistic compares the total within intra-cluster variation for different values of **k** with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (*i.e.*, that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*1KMRhZEIy0je24ksyiByxQ.png?w=456&ssl=1)


The gap stats plot shows the statistics by number of clusters (**k**) with standard errors drawn with vertical segments and the optimal value of **k** marked with a vertical dashed blue line. According to this observation **k = 2** is the optimal number of clusters in the data.

#### The Silhouette Method

Another visualization that can help determine the optimal number of clusters is called the a silhouette method. Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for **k**.

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*gtRd99n__IAB3Z1J-hOcpw.png?w=456&ssl=1)


This also suggests an optimal of 2 clusters.

#### The Sum of Squares Method

Another clustering validation method would be to choose the optimal number of cluster by minimizing the within-cluster sum of squares (a measure of how tight each cluster is) and maximizing the between-cluster sum of squares (a measure of how seperated each cluster is from the others).

![](https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*9vEjv6Xir4HX58pBjNMfkQ.png?w=456&ssl=1)


From this measurement it appears 7 clusters would be the appropriate choice.

#### NbClust

The **NbClust** package provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*QzzyNowIPofo4Ak-z42n-w.png?w=456&ssl=1)


This suggest the optimal number of clusters is 3.

#### Clustree

The statistical method above produce a single score that only considers a single set of clusters at a time. The **clustree** R package takes an alternative approach by considering how samples change groupings as the number of clusters increases. This is useful for showing which clusters are distinct and which are unstable. It doesnâ€™t explicitly tell you which choice of *optimal* clusters is but it is useful for exploring possible choices.

Letâ€™s take a look at 1 to 11 clusters.

![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*T43RY6iKGjqtKSFvr8YsLg.png?w=456&ssl=1)


In this figure the size of each node corresponds to the number of samples in each cluster, and the arrows are coloured according to the number of samples each cluster receives. A separate set of arrows, the transparent ones, called the incoming node proportion, are also coloured and shows how samples from one group end up in another groupâ€Šâ€”â€Šan indicator of cluster instability.

In this graph we see that as we move from **k=2** to **k=3** a number of species from the lookers-left cluster are reasigned to the third cluster on the right. As we move from **k=8** to **k=9** we see one node with multiple incoming edges an indicator that we over-clustered the data.

It can also be useful to overlay this dimension on other dimensions in the data, particularly those that come from dimensionality reduction techniques. We can do this using the **clustree_overlay()** function:

![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*hiVccajlxLATh9DJjacMhw.png?w=456&ssl=1)


I prefer to see it from the side, showing one of the x or y dimensions against the resolution dimension.

![](https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*mS_Ix9xschUWuAwjLHYHRg.png?w=456&ssl=1)


![](https://i1.wp.com/cdn-images-1.medium.com/max/1024/1*cLXIl9t0312kiSjvopoE1g.png?w=456&ssl=1)


This shows that we can an indication of the correct clustering resolution by examining the edges and we can overly information to assess the quality of the clustering.

### Choosing the appropriate algorithm

What about choice of appropriate clustering algorithm? The **cValid** package can be used to simultaneously compare multiple clustering algorithms, to identify the best clustering approach and the optimal number of clusters. We will compare k-means, hierarchical and PAM clustering.
