---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/01/23/distilled-news-957/
date:      2019-01-23
author:      Michael Laux
tags:
    - models
    - learning
    - learns
    - processing model
    - machines
---

**How to visualize convolutional features in 40 lines of code**

Recently, while reading Jeremy Rifkin’s book ‘The End of Work’, I came across an interesting definition of AI. Rifkin writes: ‘today when scientists talk of artificial intelligence, they generally mean ‘the art of creating machines that perform functions which require intelligence when performed by people.’ (taken from Kurzweil, Raymond, The Age of Intelligent Machines (Cambridge, MA: MIT Press, 1990), p. 14.)’. I like this definition because it avoids the hyped discussion whether AI is truly intelligent in the sense of our intelligence. As a scientist, the thought of unveiling the functional principles of our brain and creating a truly intelligent machine excites me but I think it is important to realize that a lot of current AI research is rather aimed at automizing processes that until now were not automizable. While that may sound less exciting, it still is a great thing. Just one example: the emergence of deep convolutional neural networks revolutionized computer vision and pattern recognition and will allow us to introduce a vast amount of automation in fields such as medical diagnosis. This could allow humanity to quickly bring top medical diagnosis to people in poor countries that are not able to educate the many doctors and experts they would otherwise require.

**Key Steps for Building an Effective AI Organization**

Recently, I got fascinated by the impact of Artificial Intelligence on any business from any sector (tech, banking, manufacturing, etc.) This led me to explore the subject further while trying to understand what a corporation should do to transform its processes using AI. In this article, I would love to summarize my observations into a set of actionable steps which can help any organization kickstart their AI transformation. My thoughts are heavily influenced by the amazing 12-page paper, written by Andrew Ng?-?founder of Landing AI, called ‘AI Transformation Playbook’. In addition, I have taken advice from numerous McKinsey&Company reports like ‘McKinsey on Payments: Special Edition on Advanced Analytics in Banking’.

**Visualizing Principal Component Analysis with Matrix Transforms**

Principal Component Analysis (PCA) is a method of decomposing data into correlated components by identifying eigenvalues and eigenvectors. The following is meant to help visualize what these different values represent and how they’re calculated. First I’ll show how matrices can be used to transform data, then how those matrices are used in PCA.

**AI brings speed to security**

Organizations that use security tools with artificial intelligence (AI) and machine learning (ML) see a significant decrease in incident response time, according to a survey of 457 security practitioners conducted by O’Reilly Media in conjunction with Oracle. Twenty percent of IT professionals who rely on traditional security measures said their teams can detect a malware infection or other attack within minutes, according to the survey. But among IT pros who reported using AI and ML security services, that number more than doubled to 45%. The long tail shows a similar trend: only 16% of IT professionals need days or longer to find an infection when AI or ML is involved, versus a whopping 35% for those who don’t use these technologies.

**Neural Network Models in R**

Neural Network(or Artificial Neural Network) has the ability to learn by examples. ANN is an information processing model inspired by the biological neuron system. It is composed of a large number of highly interconnected processing elements known as the neuron to solve problems. It follows the non-linear path and process information in parallel throughout the nodes. A neural network is a complex adaptive system. Adaptive means it has the ability to change its internal structure by adjusting weights of inputs.

**Introduction to Video Classification**

Many Deep Learning articles and tutorials primarily focus on three data domains: images, speech, and text. These data domains are popular for their applications in image classification, speech recognition, and text sentiment classification. Another very interesting data modality is video. From a dimensionality and size perspective, videos are one of the most interesting data types alongside datasets such as social networks or genetic codes. Video uploading platforms such as YouTube are collecting enormous datasets, empowering Deep Learning research. A video is really just a stack of images. This article will review a paper [1] on video classification research led Andrej Karpathy, currently the Director of AI at Tesla. This paper models videos with Convolutional Networks in a very similar way to how CNNs model images. This paper is a great anecdote to the powerful representation power of Convolutional Networks. Prior to this work, Video Classification research was dominated by a pipeline of visual bag-of-words features quantized into a k-means dictionary and classified with a machine learning model such as an SVM. This work highlights the power of CNNs to abstract away all of these previous feature engineering algorithms. The paper also serves as a good foundation of ideas to integrate the temporal component of videos into CNN models. This paper explores three different components of Video Classification, designing CNNs which account for temporal connectivity in videos, multi-resolution CNNs which can speed up computation, and the effectiveness of transfer learning with Video Classification.

**Data Science and the Paradox of Predictions**

Many data science projects are a hunt for knowledge. As history has taught us through the years, the mere act of knowing can change what it is we believe to know. Professor Harari explores this topic in Homo Deus with the skill we’ve become accustomed to in his work. Giving the example of Marx’s ‘Das Kapital’, Harari provides clarity to the idea that translates into a very valuable lesson.

**Introducing Manifold**

Machine learning programs defer from traditional software applications in the sense that their structure is constantly changing and evolving as the model builds more knowledge. As a result, debugging and interpreting machine learning models is one of the most challenging aspects of real world artificial intelligence(AI) solutions. Debugging, interpretation and diagnosis are active areas of focus of organizations building machine learning solutions at scale. Recently, Uber unveiled Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models. Manifold brings together some very advanced innovations in the areas of machine learning interpretability to address some of the fundamental challenges of visually debugging machine learning models. The challenge of debugging and interpreting machine learning models is nothing new and the industry has produced several tools and frameworks in this area. However, most of the existing stacks focus on evaluating a candidate model using performance metrics such as like log loss, area under curve (AUC), and mean absolute error (MAE) which, although useful, offer little insight in terms of the underlying reasons of the model’s performance. Another common challenge is that most machine learning debugging tools are constrained to a specific types of models(ex: regression or classification) and are very difficult to generalize across broader machine learning architectures. Consequently, data scientists spend tremendous amounts of time trying different model configurations until they can achieve specific performances.

**Why Feature Correlation Matters …. A Lot!**

Machine Learning models are as good or as bad as the data you have. That’s why data scientists can spend hours on pre-processing and cleansing the data. They select only the features that would contribute most to the quality of the resulting model. This process is called ‘Feature Selection’. Feature Selection is the process of selecting the attributes that can make the predicted variable more accurate or eliminating those attributes that are irrelevant and can decrease the model accuracy and quality. Data and feature correlation is considered one important step in the feature selection phase of the data pre-processing especially if the data type for the features is continuous. so what is data correlation?

**Demystifying Logistic Regression**

Logistic Regression is one of the most popular classification technique.In most of the tutorials and articles people usually explain the probabilistic interpretation of Logistic Regression.So in this article i will try to give the geometric intuition of Logistic Regression.The topics that i will cover in this article –• Geometric Intuituion of Logistic Regression• Optimisation Function• Sigmoid Function• Overfitting and Underfitting• Regularisation – L2 and L1

**Chinese Interests Take a Big Seat at the AI Governance Table**

Last summer the Chinese government released its ambitious New Generation Artificial Intelligence Development Plan (AIDP), which set the eye-catching target of national leadership in a variety of AI fields by 2030. The plan matters not only because of what it says about China’s technological ambitions, but also for its plans to shape AI governance and policy. Part of the plan’s approach is to devote considerable effort to standards-setting processes in AI-driven sectors. This means writing guidelines not only for key technologies and interoperability, but also for the ethical and security issues that arise across an AI-enabled ecosystem, from algorithmic transparency to liability, bias, and privacy. This year Chinese organizations took a major step toward putting these aspirations into action by releasing an in-depth white paper on AI standards in January and hosting a major international AI standards meeting in Beijing in April. These developments mark Beijing’s first stake in the ground as a leader in developing AI policy and in working with international bodies, even as many governments and companies around the world grapple with uncharted territory in writing the rules on AI. China is eager to participate in international standards-setting bodies on the question of whether and how to set standards around controversial aspects of AI, such as algorithmic bias and transparency in algorithmic decision making.

**Chronological Representation**

It’s crucial to know the chronological order of events to learn causality, to plan, to synchronize activities in societies and for many other reasons. However, it’s still a huge challenge for both neuroscientists to understand how time is represented in brains and for AI researchers to make agents able to operate in constantly changing environments. Usually cognitive scientists, unlike physicists, treat time completely different from space. Neuroscientists already discovered a lot of mechanisms responsible for circadian rhythms, heartbeat, brainwaves and other periodic biological ‘clocks’, as well as timers operating on the millisecond-second scale. However, generation and storage of event memories and representation of time for AI agents are still open questions.

**Neural Networks with Numpy for Absolute Beginners: Introduction**

In this tutorial, you will get a brief understanding of what Neural Networks are and how they have been developed. In the end, you will gain a brief intuition as to how the network learns. Artificial Intelligence has become one of the hottest fields in the current day and most of us willing to dive into this field start off with Neural Networks!! But on confronting the math intensive concepts of Neural Networks we just end up learning a few frameworks like Tensorflow, Pytorch etc., for implementing Deep Learning Models. Moreover, just learning these frameworks and not understanding the underlying concepts is like playing with a black box. Whether you want to work in the industry or academia, you will be working, tweaking and playing with the models for which you need to have a clear understanding. Both the industry and the academia expect you to have full clarity of these concepts including the math. In this series of tutorials, I’ll make it extremely simple to understand Neural Networks by providing step by step explanation. Also, the math you’ll need will be the level of high school. Let us start with the inception of artificial neural networks and gain some inspiration as to how it evolved.

**Interpolation with Generative Models**

In this post I am going to write about generative models. It’s gonna cover the dichotomy between generative and discriminative models, and how generative models can really learn the essence of objects of interest by being able to perform interpolations.





### Like this:

Like Loading...


*Related*

