---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/01/25/distilled-news-960/
date:      2019-01-25
author:      Michael Laux
tags:
    - data
    - filebeat
    - learned
    - learnings
    - correlated
---

**Tensor Considered Harmful**

Despite its ubiquity in deep learning, Tensor is broken. It forces bad habits such as exposing private dimensions, broadcasting based on absolute position, and keeping type information in documentation. This post presents a proof-of-concept of an alternative approach, named tensors, with named dimensions. This change eliminates the need for indexing, dim arguments, einsum- style unpacking, and documentation-based coding. The prototype PyTorch library accompanying this blog post is available as namedtensor.

**Data Science and Ethics – Why Companies Need a new CEO (Chief Ethics Officer)**

We live in a time, where individuals and organizations can store and analyze massive amounts of information; with over 2.5 ­quintillion records of data created every day. People are being defined by what they search for on the internet, what they eat, where they travel to, where they hold membership accounts, etc. Organizations and individuals can leverage this data to uncover powerful findings. Data scientists/analysts often get lost in the techniques and methods of the trade. In doing so, they can forget to ask important questions such as: Who will be affected by the work? How we are ensuring that, by doing ‘good’ for one group, we are not inadvertently harming another? Who actually owns the data that we are working with? What is the protocol for using that data to make business decisions?

**Artificial Intelligence is just a Tool**

The hype about the possibilities and possible applications of artificial intelligence (AI) seems currently unlimited. The AI procedures and solutions are praised as true panaceas. However, when viewed soberly, they are just another tool in the toolbox of IT experts.

**The basics of deploying Logstash pipelines to Kubernetes**

I’ve worked with the Elastic stack before, more specifically ElasticSearch and Kibana but I felt like there was so much more I could learn from these two products. I also wanted to gain an understanding of Logstash and see what problems it could help me solve. When I start learning something new I set a bunch of small, achievable objectives. One of the objectives I’d written was to have a fully functional, operating Logstash pipeline running in Kubernetes, ingesting data from somewhere, perform some action on it and then send it to ElasticSearch. I’m a huge fan of Kubernetes. There has been a huge shift in the past few years to containerize applications and I fully embrace this shift. I had no interest in running this pipeline I was building locally, its was Kubernetes or bust! I had a bit of a Google and struggled to find anything concrete regarding deployment, best practices etc thus led me to writing a basic article about how to get a basic Filebeat, Logstash and ElasticSearch pipeline running in Kubernetes. So what’s Filebeat? It’s a shipper that runs as an agent and forwards log data onto the likes of ElasticSearch, Logstash etc.

**Filebeat overview**

Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them to either to Elasticsearch or Logstash for indexing. Here’s how Filebeat works: When you start Filebeat, it starts one or more inputs that look in the locations you’ve specified for log data. For each log that Filebeat locates, Filebeat starts a harvester. Each harvester reads a single log for new content and sends the new log data to libbeat, which aggregates the events and sends the aggregated data to the output that you’ve configured for Filebeat.

**GeoPAT2: Entropy calculations for local landscapes**

GeoPAT 2 is an open-source software written in C and dedicated to pattern-based spatial and temporal analysis. Four main types of analysis available in GeoPAT 2 are (i) search, (ii) change detection, (iii) segmentation, and (iv) clustering. However, additional applications are also possible, including extracting information about spatial patterns.

**The Shiny Module Design Pattern**

Foremost in your mind should be the quintessential reality of R: Everything that happens in R is the result of a function call. Shiny is no exception. To write a minimal shiny app, you create an object that describes your app’s user interface, write a function describing runtime behaviors, and pass them into a function that spins up the app you’ve described. By convention, these two objects are associated with the variable names ui and server.

**Quality over quantity: building the perfect data science project**

What I want to explore here is a way to break out of that spiral, by using some key lessons we’ve learned from seeing dozens of mentees leverage their projects into job interviews and ultimately, into offers. So without further ado, here it is: a step-by-step guide to building a great data science project that will take you beyond the scikit-learn/pandas spiral, and get you interviews as fast as possible.

**3 Methods for Parallelization in Spark**

Spark is great for scaling up data science tasks and workloads! As long as you’re using Spark data frames and libraries that operate on these data structures, you can scale to massive data sets that distribute across a cluster. However, there are some scenarios where libraries may not be available for working with Spark data frames, and other approaches are needed to achieve parallelization with Spark. This post discusses three different ways of achieving parallelization in PySpark.

**Correlated longitudinal data with varying time intervals**

I was recently contacted to see if simstudy can create a data set of correlated outcomes that are measured over time, but at different intervals for each individual. The quick answer is there is no specific function to do this. However, if you are willing to assume an ‘exchangeable’ correlation structure, where measurements far apart in time are just as correlated as measurements taken close together, then you could just generate individual-level random effects (intercepts and/or slopes) and pretty much call it a day. Unfortunately, the researcher had something more challenging in mind: he wanted to generate auto-regressive correlation, so that proximal measurements are more strongly correlated than distal measurements.

**9 trends to watch in systems engineering and operations**

• AIOps• Knative vs. AWS Lambda vs. Microsoft Azure Functions vs. Google Cloud• Cloud-native infrastructure• Security• Service mesh• DevOps breaks down more silos; rise of the SRE• Kubernetes• Distributed tracing• Containers

**A Comprehensive List of Handy R Packages**

Whether Python or R is more superior for Data Science / Machine Learning is an open debate. Despite of its quirkiness and not-so-true-but-generally-perceived slowness, R really shines in exploratory data analysis (EDA), in terms of data wrangling, visualizations, dashboards, myriad choices of statistical packages (and bugs)?-?so I always found it helpful to dual wield R and Python, especially with improved inter-operability using reticulate and rpy. I spent most of the past several months working with R, while my python, Java and javascript skills became dormant. The CRAN taskviews is an awesome starting place to search for R packages, but I guess a list of packages I have used in the last few years may benefit the community. It’s also a good motivation for me to document some past learnings. So here it is- and I will try to make updates. It’s quite long, so it’s probably better to try those relevant to your subject matter first before brute force install everything. Nevertheless, there’s an observation that folks who buy a lot books hardly read any of them.

**Detecting malaria using deep learning.**

Building a convolutional neural network to quickly predict the presence of malaria parasitized cells in a thin blood smear.

**The trinity of errors in financial models: An introductory analysis using TensorFlow Probability**

An exploration of three types of errors inherent in all financial models.

**An Open Standard for Ethical Enterprise-Grade AI**

Just like the invention of the wheel, of the printing press or of the computer, Artificial Intelligence (AI) will radically reshape the way enterprises work. This is such a revolution, that all industries will be impacted – without any exception: transportation, e-commerce, education, healthcare, energy, insurance, … The standard has seven sections. The full details of each of them are explained on github, but here is a summary:1. General Information2. Initial Data3. Data Preparation4. Feature Engineering5. Training Data Audit6. Model Description7. Model Audit





### Like this:

Like Loading...


*Related*

