---
layout:     post
catalog: true
title:      My presentations on ‘Elements of Neural Networks & Deep Learning’ -Parts 6,7,8
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/51K-RVWU4CI/
date:      2019-01-20
author:      Tinniam V Ganesh
tags:
    - methods
    - deep
    - introduces
    - introducing
    - sets
---





This is the final set of presentations in my series â€˜Elements of Neural Networks and Deep Learningâ€™. This set follows the earlier 2 sets of presentations namely1. My presentations on â€˜Elements of Neural Networks & Deep Learningâ€™ -Part1,2,32. My presentations on â€˜Elements of Neural Networks & Deep Learningâ€™ -Parts 4,5

In this final set of presentations I discuss initialization methods, regularization techniques including dropout. Next I also discuss gradient descent optimization methods like momentum, rmsprop, adam etc. Lastly, I briefly also touch on hyper-parameter tuning approaches. The corresponding implementations are available in vectorized R, Python and Octave are available in my book â€˜Deep Learning from first principles:Second edition- In vectorized Python, R and Octaveâ€˜

1. **Elements of Neural Networks and Deep Learning â€“ Part 6**This part discusses initialization methods specifically like He and Xavier. The presentation also focuses on how to prevent over-fitting using regularization. Lastly the dropout method of regularization is also discusses



The corresponding implementations in vectorized R, Python and Octave of the above discussed methods are available in my post Deep Learning from first principles in Python, R and Octave â€“ Part 6

2. **Elements of Neural Networks and Deep Learning â€“ Part 7**This presentation introduces exponentially weighted moving average and shows how this is used in different approaches to gradient descent optimization. The key techniques discussed are learning rate decay, momentum method, rmsprop and adam.



The equivalent implementations of the gradient descent optimization techniques in R, Python and Octave can be seen in my post Deep Learning from first principles in Python, R and Octave â€“ Part 7

3. **Elements of Neural Networks and Deep Learning â€“ Part 8**This last part touches upon hyper-parameter tuning in Deep Learning networks



This concludes this series of presentations on â€œElements of Neural Networks and Deep Learningâ€™

![](https://gigadom.files.wordpress.com/2017/01/Untitled.png?w=456)
![](https://gigadom.files.wordpress.com/2017/01/Untitled.png?w=456)
Checkout my book â€˜Deep Learning from first principles: Second Edition â€“ In vectorized Python, R and Octaveâ€™. My book starts with the implementation of a simple 2-layer Neural Network and works its way to a generic L-Layer Deep Learning Network, with all the bells and whistles. The derivations have been discussed in detail. The code has been extensively commented and included in its entirety in the Appendix sections. My book is available on Amazon as paperback ($18.99) and and in kindle version($9.99/Rs449).

See also1. My book â€˜Practical Machine Learning in R and Python: Third editionâ€™ on Amazon2. Big Data-1: Move into the big league:Graduate from Python to Pyspark3. My travels through the realms of Data Science, Machine Learning, Deep Learning and (AI)4. Revisiting crimes against women in India5. Introducing cricket package yorkr: Part 1- Beaten by sheer pace!6. Deblurring with OpenCV: Weiner filter reloaded7. Taking a closer look at Quantum gates and their operations

To see all posts click Index of posts


*Related*








---
