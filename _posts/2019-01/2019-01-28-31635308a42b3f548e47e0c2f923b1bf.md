---
layout:     post
catalog: true
title:      Problems in Estimating GARCH Parameters in R (Part 2; rugarch)
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/6mq0MWpvQ8E/
date:      2019-01-28
author:      ntguardian
tags:
    - parameters
    - solvers
    - parameter estimates
    - functions
    - optimization
---






Now here is a blog post that has been sitting on the shelf far longer than it should have. Over a year ago I wrote an article about problems I was having when estimating the parameters of a GARCH(1,1) model in R. I documented the behavior of parameter estimates (with a focus on ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
) and perceived pathological behavior when those estimates are computed using **fGarch**. I called for help from the R community, including sending out the blog post over the R Finance mailing list.



I was not disappointed in the feedback. You can see some mailing list feedback, and there were some comments on Reddit that were helpful, but I think the best feedback I got was through my own e-mail.

Dr. Brian G. Peterson, a member of the R finance community, sent some thought provoking e-mails. The first informed me that **fGarch** is no longer the go-to package for working with GARCH models. The RMetrics suite of packages (which include **fGarch**) was maintained by Prof. Diethelm Würtz at ETH Zürich. He was killed in a car accident in 2016.

Dr. Peterson recommended I look into two more modern packages for GARCH modelling, **rugarch** (for univariate GARCH models) and **rmgarch** (for multivariate GARCH models). I had not heard of these packages before (the reason I was aware of **fGarch** was because it was referred to in the time series textbook Time Series Analysis and Its Applications with R Examples by Shumway and Stoffer), so I’m very thankful for the suggestion. Since I’m interested in univariate time series for now, I only looked at **rugarch**. The package appears to have more features and power than **fGarch**, which may explain why it seems more difficult to use. However the package’s vignette is helpful and worth printing out.

Dr. Peterson also had interesting comments about my proposed applications. He argued that intraday data should be preferred to daily data and that simulated data (including simulated GARCH processes) has idiosyncracies not seen in real data. The ease of getting daily data (particularly for USD/JPY around the time of Asian financial crises, which was an intended application of a test statistic I’m studying) motivated my interest in daily data. His comments, though, may lead me to reconsider this application. (I might try to detect the 2010 eurozone financial crises via EUR/USD instead. I can get free intraday data from HistData.com for this.) However, if standard error estimates cannot be trusted for small sample sizes, our test statistic would still be in trouble since it involves estimating parameters even for small sample sizes.

He also warned that simulated data exhibits behaviors not seen in real data. That may be true, but simulated data is important since it can be considered a statistician’s best-case scenario. Additionally, the properties of the process that generated simulated data are known *a priori*, including the values of the generating parameters and whether certain hypotheses (such as whether there is a structural change in the series) are true. This allows for sanity checks of estimators and tests. This is impossible for real-world since we don’t have the *a priori* knowledge needed.

Prof. André Portela Santos asked that I repeat the simulations but with ![](https://s0.wp.com/latex.php?latex=%5Calpha++0.6&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha++0.6&bg=ffffff&%23038;fg=444444&%23038;s=0)
 since these values are supposedly more common than my choice of ![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
. It’s a good suggestion and I will consider parameters in this range in addition to ![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
 in this post. However, my simulations seemed to suggest that when ![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
, the estimation procedures nevertheless seem to want to be near the range of large ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
. I’m also surprised since my advisor gave me the impression that GARCH processes with either ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
 or ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
 large are more difficult to work with. Finally, if the estimators are strongly biased, we might expect to see most estimated parameters to lie in that range, though that does not mean the “correct” values lie in that range. My simulations suggest **fGarch** struggles to discover ![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
 even when those parameters are “true.”” Prof. Santos’ comment leads me to desire a metastudy about what common estimates of GARCH parameters are on real world. (There may or may not be one; I haven’t checked. If anyone knows of one, please share.)

My advisor contacted another expert on GARCH models and got some feedback. Supposedly the standard error for ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
 is large, so there should be great variation in parameter estimates. Some of my simulations agreed with this behavior even for small sample sizes, but at the same time showed an uncomfortable bias towards ![](https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+1&bg=ffffff&%23038;fg=444444&%23038;s=0)
. This might be a consequence of the optimization procedures, as I hypothesized.

So given this feedback, I will be conducting more simulation experiments. I won’t be looking at **fGarch** or **tseries** anymore; I will be working exclusively with **rugarch**. I will explore different optimization procedures supported by the package. I won’t be creating plots like I did in my first post; those plots were meant only to show the existence of a problem and its severity. Instead I will be looking at properties of the resulting estimators produced by different optimization procedures.

## Introduction to **rugarch**

As mentioned above, **rugarch** is a package for working with GARCH models; a major use case is estimating their parameters, obviously. Here I will demonstrate how to specify a GARCH model, simulate data from the model, and estimate parameters. After this we can dive into simulation studies.

### Specifying a ![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C+1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 model

To work with a GARCH model we need to specify it. The function for doing this is `ugarchspec()`. I think the parameters `variance.model` and `mean.model` are the most important parameters.

`variance.model` is a list with named entries, perhaps the two most interesting being `model` and `garchOrder`. `model` is a string specify which type of GARCH model is being fitted. Many major classes of GARCH models (such as EGARCH, IGARCH, etc.) are supported; for the “vanilla” GARCH model, set this to `"sGARCH"` (or just omit it; the standard model is the default). `garchOrder` is a vector for the order of the ARCH and GARCH components of the model.

`mean.model` allows for fitting ARMA-GARCH models, and functions like `variance.model` in that it accepts a list of named entries, the most interesting being `armaOrder` and `include.mean`. `armaOrder` is like `garchOrder`; it’s a vector specifying the order of the ARMA model. `include.mean` is a boolean that, if true, allows for the ARMA part of the model to have non-zero mean.

When simulating a process, we need to set the values of our parameters. This is done via the `fixed.pars` parameter, which accepts a list of named elements, the elements of the list being numeric. They need to fit the conventions the function uses for parameters; for example, if we want to set the parameters of a ![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 model, the names of our list elements should be `"alpha1"` and `"beta1"`. If the plan is to simulate a model, every parameter in the model should be set this way.

There are other parameters interesting in their own right but I focus on these since the default specification is an ARMA-GARCH model with ARMA order of ![](https://s0.wp.com/latex.php?latex=%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 with non-zero mean and a GARCH model of order ![](https://s0.wp.com/latex.php?latex=%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
. This is not a vanilla ![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 model as I desire, so I almost always change this.

### Simulating a GARCH process

The function `ugarchpath()` simulates GARCH models specified via `ugarchspec()`. The function needs a specification objectect created by `ugarchspec()` first. The parameters `n.sim` and `n.start` specify the size of the process and the length of the burn-in period, respectively (with defaults 1000 and 0, respectively; I strongly recommend setting the burn-in period to at least 500, but I go for 1000). The function creates an object that contains not only the simulated series but also residuals and ![](https://s0.wp.com/latex.php?latex=%5Csigma_t&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Csigma_t&bg=ffffff&%23038;fg=444444&%23038;s=0)
.

The `rseed` parameter controls the random seed the function uses for generating data. Be warned that `set.seed()` is effectively ignored by this function, so if you want consistent results, you will need to set this parameter.

The `plot()` method accompanying these objects is not completely transparent; there are a few plots it could create and when calling `plot()` on a `uGARCHpath` object in the command line users are prompted to input a number corresponding to the desired plot. This is a pain sometimes so don’t forget to pass the desired plot’s number to the `which` parameter to avoid the prompt; setting `which = 2` will give the plot of the series proper.

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-4-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-4-1.png?w=456)


![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-4-2.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-4-2.png?w=456)


### Fitting a ![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Ctext%7BGARCH%7D%281%2C1%29&bg=ffffff&%23038;fg=444444&%23038;s=0)
 model

The `ugarchfit()` function fits GARCH models. The function needs a specification and a dataset. The `solver` parameter accepts a string stating which numerical optimizer to use to find the parameter estimates. Most of the parameters of the function manage interfacing with the numerical optimizer. In particular, `solver.control` can be given a list of arguments to pass to the optimizer. We will be looking at this in more detail later.

The specification used for generating the simulated data won’t be appropriate for `ugarchfit()`, since it contains fixed values for its parameters. In my case I will need to create a second specification object.

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-5-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-5-1.png?w=456)


Notice the estimated parameters and standard errors? The estimates are nowhere near the “correct” numbers even for a sample size of 1000, and there is no way a reasonable confidence interval based on the estimated standard errors would contain the correct values. It looks like the problems I documented in my last post have not gone away.

Out of curiosity, what would happen with the other specification, one in the range Prof. Santos suggested?

That’s no better Now let’s see what happens when we use different optimization routines.

## Optimization and Parameter Estimation in **rugarch**

### Choice of optimizer

`ugarchfit()`‘s default parameters did a good job of finding appropriate parameters for what I will refer to as model 2 (where ![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+0.1&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+0.1&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0.7&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+0.7&bg=ffffff&%23038;fg=444444&%23038;s=0)
) but not for model 1 (![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cbeta+%3D+0.2&bg=ffffff&%23038;fg=444444&%23038;s=0)
). What I want to know is when one solver seems to beat another.

As pointed out by Vivek Rao on the R-SIG-Finance mailing list, the “best” estimate is the estimate that maximizes the likelihood function (or, equivalently, the log-likelihood function), and I omitted inspecting the log likelihood function’s values in my last post. Here I will see which optimization procedures lead to the maximum log-likelihood.

Below is a helper function that simplifies the process of fitting a GARCH model’s parameters and extracting the log-likelihood, parameter values, and standard errors while allowing for different values to be passed to `solver` and `solver.control`.

Below I list out all optimization schemes I will consider. I only fiddle with `solver.control`, but there may be other parameters that could help the numerical optimization routines, namely `numderiv.control`, which are control arguments passed to the numerical routines responsible for standard error computation. This utilizes the package **numDeriv** which performs numerical differentiation.

Now let’s run the gauntlet of optimization choices and see which produces the estimates with the largest log likelihood for data generated by model 1. The `lbfgs` method (low-storage version of the Broyden-Fletcher-Goldfarb-Shanno method, provided in **nloptr**) unfortunately does not converge for this series, so I omit it.

According the the maximum likelihood criterion, the “best” result is achieved by `gosolnp`. The result has the unfortunate property that ![](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Capprox+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Capprox+0&bg=ffffff&%23038;fg=444444&%23038;s=0)
, which is certainly not true, but at least the standard error for ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
 would create a confidence interval that contains ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
‘s true value. Of these, my preferred estimates are produced by AUGLAG+PRAXIS, as ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
 seems reasonable and in fact the estimates are all close to the truth, (at least in the sense that the confidence intervals contain the true values), but unfortunately the estimates do *not* maximize the log likelihood, even though they are the most reasonable.

If we looked at model 2, what do we see? Again, `lbfgs` does not converge so I omit it. Unfortunately, `nlminb` does not converge either, so it too must be omitted.

Here it was PRAXIS and AUGLAG+PRAXIS that gave the “optimal” result, and it was only those two methods that did. Other optimizers gave visibly bad results. That said, the “optimal” solution is the preferred on with the parameters being nonzero and their confidence intervals containing the correct values.

What happens if we restrict the sample to size 100? (`lbfgs` still does not work.)

The results are not thrilling. The “best” result for the series generated by model 1 was attained by multiple solvers, and the 95% confidence interval (CI) for ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
 would not contain ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
‘s true value, though the CIs for the other parameters would contain their true values. For the series generated by model 2 the best result was attained by the `nlminb` solver; the parameter values are not plausible and the standard errors are huge. At least the CI would contain the correct value.

From here we should no longer stick to two series but see the performance of these methods on many simulated series generated by both models. Simulations in this post will be too computationally intensive for my laptop so I will use my department’s supercomputer to perform them, taking advantage of its many cores for parallelization.

Below is a set of helper functions I will use for the analytics I want.

I first create tables containing, for a fixed sample size and model:

- The rate at which a solver attains the highest log likelihood among all solvers for a series

- The rate at which a solver failed to converge

- The rate at which a roughly 95% confidence interval based on the solver’s solution managed to contain the true parameter value for each parameter (referred to as the “capture rate”, and using the robust standard errors)


### Model 1, ![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 1, ![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 1, ![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 2, ![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 2, ![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 2, ![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)


These tables already reveal a lot of information. In general it seems that the AUGLAG-PRAXIS method (the augmented Lagrangian method using the principal axis solver) provided in NLOpt does best for model 2 especially for large sample sizes, while for model 1 the `gosolnp` method, which uses the `solnp` solver by Yinyu Ye but with random initializations and restarts, seems to win out for larger sample sizes.

The bigger story, though, is the failure of any method to be the “best”, especially in the case of smaller sample sizes. While there are some optimizers that consistently fail to attain the maximum log-likelihood, no optimizer can claim to consistently obtain the best result. Additionally, different optimizers seem to perform better with different models. The implication for real-world data–where the true model parameters are never known–is to try every optimizer (or at least those that have a chance of maximizing the log-likelihood) and pick the results that yield the largest log-likelihood. No algorithm is trustworthy enough to be the go-to algorithm.

Let’s now look at plots of the estimated distribution of the parameters. First comes a helper function.

Now for plots.

### Estimated ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 1

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-22-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-22-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 1

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-23-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-23-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 1

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-24-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-24-1.png?w=456)


Bear in mind that there are only 1,000 simulated series and the optimizers produce solutions for each series, so in principle optimizer results should not be independent, yet the only time these density plots look the same is when the optimizer performs terribly. But even when an optimizer isn’t performing terribly (as is the case for the `gosolnp`, `PRAXIS`, and `AUGLAG-PRAXIS` methods) there’s evidence of artifacts around 0 for the estimates of ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and 1 for ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
. These artifacts are more pronounced for smaller sample sizes. That said, for the better optimizers the estimators look almost unbiased, especially for ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
 and ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
, but their spread is large even for large sample sizes, especially for ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
‘s estimator. That’s not the case for the `AUGLAG-PRAXIS` optimizer, though; it appears to produce biased estimates.

Let’s look at plots for model 2.

### Estimated ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 2

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-25-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-25-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 2

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-26-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-26-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 2

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-27-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-27-1.png?w=456)


The estimators don’t struggle as much for model 2, but the picture is still hardly rosy. The `PRAXIS` and `AUGLAG-PRAXIS` methods seem to perform well, but far from spectacularly for small sample sizes.

So far, my experiments suggest practitioners should not rely on any one optimizer but instead to try different ones and choose the results that have the largest log-likelihood. Suppose we call this optimization routine the “best” optimizer. how does this optimizer perform?

Let’s find out.

### Model 1, ![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 1, ![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 1, ![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 2, ![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+100&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 2, ![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+500&bg=ffffff&%23038;fg=444444&%23038;s=0)


### Model 2, ![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=n+%3D+1000&bg=ffffff&%23038;fg=444444&%23038;s=0)


Bear in mind that we evaluate the performance of the “best” optimizer by the CI capture rate, which should be around 95%. The “best” optimizer obviously has good performance but does not outperform all optimizers. This is disappointing; I had hoped that the “best” optimizer would have the highly desirable property of a 95% capture rate. Performance is nowhere near that except for larger sample sizes. Either the standard errors are being underestimated or for small sample sizes the Normal distribution poorly describes the actual distribution of the estimators (which means multiplying by two does not lead to intervals with the desired confidence level).

Interestingly, there is no noticeable difference in performance between the two models for this “best” estimator. This suggests to me that the seemingly better results for models often seen in actual data might be exploiting the bias of the optimizers.

Let’s look at the distribution of the estimated parameters.

### Estimated ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 1

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-34-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-34-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 1

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-35-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-35-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 1

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-36-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-36-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Comega&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 2

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-37-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-37-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Calpha&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 2

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-38-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-38-1.png?w=456)


### Estimated ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
, model 2

![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-39-1.png?w=456)
![](https://ntguardian.files.wordpress.com/2019/01/problemsestimatinggarchparameters2-39-1.png?w=456)


The plots suggest that the “best” estimator still shows some pathologies even though it behaves less poorly than the other estimators. I don’t see evidence for bias in parameter estimates regardless of choice of model but I’m not convinced the “best” estimator truly maximizes the log-likelihood, especially for smaller sample sizes. the estimates for ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
 are especially bad. Even if the standard error for ![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
![](https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&%23038;fg=444444&%23038;s=0)
 should be large I don’t think it should show the propensity for being zero or one that these plots reveal.

I initially wrote this article over a year ago and didn’t publish it until now. The reason for the hang up was because I wanted a literature review of alternative ways to estimate the parameters of a GARCH model. Unfortunately I never completed such a review, and I’ve decided to release this article regardless.

That said, I’ll share what I was reading. One article by Gilles Zumbach tried to explain why estimating GARCH parameters is hard. He noted that the quasi-likelihood equation that solvers try to maximize has bad properties, such as being non-concave and having “flat” regions that algorithms can become stuck in. He suggested an alternative procedure to finding the parameters of GARCH models, where one finds the best fit in an alternative parameter space (which supposedly has better properties than working with the original parameter space of GARCH models) and estimating one of the parameters using, say, the method of moments, without any optimization algorithm. Another article, by Fiorentini, Calzolari, and Panattoni, showed that analytic gradients for GARCH models could be computed explicitly, so gradient-free methods like those used by the optimization algorithms seen here are not actually necessary. Since numerical differentiation is generally a difficult problem, this could help ensure that no additional numerical error is being introduced that causes these algorithms to fail to converge. I also wanted to explore other estimation methods to see if they somehow can avoid numerical techniques altogether or have better numerical properties, such as estimation via method of moments. I wanted to read an article by Andersen, Chung, and Sørensen to learn more about this approach to estimation.

Life happens, though, and I didn’t complete this review. The project moved on and the problem of estimating GARCH model parameters well was essentially avoided. That said, I want to revisit this point, perhaps exploring how techniques such as simulated annealing do for estimating GARCH model parameters.

So for now, if you’re a practitioner, what should you do when estimating a GARCH model? I would say don’t take for granted that the default estimation procedure your package uses will work. You should explore different procedure and different parameter choices and go with the results that lead to the largest log-likelihood value. I showed how this could be done in an automated fashion but you should be prepared to *manually* pick the model with the best fit (as determined by the log-likelihood). If you don’t do this the model you estimated may not actually be the one for which theory works.

I will say it again, one last time, in the last sentence of this article for extra emphasis: *don’t take numerical techniques and results for granted!*

---

Packt Publishing published a book for me entitled *Hands-On Data Analysis with NumPy and Pandas*, a book based on my video course *Unpacking NumPy and Pandas*. This book covers the basics of setting up a Python environment for data analysis with Anaconda, using Jupyter notebooks, and using NumPy and pandas. If you are starting out using Python for data analysis or know someone who is, please consider buying my book or at least spreading the word about it. You can buy the book directly or purchase a subscription to Mapt and read it there.

If you like my blog and would like to support it, spread the word (if not get a copy yourself)!


*Related*








---
