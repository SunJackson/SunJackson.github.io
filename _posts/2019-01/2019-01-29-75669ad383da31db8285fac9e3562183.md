---
layout:     post
catalog: true
title:      Creating a word cloud on R-bloggers posts
subtitle:      转载自：http://feedproxy.google.com/~r/RBloggers/~3/XJ2MiY4OAAc/
date:      2019-01-29
author:      Andrew Treadway
tags:
    - all_titles
    - cleaning
    - all_pages
    - article titles scraped
    - site
---

This post will go through how to create a word cloud of article titles scraped from the [awesome R-bloggers](https://www.r-bloggers.com/). Our goal will be to use R’s **rvest** package to search through 50 successive pages on the site for article titles. The **stringr** and **tm** packages will be used for string cleaning and for creating a term document frequency matrix (with **tm**). We will then create a word cloud based off the words comprising these titles.

First, we’ll load the packages we need.

Let’s write a function that will take a webpage as input and return all the scraped article titles.

The above function takes an input, called *site*, which will be the URL of a specific webpage on [R-bloggers](https://www.r-bloggers.com/). We then use **rvest’s** *read_html* function to scrape the HTML from the webpage. Next, we parse out the titles by searching through the H2 tags, and parsing out the title attributes from the links within those header tags i.e. we search through each H2 tag, find the “a” tag (anchor, or link), and then pull the title from that tag.

The remaining code above is for cleaning up the titles we parsed. We take out any titles we parsed that are NA – i.e. any link tags that did not have a title attribute (these are not post titles). At this point, each of the title attributes we have has the words “Permalink to “. The *gsub* line of code is just getting rid of this in each title.

Now, let’s get the vector of webpages we need to scrape. Each successive page containing article links has the following pattern:

**“https://www.r-bloggers.com/page/index”** where index is some positive integer.

[https://www.r-bloggers.com/page/1](https://www.r-bloggers.com/page/1)[https://www.r-bloggers.com/page/2](https://www.r-bloggers.com/page/2)[https://www.r-bloggers.com/page/3](https://www.r-bloggers.com/page/3)[https://www.r-bloggers.com/page/4](https://www.r-bloggers.com/page/4)**…****…****…**

Thus, we can just use the *paste0* function to generate all the URLs we want.

Next, let’s scrape the post titles from each webpage using our *scrape_post_titles* function. Then, we’ll collapse the titles into a single vector.

After we have the titles scraped, we need to perform some cleaning operations, such as converting each title to lowercase, and getting rid of numbers, punctuation, and stop words.

Next, we use the **tm** package to convert our cleaned vector of titles to a corpus. On the next line, we stem each word in the titles to get the root of each word (e.g. model, models, and modeling will each count as the same word, model).

With the cleaned corpus, we can get a term document matrix. This will give us a frequency of how often each word occurs.

Lastly, we use the **wordcloud** package to generate a word cloud based off the words across all the titles.

![](https://i2.wp.com/theautomatic.net/wp-content/uploads/2019/01/r-bloggers-word-cloud.png?w=640)
![](https://i2.wp.com/theautomatic.net/wp-content/uploads/2019/01/r-bloggers-word-cloud.png?w=640)


Above, we can see that “data” is the most popular word. Variations of “model”, “analysis”, and “package” are also popular.

That’s it for this post! Click here to read more R articles.

The post Creating a word cloud on R-bloggers posts appeared first on Open Source Automation.


*Related*








---
