---
layout:     post
title:      Smooth distributed convex optimization
subtitle:   转载自：https://blogs.princeton.edu/imabandit/2017/07/05/smooth-distributed-convex-optimization/
date:       2017-07-06
author:     Sebastien Bubeck
header-img: img/background2.jpg
catalog: true
tags:
    - graphs
    - communication
    - communicated
    - algorithms
    - optimization
---

A couple of months ago we (Kevin Scaman, Francis Bach, Yin Tat Lee, Laurent Massoulie and myself) uploaded [a new paper on distributed convex optimization](https://arxiv.org/abs/1702.08704). We came up with a pretty clean picture for the optimal oracle complexity of this setting, which I will describe below. I should note that there are hundreds of papers on this topic, but the point of the post is to show our simple cute calculations and not to survey the immense literature on distributed optimization, see the paper itself for a number of pointers to other recent works.

**Distributed optimization setting**

Let ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f8c26ec73ea76200364267069eec5df4_l3.png?resize=84%2C18&ssl=1)
 be an undirected graph on ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a63eb5ff0272d3119fa684be6e7acce8_l3.png?resize=11%2C8&ssl=1)
 vertices (![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-30fa5bd34797136e7fb8e56a933c7ef6_l3.png?resize=57%2C18&ssl=1)
) and with diameter ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9cf13d01d314e721529ca1d9e2f76929_l3.png?resize=14%2C12&ssl=1)
. We will think of the nodes in ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7620c75c8772e1ee533aefe8de7019b0_l3.png?resize=14%2C12&ssl=1)
 as the computing units. To each vertex ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a495aad2f8e3d31ba82a245b559744dc_l3.png?resize=45%2C13&ssl=1)
 there is an associated convex function ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5eecc9b7556bfad76e7a1ce6a193a66f_l3.png?resize=93%2C19&ssl=1)
. For machine learning applications one can think that each computing unit has access to a “private” dataset, and ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7d6f5eea15b4dc7eae4cf3434d0bd311_l3.png?resize=40%2C18&ssl=1)
 represents the fit of the model corresponding to ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9c6f585f2818eec84b05213c11071a67_l3.png?resize=53%2C16&ssl=1)
 on this dataset (say measured on least squares loss, or logistic loss for example). The goal will be to find in a distributed way the optimal “consensus” point:

     ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f53d285f10b008bed6de46e317c78992_l3.png?resize=205%2C39&ssl=1)


The distributed processing protocol is as follows: asynchronously/in parallel, each node ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-018f9da6d6a66d752a7d1c96afba76a2_l3.png?resize=9%2C8&ssl=1)
 can (i) compute a (local) gradient ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-68003a70cb8533f74f1749033967c341_l3.png?resize=29%2C17&ssl=1)
 in time ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-21b5b4cbe9a10b6d847eeb4265b99898_l3.png?resize=7%2C13&ssl=1)
, and (ii) communicate a vector in ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3c53b3f5f2473532b6e89cb16c41d331_l3.png?resize=21%2C15&ssl=1)
 to its neighbors in ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7620c75c8772e1ee533aefe8de7019b0_l3.png?resize=14%2C12&ssl=1)
 in time ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3af6c51247895b176bb502f0ee0857ee_l3.png?resize=10%2C8&ssl=1)
. We denote by ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6ff2f8ae538d162d9f9d4f2bc23f6a82_l3.png?resize=26%2C14&ssl=1)
 the local model (essentially its guess for ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b92fd5178a7e53e81d04a267f41c9d8_l3.png?resize=16%2C13&ssl=1)
) of node ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-018f9da6d6a66d752a7d1c96afba76a2_l3.png?resize=9%2C8&ssl=1)
 at time ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-735b23a07db1fca772d8971870f088a1_l3.png?resize=51%2C17&ssl=1)
. We aim to characterize the smallest time ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d02fffe07a654ad0d12e1bed72b166e3_l3.png?resize=16%2C15&ssl=1)
 such that one can guarantee that all nodes ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-018f9da6d6a66d752a7d1c96afba76a2_l3.png?resize=9%2C8&ssl=1)
 satisfy ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2a1ef81fde4db080e4e50d8c72e8dc08_l3.png?resize=151%2C22&ssl=1)
 where ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c6615774b64c68de3e9de81f8f9701ec_l3.png?resize=115%2C22&ssl=1)
.

We focus on the case where ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e48fc46b6bd3c941aa909fff88294a60_l3.png?resize=12%2C20&ssl=1)
 is [![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-874e8d85c668e484f7277da3d75ce10e_l3.png?resize=11%2C16&ssl=1)
-smooth and ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0210468bf4cb3a50550e30ce7951201b_l3.png?resize=11%2C8&ssl=1)
-strongly convex](https://blogs.princeton.edu/imabandit/orf523-the-complexities-of-optimization) (![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b10dc7874289486e968d85c6dd61c9b_l3.png?resize=65%2C18&ssl=1)
 is the condition number), which is arguably the most challenging case since one expects linear convergence (i.e., the scaling of ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d02fffe07a654ad0d12e1bed72b166e3_l3.png?resize=16%2C15&ssl=1)
 in ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bd8c92db9d4710285ccbc2b75c276150_l3.png?resize=7%2C8&ssl=1)
 should be ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-38bfa3c1131fbae41cb358b8b685dc56_l3.png?resize=61%2C19&ssl=1)
) which a priori makes the interaction of optimization error and communication error potentially delicate (one key finding is that in fact it is not delicate!). Also, having in mind applications outside of large-scale machine learning (such as [“federated” learning](https://research.googleblog.com/2017/04/federated-learning-collaborative.html)), we will make no assumptions about the functions at different vertices relate to each other.

**A trivial answer**

Recall that [Nesterov’s accelerated gradient descent](https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization) solves the serial problem in time ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d548f48c3288ad900462b79f10fb6ce7_l3.png?resize=117%2C19&ssl=1)
. Trivially one can distribute a step of Nesterov’s accelerated gradient descent in time ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ac4a82fa71eff2e8859a6e877701179f_l3.png?resize=52%2C18&ssl=1)
 (simply designate a master node at the beginning, and everybody sends its local gradient to the master node in time ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ac4a82fa71eff2e8859a6e877701179f_l3.png?resize=52%2C18&ssl=1)
). Thus we arrive at the upper bound ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-84e3f0fa835bfc79704ccaee87746fa5_l3.png?resize=226%2C19&ssl=1)
 using a trivial (centralized) algorithm. We now show (slightly informally, see the paper for proper definitions) that this in fact optimal!

First let us recall the lower bound proof in the serial case (see for example [Theorem 3.15 here](http://sbubeck.com/Bubeck15.pdf)). The idea is to introduce the function ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b1dd103517c0a643cde8ad29c62c1dc3_l3.png?resize=254%2C20&ssl=1)
 where ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-649dd83fe5ed56224cf7f675eed3a2b9_l3.png?resize=12%2C12&ssl=1)
 is the Laplacian of the path graph on ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b3857344721fd6aa79494e06df9bb7a4_l3.png?resize=1%2C1&ssl=1)
, or in other words

     ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2560abf8d51a7544386b62eeaf853f79_l3.png?resize=237%2C39&ssl=1)


First it is easy to see that this function is indeed ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b5b48e1d1aec370eefb4d391341db602_l3.png?resize=38%2C18&ssl=1)
-smooth and ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9749b1b895749197da096fec4832ff53_l3.png?resize=37%2C18&ssl=1)
-strongly convex. The key point is that, for any algorithm starting at ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3dd21fb1ee9bc226c3ac689f6a30135b_l3.png?resize=50%2C15&ssl=1)
 and such that each iteration stays in the linear span of the previously computed gradients (a very natural assumption) then

     ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9f590fc45e1a1d75995f0e29638053c_l3.png?resize=180%2C18&ssl=1)


In words one can say that each gradient calculation “discovers” a new edge of the path graph involved in the definition of ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&ssl=1)
. Concluding the serial proof is then just a matter of brute force calculations.

Now let us move to the distributed setting, and consider two vertices ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-39adccca45c7a41f32459e2bbcfa5e37_l3.png?resize=10%2C8&ssl=1)
 and ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-018f9da6d6a66d752a7d1c96afba76a2_l3.png?resize=9%2C8&ssl=1)
 that realize the diameter of ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7620c75c8772e1ee533aefe8de7019b0_l3.png?resize=14%2C12&ssl=1)
. The idea goes as follows: let ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78759901edb21cf53d5c65a51de1950e_l3.png?resize=18%2C16&ssl=1)
 (respectively ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-830e547a9fe0aa7ff77efa61639dbe40_l3.png?resize=19%2C15&ssl=1)
) be the Laplacian of *even* edges of the path graph on ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-93cbb859a1186a42542823b7c043cfad_l3.png?resize=13%2C12&ssl=1)
 (respectively the odd edges), that is

     ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9730e1eb5bd6fdc4d12d5e5423628387_l3.png?resize=281%2C39&ssl=1)


Now define ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fb9f179848bd567802f78f104fcecfea_l3.png?resize=256%2C22&ssl=1)
, ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9f1e07d46929e290022a519cd8b7c8aa_l3.png?resize=181%2C22&ssl=1)
, and ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1cfe16f5529da3fbac4742442c97e7c6_l3.png?resize=52%2C16&ssl=1)
 for any ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ca018a75957f0626d89a409d307c30f6_l3.png?resize=79%2C18&ssl=1)
. The key observation is that node ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-39adccca45c7a41f32459e2bbcfa5e37_l3.png?resize=10%2C8&ssl=1)
 does not “know” about the even edges until it receives a message from ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-018f9da6d6a66d752a7d1c96afba76a2_l3.png?resize=9%2C8&ssl=1)
 and vice versa. Thus it fairly easy to show that in this case one has:

     ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2ccaa21af5f5c3733bade4129ad16384_l3.png?resize=269%2C19&ssl=1)


which effectively amounts to a slowdown by a factor ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4474b991892091fb6d6edd8441899d06_l3.png?resize=67%2C18&ssl=1)
 compared to the serial case and proves the lower bound ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bb61c2e21613144b1e50ffbe3b45ee6c_l3.png?resize=218%2C19&ssl=1)
.

**Not so fast!**

One can say that the algorithm proposed above defeats a bit the purpose of the distributed setting. Indeed the centralized communication protocol it relies on is not robust to various real-life issues such as machine failures, time-varying graphs, edges with different latency, etc. An elegant and practical solution is to restrict communication to be *gossip*-like. That is local computations have now to be communicated via matrix multiplication with a *walk matrix* ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-07ab7899fd193672e839942af5a08bbe_l3.png?resize=19%2C12&ssl=1)
 which we define as satisfying the following three conditions: (i) ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7a4d43e8606f33461f2530c78861961_l3.png?resize=187%2C18&ssl=1)
, (ii) ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eca0ea8fbf093997f2f748253d02dd1b_l3.png?resize=208%2C18&ssl=1)
, and (iii) ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8b7027029adcd294543da6eb9db87574_l3.png?resize=52%2C15&ssl=1)
. Let us briefly discuss these conditions: (i) simply means that if ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cb398155ea62b37aa6aba5632ec286ba_l3.png?resize=56%2C16&ssl=1)
 represents real values stored at the vertices, then ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-45c81c764aecca457a1885a3cac7f939_l3.png?resize=29%2C12&ssl=1)
 can be calculated with a distributed communication protocol; (ii) says that if there is consensus (that is all vertices hold the same value) then no communication occurs with this matrix multiplication; and (iii) will turn out to be natural in a just a moment for our algorithm based on duality. A prominent example of a walk matrix would be the (normalized) Laplacian of ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7620c75c8772e1ee533aefe8de7019b0_l3.png?resize=14%2C12&ssl=1)


We denote by ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2afeb87159a4918337409f4bf56d689a_l3.png?resize=10%2C12&ssl=1)
 the inverse condition number of ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-07ab7899fd193672e839942af5a08bbe_l3.png?resize=19%2C12&ssl=1)
 on ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bbd9c94504e8b6124ef694a8749de46e_l3.png?resize=80%2C19&ssl=1)
 (that is the ratio of the smallest non-zero eigvenvalue of ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-07ab7899fd193672e839942af5a08bbe_l3.png?resize=19%2C12&ssl=1)
 to its largest eigenvalue), also known as the spectral gap of ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7620c75c8772e1ee533aefe8de7019b0_l3.png?resize=14%2C12&ssl=1)
 when ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-07ab7899fd193672e839942af5a08bbe_l3.png?resize=19%2C12&ssl=1)
 is the Laplacian. Notice that ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2afeb87159a4918337409f4bf56d689a_l3.png?resize=10%2C12&ssl=1)
 naturally controls the number of gossip steps to reach consensus, in the sense that gossip steps corresponds to gradient descent steps on ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5dad155d0e3653e0c47045b1151d136a_l3.png?resize=109%2C19&ssl=1)
, which will converge in ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d843fb2a9263c00dbc71e76fa764c26e_l3.png?resize=120%2C20&ssl=1)
 steps. Doing an “accelerated gossip” (also known as Chebyshev gossiping) one could thus hope to essentially replace the diameter ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9cf13d01d314e721529ca1d9e2f76929_l3.png?resize=14%2C12&ssl=1)
 by ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-32f23d6ae296b80aa733d468bc409c40_l3.png?resize=134%2C22&ssl=1)
. Notice that this is hopeful thinking because in the centralized model ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9cf13d01d314e721529ca1d9e2f76929_l3.png?resize=14%2C12&ssl=1)
 steps gets you to an *exact* consensus, while in the gossip model one only reaches an ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bd8c92db9d4710285ccbc2b75c276150_l3.png?resize=7%2C8&ssl=1)
-approximate consensus and errors might compound. In fact with a bit of graph theory one can immediately see that simply replacing ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9cf13d01d314e721529ca1d9e2f76929_l3.png?resize=14%2C12&ssl=1)
 by ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-32f23d6ae296b80aa733d468bc409c40_l3.png?resize=134%2C22&ssl=1)
 is too good to be true: there are graphs (namely expanders) where ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9cf13d01d314e721529ca1d9e2f76929_l3.png?resize=14%2C12&ssl=1)
 is of order ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-869fdf40a5f8e47d44e6b773f2c438e9_l3.png?resize=47%2C18&ssl=1)
 while ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2afeb87159a4918337409f4bf56d689a_l3.png?resize=10%2C12&ssl=1)
 is of order of a constant, and thus an upper bound of the form (say) ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8846b6a7ad15605d9a88ac7fd0f7bb5b_l3.png?resize=269%2C22&ssl=1)
 would violate our previous lower bound by a factor ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-64b505a3832d1a0e733467477aa22c5f_l3.png?resize=67%2C19&ssl=1)
.

To save the day we will make extra assumptions, namely that each local function ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-da6e60910e6da6541e5e29ed1c6e0d10_l3.png?resize=16%2C16&ssl=1)
 has condition number ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cef17ccfb5c1b34066c0553bf5be7dc9_l3.png?resize=10%2C8&ssl=1)
 and that in addition to computing local gradient the vertices can also compute local gradients of the Fenchel dual functions ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5cd2f4a0b0b7594748f7b51daa28a06f_l3.png?resize=17%2C17&ssl=1)
. The latter assumption can be removed at the expense of extra logarithmic factors but we will ignore this point here (see the paper for some hints as well as further discussion on this point). For the former assumption we note that the lower bound proof given above completely breaks under this assumption. However one can save the construction for some specific graphs (finding the correct generalization to arbitrary graphs is one of our open problems). For example imagine a line graph, and cluster the vertices into three groups, the first third, the middle, and the last third. Then one could distribute the even part of the Laplacian on ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-93cbb859a1186a42542823b7c043cfad_l3.png?resize=13%2C12&ssl=1)
 in the first group, and the odd part on the last group, as well as distribute the Euclidean norm evenly among all vertices. This construction verifies that each vertex function has condition number ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e51fb454a5f909c8b5275b0790de96b6_l3.png?resize=37%2C18&ssl=1)
 and furthermore the rest of the argument still goes through. Interestingly in this case one also has ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-408d218ffbcfabbd8403289c8d415033_l3.png?resize=82%2C19&ssl=1)
 and thus this proves that for the line graph one has ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-344a36ab8db587f9befba0b373454614_l3.png?resize=245%2C22&ssl=1)
 for gossip algorithms. We will now show a matching upper bound (which holds for arbitrary graphs).

**Dual algorithm**

For ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f7695516a431930ddeef5f1f23ac975d_l3.png?resize=77%2C16&ssl=1)
 (which we think of as a set of column vectors, one for each vertex ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fa8eca47b7642321318fc4aa8b765609_l3.png?resize=85%2C18&ssl=1)
), denote ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e315c05f2fbcb79931262c7e40433e91_l3.png?resize=20%2C15&ssl=1)
 for the ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4e4850482ff981b0970daa5999b24aa1_l3.png?resize=19%2C15&ssl=1)
 column and let ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f3ced37b2825a27ac3ca19f5925861e4_l3.png?resize=164%2C20&ssl=1)
. We are interested in minimizing ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-53dc020de5338aa229c3d10379153faa_l3.png?resize=14%2C12&ssl=1)
 under the constraint that all columns are equal, which can be written as ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2d63b031cbf0494ca482819d3c4ec897_l3.png?resize=83%2C18&ssl=1)
. By definition of the Fenchel dual ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f0222836e6964f408e4f6ce8991d7af4_l3.png?resize=20%2C13&ssl=1)
 and a simple change of variable one has:

     ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d1d51a639b29db239a4beeffcc42d2d8_l3.png?resize=336%2C34&ssl=1)


Next observe that gradient ascent on ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab48baf331239642a00255b86324280a_l3.png?resize=10%2C12&ssl=1)
 can be written as

     ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ba1eb72f4a46db799c9856012026b535_l3.png?resize=216%2C21&ssl=1)


and with the notation ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b249faed0038c7f13f56184f75f5404a_l3.png?resize=78%2C20&ssl=1)
 this is simply ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f9380f8fd3d5c7f07682690bad6e0910_l3.png?resize=163%2C19&ssl=1)
. Crucially ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7f49cfd80f6fc89006000738971a1d15_l3.png?resize=79%2C18&ssl=1)
 exactly corresponds to gossiping the local conjugate gradients (which are also the local models) ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4e00ec5eabd8859b185adbc59e379a3e_l3.png?resize=127%2C20&ssl=1)
. In other words we only have to understand the condition number of the function ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-654246ea304e35d4f49b8f09d5b2158c_l3.png?resize=117%2C20&ssl=1)
. The beauty of all of this is that this condition number is precisely ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-da1eb1e7bbbc60a6970a16cd206763ae_l3.png?resize=29%2C18&ssl=1)
 (i.e. it naturally combines the condition number of the vertex functions with the “condition number” of the graph). Thus by accelerating gradient ascent we arrive at a time complexity of ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d60c04fe99dd7f1469c41fcf7f906f77_l3.png?resize=193%2C22&ssl=1)
 (recall that a gossip step takes time ![](https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3af6c51247895b176bb502f0ee0857ee_l3.png?resize=10%2C8&ssl=1)
). We call the corresponding algorithm SSDA (Single-Step Dual Accelerated). One can improve it slightly in the case of low communication cost by doing multiple rounds of communication between two gradient computations (essentially replacing ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-07ab7899fd193672e839942af5a08bbe_l3.png?resize=19%2C12&ssl=1)
 by ![](https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ac8d49fa7b6f51324adca10e2f8c937a_l3.png?resize=26%2C15&ssl=1)
). We call the corresponding algorithm MSDA (Multi-Step Dual Accelerated) and its attains the optimal (in the worst case over graphs) complexity of ![](https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-617d74890b908b6372ed7a11d28c40e1_l3.png?resize=213%2C22&ssl=1)
.
