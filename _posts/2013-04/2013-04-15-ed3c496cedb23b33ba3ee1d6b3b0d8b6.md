---
layout:     post
title:      Isotonic Regression
subtitle:   转载自：http://fa.bianp.net/blog/2013/isotonic-regression/
date:       2013-04-15
author:     Fabian Pedregosa
header-img: img/background2.jpg
catalog: true
tags:
    - algorithm
    - x_
    - data
    - estimated
    - estimation
---

My latest contribution for [scikit-learn](http://scikit-learn.org/.) is
 an implementation of the isotonic regression model that I coded with
 [Nelle Varoquaux](https://twitter.com/nvaroqua) and
 [Alexandre Gramfort](http://alexandre.gramfort.net/). This model
 finds the best least squares fit to a set of points, given the
 constraint that the fit must be a non-decreasing
 function. [The example](http://scikit-learn.sourceforge.net/dev/auto_examples/plot_isotonic_regression.html)
 on the scikit-learn website gives an intuition on this model.

![](http://fa.bianp.net/blog/static/images/2013/plot_isotonic_regression_1.png)


The original points are in red, and the estimated ones are in
green. As you can see, there is one estimation (green point) for each
data sample (red point). Calling $y \in \mathbb{R}^n$ the input data,
the model can be written concisely as an optimization problem over $x$

$$ 
\text{argmin}_x |y - x |^2 \\
\text{subject to } x_0 \leq x_1 \leq \cdots \leq x_n
$$

The algorithm implemented in scikit-learn is the pool adjacent
violators algorithm , which is an efficient linear time
$\mathcal{O}(n)$ algorithm. The algorithm sweeps through the data
looking for violations of the monotonicity constraint. When it finds
one, it adjusts the estimate to the best possible fit with
constraints. Sometimes it also needs to modify previous points to make
sure the new estimate does not violate the constraints. The following
picture shows how it proceeds at each iteration

![](http://fa.bianp.net/blog/static/images/2013/isotonic.gif)

