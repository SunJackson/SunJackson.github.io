---
layout:     post
title:      Introduction to Sentiment Analysis
subtitle:   转载自：http://www.p-value.info/2012/11/introduction-to-sentiment-analysis.html
date:       2012-11-30
author:     Carl Anderson (noreply@blogger.com)
header-img: img/background0.jpg
catalog: true
tags:
    - documents
    - trained
    - examples
    - classifier
    - classifying
---





**positive**: e.g., "that movie was awesome"


**negative**: e.g., "that movie sucked"





We may also want a third category:





**neutral**: e.g., "I saw the movie at the Odeon".








In reality, we may have a fourth category **unknown** where we cannot tell. For instance, it may contain a document in a foreign language, a set of stop words only, or a set of words that our classifier does not understand, e.g. SMS acronyms (e.g., "cu l8r"). For simplicity, we can simply not classify these documents or we can categorize them as neutral.





To train the classifier we will need to choose a set of **features**, the facets of data that the classifier will work on. The simplest scenario is to use each word in the document. (Later, we will discuss more complex features and other features.)





This is a supervised learning problem so we will need a labeled corpus of data. That is, a set of documents each of which has been labeled with one of our class names or IDs. One of the simplest ways is a CSV or a TSV file:





pos     that movie was awesome


neu     I saw the movie at the Odeon


pos     I love this place


pos     I am happy


....





We will need a **learner**, an algorithm that will learn to classify. In most examples that I've seen online, people have used a naive Bayes classifier but there are many others that one could choose.





Importantly, sentiment analysis is very context sensitive. If you train a classifier using movie review data, it likely will not fare well classifying documents about election results or your startup's product. A classifier trained on US english tweets may or may not classify UK tweets well.





Finally, we cannot expect to assign sentiment correctly 100% of the time. Even humans can often disagree about the sentiment of documents. There are several reasons:

- Cultural differences mentioned above. For example, "rocking" and "sick" are positive adjectives to members of some demographics and cultures but not others.

- English is a hard and ambiguous language. For instance, consider Chomsky's example: "old men and women". Is that [old men] and [old women] or does it mean [old men] and [women]. Another example is [Eats, Shoots & Leaves](http://www.amazon.com/Eats-Shoots-Leaves-Tolerance-Punctuation/dp/1592402038/ref=sr_1_1?ie=UTF8&qid=1354151510&sr=8-1&keywords=eats+shoots+and+leaves).

- Sarcasm, innuendo, and double entendres. This is one of the current challenges in NLP. For a fun example take a look at the problem of detecting [that's what she said](http://blog.echen.me/2011/05/05/twss-building-a-thats-what-she-said-classifier).

- That people often use a bag of words model for sentiment analysis, at least as a first pass. That is, we analyze a document as a set of words and not a phrase. Thus, we will miss that the "not" in "not good" negates "good". In general, we will miss double negatives and other qualifiers. I love this [illustrative example](http://en.wikipedia.org/wiki/Sidney_Morgenbesser):


> 
*During a lecture the Oxford linguistic philosopher J.L. Austin made the claim that although a double negative in English implies a positive meaning, there is no language in which a double positive implies a negative. To which Morgenbesser responded in a dismissive tone, "Yeah, yeah."*


Now we have the background out of the way, let's starting building a concrete example.








This is a tab-delimited file with 7086 sentences tagged as 1 or 0. 





head training.txt


1     The Da Vinci Code book is just awesome.


1     this was the first clive cussler i've ever read, but even books like Relic, and Da Vinci code were more plausible than this.


1     i liked the Da Vinci Code a lot.


1     i liked the Da Vinci Code a lot.


1     I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.


1     that's not even an exaggeration ) and at midnight we went to Wal-Mart to buy the Da Vinci Code, which is amazing of course.


1     I loved the Da Vinci Code, but now I want something better and different!..


1     i thought da vinci code was great, same with kite runner.


1     The Da Vinci Code is actually a good movie...


1     I thought the Da Vinci Code was a pretty good book.





There are many duplicates. Let's remove them:





cat training.txt | sort | uniq > uniq_training.txt





How many positive and negative samples remain?





cat uniq_training.txt | grep ^1 | wc -l


772


cat uniq_training.txt | grep ^0 | wc -l


639





We need to extract features from a document. We'll take unique, lowercase words with more than two characters:





def extract_features(document):


   features={}


   for word in set(document.split()):


       if len(word) > 2:


          features['contains(%s)' % word.lower()] = True


   return features





>>> extract_features('" A couple of very liberal people I know thought Brokeback Mountain was " stupid exploitation.')


{'contains(very)': True, 'contains(people)': True, 'contains(couple)': True, 'contains(mountain)': True, 'contains(was)': True, 'contains(brokeback)': True, 'contains(liberal)': True, 'contains(exploitation.)': True, 'contains(know)': True, 'contains(thought)': True, 'contains(stupid)': True}





We need to read in our documents:





documents=[]


f = open("uniq_training.txt","r")


for document in f.readlines():


   parts= document.strip().split("\t")


   documents.append((parts[1],bool(int(parts[0]))))





>>> documents[0]


('" A couple of very liberal people I know thought Brokeback Mountain was " stupid exploitation.', True)





Now extract features from each document in our corpus and split into a training set (80%) and a test set (20%):





import random


random.seed(1234) #so that you can reproduce my results if you wish


random.shuffle(documents)


import nltk


n_train = int(0.8*len(documents))


training_set = nltk.classify.apply_features(extract_features,documents[:n_train])


test_set = nltk.classify.apply_features(extract_features,documents[n_train:])





>>> training_set[0]


({'contains(very)': True, 'contains(people)': True, 'contains(couple)': True, 'contains(mountain)': True, 'contains(was)': True, 'contains(brokeback)': True, 'contains(liberal)': True, 'contains(exploitation.)': True, 'contains(know)': True, 'contains(thought)': True, 'contains(stupid)': True}, True)





Finally, let's now train our classifier:





>>> classifier = nltk.NaiveBayesClassifier.train(training_set)





>>> nltk.classify.accuracy(classifier, test_set)


0.911660777385159





Whoah, 91% accuracy isn't bad at all given that we had a fairly balanced training set:





ct_pos=0


for d in training_set:


    if d[1]==True: ct_pos+=1





print ct_pos, len(training_set)-ct_pos


617 511





By that I mean, I would have been suspicious if we had a very imbalanced training set and say 95% of samples were positive.
