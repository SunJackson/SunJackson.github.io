---
layout:     post
title:      Calculating CVA with Apache Spark
subtitle:   转载自：http://blog.cloudera.com/blog/2015/03/calculating-cva-with-apache-spark/
date:       2015-03-04
author:     Justin Kestelyn
header-img: img/background2.jpg
catalog: true
tags:
    - cva
    - calculating
    - calculate
    - spark
    - financial
---

**Thanks to Matthew Dixon, principal consultant at Quiota LLC and Professor of Analytics at the University of San Francisco, and Mohammad Zubair, Professor of Computer Science at Old Dominion University, for this guest post that demonstrates how to easily deploy exposure calculations on Apache Spark for in-memory analytics on scenario data.**

Since the 2007 global financial crisis, financial institutions now more accurately measure the risks of over-the-counter (OTC) products. It is now standard practice for institutions to adjust derivative prices for the risk of the counter-party’s, or one’s own, default by means of credit or debit valuation adjustments (CVA/DVA).

Calculating the CVA of a portfolio typically requires Monte-Carlo simulation with a large number of scenarios. The computation and storage requirement for what-if scenario analysis grows significantly when the portfolio is large and contains a wide array of financial instruments across multiple asset classes. To handle that complexity, distributed computing platforms offer trade-offs across performance, flexibility, modularity, and maintainability of programming infrastructure.

For example, in this post, you will learn how to efficiently deploy exposure calculations on Apache Spark for in-memory analytics on scenario data. This example application exemplifies the flexibility, maintainability, and scalability provided by Spark. Applications with cyclic dataflow graphs, such as counter-party credit risk analytics, are well suited for Spark because the scenario data can be kept in memory for fast what-if scenario and statistical analysis. (To see another example of Spark’s benefits in the financial calculations area—in this case, for calculating VaR—see [this post](http://blog.cloudera.com/blog/2014/07/estimating-financial-risk-with-apache-spark).)

## Estimating CVA on Spark

The computational complexity of estimating the CVA is significant and beyond the capability of a single workstation. The number of calculations to estimate CVA of a portfolio is given by:

*Number of Instruments × Number of time intervals × Number of scenarios*

A calculation here refers to calculating the price of an instrument for a scenario at some time interval. Consider, for example, a CVA calculation on a portfolio of 10,000 instruments, with average maturity of 10 years, and 2,000 market scenarios generated every three months—resulting in *8 billion* calculations. The time to price an instrument can vary depending on the instrument. If we assume on average that it takes 100 microseconds to price an instrument on a single workstation, it will take a total of 220 hours to perform all the calculations.

A numbers of banks are using customized parallel and distributed computing platforms to perform these calculations in a reasonable time. Often banks need to hold all the pricing of instruments in memory to calculate various statistics. This requirement results in a large memory consumption, thus the current proprietary solutions are expensive and hard to scale with respect to fault tolerance.

Alternatively, Spark can be programmed to calculate the CVA of a portfolio over a cluster of thousands of cheap commodity nodes using high-level languages such as Scala and Python, thus making it an attractive platform for prototyping and live risk estimates. The key benefit of using Spark over other distributed implementations such as MPI, OpenMP, and CUDA is that it allows the computations to scale to a large number of nodes reliably where failure of nodes is managed by the framework. Furthermore, Spark can hold all the price results of a large portfolio simulation in memory across thousands of nodes in support of calculating various statistics on demand. 

This benefit, along with the ease of implementation, comes at the expense of some performance loss. However, we can minimize this performance loss by using, say, the numpy/scipy package in Python that has been built using BLAS and LAPACK routines.

## Implementing CVA in Python on Spark

For demonstration purposes we consider a portfolio of *NI* vanilla swaps with *NT* time intervals and *NS* simulations. The total number of price calculations then is given by *NI × NT × NS*. Our implementation on Spark is based on [Python code that utilizes the QuantLib package to value a vanilla swap](http://www.pricederivatives.com/en/derivatives-CVA-example-monte-carlo-python).

![](http://blog.cloudera.com/wp-content/uploads/2015/02/cva-f1.jpg)


(Figure courtesy of Giovanni Cesari, UBS)

 

Next, we will briefly outline this computation on Spark.

1. Create a distributed collection (RDD),* randArrayRDD*, of random numbers of size *NS × NT* using the Apache Spark Python MLIB API:







||sc = SparkContext(appName = 'CVA')randArrayRDD = RandomRDDs.normalVectorRDD(sc, NS, NT,  numPartitions=NP, seed=1L)|

randArrayRDD = RandomRDDs.normalVectorRDD(sc, NS, NT,  numPartitions=NP, seed=1L)


2. Call a map function that processes the RDD in parallel and collect the pricing at the driver.







||pmat = randArrayRDD.map(lambda p:(value_swap(p, NI))).collect()pmat = np.array(pmat)pmat = pmat.reshape((NS, NI, NT))|

pmat = np.array(pmat)


The map function works on a row of RDD of random numbers in two stages:

(a) Construct *NT* discount curves, *crvVec *,one for each time interval using the single factor Hull-White short-rate model.







||for iT in xrange(1,NT): crvDate=Dates[iT]; crvDates=[crvDate]+[crvDate+Period(k,Years) for k in xrange(1,NTenors)] crvDiscounts=[1.0]+[A(T[iT],T[iT]+k)*exp(-B(T[iT],T[iT]+k)*rvec[iT]) for k in xrange(1,NTenors)] crvVec[iT]=DiscountCurve(crvDates,crvDiscounts,Actual360(),TARGET())|

 crvDate=Dates[iT];

 crvDiscounts=[1.0]+[A(T[iT],T[iT]+k)*exp(-B(T[iT],T[iT]+k)*rvec[iT]) for k in xrange(1,NTenors)]


(b) For each discount curve, value *NI* swaps at *NT* time intervals in the future to construct a price matrix, *spmat*, of size *NI × NT* for each simulation.







|1234567891011121314151617|for iT in xrange(len(T)): Settings.instance().evaluationDate=Dates[iT] allDates= list(floatingSchedule) fixingdates=[index.fixingDate(floatingSchedule[iDate]) for iDate in xrange(len(allDates)) if index.fixingDate(floatingSchedule[iDate])<=Dates[iT]] if fixingdates: for date in fixingdates[:-1]: try:index.addFixing(date,0.0) except:pass try:index.addFixing(fixingdates[-1],rmean[iT]) except:pass discountTermStructure = RelinkableYieldTermStructureHandle() swapEngine = DiscountingSwapEngine(discountTermStructure) swap1.setPricingEngine(swapEngine) crv=crvVec[iT] discountTermStructure.linkTo(crv) forecastTermStructure.linkTo(crv) npvVec[nI][iT]=swap1.NPV()|

2

4

6

8

10

12

14

16

 Settings.instance().evaluationDate=Dates[iT]

 fixingdates=[index.fixingDate(floatingSchedule[iDate]) for iDate in xrange(len(allDates)) if index.fixingDate(floatingSchedule[iDate])<=Dates[iT]]

 for date in fixingdates[:-1]:

 except:pass

 except:pass

 swapEngine = DiscountingSwapEngine(discountTermStructure)

 crv=crvVec[iT]

 forecastTermStructure.linkTo(crv)


This price matrix,* pmat*, has price distribution information for all swaps in the portfolio and can be used to calculate various exposure measures such as potential future exposure (PFE) and the expected positive exposure (EPE). The CVA can be estimated using the simulated estimate of the EPE.







||EE=np.sum(npvMat, axis=1)EE=np.mean(EE,axis=0)sum=0 for i in xrange(NT-1):sum +=   0.5*(EE[i]*crvToday.discount(T[i])+EE[i+1]*crvToday.discount(T[i+1]))*(exp(-S*T[i]/(1.0-R))-exp(-S*T[i+1]/(1.0-R)))CVA=(1.0-R)*sum|

EE=np.mean(EE,axis=0)

 

sum +=   0.5*(EE[i]*crvToday.discount(T[i])+EE[i+1]*crvToday.discount(T[i+1]))*(exp(-S*T[i]/(1.0-R))-exp(-S*T[i+1]/(1.0-R)))


Note that if you do not want to preserve the price distribution for different swaps, you can simply aggregate prices of different swaps at each worker before sending it to the driver—thereby reducing the communication cost. To do that, modify step 2(b).

For each yield curve constructed earlier, value all *NI* swaps to create a price matrix of size *NI × NT*:

 







|12345678910111213141516171819202122232425|for nI in xrange(NI): fixedSchedule=Schedule(startDate, maturity,Period("6m"), TARGET(),ModifiedFollowing,ModifiedFollowing,DateGeneration.Forward, False) floatingSchedule=Schedule(startDate, maturity,Period("6m"),TARGET() ,ModifiedFollowing,ModifiedFollowing,DateGeneration.Forward, False) swap1=VanillaSwap(VanillaSwap.Receiver, 1000000,fixedSchedule,0.05 , Actual360(),floatingSchedule, index, 0,Actual360()) for iT in xrange(len(T)): Settings.instance().evaluationDate=Dates[iT] allDates=list(floatingSchedule) fixingdates=[index.fixingDate(floatingSchedule[iDate]) for iDate in xrange(len(allDates)) if index.fixingDate(floatingSchedule[iDate])<=Dates[iT]] if fixingdates: for date in fixingdates[:-1]: try:index.addFixing(date,0.0) except:pass try:index.addFixing(fixingdates[-1],rmean[iT]) except:pass discountTermStructure=RelinkableYieldTermStructureHandle() swapEngine=DiscountingSwapEngine(discountTermStructure) swap1.setPricingEngine(swapEngine) crv=crvVec[iT] discountTermStructure.linkTo(crv) forecastTermStructure.linkTo(crv) spmat[nI][iT]=swap1.NPV()  spmat=np.array(spmat) spmat[spmat<0]=0 spmat=np.sum(spmat, axis=0)|

2

4

6

8

10

12

14

16

18

20

22

24

 fixedSchedule=Schedule(startDate, maturity,Period("6m"), TARGET(),ModifiedFollowing,ModifiedFollowing,DateGeneration.Forward, False)

 swap1=VanillaSwap(VanillaSwap.Receiver, 1000000,fixedSchedule,0.05 , Actual360(),floatingSchedule, index, 0,Actual360())

 Settings.instance().evaluationDate=Dates[iT]

 fixingdates=[index.fixingDate(floatingSchedule[iDate]) for iDate in xrange(len(allDates)) if index.fixingDate(floatingSchedule[iDate])<=Dates[iT]]

 for date in fixingdates[:-1]:

 except:pass

 except:pass

 swapEngine=DiscountingSwapEngine(discountTermStructure)

 crv=crvVec[iT]

 forecastTermStructure.linkTo(crv)

 

 spmat[spmat<0]=0


This results in simplification of computation of the mean exposure and the CVA calculations at the driver. Next, create a distributed collection (RDD) of random numbers of size *NS × NT*:







||randArrayRDD = RandomRDDs.normalVectorRDD(sc, NS, NT, numPartitions=NP, seed=1L)|


Value each swap in parallel, where one unit of work is valuing a swap under all scenarios in a row of the random matrix.







||pmat=randArrayRDD.map(lambda p: (value_swap(p, NI))).collect()|


Collect the valuations at the driver.







||pmat=np.array(pmat)pmat=pmat.reshape((NS, NT))EE=np.sum(pmat, axis=1)EE=np.mean(EE,axis=0)|

pmat=pmat.reshape((NS, NT))

EE=np.mean(EE,axis=0)


Calculate the CVA.







||for i in xrange(NT-1):    sum+=0.5*(EE[i]*crvToday.discount(T[i])+EE[i+1]*crvToday.discount(T[i+1]))*(exp(-S*T[i]/(1.0-R))-exp(-S*T[i+1]/(1.0-R)))CVA=(1.0-R)*sum|

    sum+=0.5*(EE[i]*crvToday.discount(T[i])+EE[i+1]*crvToday.discount(T[i+1]))*(exp(-S*T[i]/(1.0-R))-exp(-S*T[i+1]/(1.0-R)))


Conclusion
We have demonstrated how Spark can be programmed to calculate the CVA of a portfolio over a cluster of thousands of cheap commodity nodes using Python, thus making it an attractive platform for prototyping and live risk estimates.
![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/32x32/facebook.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/32x32/google_plus.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/32x32/linkedin.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/32x32/mail.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/64x64/facebook.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/64x64/google_plus.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/64x64/linkedin.png)

![](http://blog.cloudera.com/wp-content/plugins/social-media-feather/synved-social/image/social/regular/64x64/mail.png)

