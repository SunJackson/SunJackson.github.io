---
layout:     post
title:      Using XGBoost for time series prediction tasks
subtitle:   转载自：http://mlwhiz.github.io/blog/2017/12/26/How_to_win_a_data_science_competition/
date:       2018-07-19
author:     Rahul Agarwal
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - predictions
    - features
    - series prediction
    - created
    - data
    - final
    - target
    - lag
    - coursera
    - store
    - learning
    - learned
    - zeros
    - sales
    - modelling
    - models
    - item_category_id_avg_item_cnt_day
    - master
---

Recently Kaggle master Kazanova along with some of his friends released a ["How to win a data science competition"](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) Coursera course. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.

## Description of the Problem:

In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.

We were asked you to predict total sales for every product and store in the next month.

The evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.

The main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.

## 1. Created a dataframe of all Date_block_num, Store and Item combinations:

This is important because in the months we don't have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.



## 2. Cleaned up a little of sales data after some basic EDA:



## 3. Created Mean Encodings:

These above lines add the following 9 features :

- 'item_id_avg_item_price'

- 'item_id_sum_item_cnt_day'

- 'item_id_avg_item_cnt_day'

- 'shop_id_avg_item_price',

- 'shop_id_sum_item_cnt_day'

- 'shop_id_avg_item_cnt_day'

- 'item_category_id_avg_item_price'

- 'item_category_id_sum_item_cnt_day'

- 'item_category_id_avg_item_cnt_day'


## 4. Create Lag Features:

Next we create lag features with diferent lag periods on the following features:

- 'item_id_avg_item_price',

- 'item_id_sum_item_cnt_day'

- 'item_id_avg_item_cnt_day'

- 'shop_id_avg_item_price'

- 'shop_id_sum_item_cnt_day'

- 'shop_id_avg_item_cnt_day'

- 'item_category_id_avg_item_price'

- 'item_category_id_sum_item_cnt_day'

- 'item_category_id_avg_item_cnt_day'

- 'item_cnt_day'




## 5. Fill NA with zeros:



## 6. Drop the columns that we are not going to use in training:



## 7. Take a recent bit of data only:



## 8. Split in train and CV :



## 9. THE MAGIC SAUCE:

In the start I told that the clipping aspect of [0,20] will be important.
In the next few lines I clipped the days to range[0,40]. You might ask me why 40. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer. While if I increase it to 40 having a 20 becomes much more easier. Please note that We will clip our predictions in the [0,20] range in the end.



## 10: Modelling:

- Created a XGBoost model to get the most important features(Top 42 features)

- Use hyperopt to tune xgboost

- Used top 10 models from tuned XGBoosts to generate predictions.

- clipped the predictions to [0,20] range

- Final solution was the average of these 10 predictions.


Learned a lot of new things from this [awesome course](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0). Most recommended.
