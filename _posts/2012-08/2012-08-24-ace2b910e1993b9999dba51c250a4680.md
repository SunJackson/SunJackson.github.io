---
layout:     post
title:      Numba vs Cython
subtitle:   转载自：http://jakevdp.github.io/blog/2012/08/24/numba-vs-cython/
date:       2012-08-24
author:     Jake VanderPlas
header-img: img/background0.jpg
catalog: true
tags:
    - pythonic
    - code
    - numba
    - cython
    - languages
---

*For a more up-to-date comparison of Numba and Cython, see the*
[*newer post*](http://jakevdp.github.io/blog/2013/06/15/numba-vs-cython-take-2)
*on this subject.*

Often I'll tell people that I use python for computational analysis, and they
look at me inquisitively. "Isn't python pretty slow?" They have a point.
Python is an interpreted language, and as such cannot natively perform
many operations as quickly as a compiled language such as C or Fortran.
There is also the issue of the oft-misunderstood and much-maligned
[GIL](http://wiki.python.org/moin/GlobalInterpreterLock),
which calls into question python's ability to allow true parallel computing.

Many solutions have been proposed: [PyPy](http://pypy.org/) is a much faster
version of the core python language; 
[numexpr](http://code.google.com/p/numexpr) provides optimized performance
on certain classes of operations from within python;
[weave](http://www.scipy.org/Weave) allows inline inclusion of compiled
C/C++ code;
[cython](http://www.cython.org/) provides extra markup that allows python
and/or python-like code to be compiled into C for fast operations. But
a naysayer might point out: many of these "python" solutions in practice
are not really python at all, but clever hacks into Fortran or C.

I personally have no problem with this. I like python because it gives me a nice
work-flow: it has a clean syntax, I don't need to spend my time hunting down
memory errors, it's quick to try-out code snippets, it's easy to wrap legacy
code written in C and Fortran, and I'm much more productive when writing
python vs writing C or C++. [Numpy](http://numpy.scipy.org/.),
[scipy](http://www.scipy.org/.), and [scikit-learn](http://www.scikit-learn.org/.)
give me optimized routines for most of what I need to do on a daily basis,
and if something more specialized comes up, cython has never failed me.
Nevertheless, the whole setup is a bit clunky:
why can't I have the best of both worlds: a beautiful, scripted, dynamically
typed language like python, with the speed of C or Fortran?

In recent years, new languages like [go](http://golang.org/) and
[julia](http://julialang.org/) have popped up which try to address some of
these issues. Julia in particular has a number of nice properties (see the
[talk](http://www.youtube.com/watch?v=VCp1jUgVRgE) from Scipy 2012 for a
good introduction) and uses [LLVM](http://llvm.org/.) to enable just-in-time
(JIT) compilation and achieve some impressive benchmarks. Julia holds promise,
but I'm not yet ready to abandon the incredible code-base and user-base
of the python community.

Enter [numba](http://numba.pydata.org/). This is an attempt to bring JIT
compilation cleanly to python, using the LLVM framework. In a
[recent post](http://jakevdp.github.io/blog/2012/08/08/memoryview-benchmarks), one commenter pointed
out numba as an alternative to cython. I had heard about it before (See
Travis Oliphant's scipy 2012 talk
[here](http://www.youtube.com/watch?v=WYi1cymszqY)) but hadn't had the chance
to try it out until now. Installation is a bit involved, but the directions
on the [numba website](http://numba.pydata.org/) are pretty good.

To test this out, I decided to run some benchmarks using the
pairwise distance function I've explored before (see posts
[here](http://jakevdp.github.io/blog/2012/08/08/memoryview-benchmarks)
and [here](http://jakevdp.github.io/blog/2012/08/16/memoryview-benchmarks-2)).

### Pure Python Version

The pure python version of the function looks like this:

Not surprisingly, this is very slow. For an array consisting of 1000 points
in three dimensions, execution takes over 12 seconds on my machine:

### Numba Version

Once numba is installed, we add only a single line to our above definition
to allow numba to interface our code with LLVM:

I should emphasize that this is the *exact same* code, except for numba's
`jit` decorator. The results are pretty astonishing:

This is a three order-of-magnitude speedup, simply by adding a numba
decorator!

### Cython Version

For completeness, let's do the same thing in cython. Cython
takes a bit more than just some decorators: there are also type specifiers
and other imports required. Additionally, we'll use the `sqrt` function
from the C math library rather than from numpy. Here's the code:

Running this shows about a 30% speedup over numba:

### The Takeaway

So numba is 1000 times faster than a pure python implementation, and only
marginally slower than nearly identical cython code.
There are some caveats here: first of all, I have years of experience with
cython, and only an hour's experience with numba. I've used every optimization
I know for the cython version, and just the basic vanilla syntax for numba.
There are likely ways to tweak the numba version to make it even faster,
as indicated in the comments of
[this post](http://jakevdp.github.io/blog/2012/08/08/memoryview-benchmarks).

All in all, I should say I'm very impressed. Using numba, I added
just a *single line* to the original python code, and
was able to attain speeds competetive with a highly-optimized (and
significantly less "pythonic") cython implementation. Based on this,
I'm extremely excited to see what numba brings in the future.

All the above code is available as an ipython notebook:
[numba_vs_cython.ipynb](http://jakevdp.github.io/downloads/notebooks/numba_vs_cython.ipynb).
For information on how to view this file, see the
[IPython page](http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html)
Alternatively, you can view this notebook (but not modify it) using the
nbviewer [here](http://nbviewer.ipython.org/url/jakevdp.github.com/downloads/notebooks/numba_vs_cython.ipynb).
