---
layout:     post
title:      Memoryview Benchmarks
subtitle:   转载自：http://jakevdp.github.io/blog/2012/08/08/memoryview-benchmarks/
date:       2012-08-09
author:     Jake VanderPlas
header-img: img/background0.jpg
catalog: true
tags:
    - cython
    - typed
    - memoryviews
    - pythonic
    - distances
---

There was recently a [thread](https://groups.google.com/forum?fromgroups#!topic/cython-users/8uuxjB_wbBQ[1-25])
on cython-users which caught my eye. It has to do with 
[memoryviews](http://docs.cython.org/src/userguide/memoryviews.html), a new
way of working with memory buffers in cython.

I've been thinking recently about how to do fast
and flexible memory buffer access in cython. I contributed the
[BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html)
implementation for nearest neighbors searching in
[scikit-learn](http://www.scikit-learn.org/.), and have been actively thinking
about how to make it faster and more flexible, including adding the ability
to specify distance metrics other than euclidean and minkowski.

In order to accomplish this, I'd like to have a set of distance metric
functions which take two vectors and compute a distance. There would
be many functions with similar call signatures which could then be
plugged into a code that would iterate over a set of vectors and
compute the appropriate distances.

### Pure python version

In pure python, the implementation described above might look something
like this:

This looks promising. Let's create a function based on this which will compute
the pairwise distance between all points in a matrix (this is similar
to [pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) in scikit-learn or
[pdist](http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) in scipy). The simple form of the function might look
like this:

We could exploit symmetry to reduce the number of computations required, but
we'll skip that step for now: this simple version of the function will give
us a good benchmark for comparison with alternatives below. Using the
`timeit` magic in ipython, we can learn how fast this implementation is:

It takes nearly seven seconds to compute 250,000 distances. This is much
too slow.

### Cython Speedup

Perhaps we can speed this up using cython declarations. Before typed
memoryviews were added in cython 0.16, the way to quickly index numpy
arrays in cython was through the numpy specific syntax, adding type
information to each array that specifies its data type, its dimension, and
its order:

Notice that we're essentially running the same code, except we have added
type identifiers to speed up function calls and loops. The `mode='c'`
argument in the `np.ndarray` type says that the array is contiguous in
memory, and C-ordered.

For reference, this can be compiled in-place by running
`python setup.py build_ext --inplace` with the following
setup.py file:

We'll time the resulting function on the same sized array as we did previously:

That's a factor of 10 speedup over the pure python version! It turns out,
though, that we can do better. In particular, the slicing operation when
we call `X[i]` and `X[j]` must generate a new numpy array each time, which
leads to a lot of python overhead in reference counting, etc. This is the
reason that the cython team introduced typed memoryviews in cython v0.16.

### Typed Memoryviews

The equivalent of the above code using typed memoryviews looks like this:

The only change is that instead of using the `np.ndarray[...]` type specifier,
we use the typed memoryview `double[:, ::1]` specifier. The `::1` in the
second position means that we are passing a two-dimensional array, which
is contiguous and C-ordered. We time the results and see the following:

This gives another factor of 30 improvement over the previous version, simply
by switching to typed memoryviews rather than the numpy interface. Still,
our function is creating memoryview objects each time we slice the array. We
can determine how much overhead this is generating by using raw C pointers
instead. It is not as clean, but it should be very fast:

### Raw Pointers

The fundamental benchmark for this sort of operation should be working
directly with the pointers themselves. While this is not a very "pythonic"
way of doing things, it does lead to very fast code, as we will see:

Instead of passing around slices of arrays, we've accessed the raw memory
buffer using C pointer syntax. This is not as easy to read, and can lead
to `glibc` errors or segmentation faults if we're not careful. Testing
this implementation, we find that it is extremely fast:

This is another factor of 10 faster than the memoryview benchmark above!
Essentially, what this is telling us is that creating a memoryview slice
takes about 0.02 / 500,000 = 40 nanoseconds on our machine. This is extremely
fast, but because we're performing this operation half a million times, the
cost of the allocations is significant compared to the rest of our
computation. If our vectors were, say, length 1000, this cost may not be
a significant percentage of the total cost.

So what are we left with? Do we need to use raw pointers in all circumstances
when working with collections of small vectors? Perhaps not.

### A Faster Implementation with Memoryviews

The creation of memoryview slices, though extremely fast, is causing a problem
simply because we're creating so many slices. Here is an alternative which
uses no raw pointers, but matches the speed of raw pointers:

Timing this implementation we find the following:

Just as fast as using raw pointers, but much cleaner and easier to read.

### Summary

Here are the timing results we've seen above:

- **Python + numpy**: 6510 ms

- **Cython + numpy**: 668 ms

- **Cython + memviews (slicing)**: 22 ms

- **Cython + raw pointers**: 2.47 ms

- **Cython + memviews (no slicing)**: 2.45 ms


So what have we learned here? First of all, typed memoryviews are fast.
Blazing fast. If used correctly, they can be comparable to raw pointers,
but are much cleaner easier to debug. For example, in the last
version, if we ran into a memory error we could simply turn on bounds-checking
and quickly find the source of the problem. Slicing with memoryviews is
also fast, but should be used carefully if your operation time on each slice
is compararable to the cost of building the slice.

The moral of the story? *Use typed memoryviews.* It will lead to fast cython
code which is cleaner, more readable, and more easily debuggable than any other
alternative.

**Update**: All of the above scripts are now available as an ipython
notebook: [memview_bench.ipynb](http://jakevdp.github.io/downloads/notebooks/memview_bench.ipynb).
For information on how to view this file, see the
[IPython page](http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html)
Alternatively, you can view this notebook (but not modify it) using the
nbviewer [here](http://nbviewer.ipython.org/url/jakevdp.github.com/downloads/notebooks/memview_bench.ipynb).

Thanks to Dave for the tip.
