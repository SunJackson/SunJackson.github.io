---
layout:     post
title:      Transforming Images to Feature Vectors
subtitle:   转载自：http://www.marekrei.com/blog/transforming-images-to-feature-vectors/
date:       2015-06-22
author:     Marek
header-img: img/background1.jpg
catalog: true
tags:
    - caffe
    - images
    - models
    - files
    - layers
---

I’m keen to explore some challenges in multimodal learning, such as jointly learning visual and textual semantics. However, I would rather not start by attempting to train an image recognition system from scratch, and prefer to leave this part to researchers who are more experienced in vision and image analysis.

Therefore, the goal is to use an existing image recognition system, in order to extract useful features for a dataset of images, which can then be used as input to a separate machine learning system or neural network. We start with a directory of images, and create a text file containing feature vectors for each image.

## 1. Install Caffe

Caffe is an open-source neural network library developed in Berkeley, with a focus on image recognition. It can be used to construct and train your own network, or load one of the pretrained models. A [web demo](http://demo.caffe.berkeleyvision.org/) is available if you want to test it out.

Follow the [installation instructions](http://caffe.berkeleyvision.org/installation.html) to compile Caffe. You will need to install quite a few dependencies (Boost, OpenCV, ATLAS, etc), but at least for Ubuntu 14.04 they were all available in public repositories.

Once you’re done, run

This will run the tests and make sure the installation is working properly.

## 2. Prepare your dataset

Put all your images you want to process into one directory. Then generate a file containing the path to each image. One image per line. We will use this file to read the images, and it will help you map images to the correct vectors later.

You can run something like this:

This will find all files in subdirectory called “images” and write their paths to images.txt

## 3. Download the model

There are a number of pretrained models publically available for Caffe. Four main models are part of the original Caffe distribution, but more are available in the [Model Zoo wiki page](https://github.com/BVLC/caffe/wiki/Model-Zoo), provided by community members and other researchers.

We’ll be using the **BVLC GoogLeNet** model, which is based on the model described in [Going Deeper with Convolutions](http://arxiv.org/abs/1409.4842) by Szegedy et al. (2014). It is a 22-layer deep convolutional network, trained on ImageNet data to detect 1,000 different image types. Just for fun, here’s a diragram of the network, rotated 90 degrees:

[![](http://www.marekrei.com/blog/wp-content/uploads/2015/06/googlenet_diagram.png)
](http://www.marekrei.com/blog/wp-content/uploads/2015/06/googlenet_diagram.png)

The Caffe models consist of two parts:

1. A description of the model (in the form of *.prototxt files)

1. The trained parameters of the model (in the form of a *.caffemodel file)


The prototxt files are small, and they came included with the Caffe code. But the parameters are large and need to be downloaded separately. Run the following command in your main Caffe directory to download the parameters for the GoogLeNet model:

This will find out where to download the caffemodel file, based on information already in the models/bvlc_googlenet/ directory, and will then place it into the same directory.

In addition, run this command as well:

It will download some auxiliary files for the ImageNet dataset, including the file of class labels which we will be using later.

## 4. Process images and print vectors

Now is the time to load the model into Caffe, process each image, and print a corresponding vector into a file. I created a script for that (see below, also available as a [Gist](https://gist.github.com/marekrei/7adc87d2c4fde941cea6)):

You will first need to set the caffe_root variable to point to your Caffe installation. Then run it with:

It will first print out a lot of model-specific debugging information, and will then print a line for each input image containing the image name, the label of the most probable class, and the class probability.

At the same time, it will also print vectors into the output file. By default, it will extract the layer pool5/7x7_s1 after processing each image. This is the last layer before the final softmax in the end, and it contains 1024 elements. I haven’t experimented with choosing different layers yet, but this seemed like a reasonable place to start – it should contain all the high-level processing done in the network, but before forcing it to choose a specific class. Feel free to choose a different layer though, just change the corresponding parameter in the script. If you find that specific layers work better, let me know as well.

The outputfile will contain vectors for each image. There will be one line of values for each input image, and every line will contain 1024 values (if you printed the default layer). Mission accomplished!

## Troubleshooting

Below are some tips for when you run into problems.

First, it’s worth making sure you have compiled the python bindings in the Caffe directory:

I was getting some unusual errors when this code was in a subdirectory of the main Caffe folder. After some googling I found that others had similar problems with other projects, and apparently overlapping library names were causing the wrong dependencies to be included. The simple solution was to move this code out of the Caffe directory, and put it somewhere else.

I installed Caffe with CUDA support, and even though I turned GPU support off in the script, it was still complaining when I didn’t set the CUDA path. For example, I run the code like this (you may need to change the paths to match your system):

Finally, Caffe is compiled against a specific version of CUDA. I initially had CUDA 6.5 installed, but after upgrading to CUDA 7.0 the Caffe library had to be recompiled.

## Epilogue

There you have it – going from images to vectors. Now you can use these vectors to represent your images in various tasks, such as classification, multi-modal learning, or clustering. Ideally, you will probably want to train the whole network on a specific task, including the visual component, but for starters these pretrained vectors should be quite helpful as well.

These instructions and the script are loosely based on Caffe examples on [ImageNet classification and filter visualisation](http://nbviewer.ipython.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb). If the code here isn’t doing quite what you want it to, it’s worth looking at these other similar applications.

If you have any suggestions or fixes, let me know and I’ll be happy to incorporate them in this post.
