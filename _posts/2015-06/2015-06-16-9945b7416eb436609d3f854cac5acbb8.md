---
layout:     post
title:      Pure Python Decision Trees
subtitle:   转载自：http://kldavenport.com/pure-python-decision-trees/
date:       2015-06-16
author:     Kevin Davenport
header-img: img/background1.jpg
catalog: true
tags:
    - python
    - implementing
    - performance
    - contemporaries descision trees
    - quick google search
---

[![](http://34.211.1.181/wp-content/uploads/2015/05/decision_tree.png)
](http://34.211.1.181/wp-content/uploads/2015/05/decision_tree.png)

By now we all know what Random Forests is. We know about the great off-the-self performance, ease of tuning and parallelization, as well as it’s importance measures. It’s easy for engineers implementing RF to forget about it’s underpinnings. Unlike some of it’s more modern and advanced contemporaries, descision trees are easy to interpret. A neural net might obtain great results but it is difficult to work backwards from and explain to stake holders as the weights of the connections between two neurons have little meaning on their own. Decision trees won’t be a great choice for a feature space with complex relationships between numerical variables, but it’s great for data with a simplier mix of numerical and categorical.

I recently dusted off one of my favorite books, [Programming Collective Intelligence](https://amzn.to/DJp4uz) by Toby Segaran (2007), and was quickly reminded how much I loved all the pure python explanations of optimization and modeling. It was never enough for me to read about and work out proofs on paper, I had to implement something abstract in code to truly learn it.

I went through some of the problems sets and to my dismay, realized that the code examples were no longer hosted on his personal site. A quick google search revealed that multiple kind souls had not only shared their old copies on github, but even corrected mistakes and updated python methods.

I’ll be using some of this code as inpiration for an intro to decision trees with python.
