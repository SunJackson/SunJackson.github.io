---
layout:     post
title:      New benchmarks for approximate nearest neighbors
subtitle:   转载自：https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html
date:       2018-07-19
author:     未知
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - benchmarks
    - https
    - iâ
    - neighbor benchmarking going
    - annoy
    - include approximate algorithms
    - searches
    - people
    - hnsw
    - falconn
    - vector
    - datasets
    - empirical
    - blog
    - martin
    - feel
    - faithfull
    - word
    - nearest neighbors
    - spaces
    - faiss
    - whatâ
    - bit
---

UPDATE(2018-06-17): There are is a [later blog post with newer benchmarks](https://erikbern.com/2018-06-17-new-approximate-nearest-neighbor-benchmarks)!

One of my super nerdy interests include approximate algorithms for nearest neighbors in high-dimensional spaces. The problem is simple. You have say 1M points in some high-dimensional space. Now given a query point, can you find the nearest points out of the 1M set? Doing this fast turns out to be tricky.

Iâ€™m the author of [Annoy](https://github.com/spotify/annoy) which has more than 3,000 stars on Github. Spotify used a lot of vector spaces models for music recommendations, and I think as people embrace vector space models more and more, weâ€™ll see more attention to fast approximate searches.

Whatâ€™s bothered me about the research is that thereâ€™s a thousand papers about how to do this, but very little empirical comparison. I built [ANN-benchmarks](https://github.com/erikbern/ann-benchmarks) to address this. It pits a bunch of implementations (including Annoy) against each other in a death match: which one can return the most accurate nearest neighbors in the fastest time possible. Itâ€™s not a new project, but I havenâ€™t actively worked on it for a while.

Recently, two researchers (Martin AumÃ¼ller and Alexander Faithfull) published a [paper](http://www.itu.dk/people/maau/additional/sisap2017-preprint.pdf) featuring ANN-benchmarks and were nice enough to include me as a co-author (despite not writing a single word in the paper). They contributed a ton of useful stuff into ANN-benchmarks which made me realize that my tool could be some kind of â€œgolden standardâ€� for all approximate nearest neighbor benchmarking going forward. So I decided to spend a bit more time making ANN-benchmarks *ridiculously* easy to use for researchers or industry practitioners active in this field.

My hope is that there is a group of people who care about approximate nearest neighbor search, and hopefully people everyone can stick to the same benchmarks going forward. That would be great, because it makes everyoneâ€™s lives easier (kind of like how ImageNet made it easier for the deep learning crowd).

## Whatâ€™s new in ANN-benchmarks?

What are the things Iâ€™ve added in the last few months? A bunch of stuff:

- All algorithms are now Dockerized. This means you donâ€™t have to install a bunch of stuff on the host computer, and deal with all the mess that that entails. All you need to do to add a new algorithm is to create a Dockerfile and some more config. Very nice!

- It comes with pre-computed datasets. Iâ€™ve collected a bunch of different vector datasets (MNIST and many other ones), split in train and test sets, and computed the nearest neighbors for the test set. Everyone can just download the dataset and use it.

- I finally got the [Travis-CI test working](https://travis-ci.org/erikbern/ann-benchmarks) kind of (itâ€™s still a bit flaky).


I re-ran all benchmarks, which took a few days and about $100 in EC2 costs. The results depend on what dataset you use, but are somewhat consistent. Iâ€™m not going to get into the details. All you need to know for now is that the further up and to the right is better. Feel free to check out [ANN-benchmarks](https://github.com/erikbern/ann-benchmarks) for more info.

glove-100-angular:
![](https://erikbern.com/assets/glove-100-angular.png)


sift-128-euclidean:
![](https://erikbern.com/assets/sift-128-euclidean.png)


fashion-mnist-784-euclidean:
![](https://erikbern.com/assets/fashion-mnist-784-euclidean.png)


gist-960-euclidean:
![](https://erikbern.com/assets/gist-960-euclidean.png)


In almost all the datasets the top 5 are in the following order:

1. HNSW (hierarchical navigable small world) from [NMSLIB](https://github.com/searchivarius/nmslib) (non metric search library) knocks it out of the park. Itâ€™s over 10x faster than Annoy.

1. [KGraph](https://github.com/aaalgo/kgraph) is not far behind, which is another graph-based algorithm

1. SW-graph from NMSLIB

1. FAISS-IVF from [FAISS](https://github.com/facebookresearch/faiss) (from Facebook)

1. [Annoy](https://github.com/spotify/annoy) (I wish it was a bit faster, but think this is still honorable!)


In previous benchmarks, [FALCONN](https://github.com/FALCONN-LIB/FALCONN) used to perform very well, but Iâ€™m not sure whatâ€™s up with the latest benchmarks â€“ seems like a huge regression. If any of the authors are reading this, Iâ€™d love it if you can figure out whatâ€™s going on. FALCONN somewhat interesting because itâ€™s the the only library Iâ€™ve seen that gets decent results using [locality sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing). Other than that, I havenâ€™t been very impressed by LSH. Graph-based algorithms seem be the state of the art, in particular HNSW. Annoy uses a very different algorithms (recursively partitions the space using a two-means algorithm).

## A final word

Going forward, if I see a paper about fast approximate nearest neighbor queries, and it doesnâ€™t include proper benchmarks against any of the top libraries, Iâ€™m not going to give a ğŸ’©! ANN-benchmarks makes it too easy not to have an excuse for it!
