---
layout:     post
title:      Weekly Review： 12/03/2017
subtitle:   转载自：https://codesachin.wordpress.com/2017/12/04/weekly-review-12-03-2017/
date:       2017-12-03
author:     srjoglekar246
header-img: img/background0.jpg
catalog: true
tags:
    - learning
    - cs
    - training
    - values
    - numenta
---

Missed a post last week due to the Thanksgiving long weekend :-). We had gone to San Francisco to see the city and try out a couple of hikes). Just FYI – strolling around SF is also as much a hike as any of the real trails at Mt Sutro – with all the uphill & downhill roads! As for Robotics, I am currently on Week 3 of the [Mobility course](https://www.coursera.org/learn/robotics-mobility/home/welcome), which is more of physics than ‘computer science’; its a welcome change of pace from all the ML/CS stuff I usually do.

**[Numenta – Secret to Strong AI](https://medium.com/@Numenta/the-secret-to-strong-ai-61d153e26273)**

In this article, [Numenta](https://numenta.com/)‘s cofounder discusses what we would need to push current AI systems towards *general intelligence*. He points out that many industry experts (including Jeff Bezos & Geoffrey Hinton) have opined that it would take far more than scaling up current intelligent systems, to achieve the next ‘big leap’.

Numenta’s goal as such is to take inspiration from the human brain (especially the [neocortex](https://numenta.com/)) to design the next generation of machine intelligence. The article describes how the neocortex uses abstract ‘locations’ to understand sensory input and form mental representations. To read more of Numenta’s research, visit [this page](https://numenta.com/papers-videos-and-more).

**[Transfer Learning](https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0)**

This article, though not presenting any ‘new findings’, is a fun-to-read introduction to Transfer Learning. It focusses on the different ways TL can be applied in the context of Neural Networks.

It provides examples of how pre-trained networks can be ‘retrained’ over new data by freezing/unfreezing certain layers during backpropagation. The blogpost also provides a bunch of useful links, such as [this discussion](http://cs231n.github.io/transfer-learning) on Stanford CS231.

**[Structured Deep Learning](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848)**

This article motivates the need for [embedding vectors](https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12) in Deep Learning. One of the challenges of using SQL-ish data for deep learning, is the involvement of [categorical attributes](https://en.wikipedia.org/wiki/Categorical_variable). The usual ways of dealing with such variables in ML is to use one-hot encodings, or find an integer representation for each possible value.

However, 1) one-hot encodings increase the memory footprint of a NN & 2) assigning integers to ordinal values implies a wrong meaning to neural networks, which are inherently continuous/numeric in nature. For example, Sunday=1 & Saturday=7 for a ‘week’ enum might lead the NN to believe that Sundays and Saturdays are very far apart, which is not usually true.

Hence, learning vectorial embeddings for ordinal attributes is perhaps the right way to go for most applications. While we usually know embeddings in the context of words (Word2Vec, LDA, etc), similar techniques can be used to other enum-style values as well.

**[Population-based Training](https://deepmind.com/blog/population-based-training-neural-networks)**

This blog-post by Deepmind presents a novel approach to coming up with the hyperparameters for Neural-Network training. It essentially brings in the methodology of [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm) for designing optimal network architectures.

While standard hyperparameter-tuning methods perform some kind of random search, Population-based training (PBT) allows each candidate ‘worker’ to take inspiration from the best candidates in the current population (similar to mating in GAs) while allowing for random perturbations in parameters for exploration (a.la. GA mutations.)

 





### Like this:

Like Loading...
