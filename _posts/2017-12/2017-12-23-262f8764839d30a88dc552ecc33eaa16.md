---
layout:     post
title:      Weekly Review： 12/23/2017
subtitle:   转载自：https://codesachin.wordpress.com/2017/12/24/weekly-review-12-23-2017/
date:       2017-12-23
author:     srjoglekar246
header-img: img/background0.jpg
catalog: true
tags:
    - learning
    - learned
    - data
    - xgboost
    - indexing
---

***Happy Holidays people!*** If you live in the Bay Area then the next week is probably your time off, so I hope you have fun and enjoy the holiday season! As for Robotics, I just finished Week 2 of [Perception](https://www.coursera.org/learn/robotics-perception/home/welcome), and will probably kick off Week 3 in 2018. I am excited for the last ‘real’ course (Estimation & Learning), and then building my own robot as part of the ‘Capstone’ project after that :-D.

This week’s articles:

**[XGBoost](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning)**

I recently came across XGBoost (*eXtreme Gradient Boosting*), an improvement over standard Gradient Boosting – thats actually a shame, considering how popular this method is in Data Science. If you are rusty on ensemble learning, take a look at this [article on bagging/random Forests](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning), and my own [intro to Boosting.](https://codesachin.wordpress.com/2016/03/06/a-small-introduction-to-boosting)

XGBoost is one of the most efficient versions of Gradient Boosting, and apparently works really well on structured/tabular data. It also provides features such as sparse-awareness (being able to handle missing values), and the ability to update models with ‘continued training’. Its effectiveness for tabular data has made it very popular with Kaggle winners, with one of them quoting: *“When in doubt, use xgboost”*!

Take a look at the [original paper](https://arxiv.org/pdf/1603.02754.pdf) to dig deeper.

**[Quantum Computing + Machine Learning](https://www.technologyreview.com/s/609804/a-startup-uses-quantum-computing-to-boost-machine-learning)**

A lot of companies, such as Google, Microsoft, etc have recently shown interest in the domain of Quantum Computing. [Rigetti](https://www.rigetti.com/) happens to be a startup that aims to rival these juggernauts with its great solution to cloud-Quantum Computing (called [Forest](https://www.rigetti.com/forest)). They even have their own Python integration!

The article in question details their efforts to prototype simple clustering with quantum computing. It is still pretty crude, and is by no means a replacement to traditional systems – for now. One of the major critical points is “Applying Quantum Computing to Machine Learning will only make a black-box system more difficult to understand”. This is infact true, but the author suggests that ML could actually/maybe help us understand the behavior of Quantum Computers by modelling them!

**[Breaking a CAPTCHA with ML](https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710)**

A simple, easy-to-read, fun article on how you could break the simplest CAPTCHA algorithms with CV+Deep Learning.

**[Learning Indexing Structures with ML](https://arxiv.org/abs/1712.01208)**

*Indexing structures* are essentially data structures meant for efficient data access. For example, a [B-Tree Index](https://en.wikipedia.org/wiki/B-tree) is used for efficient range-queries, a Hash-table is used for fast key-based access, etc. However, all of these data structures are pretty rigid in their behavior – they do not fine-tune/change their parameters based on the structure of the data.

This paper (that includes the Google legend Jeff Dean as an author) explores the possibility of using Neural Networks (infact, a hierarchy of them) as indexing structures. Basically, you would use a Neural Network to compute the function – *f: data -> hash/position*.

Some key takeaways from the paper:

1. Range Index models essentially ‘learn’ a cumulative distribution function.

The overall ‘learned index’ by this paper is a hierarchy of models (but not a tree, since two models at a certain layer can point to the same model in the next layer)

1. As you go down the layers, the models deal with smaller and smaller subsets of the data.


**[Tacotron 2](https://research.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html)**

This post on the Google Research blog details the development of a [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio)-like framework to generate Human Speech from text.

 





### Like this:

Like Loading...
