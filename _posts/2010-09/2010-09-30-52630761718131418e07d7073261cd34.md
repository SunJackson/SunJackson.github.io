---
layout:     post
title:      LARS algorithm
subtitle:   转载自：http://fa.bianp.net/blog/2010/lars-algorithm/
date:       2010-09-30
author:     Fabian Pedregosa
header-img: img/background3.jpg
catalog: true
tags:
    - working lately
    - stagewise
    - gram matrix
    - maximum number
    - tiny parts
---

I've been working lately with [Alexandre Gramfort](http://www-sop.inria.fr/members/Alexandre.Gramfort) coding the LARS
algorithm in [scikits.learn](hhtp://scikit-learn.sf.net/.). This algorithm computes the solution to
several general linear models used in machine learning: LAR, Lasso,
Elasticnet and Forward Stagewise. Unlike the implementation by
coordinate descent, the LARS algorithm gives the full coefficient path
along the regularization parameter, and thus it is specially well suited
for performing model selection.
![](http://scikit-learn.sourceforge.net/_images/plot_lasso_lars.png)


The algorithm is coded
mostly in python, with some tiny parts in C (because I already had the
code for Cholesky deletes in C) and a cython interface for the BLAS
function dtrsv, which will be proposed to scipy once I stabilize this
code. The algorithm is mostly complete, allowing some optimizations,
like using a precomputed Gram matrix or specify maximum number of
features/iterations, but could still be extended to compute other
models, like ElasticNet or Forward Stagewise. I haven't done any
benchmarks yet, but preliminary ones by Alexandre Gramfort showed that
it is roughly equivalent to this [Matlab implementation](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3897). Using
[PyMVPA](http://pymvpa.org/.), it shouldn't be difficult to benchmark it against the R
implementation, though.
