---
layout:     post
title:      Frequentism and Bayesianism： A Practical Introduction
subtitle:   转载自：http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/
date:       2014-03-11
author:     Jake VanderPlas
header-img: img/background1.jpg
catalog: true
tags:
    - f_i
    - d_i
    - e_i
    - likelihood
    - frac
---

We'll start with the classical frequentist **maximum likelihood** approach. Given a single observation $D_i = (F_i, e_i)$, we can compute the probability distribution of the measurement given the true flux $F_{\rm true}$ given our assumption of Gaussian errors:

$$ P(D_i~|~F_{\rm true}) = \frac{1}{\sqrt{2\pi e_i^2}} \exp{\left[\frac{-(F_i - F_{\rm true})^2}{2 e_i^2}\right]} $$

This should be read "the probability of $D_i$ given $F_{\rm true}$ equals ...". You should recognize this as a normal distribution with mean $F_{\rm true}$ and standard deviation $e_i$.

We construct the **likelihood function** by computing the product of the probabilities for each data point:

$$\mathcal{L}(D~|~F_{\rm true}) = \prod_{i=1}^N P(D_i~|~F_{\rm true})$$

Here $D = \{D_i\}$ represents the entire set of measurements. Because the value of the likelihood can become very small, it is often more convenient to instead compute the log-likelihood. Combining the previous two equations and computing the log, we have

$$\log\mathcal{L} = -\frac{1}{2} \sum_{i=1}^N \left[ \log(2\pi e_i^2) + \frac{(F_i - F_{\rm true})^2}{e_i^2} \right]$$

What we'd like to do is determine $F_{\rm true}$ such that the likelihood is maximized. For this simple problem, the maximization can be computed analytically (i.e. by setting $d\log\mathcal{L}/dF_{\rm true} = 0$). This results in the following observed estimate of $F_{\rm true}$:

$$ F_{\rm est} = \frac{\sum w_i F_i}{\sum w_i};~~w_i = 1/e_i^2 $$

Notice that in the special case of all errors $e_i$ being equal, this reduces to

$$ F_{\rm est} = \frac{1}{N}\sum_{i=1}^N F_i $$

That is, in agreement with intuition, $F_{\rm est}$ is simply the mean of the observed data when errors are equal.

We can go further and ask what the error of our estimate is. In the frequentist approach, this can be accomplished by fitting a Gaussian approximation to the likelihood curve at maximum; in this simple case this can also be solved analytically. It can be shown that the standard deviation of this Gaussian approximation is:

$$ \sigma_{\rm est} = \left(\sum_{i=1}^N w_i \right)^{-1/2} $$

These results are fairly simple calculations; let's evaluate them for our toy dataset:
