---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/09/05/distilled-news-1187/
date:      2019-09-05
author:      Michael Laux
tags:
    - data
    - models
    - trained
    - training
    - generate new kernel
---

**Models as Web Endpoints**

In the second chapter of Data Science in Production, I discuss how to set up predictive models as web endpoints. This is a useful skill, because it enables data scientists to shift from batch model application, such as outputting CSV files, to hosting models that other applications can use in real time. I previously wrote about this process using Keras, but the result was not a scalable solution. The goal of this post is to focus on the tools you can use in the Python stack to host a predictive model that can scale to meet demand.

**Learn how to call state of the art models with some clicks using Azure, Colab Notebooks and a lot of snippets. There is a fleet of clouds with their own minds floating over the internet trying to take control of the winds. They’ve been pushing very aggressively all kinds of services into the world and absorbed data from every possible source. Among this big bubble of services, an increasing number of companies and applications are relying on pre-made AI resources to extract insights, predict outcomes and gain value out of unexplored information. If you are wondering how to try them out, I’d like to give you an informal overview of what you can expect from sending different types of data to these services. In short, we’ll be sending images, text and audio files high into the clouds and explore what we get back. While this way of using AI doesn’t give you direct, full control over what’s happening (as you would using machine learning frameworks) its a quick way for you to play around with several kinds of models and use them in your applications. It’s also a nice way to get to know what’s already out there.**


**Top 5 Natural Language Processing Python Libraries for Data Scientist.**

• spaCy• Gensim• Pattern• Natural Language Tool KIT [NLTK]• TextBlob

**Generating Titles for Kaggle Kernels with LSTM**

(Kernels are the notebooks in R or Python published on Kaggle by the users.) When I first found out about sequence models, I was amazed by how easily we can apply them to a wide range of problems: text classification, text generation, music generation, machine translation, and others. In this article, I would like to focus on the step-by-step process of creating a model and won’t cover sequence models and LSTMs theory. I got an idea to use Meta Kaggle dataset to train a model to generate new kernel titles for Kaggle. Kernels are the notebooks in R or Python published on Kaggle by the users. Kaggle users can upvote kernels. Depending on the number of the upvotes, kernels receive medals. Model, which generates kernel titles, can help to capture trends for Kaggle kernels and serve as an inspiration for writing new kernels and get medals.

**Transfer Learning in NLP**

Using already trained robust models to complete NLP tasks in shorter time and using less resources. First what is Transfer Learning? In these recent times, we have become very good at predicting a very accurate outcome with very good training models. But considering most of the machine learning tasks are domain specific, the trained models usually fail to generalize the conditions that it has never seen before. The real world is not like the trained data set, it contains lot of messy data and the model will make a ill prediction in such condition. The ability to transfer the knowledge of a pre-trained model into a new condition is generally referred to as transfer learning.

**Neural Knapsack**

Solving the knapsack problem using neural networks. In some cases of data science, it is needed to run a specific algorithm on the output of the model to get the result. Sometimes it is as simple as finding the index of the maximum output, other times, more advanced algorithms are needed. You may run the algorithm after running the inference. However, designing a model that can run the algorithm internally have some advantages. Solving the problem of knapsack using neural networks not only helps the model run the knapsack algorithm internally but also allows the model to be trained end to end.

**Bayesian Neural Networks (LSTM): implementation**

Bayesian inferences permit quantification of uncertainty and thus, enable the development of robust machine learning models. In practice, one needs to employ sampling methods to approximate the posterior distribution/integrals encountered in the Bayesian setting. Sampling methods are computationally intensive for large neural networks typical employed in practice. Variational inference methods have been developed to overcome this limitation. In an earlier post, we discussed theoretical aspects of practical, variational inference algorithm, Back Prop by Bayes (BBB). BBB provides an effective means to reduce epistemic uncertainty, by placing priors of weights. Epistemic uncertainty, is generally related to lack of training data. In the post, we consider the practical implementation of BBB, specially for LSMT’s.

**Charting Newborn Territory**

You might have seen some kind of parenthood-in-numbers stats such as ‘a baby goes through 6,000 diapers before potty training on average’. Well, I am currently getting to live that in real life: in the very first month of my daughter’s life, she has already had 233 meals/diaper changes. As much I was mesmerized by her milk-drunk smiles and chubby hands and feet, I was wondering how can I represent this experience to show the commitment and energy that it takes for a new mother (and her support team) to tend to her baby. As they say, ‘the days go by slow and the years go by fast’.

**Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model**

• Learn how you can use computer vision and deep learning techniques to work with video data• We will build our own video classification model in Python• This is a very hands-on tutorial for video classification – so get your Jupyter notebooks ready

**Spatial regression in R part 1: spaMM vs glmmTMB**

Many datasets these days are collected at different locations over space which may generate spatial dependence. Spatial dependence (observation close together are more correlated than those further apart) violate the assumption of independence of the residuals in regression models and require the use of a special class of models to draw the valid inference.

**6 Tips for Building a Training Data Strategy for Machine Learning**

Without a well-defined approach for collecting and structuring training data, launching an AI initiative becomes an uphill battle. These six recommendations will help you craft a successful strategy.1: Establish a Budget for Training Data2: Source Appropriate Data3: Ensure Data Quality4: Be Aware of and Mitigate Data Biases5: When Necessary, Implement Data Security Safeguards6: Select Appropriate Technology

**Using Spark from R for performance with arbitrary code – Part 1 – Spark SQL translation, custom functions, and Arrow**

Apache Spark is a popular open-source analytics engine for big data processing and thanks to the sparklyr and SparkR packages, the power of Spark is also available to R users. This series of articles will attempt to provide practical insights into using the sparklyr interface to gain the benefits of Apache Spark while still retaining the ability to use R code organized in custom-built functions and packages.

**1 giraffe, 2 giraffe, GO!**

I am beyond excited to finally be able to announce a new version of ggraph. This release, like the ggforce 0.3.0 release, has been many years in the making, laying dormant for long periods first waiting for ggplot2 to get updated and then waiting for me to have time to finally finish it off. All that is in the past now as ggraph 2.0.0 has finally landed on CRAN, filled with numerous new features, a massive amount of bug fixes, and a slew of breaking changes. If you are new to ggraph, a short description follows: It is an extension of ggplot2 that implement an extended grammar for relational data (e.g. trees and networks). It provides a huge variety of geoms for drawing nodes and edges, along with an assortment of layouts making it possible to produce a very wide range of network visualization types. It is to my knowledge the most feature packed network visualization framework available in R (and potentially in other languages as well), all building on top of the familiar ggplot2 API. If you want to learn more I invite you to browse the new pkgdown website that has been made available.

**How short-term forecasting with neural nets can inform long-term decisions.**

Electric utilities can detect monthly peaks with only a three-day forecast.

**Linear Programming for Data Scientists**

As Data Scientists we become acquainted with the concept of optimization very early in our careers. Optimization lies at the heart of every machine learning model. But our relationship with optimization goes way back; we’ve been [unknowingly] solving optimization problems since before we can remember:• The fastest way to get to work• Organizing our budget to get the most out of it• Planning workouts for maximum impact in the least amount of time• Doing meal-prep every Sunday• Packing a suitcase for a long vacationThese are just a few examples of everyday optimization. Optimization is the way of life. It can be as simple as the examples I just mentioned, or as complex as The Traveling Salesman Problem.

**Analyzing a binary outcome arising out of within-cluster, pair-matched randomization**

A key motivating factor for the simstudy package and much of this blog is that simulation can be super helpful in understanding how best to approach an unusual, or least unfamiliar, analytic problem. About six months ago, I described the DREAM Initiative (Diabetes Research, Education, and Action for Minorities), a study that used a slightly innovative randomization scheme to ensure that two comparison groups were evenly balanced across important covariates. At the time, we hadn’t finalized the analytic plan. But, now that we have started actually randomizing and recruiting (yes, in that order, oddly enough), it is important that we do that, with the help of a little simulation.

### Like this:

Like Loading...
