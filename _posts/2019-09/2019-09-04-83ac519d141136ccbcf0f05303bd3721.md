---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/09/05/distilled-news-1186/
date:      2019-09-04
author:      Michael Laux
tags:
    - modeling
    - models
    - data
    - learning
    - eleanor
---

**Guide to R and Python in a Single Jupyter Notebook**

The war between R and Python users has been raging for several years. With most of the old school statisticians being trained on R and most computer science and data science departments in universities instead preferring Python, both have pros and cons. The main cons I have noticed in practice are in the packages that are available for each language. As of 2019, the R packages for cluster analysis and splines are superior to the Python packages of the same kind. In this article, I will show you, with coded examples, how to take R functions and datasets and import and utilize then within a Python-based Jupyter notebook.

**Deep Reuse: Streamline CNN Inference On the Fly via Coarse-Grained Computation Reuse**

This paper presents deep reuse, a method for speeding up CNN inferences by detecting and exploiting deep reusable computations on the fly. It empirically reveals the massive similarities among neuron vectors in activation maps, both within CNN inferences on an input and across inputs. It gives an in-depth study on how to effectively turn the similarities into beneficial computation reuse to speed up CNN inferences. The investigation covers various factors, ranging from the clustering methods for similarity detection, to clustering scopes, similarity metrics, and neuron vector granularities. The insights help create deep reuse. As an on-line method, deep reuse is easy to apply, and adapts to each CNN (compressed or not) and its input. Using no special hardware support or CNN model changes, this method speeds up inferences by 1.77-2X (up to 4.3X layer-wise) on the fly with virtually no (<0.0005) loss in accuracy

**All-optical neural network for deep learning**

New approach could enable parallel computation with light. In a key step toward making large-scale optical neural networks practical, researchers have demonstrated a first-of-its-kind multilayer all-optical artificial neural network. Researchers detail their two-layer all-optical neural network and successfully apply it to a complex classification task.

**How organizations are sharpening their skills to better understand and use AI**

Continuous learning is critical to business success, but providing employees with an easily accessible, results-driven solution they can access from wherever they are, whenever they need it, is no easy feat. Additionally, delivering valuable content in a variety of formats – whether that is through books, videos, or live online training – is crucial to supporting employees to upskill and reskill on the job. These are some of the features O’Reilly Online Learning provides to its 2.25 million platform users to encourage personal and professional development, and there’s no better time to take advantage. According to Deloitte, evolving work demands and skills requirements are one big reason why continuous learning is critical, and there is no sector experiencing this more abruptly than technology. Executives and employees alike are worried about how emerging tech, such as robotics and AI, are changing jobs and how people should prepare for them. In fact, a recent World Economic Forum report found that more than half (54%) of all employees will require significant reskilling and upskilling in just three years. So, what exactly are the skills data scientists and other tech titles are honing in response to this shift?

**Microsoft Icecaps**

With natural language processing rapidly increasing in popularity, more and more tools have become available to the public to build large systems. Some of these tools are intended for general-purpose NLP, while others focus on specific domains such as language modeling and text generation. However, few are designed to target conversational scenarios and the specific needs they entail. Microsoft Icecaps was created to offer researchers and developers an open-source toolkit with a focus on conversational modeling. With a design emphasizing flexibility, modularity, and ease of use, Icecaps empowers users to build customized neural conversational systems that produce personalized, diverse, and informed responses. Icecaps 0.1 provides a wide array of features for users to build and customize conversational systems.• Icecaps’ design is based on a component-chaining architecture, where models are represented as chains of components (e.g. encoders and decoders) that data flows through. This enables complex multi-task learning environments with shared components between tasks.• Personalization embeddings, SpaceFusion, and MRC-based knowledge grounding models are recent advances in conversational modeling included in our toolkit.• We provide customized decoding tools that allow users to employ maximum mutual information, token filtering, and repetition penalties to improve response quality and diversity.• Data processing tools are provided for users to easily convert their text data sets into binarized TFRecords. Our data processor features various text preprocessing tools, including byte pair encoding and fixed-length multi-turn context extraction.

**Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!**

• pyforest – Importing all Python Data Science Libraries in One Line of Code• HungaBunga – A Different Way of Building Machine Learning Models using sklearn• Behavior Suite for Reinforcement Learning (bsuite) by DeepMind• DistilBERT – A Lighter and Cheaper Version of Google’s BERT• ShuffleNet Series – An Extremely Efficient Convolutional Neural Network for Mobile Devices• RAdam – Improving the Variance of Learning Rates• ggtext – Improved Text Rendering for ggplot2

**Use ExPanD to Create a Notebook for Your EDA**

The ‘ExPanDaR’ package offers a toolbox for interactive exploratory data analysis (EDA). You can read more about it here. The ‘ExPanD’ shiny app allows you to customize your analysis to some extent but often you might want to continue and extend your analysis with additional models and visualizations that are not part of the ‘ExPanDaR’ package. Thus, I am currently developing an option to export the ‘ExPanD’ data and analysis to an R Notebook. While it is not ready for CRAN yet, it seems to work reasonably well and I would love to see some people trying it and letting me know about any bugs or other issues that they encounter. Hence, this blog post.

**September Edition: Deploying Machine Learning Models**

The applications of machine learning are seemingly endless (as Juan De Dios Santos demonstrates, building a pikachu detection app). But whilst an increasing number of data scientists are familiarising themselves with the technology, finding successful use cases that are scaled and robust is still difficult. This is, at least in part, due to the fact that building an accurate and unbiased model can barely considered to be half the battle. Other people need to use it! As Ian Xiao discusses in another of our Edition articles, even if you can get the model out into the world there are interaction, execution and feedback problems that lie in wait to disrupt your AI aspirations… Our picks for this month are here to help you pick the right stack for releasing your model, properly.

**Forward Mode Automatic Differentiation & Dual Numbers**

We discuss different ways of differentiating computer programs. More specifically, we compare forward mode and reverse mode (backprop) automatic differentiation. Ultimately, we implement forward mode AD with dual dual numbers for a simple logistic regression problem.

**Feature Selection in Python – Recursive Feature Elimination**

Finding optimal features to use for Machine learning model training can sometimes be a difficult task to accomplish. I’m not saying that the process itself is difficult, there are just so many methods to choose from. You’ve probably read a lot about methods like Principal Component Analysis, and it is by no means a bad method, but it just won’t tell you which features are most important – it will return principal components which are actually combinations of features (to explain in the most simple words possible).

**eXplainable AI (XAI) research must go beyond the four walls of research labs**

Eleanor, 65, wants to buy a car for her only grandchild, Tim, who is turning 21 in a few months. She wants to gift him a car so that he can travel to job interviews in relative comfort instead of traveling by bus. Eleanor goes to XYZ Bank to request a $10,000 loan. She speaks with an Associate, Kirby. Kirby invites Eleanor to fill out the loan application. On completing the form, Eleanor submits her request for a new credit line. Kirby enters her details into the software, the information then gets sent to a black-box model. The model does its magic and instantly sends its decisions back to Kirby. Dismayed, he looks at Eleanor, and immediately, her heart plummets to the floor. She knows right away; her loan request did not go through. She asks Kirby the reason for the refusal. Kirby, unfortunately, has no idea. He tells her, ‘A mathematical model makes decisions, and we listen to it.’

### Like this:

Like Loading...
