---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/09/03/distilled-news-1185/
date:      2019-09-03
author:      Michael Laux
tags:
    - likes
    - memory
    - memories
    - machine learning model
    - functions
---

**Build your own Knowledge Graph**

Do you have a lot of text documents stored on hard disks or in the cloud, and you don’t use its textual information directly in your business? Then this article is for you. Learn how you can leverage artificial intelligence to use that dark data and turn it into valuable business insights, using a Knowledge Graph.

**Explaining Predictions: Random Forest Post-hoc Analysis (randomForestExplainer package)**

This is a continuation on the explanation of machine learning model predictions. Specifically, random forest models. We can depend on the random forest package itself to explain predictions based on impurity importance or permutation importance. Today, we will explore external packages which aid in explaining random forest predictions.

**Great Developers Never Stop Learning**

As software engineers, developers or architects, it is pivotal to stay current and relevant within the technologies and the domains we work. Some of the skills we studied a few years ago at uni have become outdated, so unless we get in charge of our learning we will lose our competitiveness. Also, we quite often sacrifice learning and development in favour of project deadlines. This short-term thinking carries a long-term price! Although it is impossible to stay up to speed with everything in this rapidly changing world of I.T., we need to choose a few areas that interest us (whether these areas are related to our current job, or the one we want in the future) and carve time out to up-skill.

**Reinforcement Learning: Markov-Decision Process (Part 2)**

This story is in continuation with the previous, Reinforcement Learning : Markov-Decision Process (Part 1) story, where we talked about how to define MDPs for a given environment.We also talked about Bellman Equation and also how to find Value function and Policy function for a state.In this story we are going to go a step deeper and learn about Bellman Expectation equation , how we find the optimal Value and Optimal Policy function for a given state and then we will define Bellman Optimality Equation.

**Lesser known dplyr functions**

However, dplyr comes with several other functions that are not mentioned in the vignettes (or at least, not at length). In this post I’ll talk about some of them. (For the full list of dplyr functions, see the reference manual.)• Counting functions: n() and n_distinct()• If-else functions: if_else() and case_when()• Comparison functions: between() and near()• Selecting specific elements based on position: nth(), first() and last()• Selecting specific rows based on position/value: slice() and top_n()• Utilities: coalesce() and pull()

**Jacobian regularization – Generalization of L1 and L2 regularization**

L1 and L2 regularization, also known as Lasso and Ridge, are well known regularization techniques, used for a variety of algorithms. The idea of these methods is to impose smoothness of the prediction function and avoid overfitting.

**Convergence of Random Variables**

There are three different situations we have to take into account:• Convergence in Probability• Convergence in Quadratic Mean• Convergence in DistributionLet’s examine all of them.

**An Introduction to Graph Databases**

The last few years have seen an explosion of new paradigms in databases. Previously the relational database management system (RDBMS) as epitomized by the likes of Microsoft’s SQLServer or Oracles MySQL had been the de facto route for those looking for a database. I touched on the reasons for this, and looked at some of the newer, or re-discovered, alternatives in one of my earlier pieces; in this article I’m going to dig deeper into one of these, the Graph Database, to explore what they can do, and to show some use cases where they shine.

**Does a neural network know when it doesn’t know?**

If you are keeping up with data science and machine learning, you probably know that in recent years, deep neural networks revolutionized artificial intelligence and computer vision. Ever since AlexNet won the ImageNet challenge in 2012 by a large margin, deep neural nets have conquered many previously unsolvable tasks. Neural networks are mind-blowing. They can learn how to turn winter landscapes to summer landscapes, put zebra stripes to a horse, learn semantic representations between words with basically no supervision, generate photorealistic images from sketches, and many more amazing feats. The technology has advanced so far such that basically everyone with a notebook can use and build neural network architectures capable of previously unattainable feats. Many open source deep learning frameworks – such as TensorFlow and PyTorch – are available, bringing this amazing technology to your arm’s reach.

**Fastest Way to Install & Load Libraries in R**

After working collaboratively with a classmate, it became apparent that I needed a new way of loading libraries from what I was taught in school. Not everyone has the same libraries installed and this can run into errors. I wanted the code to run seamlessly for everyone. Additionally, it is painful to have to write the same functions over and over again to install and load different libraries. My classmate and I worked to find a simple way to do this. I then used a package called tictoc to measure the speed of different methods.

**How to create multiple variables with a single line of code in R**

hen I have a dataset with many variables and want to create a new variable for each of them, then the first thing comes into my mind is to write a new line of code for each transformation (e.g., new variable). It is time-consuming when I have more than 10 variables. Therefore, as ‘an advanced R user,’ I will use mutate_all to create a new variable for each variable included in the dataset. However, in most ‘real life’ circumstances, I don’t want to create a new variable for all variables in the dataset, but only for a few of them. For example, I don’t want to create a new variable for ID or categorical variables. In this situation, the mutate_at became a useful function in the tidyverse package.

**Analysis of the Light Rail Network using Python (Pandas, Plotly, SodaPy)**

I remember how excited I was when I heard about Canberra’s newest public transport. I’ve only been to a few places in Australia but I felt like Canberra missed out considering the light rail system in Sydney and Melbourne because they’re pretty convenient and it just feels amazing ?? I enjoy how smooth the light rail moves *(except maybe that small bit just before arriving at the terminal stop in Gungahlin where it feels like the tram’s scratching off tax money from the tracks)*, how it saves money from petrol, parking, or just travel costs in general (concession rate – I’m at uni and I get discount, the government understands my financial struggles because I spend it all on rice *sob*). Riding the tram to get to and from the city just makes sense to me and it’s great!

**On Custom Loss Functions in Keras**

The Keras library already provides various losses like mse, mae, binary cross entropy, categorical or sparse categorical losses cosine proximity etc. These losses are well suited for widely used image regressions and image classification problems. However, for many specific cases, a user would need to define custom loss functions. This functionality is very will built in Keras with easy implementation. The loss functions in Keras work with tensors and you are not recommended to use numpy arrays with them. In this post, I would try to cover how to build a custom loss function in Keras that I was recently exploring for depth estimation on images and share few insights and gotchas that got me scraping my head for days.

**Transformer vs RNN and CNN for Translation Task**

Google Brain and their collaborators have published an article introducing a new architecture, the Transformer, based only on attention mechanisms (see reference [1]). It surpasses any other NMT models seen before such as Google Neural Machine Translation (GNMT) alias Google Translate. The Transformer has been able to reach a new state of the art in translation. In addition to major improvements in the quality of translation, it also allows the realization of many other natural language processing (NLP) tasks.

**Neuroscience and Cognitive Psychology can Help Us to Understand Memory in AI Agents**

Incorporating memory into deep neural networks is one of the most active areas of research in the AI ecosystem. Memory is one of the magic abilities of human cognition and one that has proven hard to recreate in artificial intelligence(AI) systems. Memory modeling is an active area of research in the deep learning space. In recent years, techniques such as Neural Turing Machines(NTM) have made significant progress setting up the foundation for building human-like memory structures in deep learning systems. In the past, I’ve written extensibly about the role of memory in artificial intelligence(AI) so I am not planning to bore you restating the same points. Instead, I would like to approach the subject from a different angle and attempt to answer three fundamental questions that we should have in mind when thinking about memory in deep learning models:• What makes memory such a complex subject in deep learning systems?• Where can we draw inspiration about memory architectures?• What are the main techniques used to represent memories in deep learning models?

**Uber’s Ludwig Gets a Second Version to Help You Build Machine Learning Models Without Writing Code**

In the last couple of years, Uber has quietly become one of the most active contributors to open source machine learning technologies. From training frameworks like Horovod, statistical languages like Pyro or conversational stacks like the Plato Research Dialogue System, Uber has been pushing boundaries of innovation in the machine learning space with practical technologies rather than exoteric research. One Uber’s most popular contributions to the machine learning ecosystem has been Ludwig, a framework for training and testing machine learning models without the need to write code. Recently, Uber released a second version of Ludwig that includes major enhancements in order to enable mainstream no-code experiences for machine learning developers.

**NLP Text Preprocessing: A Practical Guide and Template**

Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.

### Like this:

Like Loading...
