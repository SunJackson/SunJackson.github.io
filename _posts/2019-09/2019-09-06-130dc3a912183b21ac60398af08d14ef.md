---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/09/06/distilled-news-1188/
date:      2019-09-06
author:      Michael Laux
tags:
    - learning
    - principles
    - plot
    - google
    - chatbots
---

**4 Design Principles for Data Processing**

For data science, many people may have asked the same question: does data science programming have design patterns? I would say yes. However, in order to differentiate them from OOP, I would call them Design Principles for data science, which essentially means the same as Design Patterns for OOP, but at a somewhat higher level. As inspired by Robert Martin’s book ‘Clean Architecture’, this article focuses on 4 top design principles for data processing and data engineering.Design Principle 1: Always Start with Design of Datasets and Data EntitiesDesign Principle 2: Separate Business Rules from Processing LogicDesign Principle 3: Build Exceptions from the BeginningDesign Principle 4: Easy to Integrate using Standard Input and Output

**Synthetic datasets: A non-technical primer for the biobehavioral sciences**

Open research data provides considerable scientific, societal, and economic benefits. However, disclosure risks can sometimes limit the sharing of open data, especially in datasets that include information from individuals with rare disorders. This article introduces the concept of synthetic datasets, which is an emerging method originally developed to permit the sharing of confidential census data. Synthetic datasets mimic real datasets by preserving their statistical properties and the relationships between variables. Importantly, this method also reduces disclosure risk to essentially nil as no record in the synthetic dataset represents a real individual. This practical guide with accompanying R script enables biobehavioral researchers to create synthetic datasets and assess their utility via the synthpop R package. By sharing synthetic datasets that mimic original datasets that could not otherwise be made open, researchers can ensure the reproducibility of their results and facilitate data exploration while maintaining participant privacy.

**Google Federated Learning and AI**

I have heard mention of federated learning before. However diving into Semi-Supervised Learning the last few days made me aware of this concept and its usage. In this article I will give a brief explanation of the concept without going into extensive technical details. Facebook was recently hit by a $5 billion fine by the Federal Trade Commission due to its negligent management of user data. Google has equally faced billion dollar formal charges from the European Commission. How can these companies improve? Google proposes part of a solution.

**Natural Language Processing for Automated Feature Engineering**

When trying to leverage real-world data in a machine learning pipeline, it is common to come across text. Text data can include a lot of valuable information, but is often ignored because it is hard to translate text into meaningful numbers that algorithms can interpret. In Featuretools, primitive functions are used to automatically create features for different types of data, enabling machine learning models to make the most of data from your dataset. In this article, I explore the use of the nlp-primitives library to create features from text data by using an example dataset to investigate these additional features in a machine learning model. Then, I explain why these primitives have such an outsized effect on the accuracy of the model.

**Hello World in Speech Recognition**

This blog will help you write a basic end to end ASR system using Tensorflow. I will go over each component of a minimal neural network and a prefix beam search decoder required to generate a readable transcript from audio. I come across a lot of resources on building basic machine learning systems around computer vision and natural language processing tasks, but very few when it comes to speech recognition. This is an attempt to fill that gap and make this field less daunting for beginners.

**Importance Sampling Introduction**

Importance sampling is an approximation method instead of sampling method. It derives from a little mathematic transformation and is able to formulate the problem in another way. In this post, we are going to:â€¢ Learn the idea of importance samplingâ€¢ Get deeper understanding by implementing the processâ€¢ Compare results from different sampling distribution

**What exactly is Kubernetes?**

Since I discovered Docker in late 2015 I was impressed by the fact that deployment could have been processed like source code. Yes, I think this was the gap that containers fill: make source code and serve the same thing. As you commit your changes on the order controller, you can commit the new server that hosts the order controller. That was a revolution, thinking to the time when developer manually uploads file to production or move with DevOps tool if they were lucky. This opens to new scenarios, where also the infrastructure is deployed by DevOps tools. Honestly speaking, not all the applications need an infrastructure that can be replicated by a click. It is a requirement for Saas software, but an expensive plus for most of simples business use case. However, coming back to Docker, there still was some effort to deploy it and scale. I used it on dev, test, and production environment, no blame on docker. I love it. But, it needed to convert old way developers to use container also in development to gain all benefits of a lean process to the production environment. We did it so that, more or less, most of the developers have docker in their PC, can run a container or run ‘docker-compose up’ and use a cluster of servers.

**Build a Simple ChatBot with Python and Google Search**

Today we are going to build a Python 3 ChatBot API and web interface. ChatBots are challenging to build because there are an infinite number of inputs. Because of that, a ChatBot that can consistently come up with good answers needs immense knowledge. It is common for developers to apply machine learning algorithms, NLP, and corpora of predefined answers into their ChatBot system design. We are going to keep our code basic, so we will bypass creating a complex ‘brain’ for our ChatBot. Instead of building an AI brain, we will use one that is free and already built: Google Search. Our ChatBot will perform a Google Search of a user’s query, scrape the text from the first result, and reply to the user with the first sentence of that page’s text. Let’s get started! By the way, all the code mentioned is in the Python ChatBot GitHub repository.

**Why Artificial Intelligence Projects Fail**

Over the last few months, I have seen the number of AI projects taken up significantly and most of the folks working on AI projects in their firms are planning to increase their AI initiatives even further over the next 12 months. Many of these initiatives come with high expectations but AI projects are far from fool-proof. In fact, there are predictions that more than half of all AI projects will fail to deliver against their expectations. Failure can happen for many reasons, however, there are a few glaring dangers that will cause any AI project to crash and burn. Based on my experience, discussing with fellow AI practitioners in various organizations and going through many surveys done in the last few months, I know these mistakes are all too frequent. One thing they have in common is they are all caused by a lack of adequate strategy & planning.

**The ultimate EDA visualization in R**

Recently I was doing EDA (exploratory data analysis) for a research project. My colleague introduced a plot to me called rain cloud plot. It is a combination of dot plot, box plot and kernel density (or you could think of it as a half violin plot), which really conveys a myriad of information yet in a visually pleasant way (thus a lot of times we mistakenly call it rainbow plot ðŸ™‚ ).

**Forget about Excel, Use these R Shiny Packages Instead**

Transferring your Excel sheet to a Shiny app can be the easiest way to create an enterprise ready dashboard. In this post, I present 6 Shiny alternatives for the table-like data that Excel users love.

**Probability Learning IV : The Math Behind Bayes**

After the two previous posts about Bayes’ Theorem, I got a lot of requests asking for a deeper explanation on the maths behind the regression and classification uses of the theorem. Because of that, in the previous post we covered the maths behind the Maximum Likelihood principle, to construct a solid basis from where we can easily understand and enjoy the maths behind Bayes.

**A Hands-On Guide To Text Classification With Transformer Models (XLNet, BERT, XLM, RoBERTa)**

A step-by-step tutorial on using Transformer Models for Text Classification tasks. Learn how to load, fine-tune, and evaluate text classification tasks with the Pytorch-Transformers library. Includes ready-to-use code for BERT, XLNet, XLM, and RoBERTa models.

### Like this:

Like Loading...
