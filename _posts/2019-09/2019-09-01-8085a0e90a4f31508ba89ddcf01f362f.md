---
layout:     post
catalog: true
title:      Distilled News
subtitle:      转载自：https://analytixon.com/2019/09/01/distilled-news-1182/
date:      2019-09-01
author:      Michael Laux
tags:
    - data
    - models
    - machine
    - presidio
    - graphs
---

**ARIMA Models with Turing.jl**

This article is a part of my work done in Julia Season of Contributions (JSoC) 2019. It describes the Julia implementation of ARIMA models using the Probabilistic Programming Language (PPL) Turing.jl, which provides great ease in defining probabilistic models. This aspect of Turing will become more obvious when we look at model definitions later on in the article. Furthermore, Turing supports the use of custom distributions for specifying models. Okay, so let’s get straight onto it!

**The Curse of Dimensionality**

Most of us have very reasonable intuition that more information is always better. For example, the more I know about a potential borrower, the better I can predict whether that borrower will default on a loan. Surprisingly, this intuition is false, sometimes spectacularly false. I’ve written separately about how extra information can be worse than useless because fields in a sample of data can cluster or correlate in misleading ways. Such irrelevant or random chance effects can obscure real ones. In fact, matters get worse. Extra information can cause problems even if it’s useful. This surprising fact is due to phenomena that arise only in high dimensions and is known as The Curse of Dimensionality.

**Multivariable time series forecasting using Stateless Neural Networks**

Forecasting with multiple variables that aids a target variable with stateless deep learning.Time series. Datasets that have a time element with them. Such data allow us to think about the combination of 2 properties of time series:• Seasonality – Patterns in data that tends to repeat over and over at a specific length of time.• Trend – This is similar to regression, where we are capturing the global pattern of the series.• The relevance of data tends to center at present time, meaning past and data close to present time is of greater influence and accuracy of future predictions is better when closer to present data (principle of entropy).

**10 Most Popular Machine Learning Software Tools in 2019**

1. TensorFlow2. Google Cloud ML Engine3. Accord.NET4. Apache Mahout5. Shogun6. Oryx 27. Apache Singa8. Apache Spark MLlib9. Google ML Kit for Mobile10. Apple’s Core ML

**Introducing AI Explainability 360: A New Toolkit to Help You Understand what Machine Learning Models are Doing**

Interpretability is one of the most difficult challenges in modern machine learning solutions. While building sophisticated machine learning models is getting easier, understanding how models develop knowledge and arrive to conclusions remains a very difficult challenge. Typically, the more accurate the models the harder they are to interpret. Recently, AI researchers from IBM open sourced AI Explainability 360, a new toolkit of state-of-the-art algorithms that support the interpretability and explainability of machine learning models. The release of AI Explainability 360 is the first practical implementation of ideas outlined in dozens of research papers in the last few years. In the same way traditional software applications incorporate instrumentation code to help its runtime monitoring, machine learning models need to add interpretability techniques to facilitate debugging, troubleshooting and versioning. However, interpreting machine learning models is a multiple of complexity higher than traditional software applications. For starters, we need have very little understanding of what makes a machine learning model interpretable.

**Reconciling Your Data and the World with Knowledge Graphs**

Knowledge is at the core of any successful initiative. In the age of data, one would think that the task of incorporating knowledge would be easier than ever. However, the three Vs – volume, velocity and veracity of data – make the task anything but easy. What is an effective way to accomplish the same? Enter knowledge graphs. Graphical representation of knowledge has been around for decades now, dating back to the 1960s. In our previous article, we talked about graph tech (databases, processing, and visualization). We also explored how it solves multiple pain points that impede data-driven initiatives from operating at high efficiency. A key point in the rise of graph tech was Google unveiling its knowledge graph (KG) back in 2012.

**TensorFlow 2.0: Dynamic, Readable, and Highly Extended**

Considering learning a new Python framework for deep learning? If you already know some TensorFlow and are looking for something with a little more dynamism, you no longer have to switch all the way to PyTorch, thanks to some substantial changes coming as part of TensorFlow 2.0. In fact, many of the changes in 2.0 specifically address the alleged shortcomings of TensorFlow. With eager execution by default, you no longer have to pre-define a static graph, initialize sessions, or worry about tensors falling outside of the proper scope when you get over-zealous in your object oriented principles. TensorFlow still has about 3 times the user base of PyTorch (judging from the repositories on GitHub referencing each framework), and that means more extensions, more tutorials, and more developers collaboratively exploring the space of all possible code errors on Stack Overflow. You’ll also find that, despite the major changes starting with TensorFlow 2.0, the project developers have taken many steps to ensure that backwards compatibility can be maintained.

**Microsoft Academic Knowledge Graph (MAKG)**

We present the Microsoft Academic Knowledge Graph (MAKG), a large RDF data set with over eight billion triples with information about scientific publications and related entities, such as authors, institutions, journals, and fields of study. The data set is based on the Microsoft Academic Graph and licensed under the Open Data Attributions license. Furthermore, we provide entity embeddings for all 210M represented scientific papers.

**Presidio – Data protection and anonymization API**

Presidio (Origin from Latin praesidium ‘protection, garrison’) helps to ensure sensitive text is properly managed and governed. It provides fast analytics and anonymization for sensitive text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers and financial data. Presidio analyzes the text using predefined or custom recognizers to identify entities, patterns, formats, and checksums with relevant context. Presidio leverages docker and kubernetes for workloads at scale.

**Interactive Dashboards for Data Science**

Creating an online dashboard in Python to analyse Facebook Stock Market Prices and Performance Metrics.

**On adding negative feedback synapses to a neural network**

The missing link in deep neural networks. The special thing about adding negative recurrent synapses to a neural network is that they introduce inner states within the network.

**Summary: Structured Attention Networks**

Since its inception, attention has become an essential component of neural architectures that perform sequence transduction. Originally designed as a differentiable analog of the alignment matrix used in phrase-based machine translation , attention has also proven useful for tasks such as image captioning and constituency parsing. In fact, some have argued that it is all you need to build a state-of-the-art sequence transduction model. In Structured Attention Networks , Kim et al. formulate attention as a graphical model which simulates selection from a set of inputs. Using this framework, they then demonstrate how more complex graphical models can be used to incorporate richer structural biases into a model (while still leaving it able to be trained in an end-to-end manner).

**The Trap of Classification**

This post is about the practice of classifying data into finite classes, and the trap of specifying a right answer. If high-dimensional representations are the principle behind the success of neural networks, the practice of discrete classification should have been made redundant. A network model should not be forced to collapse it’s output to such a sparse vector, such as the labels ‘1-9’. I don’t believe the brain ever processes discrete categories of digits ‘1-9’, and likewise it is the wrong approach for network models. The challenge for models, and what the brain is able to do, is to communicate the output ‘3’, while still maintaining a distributed representation of that concept.

### Like this:

Like Loading...
