---
layout:     post
title:      Deep learning architecture diagrams
subtitle:   转载自：http://fastml.com/deep-learning-architecture-diagrams/
date:       2018-07-19
author:     未知
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - architectures
    - https
    - lstm diagrams
    - understanding
    - neural networks
    - neurons
    - machines
    - pdf
    - units
    - code
    - deep learning
    - recurrent
    - compiler
    - streams forming
    - called
    - practice
    - savanna
    - weighted connections
    - gru
    - esn
    - memories
    - memory
    - gradients
    - feature
    - perceptrons
---

As a wild stream after a wet season in African savanna diverges into many smaller streams forming lakes and puddles, so deep learning has diverged into a myriad of specialized architectures. Each architecture has a diagram. Here are some of them.










Neural networks are conceptually simple, and that’s their beauty. A bunch of homogenous, uniform units, arranged in layers, weighted connections between them, and that’s all. At least in theory. Practice turned out to be a bit different. Instead of feature engineering, we now have [architecture engineering](http://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html), as described by Stephen Merrity:

> The romanticized description of deep learning usually promises that the days of hand crafted feature engineering are gone - that the models are advanced enough to work this out themselves. Like most advertising, this is simultaneously true and misleading.
Whilst deep learning has simplified feature engineering in many cases, it certainly hasn’t removed it. As feature engineering has decreased, the architectures of the machine learning models themselves have become increasingly more complex. Most of the time, these model architectures are as specific to a given task as feature engineering used to be.
To clarify, this is still an important step. Architecture engineering is more general than feature engineering and provides many new opportunities. Having said that, however, we shouldn’t be oblivious to the fact that where we are is still far from where we intended to be.

Not quite as bad as doings of [architecture astronauts](http://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html), but not too good either.

![](http://fastml.com/images/deep_learning_diagrams/kardash_chair_600.jpg)
An example of architecture specific to a given task

## LSTM diagrams

How to explain those architectures? Naturally, with a diagram. A diagram will make it all crystal clear.

Let’s first inspect the two most popular types of networks these days, CNN and LSTM. You’ve already seen a [convnet diagram](http://fastml.com/object-recognition-in-images-with-cuda-convnet#architecture), so turning to the iconic LSTM:

![](http://fastml.com/images/deep_learning_diagrams/lstm.png)


It’s easy, just take a closer look:

![](http://fastml.com/images/deep_learning_diagrams/lstm_otoro.png)


As they say, in mathematics you don’t understand things, you just get used to them.

Fortunately, there are good explanations, for example [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs) and
[Written Memories: Understanding, Deriving and Extending the LSTM](http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html).

LSTM still too complex? Let’s try a simplified version, GRU (Gated Recurrent Unit). Trivial, really.

![](http://fastml.com/images/deep_learning_diagrams/lstm_and_gru.png)


Especially this one, called *minimal GRU*.

![](http://fastml.com/images/deep_learning_diagrams/minimal_gru.jpg)


## More diagrams

Various modifications of LSTM are now common. Here’s one, called deep bidirectional LSTM:

![](http://fastml.com/images/deep_learning_diagrams/db-lstm.jpg)
DB-LSTM, [PDF](https://pdfs.semanticscholar.org/c34e/41312b47f60986458759d5cc546c2b53f748.pdf)

![](http://fastml.com/images/deep_learning_diagrams/db-lstm2.png)


The rest are pretty self-explanatory, too. Let’s start with a combination of CNN and LSTM, since you have both under your belt now:

![](http://fastml.com/images/deep_learning_diagrams/crmn.jpg)
Convolutional Residual Memory Network, [1606.05262](https://arxiv.org/abs/1606.05262)

![](http://fastml.com/images/deep_learning_diagrams/dntm.jpg)
Dynamic NTM, [1607.00036](http://arxiv.org/abs/1607.00036)

![](http://fastml.com/images/deep_learning_diagrams/entm.jpg)
Evolvable Neural Turing Machines, [PDF](http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf)

![](http://fastml.com/images/deep_learning_diagrams/unsupervised_domain_adaptation_by_backpropagation.svg)
Unsupervised Domain Adaptation By Backpropagation, [1409.7495](https://arxiv.org/abs/1409.7495)

![](http://fastml.com/images/deep_learning_diagrams/deeply_recursive_cnn_for_super_resolution.jpg)
Deeply Recursive CNN For Image Super-Resolution, [1511.04491](http://arxiv.org/abs/1511.04491)

![](http://fastml.com/images/deep_learning_diagrams/recurrent_model_of_visual_attention.jpg)
Recurrent Model Of Visual Attention, [1406.6247](https://arxiv.org/abs/1406.6247)

This diagram of multilayer perceptron with synthetic gradients scores high on clarity:

![](http://fastml.com/images/deep_learning_diagrams/mlp_with_synthetic_gradients.png)
MLP with synthetic gradients, [1608.05343](https://arxiv.org/abs/1608.05343)

![](http://fastml.com/images/deep_learning_diagrams/drinking_cat.jpg)


Every day brings more. Here’s a fresh one, again from Google:

![](http://fastml.com/images/deep_learning_diagrams/google_neural_machine_translation_system.jpg)
Google’s Neural Machine Translation System, [1609.08144](http://arxiv.org/abs/1609.08144)

## And Now for Something Completely Different

Drawings from the [Neural Network ZOO](http://www.asimovinstitute.org/neural-network-zoo) are pleasantly simple, but, unfortunately, serve mostly as eye candy. For example:

![](http://fastml.com/images/deep_learning_diagrams/zoo_lsm_200.png)

![](http://fastml.com/images/deep_learning_diagrams/zoo_esn_200.png)

![](http://fastml.com/images/deep_learning_diagrams/zoo_elm_200.png)
ESM, ESN and ELM

These look like not-fully-connected perceptrons, but are supposed to represent a *Liquid State Machine*, an *Echo State Network*, and an *Extreme Learning Machine*.

How does LSM differ from ESN? That’s easy, it has green neuron with triangles. But how does ESN differ from ELM? Both have blue neurons.

Seriously, while similar, ESN is a recurrent network and ELM is not. And this kind of thing should probably be visible in an architecture diagram.

![](http://fastml.com/images/deep_learning_diagrams/there_is_no_escape.jpg)


## P.S.

You haven’t seen anything till you’ve seen A [Neural](https://twitter.com/notmisha/status/793619497118752768) [Compiler](http://www.sciencedirect.com/science/article/pii/0304397594002003):

![](http://fastml.com/images/deep_learning_diagrams/neural_compiler_1.jpg)
The input of the compiler is a PASCAL Program.

![](http://fastml.com/images/deep_learning_diagrams/neural_compiler_2.jpg)
The compiler produces a neural network that computes what is specified by the PASCAL program.

![](http://fastml.com/images/deep_learning_diagrams/neural_compiler_4.jpg)
The compiler generates an intermediate code called cellular code.

Weird, huh?
