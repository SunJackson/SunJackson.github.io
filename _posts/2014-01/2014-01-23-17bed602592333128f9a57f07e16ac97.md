---
layout:     post
title:      Five Interesting Papers
subtitle:   转载自：http://inverseprobability.com/2014/01/23/five-interesting-papers
date:       2014-01-23
author:     未知
header-img: img/background0.jpg
catalog: true
tags:
    - probabilistic
    - learning
    - papers
    - bayesian
    - shared
---

[Originally Shared Publicly via Google+](https://plus.google.com/+NeilLawrence/posts/RzWppAJgQdz) check there for comments

Had a very nice dinner with Mike Tipping and David Duvenaud on Tuesday night in Cambridge. Great to catch up with Mike, who’s had a big influence on ML with probabilistic PCA and sparse Bayesian learning. He’s been working in the commercial sector for a few years (but it was really nice when David mentioned [in passing] perspectives about sparsity that were originated by Mike!).

Mike followed up with a mail asking the following question (shared with permission): “If I were to read 5 papers from the last couple of years that capture the interesting/important stuff happening in ML, what would they be?”

So below’s my answer: I love the fact that four of them are on arxiv. I also know that at least two of them had trouble getting published (either delayed in publication or reviewers not enthusiastic … etc).

They are chosen partly as reflections of where I think the field is going, and partly as reflections of where I think the field should be going. And of course the list is totally subjective and missing great papers by some of my favourite researchers: it’s a personal list, but Mike and I share similar tastes. It will be interesting to hear Mike’s opinion about them when he’s done.

[Stochastic variational inference by Hoffman, Wang, Blei and Paisley](http://arxiv.org/abs/1206.7051)
A way of doing approximate inference for probabilistic models with potentially billions of data … need I say more?

[Austerity in MCMC Land: Cutting the Metropolis Hastings by Korattikara, Chen and Welling](http://arxiv.org/abs/1304.5299)
Oh … I do need to say more … because these three are at it as well but from the sampling perspective. Probabilistic models for big data … an idea so important it needed to be in the list twice.

[Practical Bayesian Optimization of Machine Learning Algorithms by Snoek, Larochelle and Adams](http://arxiv.org/abs/1206.2944)
This paper represents the rise in probabilistic numerics, I could also have chosen papers by Osborne, Hennig or others. There are too many papers out there already. Definitely an exciting area, be it optimisation, integration, differential equations. I chose this paper because it seems to have blown the field open to a wider audience, focussing as it did on deep learning as an application, so it let’s me capture both an area of developing interest and an area that hits the national news.

[Kernel Bayes Rule by Fukumizu, Song, Gretton](http://arxiv.org/abs/1009.5736)
One of the great things about ML is how we have different (and competing) philosophies operating under the same roof. But because we still talk to each other (and sometimes even listen to each other) these ideas can merge to create new and interesting things. Kernel Bayes Rule makes the list.

[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)
An obvious choice, but you don’t leave the Beatles off lists of great bands just because they are an obvious choice.﻿
