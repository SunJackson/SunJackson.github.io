---
layout:     post
title:      Introducing vbench, new code performance analysis and monitoring tool
subtitle:   转载自：http://wesmckinney.com/blog/introducing-vbench-new-code-performance-analysis-and-monitoring-tool/
date:       2011-12-18
author:     Wes McKinney
header-img: img/background2.jpg
catalog: true
tags:
    - pandas benchmarks
    - vbench
    - performance
    - web
    - matplotlib
---





** Sun 18 December 2011

 

Do you know how fast your code is? Is it faster than it was last week? Or a month ago? How do you know if you accidentally made a function slower by changes elsewhere? Unintentional performance regressions are extremely common in my experience: it's hard to unit test the performance of your code. Over time I have gotten tired of playing the game of "performance whack-a-mole". Thus, I started hacking together a little weekend project that I'm calling **[vbench](http://github.com/pydata/vbench)**. If someone thinks up a cleverer name, I'm all ears.

[Link to pandas benchmarks page produced using vbench](http://pandas.sourceforge.net/vbench.html )

## What is vbench?

vbench is a super-lightweight Python library for running a collection of performance benchmarks over the course of your source repository's history. Since I'm a GitHub user, it only does git for now, but it could be generalized to support other VCSs. Basically, you define a benchmark:

Then you write down the information about your repository and how to build any relevant DLLs, etc., that vary from revision to revision:

Then you pass this info, plus a list of your benchmark objects, to the `BenchmarkRunner` class:

Now, the `BenchmarkRunner` makes a clone of your repo, then runs all of the benchmarks once for each revision in the repository (or some other rule, e.g. I've set `run_option='eod'` to only take the last snapshot on each day). It persists the results in a SQLite database so that you can rerun the process and it will skip benchmarks it's already run (this is key when you add new benchmarks, only the new ones will be updated). Benchmarks are uniquely identified by the MD5 hash of their source code.

This is the resulting plot over time for the above GroupBy benchmark related to some Cython code that I worked on late last week (where I made a major performance improvement in this case):

[![](http://wesmckinney.com/blog/wp-content/uploads/2011/12/vbench_demo1.png)
](http://wesmckinney.com/blog/images/vbench_demo1.png)

Here is a [fully-formed vbench suite](https://github.com/wesm/pandas/blob/5d4bf8febdad007d7804c2e91c5bead01ca92637/vb_suite/benchmarks.py) in the pandas git repository.

## Kind of like [codespeed](https://github.com/tobami/codespeed) and [speed.pypy.org](http://speed.pypy.org/.)?

Before starting to write a new project I looked briefly at codespeed and the excellent work that the PyPy guys have done with **speed.pypy.org**. But then I started thinking, you know, Django web application, JSON requests to upload benchmark results? Seemed like far too much work to do something relatively simple. The dealbreaker is that codespeed is just a web application. It doesn't actually (to my knowledge, someone correct me if I'm wrong?) have any kind of a framework for orchestrating the running of benchmarks throughout your code history. That is what this new project is for. I actually see a natural connection between vbench and codespeed, all you need to do is **write a script to upload your vbench results to a codespeed web app!**

At some point I'd like to build a simple web front end or wx/Qt viewer for the generated vbench database. I've never done any JavaScript, but it would be a good opportunity to learn. Knowing me, I might break down and hack out a stupid little wxPython app with an embedded matplotlib widget anyway.

Anyway, I'm really excited about this project. It's very prototype-y at the moment but I tried to design it in a nice and extensible way. I also plan to put all my git repo analysis tools in there (like code churn graphs etc.) so it should become a nice little collection of tools.

## Other ideas for extending

- Dealing with API changes

- Multiple library versions (e.g. NumPy) or Python versions

