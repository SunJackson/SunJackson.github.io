---
layout:     post
title:      A Year of Approximate Inference： Review of the NIPS 2015 Workshop
subtitle:   转载自：http://blog.shakirm.com/2015/12/a-year-of-approximate-inference/
date:       2015-12-18
author:     shakirm
header-img: img/background2.jpg
catalog: true
tags:
    - inferences
    - variational
    - variates
    - methods
    - approximate
---

Probabilistic inference lies no longer at the fringe. The importance of how we connect our observed data to the assumptions made by our statistical models—the task of inference—was a central part of this year's Neural Information Processing Systems (NIPS) conference: Zoubin Ghahramani's opening keynote set the stage and the pageant of inference included forecasting, compression, decision making, personalised modelling, and automating the scientific method; Josh Tenenbaum inspired us to think about the central role of of probabilistic inference in building intelligent systems and for our understanding of brains, minds and machines.

I was fortunate to be one of the organisers of this year's [NIPS 2015 Workshop on Advances in Approximate Bayesian Inference](http://approximateinference.org/), where we debated the past, present and future of approximate inference. This post is my brief reflection of a year in inference and the advances we have made since the last workshop; *my review of that workshop is here: [Variational Inference: Tricks of the Trade](http://blog.shakirm.com/2015/01/variational-inference-tricks-of-the-trade)".*
![](http://blog.shakirm.com/wp-content/uploads/2015/12/ABIworkshop-1024x367.jpg)


Full house at the NIPS 2015 Workshop on Advances in Approximate Bayesian Inference

Stochastic approximation remains the driving force for scaling inference. At the workshop we explored stochastic approximation schemes for expectation propagation (EP), building on work published earlier this year. Three important explorations of EP at the workshop were:

EP remains one of the best inference methods for GP classification and these approaches make this even more so, and bring EP back into the conversation on availability and limitations of different approximate inference methods.

Stochastic approximation schemes were established for variational free energy methods over the past few years, and this thinking has matured further. The main tricks driving variational inference remain the use of Monte Carlo gradient estimators using either [pathwise-derivative](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks) or [score-function estimators](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick), allowing us to apply the variational principle for inference in both discrete and continuous models. As a field, we are now interrogating these estimators further and attempting to tame their variance in different ways, including methods using [local expectation gradients](http://arxiv.org/abs/1503.01494) or [delta method-derived control variates](http://arxiv.org/abs/1511.05176).

- At the workshop, we explored one new approach to this problem through the [unification of variational inference and reinforcement learning](http://approximateinference.org/accepted/WeberEtAl2015.pdf). This will allow for variance reduction and other approximation tools from reinforcement learning, such as value and advantage functions, to be applied within the variational inference setting.


The use of inference models and amortised inference schemes has had much wider use. We now have a variety of ways in which to deploy such inference networks in our algorithms: in expectation propagation, we can learn to [pass messages using kernels](http://www.researchgate.net/profile/Zoltan_Szabo17/publication/276354183_Just-In-Time_Kernel_Regression_for_Expectation_Propagation/links/5557a57f08ae6943a874b20f.pdf); for posterior predictive distributions, we can distill this computation as done in [Bayesian Dark Knowledge](http://arxiv.org/pdf/1506.04416.pdf); in variational inference we can develop sequential extensions, such as that used in [DRAW](http://arxiv.org/pdf/1502.04623.pdf).

The simplicity and deeper understanding we now have of the gradient estimators makes it easy to promote further automation and enhance existing software systems. This generality allows us to equip any automatic differentiation system with the ability to do variational inference.

- We had an in depth panel discussion that explored this, highlighting new tools such as [STAN](http://arxiv.org/abs/1506.03431) and [python-autograd](https://github.com/HIPS/autograd) that make variational inference even more accessible and easy to use.

- And there are many ways to formalise and make easier the mixing of the two types of gradient estimators that we have been using, such as that developed in [stochastic computational graphs](http://arxiv.org/abs/1506.05254).

- These tools allow variational inference to be implemented in about 5 lines of code. Variational inference is on a path that will soon allow it to be a default method for statistical inference due to these far-sighted efforts.


The question that is most often asked in variational inference, is how we can obtain better posterior inferences. Two paths have emerged, one looking at modified objective functions and the other at developing richer classes of approximate posterior distributions.

***Learning objectives.***

***Richer posterior distributions.*** There has been much development of richer posteriors this year.

- The workshop showed that more robust inference can be obtained by improving covariance estimates obtained from mean-field inference, using [robust variational inference.](http://approximateinference.org/accepted/GiordanoEtAl2015.pdf)

- Earlier in the year [MCMC and variational inference](http://jmlr.org/proceedings/papers/v37/salimans15.pdf) (VI-HMC) was proposed as one way of achieving this. [Normalising flows](http://jmlr.org/proceedings/papers/v37/rezende15.pdf) were proposed as another approach. At this workshop, we obtained a new tool for constructing rich posterior distribution by combining the auxiliary variational bound from VI-HMC with normalising flows, giving us *[hierarchical variational models](http://approximateinference.org/accepted/RanganathEtAl2015.pdf)*.


Our constant request is for more theory that can guide our efforts and provide more insight. And there are many efforts in this regard.

As always more theoretical work is needed as well as new ways of communicating theoretical insights to the wider approximate inference community.

There was so much more that was discussed: better ways of doing optimisation (multicanonical methods, incremental variational inference, and inference in sequential/temporal models), testing and evaluation, and the types of guarantees that might be needed when using approximate inference in production and scientific settings. Approximate inference is clearly a healthy and active research area. There are many directions going forward, and these will include a continued search for rich posterior approximations, better optimisation algorithms, and more general purpose software, as well as new directions for greater interpretability, tighter integration with other systems such as differentiable rendering engines, more diverse applications in compression and decision making, and greater cross-pollination with other approximate inference approaches. I can't wait to see what one more year will bring.




*Related*

