---
layout:     post
title:      Sample Variance Penalization
subtitle:   转载自：http://www.machinedlearnings.com/2015/11/sample-variance-penalization.html
date:       2015-12-01
author:     Paul Mineiro (noreply@blogger.com)
header-img: img/background3.jpg
catalog: true
tags:
    - learning
    - batch
    - modeling
    - rates
    - initial
---












### 
[Sample Variance Penalization](http://www.machinedlearnings.com/2015/11/sample-variance-penalization.html)


This didn't really take off, as far as I can tell (although [Conterfactual Risk Minimization](http://arxiv.org/abs/1502.02362) uses it and that's pretty cool). The objective is non-convex, which perhaps was a negative feature at the time. The objective also involves batch quantities, and maybe this was a minus. Nowadays we're all doing mini-batch training of non-convex objectives anyway, so SVP deserves another look. If you turn the crank on this, you get \[\nabla_w f (w) = \mathbb{E}\left[ \left( 1 + \kappa \frac{l (y, h (x; w)) - \mu (l; w)}{\sigma (l; w)} \right) \nabla_w l (y, h (x; w)) \right],\] which looks like SGD with a variable learning rate: examples that have worse than average loss get a larger learning rate, and examples that have better than average loss get a smaller (possibly negative!) learning rate. The unit of measurement defining “worse” and “better” is the loss variance. In practice I find negative learning rates distasteful, so I lower bound at zero, but for the values of $\kappa$ where this is helpful (0.25 is a good initial guess), it typically doesn't matter.

The batch quantities $\mu (l; w)$ and $\sigma (l; w)$ look painful but in my experience you can replace them with mini-batch estimates and it is still helpful. I've gotten modest but consistent lifts across several problems using this technique, including [extreme learning](http://research.microsoft.com/en-us/um/people/manik/events/xc15) problems such as (neural) language modeling. Of course, you should only think about applying this technique on a problem where you suspect your desired model class will overfit and regularization is important: extreme learning problems have that structure because many of the tail classes have near singleton support. YMMV.












