---
layout:     post
title:      ICCV 2015, Day 2
subtitle:   转载自：http://www.nowozin.net/sebastian/blog/iccv-2015-day-2.html
date:       2015-12-15
author:     Sebastian Nowozin
header-img: img/background3.jpg
catalog: true
tags:
    - papers
    - prize
    - awards
    - pami
    - deep
---

This article summarizes the second day of the ICCV
2015 conference, the International Conference on
Computer Vision.
A summary of the [first day](http://www.nowozin.net/sebastian/blog/iccv-2015-day-1.html) is also available.

## Awards

The following awards were given at ICCV 2015.

### Achievement awards

- PAMI Distinguished Researcher Award (1): **Yann LeCun**

- PAMI Distinguished Researcher Award (2): **David Lowe**

- PAMI Everingham Prize Winner (1): **Andrea Vedaldi** for [VLFeat](http://www.vlfeat.org/)

- PAMI Everingham Prize Winner (2): **Daniel Scharstein** and **Rick Szeliski** for the [Middlebury Datasets](http://vision/middlebury.edu/stereo/data)


## Paper awards

- PAMI Helmholtz Prize (1): **David Martin**, **Charles Fowlkes**, **Doron Tal**, and **Jitendra Malik** for their ICCV 2001 paper "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics".

- PAMI Helmholtz Prize (2): **Serge Belongie**, **Jitendra Malik**, and **Jan Puzicha**, for their ICCV 2001 paper "Matching Shapes".

- Marr Prize: **Peter Kontschieder**, **Madalina Fiterau**, **Antonio Criminisi**, and **Samual Rota Bulo**, for ["Deep Neural Decision Forests"](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf).

- Marr Prize honorable mention: **Saining Xie** and **Zhuowen Tu** for ["Holistically-Nested Edge Detection"](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf).


## Interesting Papers

The above Marr prize winning papers are very nice, but here I also want to
highlight three other papers I found interesting today.

### Fast R-CNN

By Ross Girshick.

Since 2014 the standard object detection pipeline for natural images is the
R-CNN system which first extracts a set of object proposals then scores them
using a convolutional neural network.
The two key weaknesses of the approach are: first, the separation between
proposal generation and scoring, preventing joint training of model parameters;
and second the separate scoring of each hypothesis which leads to significant
runtime overhead.
This work and the follow-up work ("Faster R-CNNs" at NIPS this year) addresses
both issues by proposing a joint model that is trained end-to-end, including
proposal generation, leading to a new state of the art in object detection.

[Code](http://github.com/rbgirshick/fast-rcnn),
[paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf).

### Unsupervised Visual Representation Learning by Context Prediction

By Carl Doersch, Abhinav Gupta, and Alexei A. Efros.

Supervised deep learning needs lots of labeled training data to achieve good performance.
This paper investigates whether we can create and train deep neural networks on
artificial tasks for which we can create large amounts of training data. In
particular, the paper proposes to predict where a certain patch appears within
the image. For this task, an almost infinite amount of training data is easily
created.
Perhaps surprisingly the resulting network, despite being trained on this
artificial task, has learned useful representations for real vision tasks such
as image classification.

[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf).

### Deep Fried Convnets

By Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song,
and Ziyu Wang.

In deep convolutional networks the last few densely connected layers have the
most parameters and thus most of the required memory during test time and
training.
This work proposes to leverage the *fastfood* kernel approximation to replace
densely connected layers with specific efficient and low parameter operations.

The empirical results are impressive and the fastfood justification is
plausible, but I wonder if this work may even provide a hint at a more general
approach to construct efficient neural network architectures by using arbitrary
dense but efficient matrix operations (FFT, DCT, Walsh-Hadamard, etcetera).

[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_Deep_Fried_Convnets_ICCV_2015_paper.pdf).
