---
layout:     post
title:      Festival Chatter (Part 4) - Some Simple Sentiment Analysis
subtitle:   转载自：http://blog.ethanrosenthal.com/2014/11/25/festival-chatter-part4/
date:       2014-11-25
author:     Ethan Rosenthal
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - http
    - bonnaroo tweets
    - rates
    - word
    - negatively
    - lastly
    - lasts
    - scores
    - scoring
    - scored
    - festival
    - positive
    - temporal
    - assignment
    - artists
    - people
    - separating
    - seaborn
    - worry
    - calculate_sentiment
    - scripts
    - chatter
    - coursera
---

## 
[Festival Chatter (Part 4) - Some Simple Sentiment Analysis](http://blog.ethanrosenthal.com/2014/11/25/festival-chatter-part4)


I think this post will probably conclude my Festival Chatter series on analyzing Bonnaroo tweets in Python ([part 1](http://blog.ethanrosenthal.com/2014/08/31/festival-chatter-part1), [part 2](http://blog.ethanrosenthal.com/2014/09/09/festival-chatter-part2), [part 3](http://blog.ethanrosenthal.com/2014/10/06/festival-chatter-part3)). I've had a lot of fun messing around with this dataset, but I think it's time to move on to playing with something else. For this last post, though, I will show some simple [sentiment analysis](http://en.wikipedia.org/wiki/Sentiment_analysis) of the collected tweets. There are a whole bunch of issues with this method of sentiment analysis. I'll mention some of these after presenting the findings.

## Words to Numbers

The sentiment analysis used here consists of associating a number with an English word where the value of the number corresponds to how "positive" or "negative" the word seems. Presumably, I could go through the dictionary and subjectively assign numbers to words, but thankfully somebody else has less of a life than me (maybe). So, if you click [here](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), you can download a tab-separated text file called `AFINN-111.txt` that contains a dictionary of 2477 words and associated integer sentiment score. Larger positive (negative) values correspond to more positive (negative) sentiment. On a total side note, I often see versions of my previous sentence in scientific papers where one attempts to describe two often opposing concepts and puts the antonyms in parentheses in order to save space. I wonder if there is a name for this "literary device"? Anyway, good words like "love" get a value of +3 on the sentiment score, while hate is worth -3.

I wrote a small script `calculate_sentiment.py` that reads in `AFINN-111.txt` and calculates the sentiment score of every tweet. This score is just the sum of the sentiment scores for all words in the tweet that are also in the dictionary. I originally wrote this script for a homework assignment from the Coursera [Introduction to Data Science](https://www.coursera.org/course/datasci) course. Things like separating the scores into "positive" and "negative" buckets are unnecessary now and just left over from the assignment.

## "But how did it make you feel?"

With this machinery, we can now take a look at the Bonnaroo dataset.

We now have a single `text_sentiment` number associated with each tweet.

We can see that there is a positive average sentiment, but the range is quite large (-20, +24). Let's take a look at those minimum and maximum tweets:

As you can see, the most negative tweet is no surprise. Although our sentiment scoring algorithm captured this tweet's sentiment fairly well, you will see in a bit how the algorithm often fails because "inflamatory language" is *always* scored so negatively. Meanwhile, the frequent use of the word "love" in the positive tweet makes it clear why this tweet ranked so high on the sentiment score.

## Temporal Vibes

Now that we have our sentiment score for each tweet, I thought it would be interesting to look at how average sentiment changed with time. For this, I employ a similar approach to the last post by resampling the data. I also resample at different rates which reveals the tradeoff between temporal resolution and noise. Lastly, I've switched from using prettyplotlib to [seaborn](http://stanford.edu/~mwaskom/software/seaborn). Prettyplotlib was abandoned by the creator, and seaborn seems to be pretty popular and easy to use.

![](http://blog.ethanrosenthal.com/assets/img/avg_tweet_sent.png)


For the high frequency resampling rate (5 min), you can see that the signal gets quite noisey near the morning of June 13th. It turns out that there were very few tweets during this time, so any single tweet with a sizeable sentiment score would dominate the 5 minute bin over which I was averaging.

By decreasing the sampling rate to 60 minute bins, you can see a clear upturn in the average sentiment starting around midnight on June 15th which was the last night of the festival. I decided to look at some of the very positive tweets during this time.

This is pretty cool - it seems like many of these tweets are people looking fondly back on their time at Bonnaroo and talking about how much they enjoyed it. It would be interesting to look at how long this "after-glow" lasts.

## Festival Chatter Scatter

Lastly, to get back to some insight into the artists, I decided to create a scatterplot of the average sentiment for each of the top 40 most popular artists on the x-axis, and the sentiment standard deviation on the y-axis. The idea was that maybe the standard deviation would indicate "controversial" artists.

People clearly loved this show, but their language is not well suited to be captured by `AFINN-111`. And while I'm sure that there are many ways to attempt to alleviate this issue, I'll worry abou that another day. For now, all the scripts used in this series are on my [GitHub](https://github.com/EthanRosenthal/festival-chatter).
