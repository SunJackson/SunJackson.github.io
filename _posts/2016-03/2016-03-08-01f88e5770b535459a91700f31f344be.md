---
layout:     post
title:      Second Annual Data Science Bowl – Part 3 – Automatically Finding the Heart Location in an MRI Image
subtitle:   转载自：https://colinpriest.com/2016/03/08/second-annual-data-science-bowl-part-3-automatically-finding-the-heart-location-in-an-mri-image/
date:       2016-03-08
author:     Colin Priest
header-img: img/background2.jpg
catalog: true
tags:
    - images
    - training
    - trained
    - modelling
    - models
---

My [last blog](https://colinpriest.com/2016/03/07/second-annual-data-science-bowl-part-2) wasn’t so sexy, what with all the data cleansing, and no predictive modelling. But in this blog I do something really cool – I train a machine learning model to find the left ventricle of the heart in an MRI image. And I couldn’t have done it without all of that boring data cleansing. #kaggle @kaggle

Aside from being a really cool thing to do, there is a purpose to this modelling. I want to find the boundaries of the heart chamber, and that is much easier and faster to do when I remove distractions. Once I have found the location of the heart chamber, I can crop the image to a much smaller square.

The input to the model will be a set of images. In order to simply what the model learns, I only gave it training images from sax locations near the centre of the heart.

The output from the model will be the row number and column number of the centroid of the left ventricle heart chamber (the red dot in the images above).

I had to manually define those centroid locations for a training set of a few hundred of the images. This was laborious and time consuming, even after I automated some of the process. But it needed to be done, because otherwise the machine learning algorithm has no way of knowing what the true answers should be.

Even though I am much more comfortable coding in R than in Python, I used Python for this step because I wanted to use [Daniel Nouri](http://danielnouri.org/)‘s [nolearn](https://pypi.python.org/pypi/nolearn) library, which sits above [lasagne](http://lasagne.readthedocs.org/en/latest) and [theano](http://deeplearning.net/software/theano), and these libraries are not available in R. The convolution neural network architecture was based upon the architecture in [Daniel Nouri’s tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial) for the [Facial Keypoints Detection competition in Kaggle](https://www.kaggle.com/c/facial-keypoints-detection).

OK, so this part isn’t all that sexy either. But it’s the engine for all the cool modelling that is about to be done.

 

I used [jupyter notebook](http://jupyter.org/) as the development environment to set up and run my Python scripts. While there’s a lot to like about jupyter, one thing that annoys me is that print commands run in jupyter don’t immediately show text on the screen. But here’s a trick to work around that:

Using this helper function instead of the print function results in text immediately appearing in the output. This is particular helpful for progress messages on long training runs.

I like to use R’s [expand.grid](https://stat.ethz.ch/R-manual/R-devel/library/base/html/expand.grid.html) function, but it isn’t built in to Python. So I wrote my own helper function in Python that mimics the functionality:

This next function reads a cleaned up image, checking that it has the correct aspect ratio, then resizing it to 96 x 96 pixels. The resizing is done to reduce memory usage in my GPU, and to speed up the training and scoring.

My initial models were not performing as well as I would like. So to force the the model to generalise, I added some image transformations (rotation and reflection) to the training data. This required helper functions:

There are two feature sets that enable the machine learning model to find the heart:

1. shape in the image – by looking at shape in the image it can find the heart

1. movement between images at different points of time – the heart is moving but most of the chest is stationary


So I set up the network architecture to use two channels. The first channel is the image, and the second channel is the difference between the image at this point of time and the image 8 time periods in the future.

I used early stopping to help reduce overfitting:

As well as reading the training data, I shuffled the order of the data. This allowed me to use batch training.

The network architecture used deep convolutional layers to find features in the image, then fully connected layers to convert these features into the centroid location:

![](https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image05.png?w=529)


Based upon how quickly the training converged, the network could possibly have been simplified, reducing the number of layers, or using fewer neurons in the fully connected layers. But I didn’t have time to experiment with different architectures.

![](https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image06.png?w=529)


The GPU quickly ran out of RAM unless I used the batch iterator. I found the batch size via trial and error. Large batch sizes caused the GPU to run out of RAM. Small batch sizes ran much slower.

Just like humans, all models make mistakes. The heart chamber segmentation algorithms I used later in this project were sensitive to how well the heart chamber was centred in the image. But as long as the model output was a centroid that was inside the heart chamber, things usually went OK. Early versions of my model made mistakes that placed the centroid outside the heart chamber, sometimes even far away from the heart.Tweaks to the training data (especially enhancing the data with rotation and reflection) and the architecture (especially dropout layers) improved the performance.

![](https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-image07.png?w=529)


After many failed models, I was excited when the two worst training errors were still close to the centre of the heart chamber ðŸ™‚

The main point of building a heart finder machine learning model is to automate the process of finding the left ventricle in the test images that will be used as part of the competition submission. These are images that the model has never seen before.

In the animated gif below, you can see the left ventricle centroid location that has been automatically fitted, displayed as a dark rectangle moving around near the centre of the heart chamber. The machine learning algorithm was not trained on this patient’s images – so what you see here is artificial intelligence in action!

![](https://colinpriestdotcom.files.wordpress.com/2016/03/20160308-submission-images.gif?w=529)


### Like this:

Like Loading...


*Related*

