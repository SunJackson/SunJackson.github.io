---
layout:     post
title:      Logistic Regression and Optimization
subtitle:   转载自：http://learningwithdata.com/logistic-regression-and-optimization.html
date:       2015-04-29
author:     Tyler Folkman
header-img: img/background1.jpg
catalog: true
tags:
    - probability
    - probabilities
    - function
    - prod_
    - x value
---

You can see why this is a great function for a probability measure. The y-value represents the probability and only ranges between 0 and 1. Also, for an x value of zero you get a .5 probability and as you get more positive x values you get a higher probability and more negative x values a lower probability.

Okay - so this is nice, but how the heck do we use it? Well we know we have two attributes - Sepal length and Sepal width - that we need to somehow use in our logistic function. One pretty obvious thing we could do is:


Where SW is our value for sepal width and SL is our value for sepal length. For those of you familiar with [Linear Regression](http://en.wikipedia.org/wiki/Linear_regression) this looks very familiar. Basically we are assuming that x is a linear combination of our data plus an intercept. For example, say we have a plant with a sepal width of 3.5 and a sepal length of 5 and some oracle tells us that $\beta_{0} = 1$, $\beta_{1} = 2$, and $\beta_{2} = 4$. This would imply:


Plugging this into our logistic function gives:


So we would give a 99% probability to a plant with those dimensions as being Setosa.

Okay - makes sense. But who is this oracle giving us our $\beta$ values? Good question! This is where the learning in machine learning comes in :). We will learn our $\beta$ values.

## Step 1 - Define your cost function[¶](http://learningwithdata.com/logistic-regression-and-optimization.html#Step-1---Define-your-cost-function)

If you have been around machine learning, you probably hear the phrase "cost function" thrown around. Before we get to that, though, let's do some thinking. We are trying to choose $\beta$ values in order to maximize the probability of correctly classifying our plants. That is just the definition of our problem. Let's say someone did give us some $\beta$ values, how would we determine if they were good values or not? We saw above how to get the probability for one example. Now imagine we did this for all our plant observations - all 100. We would now have 100 probability scores. What we would hope is that for the Setosa plants, the probability values are close to 1 and for the Versicolor plants the probability is close to 0.

But we don't care about getting the correct probability for just one observation, we want to correctly classify all our observations. If we assume our data are [independent and identically distributed](http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables), we can just take the product of all our individually calculated probabilities and that is the value we want to maximize. So in math: $$\prod_{Setosa}\frac{1}{1 + e^{-(\beta_{0} + \beta{1}SW + \beta_{2}SL)}}\prod_{Versicolor}1 - \frac{1}{1 + e^{-(\beta_{0} + \beta{1}SW + \beta_{2}SL)}}$$ If we define the logistic function as: $$h(x) = \frac{1}{1 + e^{-x}}$$ and x as: $$x = \beta_{0} + \beta{1}SW + \beta_{2}SL$$ This can be simplified to: $$\prod_{Setosa}h(x)\prod_{Versicolor}1 - h(x)$$

The $\prod$ symbol means take the product for the observations classified as that plant. Here we are making use of the fact that are data are labeled, so this is called supervised learning. Also, you will notice that for Versicolor observations we are taking 1 minus the logistic function. That is because we are trying to find a value to maximize, and since Versicolor observations should have a probability close to zero, 1 minus the probability should be close to 1. So now we know that we want to maximize the following: $$\prod_{Setosa}h(x)\prod_{Versicolor}1 - h(x)$$

So we now have a value we are trying to maximize. Typically people switch this to minimization by making it negative: $$-\prod_{Setosa}h(x)\prod_{Versicolor}1 - h(x)$$ Note: minimizing the negative is the same as maximizing the positive. The above formula would be called our cost function.

## Step 2 - Gradients[¶](http://learningwithdata.com/logistic-regression-and-optimization.html#Step-2---Gradients)

So now we have a value to minimize, but how do we actually find the $\beta$ values that minimize our cost function? Do we just try a bunch? That doesn't seem like a good idea...

This is where [convex optimization](http://en.wikipedia.org/wiki/Convex_optimization) comes into play. We know that the logistic cost function is [convex](http://en.wikipedia.org/wiki/Convex_function) - just trust me on this. And since it is convex, it has a single global minimum which we can converge to using [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent).

Here is an image of a convex function:
