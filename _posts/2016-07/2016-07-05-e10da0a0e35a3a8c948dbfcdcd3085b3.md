---
layout:     post
title:      ICML 2016 Thoughts
subtitle:   转载自：http://www.machinedlearnings.com/2016/07/icml-2016-thoughts.html
date:       2016-07-05
author:     Paul Mineiro (noreply@blogger.com)
header-img: img/background2.jpg
catalog: true
tags:
    - memory
    - attention
    - optimization
    - optimal
    - actual
---












### 
[ICML 2016 Thoughts](http://www.machinedlearnings.com/2016/07/icml-2016-thoughts.html)



ICML is too big for me to ``review'' it per se, but I can provide a myopic perspective.

The heavy hitting topics were Deep Learning, Reinforcement Learning, and Optimization; but there was a heavy tail of topics receiving attention. It felt like deep learning was less dominant this year; but the success of deep learning has led to multiple application specific alternative venues (e.g., CVPR, EMNLP), and ICLR is also a prestigious venue; so deep learning at ICML this year was heavyweight in either the more theoretical or multimodal works. Arguably, reinforcement learning and optimization both should partially count towards deep learning's footprint; reinforcement learning has been this way for at least a year, but optimization has recently developed more interest in non-convex problems, especially the kind that are empirically tractable in deep learning (sometimes, although seemingly innocuous architecture changes can spoil the pudding; I suppose one dream of the optimization community would be the identification of a larger-than-convex class of problems which are still tractable, to provide guidance).

Here are some papers I liked:- [Strongly-Typed Recurrent Neural Networks](http://jmlr.org/proceedings/papers/v48/balduzzi16.pdf)The off-putting title makes sense if you are into type theory, or if you've ever been a professional Haskell programmer and have had to figure out wtf a monad is. tl;dr: if you put units of measurement on the various components of a recurrent neural network, you'll discover that you are adding apples and oranges. T-LSTM, a modification of the standard LSTM to fix the problem, behaves similarly empirically; but is amenable to analysis. Theorem 1 was the nice part for me: the modified architectures are shown to compute temporal convolutions with dynamic pooling. Could type consistency provide a useful prior on architectures? That'd be a welcome development.
- [Ask Me Anything:**Dynamic Memory Networks for Natural Language Processing](http://arxiv.org/abs/1503.08895) and [Dynamic Memory Networks for Visual and Textual Question Answering](http://jmlr.org/proceedings/papers/v48/xiong16.pdf)More titles I'm not over the moon about: everybody seems to be equating “memory” = “attention over current example substructure”. If you ask for the layperson's definition, they would say that memory is about stuff you *can't* see at the moment (note: Jason started this particular abuse of terminology with [End-to-End Memory Networks](http://arxiv.org/abs/1503.08895)). Pedantry aside, undeniably these iterated attention architectures** have become the state of the art in question-answering style problems and the baseline to beat. Note since the next step in iterated attention is to incorporate previously seen and stored examples, the use of the term “memory” will soon become less objectionable.
- [From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification](http://jmlr.org/proceedings/papers/v48/martins16.pdf) This is an alternative to the softmax layer (“link function”) used as the last layer of a neural network. Softmax maps $\mathbb{R}^n$ onto the (interior of the) simplex, whereas sparsemax projects onto the simplex. One big difference is that sparsemax can “hit the corners”, i.e., zero out some components. Empirically the differences in aggregate task performance when swapping softmax with sparsemax are modest and attributable to the selection pressures on experimental sections. So why care? Attention mechanisms are often implemented with softmax, and it is plausible that a truly sparse attention mechanism might scale better (either computationally or statistically) to larger problems (such as those involving *actual* memory, c.f., previous paragraph). 
- [Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization](http://jmlr.org/proceedings/papers/v48/finn16.pdf)I find Inverse RL unintuitive: didn't Vapnik say not to introduce difficult intermediate problems? Nonetheless, it seems to work well. Perhaps requiring the learned policy to be “rational” under some cost function is a useful prior which mitigates sample complexity? I'm not sure, I have to noodle on it. In the meantime, cool videos of robots doing the dishes!
- [Dueling Network Architectures for Deep Reinforcement Learning](http://jmlr.org/proceedings/papers/v48/wangf16.pdf).Best paper, so I'm not adding any value by pointing it out to you. However, after reading it, meditate on why learning two things is better than learning one. Then re-read the discussion section. Then meditate on whether a similar variance isolation trick applies to your current problem.
From the workshops, some fun stuff I heard:- Gerald Tesauro dusted off his old [Neurogammon](https://en.wikipedia.org/wiki/Neurogammon) code, ran it on a more powerful computer (his current laptop), and got much better results. Unfortunately, we cannot conclude that NVIDIA will solve AI for us if we wait long enough. In 2 player games or in simulated environments more generally, computational power equates to more data resources, because you can simulate more. In the real world we have sample complexity constraints: you have to perform actual actions to get actual rewards. However, in the same way that cars and planes are faster than people because they have unfair energetic advantages (we are 100W machines; airplanes are [much higher](http://aviation.stackexchange.com/questions/19569/how-many-kilowatts-to-get-an-electric-747-8-airborne)), I think “superhuman AI”, should it come about, will be because of sample complexity advantages, i.e., a distributed collection of robots that can perform more actions and experience more rewards (and remember and share all of them with each other). So really Boston Dynamics, not NVIDIA, is the key to the singularity. (In the meantime … buy my vitamins!)
- Ben Recht talked about the virtues of [random hyperparameter optimization](http://www.argmin.net/2016/06/20/hypertuning) and an [acceleration technique](http://arxiv.org/abs/1603.06560) that looks like a cooler version of [sub-linear debugging](https://blogs.technet.microsoft.com/machinelearning/2014/09/24/online-learning-and-sub-linear-debugging). This style, in my experience, works.
- Leon Bottou pointed out that first order methods are now within constant factors of optimal convergence, with the corollary that any putative improvement has to be extremely cheap to compute since it can only yield a constant factor. He also presented a plausible improvement on batch normalization in the same talk.


1. Gerald Tesauro dusted off his old [Neurogammon](https://en.wikipedia.org/wiki/Neurogammon) code, ran it on a more powerful computer (his current laptop), and got much better results. Unfortunately, we cannot conclude that NVIDIA will solve AI for us if we wait long enough. In 2 player games or in simulated environments more generally, computational power equates to more data resources, because you can simulate more. In the real world we have sample complexity constraints: you have to perform actual actions to get actual rewards. However, in the same way that cars and planes are faster than people because they have unfair energetic advantages (we are 100W machines; airplanes are [much higher](http://aviation.stackexchange.com/questions/19569/how-many-kilowatts-to-get-an-electric-747-8-airborne)), I think “superhuman AI”, should it come about, will be because of sample complexity advantages, i.e., a distributed collection of robots that can perform more actions and experience more rewards (and remember and share all of them with each other). So really Boston Dynamics, not NVIDIA, is the key to the singularity. (In the meantime … buy my vitamins!)

1. Ben Recht talked about the virtues of [random hyperparameter optimization](http://www.argmin.net/2016/06/20/hypertuning) and an [acceleration technique](http://arxiv.org/abs/1603.06560) that looks like a cooler version of [sub-linear debugging](https://blogs.technet.microsoft.com/machinelearning/2014/09/24/online-learning-and-sub-linear-debugging). This style, in my experience, works.

1. Leon Bottou pointed out that first order methods are now within constant factors of optimal convergence, with the corollary that any putative improvement has to be extremely cheap to compute since it can only yield a constant factor. He also presented a plausible improvement on batch normalization in the same talk.












